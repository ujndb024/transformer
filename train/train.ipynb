{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\") # Make module visible across folders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "from tokenizers.implementations import BertWordPieceTokenizer\n",
    "\n",
    "from re import sub\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam, Optimizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from typing import List, Tuple\n",
    "\n",
    "from transformer.module.transformer import Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and Save the Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2004: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15124"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = \"./train_data/ChatbotData.csv\"\n",
    "train_data_csv = pd.read_csv(train_data)\n",
    "\n",
    "tokenizer = BertWordPieceTokenizer(lowercase=False, strip_accents=False)\n",
    "\n",
    "# tokenizer.train(\n",
    "#     files=train_data,\n",
    "#     vocab_size=2**14,\n",
    "#     min_frequency=2,\n",
    "#     special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\", \"[SOS]\", \"[EOS]\"],\n",
    "# )\n",
    "\n",
    "# tokenizer.save_model(\"train_data/\")\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"train_data/vocab.txt\", do_basic_tokenize=False)\n",
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set the Maximum Length of the Sequence, Get the Index of the `SOS`, `EOS`, `PAD`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 6 0\n"
     ]
    }
   ],
   "source": [
    "max_length: int = 40\n",
    "\n",
    "SOS = tokenizer.get_vocab()[\"[SOS]\"]\n",
    "EOS = tokenizer.get_vocab()[\"[EOS]\"]\n",
    "PAD = tokenizer.get_vocab()[\"[PAD]\"]\n",
    "\n",
    "print(SOS, EOS, PAD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Punctuations, Integer Encoding, Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = []\n",
    "for sentence in train_data_csv[\"Q\"]:\n",
    "    sentence = sub(pattern=r\"([?.!,])\", repl=r\" \\1\", string=sentence)\n",
    "    sentence = sentence.strip()\n",
    "    encoded_sentence = tokenizer.encode(sentence)\n",
    "    encoded_sentence = encoded_sentence[1:-1]\n",
    "    sentence = torch.IntTensor(encoded_sentence) # Encoder's input. No need to add SOS or EOS tokens.\n",
    "    padding = nn.ZeroPad1d((0, max_length - 1 - sentence.shape[0]))(sentence)\n",
    "    questions.append(padding.tolist())\n",
    "\n",
    "questions = torch.IntTensor(questions)\n",
    "\n",
    "answers = []\n",
    "for sentence in train_data_csv[\"A\"]:\n",
    "    sentence = sub(pattern=r\"([?.!,])\", repl=r\" \\1\", string=sentence)\n",
    "    sentence = sentence.strip()\n",
    "    encoded_sentence = tokenizer.encode(sentence)\n",
    "    encoded_sentence = encoded_sentence[1:-1]\n",
    "    sentence = torch.IntTensor([SOS] + encoded_sentence + [EOS]) # Decoder's input And Output. Decoder's input only contains SOS token and Output only contains EOS token.\n",
    "    padding = nn.ZeroPad1d((0, max_length - sentence.shape[0]))(sentence)\n",
    "    answers.append(padding.tolist())\n",
    "\n",
    "answers = torch.IntTensor(answers)\n",
    "\n",
    "# print(questions[:5], answers[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define `batch_size`, Make Several Datasets, Load with DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size: int = 128\n",
    "\n",
    "# QADataset in GitHub\n",
    "# Link: https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData.csv\n",
    "\n",
    "\n",
    "class QADataset(Dataset):\n",
    "    def __init__(self, questions: torch.IntTensor, answers: torch.IntTensor) -> None:\n",
    "        self.inputs = questions\n",
    "        self.dec_inputs = answers[:, :-1]\n",
    "        self.outputs = answers[:, 1:]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[dict, dict]:\n",
    "        return {\"inputs\": self.inputs[idx], \"dec_inputs\": self.dec_inputs[idx]}, {\n",
    "            \"outputs\": self.outputs[idx]\n",
    "        }\n",
    "\n",
    "\n",
    "dataset = QADataset(questions, answers)\n",
    "\n",
    "QA_train_dataloader = DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "QA_validation_dataloader = DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set the Hyperparameter of the `Transformer` Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15124 15124\n"
     ]
    }
   ],
   "source": [
    "encoder_vocab_size = tokenizer.vocab_size\n",
    "decoder_vocab_size = tokenizer.vocab_size\n",
    "pad_token = PAD\n",
    "num_layers: int = 2\n",
    "d_model: int = 256\n",
    "num_heads: int = 8\n",
    "max_len: int = max_length - 1\n",
    "dff: int = 512\n",
    "dropout: float = 0.1\n",
    "device = torch.device(\n",
    "    \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    ")  # if CPU is used, you should remove .to(device) in all train and validation datasets otherwise error will be occurred due to different placeholder storage.\n",
    "print(encoder_vocab_size, decoder_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set the Parameter of the `Transformer` Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(\n",
    "    pad_token=pad_token,\n",
    "    encoder_vocab_size=encoder_vocab_size,\n",
    "    decoder_vocab_size=decoder_vocab_size,\n",
    "    num_layers=num_layers,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    max_len=max_len,\n",
    "    dff=dff,\n",
    "    dropout=dropout,\n",
    "    device=device,\n",
    ").to(device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Number of Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14266132"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_parameters(model: Transformer) -> int:\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize the Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): TransformerEmbedding(\n",
       "      (token_embedding): Embedding(15124, 256)\n",
       "      (positional_encoding): PositionalEncoding(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x EncoderLayer(\n",
       "        (attention): MultiHeadAttention(\n",
       "          (attention): ScaledDotProductAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (query_dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (key_dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (value_dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (layer_norm1): LayerNorm()\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (ffnn): PositionWiseFeedForward(\n",
       "          (weight_tensor1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (weight_tensor2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (layer_norm2): LayerNorm()\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): TransformerEmbedding(\n",
       "      (token_embedding): Embedding(15124, 256)\n",
       "      (positional_encoding): PositionalEncoding(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x DecoderLayer(\n",
       "        (self_attention): MultiHeadAttention(\n",
       "          (attention): ScaledDotProductAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (query_dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (key_dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (value_dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (layer_norm1): LayerNorm()\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (encoder_decoder_attention): MultiHeadAttention(\n",
       "          (attention): ScaledDotProductAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (query_dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (key_dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (value_dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (layer_norm2): LayerNorm()\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (ffnn): PositionWiseFeedForward(\n",
       "          (weight_tensor1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (weight_tensor2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (layer_norm3): LayerNorm()\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (linear): Linear(in_features=256, out_features=15124, bias=True)\n",
       "  )\n",
       "  (padding_mask): CreatePaddingMask()\n",
       "  (look_ahead_mask): CreateLookAheadMask()\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def initialize_weights(model) -> None:\n",
    "    if hasattr(model, \"weight\") and model.weight.dim() > 1:\n",
    "        nn.init.kaiming_uniform_(model.weight.data) # Use Kaiming uniform due to ReLU activation\n",
    "\n",
    "model.apply(initialize_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make `TransformerScheduler`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerScheduler(Optimizer):\n",
    "    \"\"\"\n",
    "    Custom Scheduler for Transformer which was introduced from the original paper \"Attention is all you need, 2017\".\n",
    "\n",
    "    Parameters:\n",
    "        optimizer: Optimizer\n",
    "            optimizer for model training.\n",
    "        d_model: int, required\n",
    "            the embed dimension of the model.\n",
    "        warmup_steps: int, optional (default=4000)\n",
    "            boundary step between linear increase and proportional decrease.\n",
    "\n",
    "    Returns:\n",
    "        lr: float\n",
    "            corresponding learning rate.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        optimizer: Optimizer,\n",
    "        d_model: int,\n",
    "        warmup_steps: int = 4000, # Paper-based value\n",
    "    ) -> None:\n",
    "        self.optimizer = optimizer\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def get_lr(self, steps: int) -> float:\n",
    "        scale = self.d_model**-0.5\n",
    "        if steps < self.warmup_steps:\n",
    "            lr = scale * (steps + 1) * (self.warmup_steps**-1.5)\n",
    "        else:\n",
    "            lr = scale * ((steps + 1) ** -0.5)\n",
    "        return lr\n",
    "\n",
    "    def step(self, steps: int) -> None:\n",
    "        lr = self.get_lr(steps)\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group[\"lr\"] = lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set the Parameters required for optimizer and criterion(loss)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr: float = 1e-5 # There was no initial rate written in the based paper, I manually set it to 1e-5. You can adjust it if you want.\n",
    "betas: Tuple[float, float] = [0.9, 0.98]\n",
    "eps: float = 1e-9\n",
    "epochs: int = 70\n",
    "clip: float = 1.0\n",
    "total_steps: int = 0\n",
    "\n",
    "optimizer = Adam(params=model.parameters(), lr=lr, betas=betas, eps=eps)\n",
    "scheduler = TransformerScheduler(optimizer=optimizer, d_model=d_model) # Adam Optimizer is wrapped in the `TransformerScheduler` Class.\n",
    "criterion = CrossEntropyLoss(ignore_index=PAD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    epoch: int,\n",
    "    epochs: int,\n",
    "    model: Transformer,\n",
    "    iterator: DataLoader,\n",
    "    optimizer: Optimizer,\n",
    "    scheduler: TransformerScheduler,\n",
    "    criterion: CrossEntropyLoss,\n",
    "    clip: int,\n",
    ") -> float:\n",
    "    model.train()\n",
    "    training_total_loss: float = 0.0\n",
    "    previous_loss: float = 0.0\n",
    "    global total_steps\n",
    "\n",
    "    for i, (all_inputs, outputs) in enumerate(iterator):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = model(\n",
    "            encoder_input=all_inputs[\"inputs\"].to(device=device),\n",
    "            decoder_input=all_inputs[\"dec_inputs\"].to(device=device),\n",
    "        )\n",
    "        logits = logits.contiguous().view(-1, logits.shape[-1]).to(device=device)\n",
    "        outputs = outputs[\"outputs\"].view(-1).type(torch.LongTensor).to(device=device)\n",
    "\n",
    "        loss = criterion(input=logits, target=outputs).to(device=device)\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=clip)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        total_steps += 1\n",
    "        scheduler.step(steps=total_steps)\n",
    "\n",
    "        training_total_loss += loss.item()\n",
    "\n",
    "        if max(previous_loss, loss.item()) == loss.item():\n",
    "            print(\n",
    "                f\"\\033[0m\\033[1mEpoch: [{epoch + 1}/{epochs}], Progress: {i / len(iterator) * 100:.2f} %, Steps: {total_steps}, Current Learning Rate: {scheduler.get_lr(total_steps):.7f}, \\033[91mTrain Loss: {loss.item():.3f}\"\n",
    "            )\n",
    "            previous_loss = loss.item()\n",
    "        else:\n",
    "            print(\n",
    "                f\"\\033[0m\\033[1mEpoch: [{epoch + 1}/{epochs}], Progress: {i / len(iterator) * 100:.2f} %, Steps: {total_steps}, Current Learning Rate: {scheduler.get_lr(total_steps):.7f}, \\033[96mTrain Loss: {loss.item():.3f}\"\n",
    "            )\n",
    "            previous_loss = loss.item()\n",
    "\n",
    "    return training_total_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(\n",
    "    model: Transformer,\n",
    "    iterator: DataLoader,\n",
    "    criterion: CrossEntropyLoss,\n",
    ") -> float:\n",
    "    model.eval()\n",
    "    validation_total_loss: float = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, (all_inputs, outputs) in enumerate(iterator):\n",
    "            logits = model(\n",
    "                encoder_input=all_inputs[\"inputs\"].to(device=device),\n",
    "                decoder_input=all_inputs[\"dec_inputs\"].to(device=device),\n",
    "            )\n",
    "            logits = logits.contiguous().view(-1, logits.shape[-1]).to(device=device)\n",
    "            outputs = (\n",
    "                outputs[\"outputs\"].view(-1).type(torch.LongTensor).to(device=device)\n",
    "            )\n",
    "\n",
    "            loss = criterion(input=logits, target=outputs).to(device=device)\n",
    "            validation_total_loss += loss.item()\n",
    "\n",
    "    return validation_total_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m= Epoch [1/70] ==========================================\n",
      "\u001b[0m\u001b[1mEpoch: [1/70], Progress: 0.00 %, Steps: 1, Current Learning Rate: 0.0000005, \u001b[91mTrain Loss: 26.408\n",
      "\u001b[0m\u001b[1mEpoch: [1/70], Progress: 1.08 %, Steps: 2, Current Learning Rate: 0.0000007, \u001b[96mTrain Loss: 25.281\n",
      "\u001b[0m\u001b[1mEpoch: [1/70], Progress: 2.15 %, Steps: 3, Current Learning Rate: 0.0000010, \u001b[91mTrain Loss: 26.905\n",
      "\u001b[0m\u001b[1mEpoch: [1/70], Progress: 3.23 %, Steps: 4, Current Learning Rate: 0.0000012, \u001b[91mTrain Loss: 26.992\n",
      "\u001b[0m\u001b[1mEpoch: [1/70], Progress: 4.30 %, Steps: 5, Current Learning Rate: 0.0000015, \u001b[96mTrain Loss: 26.470\n",
      "\u001b[0m\u001b[1mEpoch: [1/70], Progress: 5.38 %, Steps: 6, Current Learning Rate: 0.0000017, \u001b[91mTrain Loss: 26.524\n",
      "\u001b[0m\u001b[1mEpoch: [1/70], Progress: 6.45 %, Steps: 7, Current Learning Rate: 0.0000020, \u001b[96mTrain Loss: 26.202\n",
      "\u001b[0m\u001b[1mEpoch: [1/70], Progress: 7.53 %, Steps: 8, Current Learning Rate: 0.0000022, \u001b[91mTrain Loss: 26.317\n",
      "\u001b[0m\u001b[1mEpoch: [1/70], Progress: 8.60 %, Steps: 9, Current Learning Rate: 0.0000025, \u001b[96mTrain Loss: 26.137\n",
      "\u001b[0m\u001b[1mEpoch: [1/70], Progress: 9.68 %, Steps: 10, Current Learning Rate: 0.0000027, \u001b[91mTrain Loss: 26.475\n",
      "\u001b[0m\u001b[1mEpoch: [1/70], Progress: 10.75 %, Steps: 11, Current Learning Rate: 0.0000030, \u001b[96mTrain Loss: 25.815\n",
      "\u001b[0m\u001b[1mEpoch: [1/70], Progress: 11.83 %, Steps: 12, Current Learning Rate: 0.0000032, \u001b[91mTrain Loss: 25.892\n",
      "\u001b[0m\u001b[1mEpoch: [1/70], Progress: 12.90 %, Steps: 13, Current Learning Rate: 0.0000035, \u001b[91mTrain Loss: 26.516\n",
      "\u001b[0m\u001b[1mEpoch: [1/70], Progress: 13.98 %, Steps: 14, Current Learning Rate: 0.0000037, \u001b[96mTrain Loss: 26.204\n",
      "\u001b[0m\u001b[1mEpoch: [1/70], Progress: 15.05 %, Steps: 15, Current Learning Rate: 0.0000040, \u001b[96mTrain Loss: 25.096\n",
      "\u001b[0m\u001b[1mEpoch: [1/70], Progress: 16.13 %, Steps: 16, Current Learning Rate: 0.0000042, \u001b[96mTrain Loss: 25.019\n",
      "\u001b[0m\u001b[1mEpoch: [1/70], Progress: 17.20 %, Steps: 17, Current Learning Rate: 0.0000044, \u001b[91mTrain Loss: 25.494\n",
      "\u001b[0m\u001b[1mEpoch: [1/70], Progress: 18.28 %, Steps: 18, Current Learning Rate: 0.0000047, \u001b[96mTrain Loss: 24.618\n",
      "\u001b[0m\u001b[1mEpoch: [1/70], Progress: 19.35 %, Steps: 19, Current Learning Rate: 0.0000049, \u001b[96mTrain Loss: 24.473\n",
      "\u001b[0m\u001b[1mEpoch: [1/70], Progress: 20.43 %, Steps: 20, Current Learning Rate: 0.0000052, \u001b[96mTrain Loss: 24.188\n",
      "\u001b[0m\u001b[1mEpoch: [1/70], Progress: 21.51 %, Steps: 21, Current Learning Rate: 0.0000054, \u001b[91mTrain Loss: 24.518\n",
      "\u001b[0m\u001b[1mEpoch: [1/70], Progress: 22.58 %, Steps: 22, Current Learning Rate: 0.0000057, \u001b[96mTrain Loss: 23.930\n",
      "\u001b[0m\u001b[1mEpoch: [1/70], Progress: 23.66 %, Steps: 23, Current Learning Rate: 0.0000059, \u001b[91mTrain Loss: 24.735\n",
      "\u001b[0m\u001b[1mEpoch: [1/70], Progress: 24.73 %, Steps: 24, Current Learning Rate: 0.0000062, \u001b[91mTrain Loss: 25.020\n",
      "\u001b[0m\u001b[1mEpoch: [1/70], Progress: 25.81 %, Steps: 25, Current Learning Rate: 0.0000064, \u001b[96mTrain Loss: 24.025\n",
      "\u001b[0m\u001b[1mEpoch: [1/70], Progress: 26.88 %, Steps: 26, Current Learning Rate: 0.0000067, \u001b[96mTrain Loss: 23.648\n",
      "\u001b[0m\u001b[1mEpoch: [1/70], Progress: 27.96 %, Steps: 27, Current Learning Rate: 0.0000069, \u001b[96mTrain Loss: 23.395\n",
      "\u001b[0m\u001b[1mEpoch: [1/70], Progress: 29.03 %, Steps: 28, Current Learning Rate: 0.0000072, \u001b[96mTrain Loss: 23.380\n",
      "\u001b[0m\u001b[1mEpoch: [1/70], Progress: 30.11 %, Steps: 29, Current Learning Rate: 0.0000074, \u001b[91mTrain Loss: 24.294\n",
      "\u001b[0m\u001b[1mEpoch: [1/70], Progress: 31.18 %, Steps: 30, Current Learning Rate: 0.0000077, \u001b[96mTrain Loss: 23.059\n",
      "\u001b[0m\u001b[1mEpoch: [1/70], Progress: 32.26 %, Steps: 31, Current Learning Rate: 0.0000079, \u001b[91mTrain Loss: 23.147\n",
      "\u001b[0m\u001b[1mEpoch: [1/70], Progress: 33.33 %, Steps: 32, Current Learning Rate: 0.0000082, \u001b[96mTrain Loss: 23.002\n",
      "\u001b[0m\u001b[1mEpoch: [1/70], Progress: 34.41 %, Steps: 33, Current Learning Rate: 0.0000084, \u001b[96mTrain Loss: 22.980\n",
      "\u001b[0m\u001b[1mEpoch: [1/70], Progress: 35.48 %, Steps: 34, Current Learning Rate: 0.0000086, \u001b[96mTrain Loss: 22.365\n",
      "\u001b[0m\u001b[1mEpoch: [1/70], Progress: 36.56 %, Steps: 35, Current Learning Rate: 0.0000089, \u001b[96mTrain Loss: 22.156\n",
      "\u001b[0m\u001b[1mEpoch: [1/70], Progress: 37.63 %, Steps: 36, Current Learning Rate: 0.0000091, \u001b[91mTrain Loss: 22.179\n",
      "\u001b[0m\u001b[1mEpoch: [1/70], Progress: 38.71 %, Steps: 37, Current Learning Rate: 0.0000094, \u001b[96mTrain Loss: 21.527\n",
      "\u001b[0m\u001b[1mEpoch: [1/70], Progress: 39.78 %, Steps: 38, Current Learning Rate: 0.0000096, \u001b[96mTrain Loss: 21.515\n",
      "\u001b[0m\u001b[1mEpoch: [1/70], Progress: 40.86 %, Steps: 39, Current Learning Rate: 0.0000099, \u001b[96mTrain Loss: 21.189\n",
      "\u001b[0m\u001b[1mEpoch: [1/70], Progress: 41.94 %, Steps: 40, Current Learning Rate: 0.0000101, \u001b[91mTrain Loss: 21.871\n",
      "\u001b[0m\u001b[1mEpoch: [1/70], Progress: 43.01 %, Steps: 41, Current Learning Rate: 0.0000104, \u001b[96mTrain Loss: 21.117\n",
      "\u001b[0m\u001b[1mEpoch: [1/70], Progress: 44.09 %, Steps: 42, Current Learning Rate: 0.0000106, \u001b[96mTrain Loss: 20.911\n",
      "\u001b[0m\u001b[1mEpoch: [1/70], Progress: 45.16 %, Steps: 43, Current Learning Rate: 0.0000109, \u001b[96mTrain Loss: 20.448\n",
      "\u001b[0m\u001b[1mEpoch: [1/70], Progress: 46.24 %, Steps: 44, Current Learning Rate: 0.0000111, \u001b[96mTrain Loss: 20.168\n",
      "\u001b[0m\u001b[1mEpoch: [1/70], Progress: 47.31 %, Steps: 45, Current Learning Rate: 0.0000114, \u001b[96mTrain Loss: 19.537\n",
      "\u001b[0m\u001b[1mEpoch: [1/70], Progress: 48.39 %, Steps: 46, Current Learning Rate: 0.0000116, \u001b[91mTrain Loss: 20.157\n",
      "\u001b[0m\u001b[1mEpoch: [1/70], Progress: 49.46 %, Steps: 47, Current Learning Rate: 0.0000119, \u001b[96mTrain Loss: 20.082\n",
      "\u001b[0m\u001b[1mEpoch: [1/70], Progress: 50.54 %, Steps: 48, Current Learning Rate: 0.0000121, \u001b[96mTrain Loss: 19.464\n",
      "\u001b[0m\u001b[1mEpoch: [1/70], Progress: 51.61 %, Steps: 49, Current Learning Rate: 0.0000124, \u001b[91mTrain Loss: 19.838\n",
      "\u001b[0m\u001b[1mEpoch: [1/70], Progress: 52.69 %, Steps: 50, Current Learning Rate: 0.0000126, \u001b[96mTrain Loss: 19.248\n",
      "\u001b[0m\u001b[1mEpoch: [1/70], Progress: 53.76 %, Steps: 51, Current Learning Rate: 0.0000128, \u001b[96mTrain Loss: 19.076\n",
      "\u001b[0m\u001b[1mEpoch: [1/70], Progress: 54.84 %, Steps: 52, Current Learning Rate: 0.0000131, \u001b[96mTrain Loss: 18.900\n",
      "\u001b[0m\u001b[1mEpoch: [1/70], Progress: 55.91 %, Steps: 53, Current Learning Rate: 0.0000133, \u001b[96mTrain Loss: 18.551\n",
      "\u001b[0m\u001b[1mEpoch: [1/70], Progress: 56.99 %, Steps: 54, Current Learning Rate: 0.0000136, \u001b[96mTrain Loss: 18.212\n",
      "\u001b[0m\u001b[1mEpoch: [1/70], Progress: 58.06 %, Steps: 55, Current Learning Rate: 0.0000138, \u001b[91mTrain Loss: 18.637\n",
      "\u001b[0m\u001b[1mEpoch: [1/70], Progress: 59.14 %, Steps: 56, Current Learning Rate: 0.0000141, \u001b[91mTrain Loss: 18.729\n",
      "\u001b[0m\u001b[1mEpoch: [1/70], Progress: 60.22 %, Steps: 57, Current Learning Rate: 0.0000143, \u001b[96mTrain Loss: 17.991\n",
      "\u001b[0m\u001b[1mEpoch: [1/70], Progress: 61.29 %, Steps: 58, Current Learning Rate: 0.0000146, \u001b[91mTrain Loss: 18.272\n",
      "\u001b[0m\u001b[1mEpoch: [1/70], Progress: 62.37 %, Steps: 59, Current Learning Rate: 0.0000148, \u001b[96mTrain Loss: 17.869\n",
      "\u001b[0m\u001b[1mEpoch: [1/70], Progress: 63.44 %, Steps: 60, Current Learning Rate: 0.0000151, \u001b[96mTrain Loss: 17.301\n",
      "\u001b[0m\u001b[1mEpoch: [1/70], Progress: 64.52 %, Steps: 61, Current Learning Rate: 0.0000153, \u001b[96mTrain Loss: 17.212\n",
      "\u001b[0m\u001b[1mEpoch: [1/70], Progress: 65.59 %, Steps: 62, Current Learning Rate: 0.0000156, \u001b[96mTrain Loss: 16.686\n",
      "\u001b[0m\u001b[1mEpoch: [1/70], Progress: 66.67 %, Steps: 63, Current Learning Rate: 0.0000158, \u001b[91mTrain Loss: 16.756\n",
      "\u001b[0m\u001b[1mEpoch: [1/70], Progress: 67.74 %, Steps: 64, Current Learning Rate: 0.0000161, \u001b[91mTrain Loss: 16.999\n",
      "\u001b[0m\u001b[1mEpoch: [1/70], Progress: 68.82 %, Steps: 65, Current Learning Rate: 0.0000163, \u001b[96mTrain Loss: 16.484\n",
      "\u001b[0m\u001b[1mEpoch: [1/70], Progress: 69.89 %, Steps: 66, Current Learning Rate: 0.0000166, \u001b[91mTrain Loss: 16.699\n",
      "\u001b[0m\u001b[1mEpoch: [1/70], Progress: 70.97 %, Steps: 67, Current Learning Rate: 0.0000168, \u001b[91mTrain Loss: 16.847\n",
      "\u001b[0m\u001b[1mEpoch: [1/70], Progress: 72.04 %, Steps: 68, Current Learning Rate: 0.0000170, \u001b[96mTrain Loss: 16.088\n",
      "\u001b[0m\u001b[1mEpoch: [1/70], Progress: 73.12 %, Steps: 69, Current Learning Rate: 0.0000173, \u001b[91mTrain Loss: 17.038\n",
      "\u001b[0m\u001b[1mEpoch: [1/70], Progress: 74.19 %, Steps: 70, Current Learning Rate: 0.0000175, \u001b[96mTrain Loss: 16.710\n",
      "\u001b[0m\u001b[1mEpoch: [1/70], Progress: 75.27 %, Steps: 71, Current Learning Rate: 0.0000178, \u001b[96mTrain Loss: 15.971\n",
      "\u001b[0m\u001b[1mEpoch: [1/70], Progress: 76.34 %, Steps: 72, Current Learning Rate: 0.0000180, \u001b[96mTrain Loss: 15.889\n",
      "\u001b[0m\u001b[1mEpoch: [1/70], Progress: 77.42 %, Steps: 73, Current Learning Rate: 0.0000183, \u001b[91mTrain Loss: 15.991\n",
      "\u001b[0m\u001b[1mEpoch: [1/70], Progress: 78.49 %, Steps: 74, Current Learning Rate: 0.0000185, \u001b[91mTrain Loss: 16.153\n",
      "\u001b[0m\u001b[1mEpoch: [1/70], Progress: 79.57 %, Steps: 75, Current Learning Rate: 0.0000188, \u001b[96mTrain Loss: 15.989\n",
      "\u001b[0m\u001b[1mEpoch: [1/70], Progress: 80.65 %, Steps: 76, Current Learning Rate: 0.0000190, \u001b[96mTrain Loss: 15.749\n",
      "\u001b[0m\u001b[1mEpoch: [1/70], Progress: 81.72 %, Steps: 77, Current Learning Rate: 0.0000193, \u001b[96mTrain Loss: 15.310\n",
      "\u001b[0m\u001b[1mEpoch: [1/70], Progress: 82.80 %, Steps: 78, Current Learning Rate: 0.0000195, \u001b[91mTrain Loss: 15.511\n",
      "\u001b[0m\u001b[1mEpoch: [1/70], Progress: 83.87 %, Steps: 79, Current Learning Rate: 0.0000198, \u001b[91mTrain Loss: 15.714\n",
      "\u001b[0m\u001b[1mEpoch: [1/70], Progress: 84.95 %, Steps: 80, Current Learning Rate: 0.0000200, \u001b[96mTrain Loss: 14.964\n",
      "\u001b[0m\u001b[1mEpoch: [1/70], Progress: 86.02 %, Steps: 81, Current Learning Rate: 0.0000203, \u001b[91mTrain Loss: 15.949\n",
      "\u001b[0m\u001b[1mEpoch: [1/70], Progress: 87.10 %, Steps: 82, Current Learning Rate: 0.0000205, \u001b[96mTrain Loss: 15.413\n",
      "\u001b[0m\u001b[1mEpoch: [1/70], Progress: 88.17 %, Steps: 83, Current Learning Rate: 0.0000208, \u001b[96mTrain Loss: 15.263\n",
      "\u001b[0m\u001b[1mEpoch: [1/70], Progress: 89.25 %, Steps: 84, Current Learning Rate: 0.0000210, \u001b[96mTrain Loss: 15.231\n",
      "\u001b[0m\u001b[1mEpoch: [1/70], Progress: 90.32 %, Steps: 85, Current Learning Rate: 0.0000212, \u001b[96mTrain Loss: 15.061\n",
      "\u001b[0m\u001b[1mEpoch: [1/70], Progress: 91.40 %, Steps: 86, Current Learning Rate: 0.0000215, \u001b[91mTrain Loss: 15.158\n",
      "\u001b[0m\u001b[1mEpoch: [1/70], Progress: 92.47 %, Steps: 87, Current Learning Rate: 0.0000217, \u001b[96mTrain Loss: 14.776\n",
      "\u001b[0m\u001b[1mEpoch: [1/70], Progress: 93.55 %, Steps: 88, Current Learning Rate: 0.0000220, \u001b[91mTrain Loss: 14.964\n",
      "\u001b[0m\u001b[1mEpoch: [1/70], Progress: 94.62 %, Steps: 89, Current Learning Rate: 0.0000222, \u001b[96mTrain Loss: 14.820\n",
      "\u001b[0m\u001b[1mEpoch: [1/70], Progress: 95.70 %, Steps: 90, Current Learning Rate: 0.0000225, \u001b[96mTrain Loss: 14.416\n",
      "\u001b[0m\u001b[1mEpoch: [1/70], Progress: 96.77 %, Steps: 91, Current Learning Rate: 0.0000227, \u001b[91mTrain Loss: 14.685\n",
      "\u001b[0m\u001b[1mEpoch: [1/70], Progress: 97.85 %, Steps: 92, Current Learning Rate: 0.0000230, \u001b[96mTrain Loss: 14.312\n",
      "\u001b[0m\u001b[1mEpoch: [1/70], Progress: 98.92 %, Steps: 93, Current Learning Rate: 0.0000232, \u001b[96mTrain Loss: 14.042\n",
      "\u001b[1m\u001b[93mComputing Validation Loss...\n",
      "\u001b[1mEpoch 1 Completed! Average Train Loss: 20.240, Average Validation Loss: 12.947\n",
      "Best Model saved in best_model/best_model.pt.\n",
      "\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m= Epoch [2/70] ==========================================\n",
      "\u001b[0m\u001b[1mEpoch: [2/70], Progress: 0.00 %, Steps: 94, Current Learning Rate: 0.0000235, \u001b[91mTrain Loss: 14.357\n",
      "\u001b[0m\u001b[1mEpoch: [2/70], Progress: 1.08 %, Steps: 95, Current Learning Rate: 0.0000237, \u001b[96mTrain Loss: 13.919\n",
      "\u001b[0m\u001b[1mEpoch: [2/70], Progress: 2.15 %, Steps: 96, Current Learning Rate: 0.0000240, \u001b[91mTrain Loss: 14.223\n",
      "\u001b[0m\u001b[1mEpoch: [2/70], Progress: 3.23 %, Steps: 97, Current Learning Rate: 0.0000242, \u001b[96mTrain Loss: 13.911\n",
      "\u001b[0m\u001b[1mEpoch: [2/70], Progress: 4.30 %, Steps: 98, Current Learning Rate: 0.0000245, \u001b[91mTrain Loss: 14.594\n",
      "\u001b[0m\u001b[1mEpoch: [2/70], Progress: 5.38 %, Steps: 99, Current Learning Rate: 0.0000247, \u001b[96mTrain Loss: 14.295\n",
      "\u001b[0m\u001b[1mEpoch: [2/70], Progress: 6.45 %, Steps: 100, Current Learning Rate: 0.0000250, \u001b[96mTrain Loss: 13.865\n",
      "\u001b[0m\u001b[1mEpoch: [2/70], Progress: 7.53 %, Steps: 101, Current Learning Rate: 0.0000252, \u001b[96mTrain Loss: 13.650\n",
      "\u001b[0m\u001b[1mEpoch: [2/70], Progress: 8.60 %, Steps: 102, Current Learning Rate: 0.0000254, \u001b[91mTrain Loss: 13.755\n",
      "\u001b[0m\u001b[1mEpoch: [2/70], Progress: 9.68 %, Steps: 103, Current Learning Rate: 0.0000257, \u001b[91mTrain Loss: 14.259\n",
      "\u001b[0m\u001b[1mEpoch: [2/70], Progress: 10.75 %, Steps: 104, Current Learning Rate: 0.0000259, \u001b[96mTrain Loss: 13.835\n",
      "\u001b[0m\u001b[1mEpoch: [2/70], Progress: 11.83 %, Steps: 105, Current Learning Rate: 0.0000262, \u001b[96mTrain Loss: 13.655\n",
      "\u001b[0m\u001b[1mEpoch: [2/70], Progress: 12.90 %, Steps: 106, Current Learning Rate: 0.0000264, \u001b[96mTrain Loss: 13.260\n",
      "\u001b[0m\u001b[1mEpoch: [2/70], Progress: 13.98 %, Steps: 107, Current Learning Rate: 0.0000267, \u001b[91mTrain Loss: 13.494\n",
      "\u001b[0m\u001b[1mEpoch: [2/70], Progress: 15.05 %, Steps: 108, Current Learning Rate: 0.0000269, \u001b[91mTrain Loss: 13.927\n",
      "\u001b[0m\u001b[1mEpoch: [2/70], Progress: 16.13 %, Steps: 109, Current Learning Rate: 0.0000272, \u001b[96mTrain Loss: 13.890\n",
      "\u001b[0m\u001b[1mEpoch: [2/70], Progress: 17.20 %, Steps: 110, Current Learning Rate: 0.0000274, \u001b[96mTrain Loss: 13.585\n",
      "\u001b[0m\u001b[1mEpoch: [2/70], Progress: 18.28 %, Steps: 111, Current Learning Rate: 0.0000277, \u001b[96mTrain Loss: 13.575\n",
      "\u001b[0m\u001b[1mEpoch: [2/70], Progress: 19.35 %, Steps: 112, Current Learning Rate: 0.0000279, \u001b[96mTrain Loss: 13.508\n",
      "\u001b[0m\u001b[1mEpoch: [2/70], Progress: 20.43 %, Steps: 113, Current Learning Rate: 0.0000282, \u001b[91mTrain Loss: 13.529\n",
      "\u001b[0m\u001b[1mEpoch: [2/70], Progress: 21.51 %, Steps: 114, Current Learning Rate: 0.0000284, \u001b[96mTrain Loss: 12.830\n",
      "\u001b[0m\u001b[1mEpoch: [2/70], Progress: 22.58 %, Steps: 115, Current Learning Rate: 0.0000287, \u001b[91mTrain Loss: 13.611\n",
      "\u001b[0m\u001b[1mEpoch: [2/70], Progress: 23.66 %, Steps: 116, Current Learning Rate: 0.0000289, \u001b[96mTrain Loss: 13.363\n",
      "\u001b[0m\u001b[1mEpoch: [2/70], Progress: 24.73 %, Steps: 117, Current Learning Rate: 0.0000292, \u001b[96mTrain Loss: 12.894\n",
      "\u001b[0m\u001b[1mEpoch: [2/70], Progress: 25.81 %, Steps: 118, Current Learning Rate: 0.0000294, \u001b[91mTrain Loss: 13.191\n",
      "\u001b[0m\u001b[1mEpoch: [2/70], Progress: 26.88 %, Steps: 119, Current Learning Rate: 0.0000296, \u001b[96mTrain Loss: 13.148\n",
      "\u001b[0m\u001b[1mEpoch: [2/70], Progress: 27.96 %, Steps: 120, Current Learning Rate: 0.0000299, \u001b[96mTrain Loss: 12.947\n",
      "\u001b[0m\u001b[1mEpoch: [2/70], Progress: 29.03 %, Steps: 121, Current Learning Rate: 0.0000301, \u001b[91mTrain Loss: 12.959\n",
      "\u001b[0m\u001b[1mEpoch: [2/70], Progress: 30.11 %, Steps: 122, Current Learning Rate: 0.0000304, \u001b[91mTrain Loss: 13.064\n",
      "\u001b[0m\u001b[1mEpoch: [2/70], Progress: 31.18 %, Steps: 123, Current Learning Rate: 0.0000306, \u001b[96mTrain Loss: 12.992\n",
      "\u001b[0m\u001b[1mEpoch: [2/70], Progress: 32.26 %, Steps: 124, Current Learning Rate: 0.0000309, \u001b[96mTrain Loss: 12.987\n",
      "\u001b[0m\u001b[1mEpoch: [2/70], Progress: 33.33 %, Steps: 125, Current Learning Rate: 0.0000311, \u001b[96mTrain Loss: 12.438\n",
      "\u001b[0m\u001b[1mEpoch: [2/70], Progress: 34.41 %, Steps: 126, Current Learning Rate: 0.0000314, \u001b[91mTrain Loss: 12.496\n",
      "\u001b[0m\u001b[1mEpoch: [2/70], Progress: 35.48 %, Steps: 127, Current Learning Rate: 0.0000316, \u001b[91mTrain Loss: 12.531\n",
      "\u001b[0m\u001b[1mEpoch: [2/70], Progress: 36.56 %, Steps: 128, Current Learning Rate: 0.0000319, \u001b[96mTrain Loss: 12.442\n",
      "\u001b[0m\u001b[1mEpoch: [2/70], Progress: 37.63 %, Steps: 129, Current Learning Rate: 0.0000321, \u001b[91mTrain Loss: 12.647\n",
      "\u001b[0m\u001b[1mEpoch: [2/70], Progress: 38.71 %, Steps: 130, Current Learning Rate: 0.0000324, \u001b[96mTrain Loss: 12.541\n",
      "\u001b[0m\u001b[1mEpoch: [2/70], Progress: 39.78 %, Steps: 131, Current Learning Rate: 0.0000326, \u001b[96mTrain Loss: 12.460\n",
      "\u001b[0m\u001b[1mEpoch: [2/70], Progress: 40.86 %, Steps: 132, Current Learning Rate: 0.0000329, \u001b[91mTrain Loss: 12.787\n",
      "\u001b[0m\u001b[1mEpoch: [2/70], Progress: 41.94 %, Steps: 133, Current Learning Rate: 0.0000331, \u001b[96mTrain Loss: 12.414\n",
      "\u001b[0m\u001b[1mEpoch: [2/70], Progress: 43.01 %, Steps: 134, Current Learning Rate: 0.0000334, \u001b[91mTrain Loss: 12.470\n",
      "\u001b[0m\u001b[1mEpoch: [2/70], Progress: 44.09 %, Steps: 135, Current Learning Rate: 0.0000336, \u001b[91mTrain Loss: 12.612\n",
      "\u001b[0m\u001b[1mEpoch: [2/70], Progress: 45.16 %, Steps: 136, Current Learning Rate: 0.0000338, \u001b[96mTrain Loss: 12.397\n",
      "\u001b[0m\u001b[1mEpoch: [2/70], Progress: 46.24 %, Steps: 137, Current Learning Rate: 0.0000341, \u001b[96mTrain Loss: 11.985\n",
      "\u001b[0m\u001b[1mEpoch: [2/70], Progress: 47.31 %, Steps: 138, Current Learning Rate: 0.0000343, \u001b[91mTrain Loss: 12.643\n",
      "\u001b[0m\u001b[1mEpoch: [2/70], Progress: 48.39 %, Steps: 139, Current Learning Rate: 0.0000346, \u001b[91mTrain Loss: 12.733\n",
      "\u001b[0m\u001b[1mEpoch: [2/70], Progress: 49.46 %, Steps: 140, Current Learning Rate: 0.0000348, \u001b[96mTrain Loss: 12.203\n",
      "\u001b[0m\u001b[1mEpoch: [2/70], Progress: 50.54 %, Steps: 141, Current Learning Rate: 0.0000351, \u001b[91mTrain Loss: 12.506\n",
      "\u001b[0m\u001b[1mEpoch: [2/70], Progress: 51.61 %, Steps: 142, Current Learning Rate: 0.0000353, \u001b[96mTrain Loss: 12.054\n",
      "\u001b[0m\u001b[1mEpoch: [2/70], Progress: 52.69 %, Steps: 143, Current Learning Rate: 0.0000356, \u001b[96mTrain Loss: 11.926\n",
      "\u001b[0m\u001b[1mEpoch: [2/70], Progress: 53.76 %, Steps: 144, Current Learning Rate: 0.0000358, \u001b[91mTrain Loss: 12.182\n",
      "\u001b[0m\u001b[1mEpoch: [2/70], Progress: 54.84 %, Steps: 145, Current Learning Rate: 0.0000361, \u001b[96mTrain Loss: 12.094\n",
      "\u001b[0m\u001b[1mEpoch: [2/70], Progress: 55.91 %, Steps: 146, Current Learning Rate: 0.0000363, \u001b[91mTrain Loss: 12.247\n",
      "\u001b[0m\u001b[1mEpoch: [2/70], Progress: 56.99 %, Steps: 147, Current Learning Rate: 0.0000366, \u001b[96mTrain Loss: 12.052\n",
      "\u001b[0m\u001b[1mEpoch: [2/70], Progress: 58.06 %, Steps: 148, Current Learning Rate: 0.0000368, \u001b[96mTrain Loss: 11.919\n",
      "\u001b[0m\u001b[1mEpoch: [2/70], Progress: 59.14 %, Steps: 149, Current Learning Rate: 0.0000371, \u001b[91mTrain Loss: 12.320\n",
      "\u001b[0m\u001b[1mEpoch: [2/70], Progress: 60.22 %, Steps: 150, Current Learning Rate: 0.0000373, \u001b[96mTrain Loss: 12.143\n",
      "\u001b[0m\u001b[1mEpoch: [2/70], Progress: 61.29 %, Steps: 151, Current Learning Rate: 0.0000376, \u001b[96mTrain Loss: 11.589\n",
      "\u001b[0m\u001b[1mEpoch: [2/70], Progress: 62.37 %, Steps: 152, Current Learning Rate: 0.0000378, \u001b[91mTrain Loss: 12.381\n",
      "\u001b[0m\u001b[1mEpoch: [2/70], Progress: 63.44 %, Steps: 153, Current Learning Rate: 0.0000380, \u001b[96mTrain Loss: 11.941\n",
      "\u001b[0m\u001b[1mEpoch: [2/70], Progress: 64.52 %, Steps: 154, Current Learning Rate: 0.0000383, \u001b[96mTrain Loss: 11.715\n",
      "\u001b[0m\u001b[1mEpoch: [2/70], Progress: 65.59 %, Steps: 155, Current Learning Rate: 0.0000385, \u001b[96mTrain Loss: 11.452\n",
      "\u001b[0m\u001b[1mEpoch: [2/70], Progress: 66.67 %, Steps: 156, Current Learning Rate: 0.0000388, \u001b[91mTrain Loss: 11.867\n",
      "\u001b[0m\u001b[1mEpoch: [2/70], Progress: 67.74 %, Steps: 157, Current Learning Rate: 0.0000390, \u001b[96mTrain Loss: 11.779\n",
      "\u001b[0m\u001b[1mEpoch: [2/70], Progress: 68.82 %, Steps: 158, Current Learning Rate: 0.0000393, \u001b[96mTrain Loss: 11.669\n",
      "\u001b[0m\u001b[1mEpoch: [2/70], Progress: 69.89 %, Steps: 159, Current Learning Rate: 0.0000395, \u001b[91mTrain Loss: 11.960\n",
      "\u001b[0m\u001b[1mEpoch: [2/70], Progress: 70.97 %, Steps: 160, Current Learning Rate: 0.0000398, \u001b[96mTrain Loss: 11.313\n",
      "\u001b[0m\u001b[1mEpoch: [2/70], Progress: 72.04 %, Steps: 161, Current Learning Rate: 0.0000400, \u001b[96mTrain Loss: 11.293\n",
      "\u001b[0m\u001b[1mEpoch: [2/70], Progress: 73.12 %, Steps: 162, Current Learning Rate: 0.0000403, \u001b[91mTrain Loss: 11.845\n",
      "\u001b[0m\u001b[1mEpoch: [2/70], Progress: 74.19 %, Steps: 163, Current Learning Rate: 0.0000405, \u001b[91mTrain Loss: 11.893\n",
      "\u001b[0m\u001b[1mEpoch: [2/70], Progress: 75.27 %, Steps: 164, Current Learning Rate: 0.0000408, \u001b[96mTrain Loss: 11.607\n",
      "\u001b[0m\u001b[1mEpoch: [2/70], Progress: 76.34 %, Steps: 165, Current Learning Rate: 0.0000410, \u001b[91mTrain Loss: 11.810\n",
      "\u001b[0m\u001b[1mEpoch: [2/70], Progress: 77.42 %, Steps: 166, Current Learning Rate: 0.0000413, \u001b[96mTrain Loss: 11.626\n",
      "\u001b[0m\u001b[1mEpoch: [2/70], Progress: 78.49 %, Steps: 167, Current Learning Rate: 0.0000415, \u001b[96mTrain Loss: 11.606\n",
      "\u001b[0m\u001b[1mEpoch: [2/70], Progress: 79.57 %, Steps: 168, Current Learning Rate: 0.0000418, \u001b[96mTrain Loss: 11.487\n",
      "\u001b[0m\u001b[1mEpoch: [2/70], Progress: 80.65 %, Steps: 169, Current Learning Rate: 0.0000420, \u001b[91mTrain Loss: 11.574\n",
      "\u001b[0m\u001b[1mEpoch: [2/70], Progress: 81.72 %, Steps: 170, Current Learning Rate: 0.0000422, \u001b[91mTrain Loss: 11.784\n",
      "\u001b[0m\u001b[1mEpoch: [2/70], Progress: 82.80 %, Steps: 171, Current Learning Rate: 0.0000425, \u001b[96mTrain Loss: 11.492\n",
      "\u001b[0m\u001b[1mEpoch: [2/70], Progress: 83.87 %, Steps: 172, Current Learning Rate: 0.0000427, \u001b[96mTrain Loss: 11.326\n",
      "\u001b[0m\u001b[1mEpoch: [2/70], Progress: 84.95 %, Steps: 173, Current Learning Rate: 0.0000430, \u001b[91mTrain Loss: 12.131\n",
      "\u001b[0m\u001b[1mEpoch: [2/70], Progress: 86.02 %, Steps: 174, Current Learning Rate: 0.0000432, \u001b[96mTrain Loss: 11.374\n",
      "\u001b[0m\u001b[1mEpoch: [2/70], Progress: 87.10 %, Steps: 175, Current Learning Rate: 0.0000435, \u001b[91mTrain Loss: 11.422\n",
      "\u001b[0m\u001b[1mEpoch: [2/70], Progress: 88.17 %, Steps: 176, Current Learning Rate: 0.0000437, \u001b[96mTrain Loss: 11.229\n",
      "\u001b[0m\u001b[1mEpoch: [2/70], Progress: 89.25 %, Steps: 177, Current Learning Rate: 0.0000440, \u001b[91mTrain Loss: 11.574\n",
      "\u001b[0m\u001b[1mEpoch: [2/70], Progress: 90.32 %, Steps: 178, Current Learning Rate: 0.0000442, \u001b[96mTrain Loss: 11.365\n",
      "\u001b[0m\u001b[1mEpoch: [2/70], Progress: 91.40 %, Steps: 179, Current Learning Rate: 0.0000445, \u001b[96mTrain Loss: 11.132\n",
      "\u001b[0m\u001b[1mEpoch: [2/70], Progress: 92.47 %, Steps: 180, Current Learning Rate: 0.0000447, \u001b[91mTrain Loss: 11.401\n",
      "\u001b[0m\u001b[1mEpoch: [2/70], Progress: 93.55 %, Steps: 181, Current Learning Rate: 0.0000450, \u001b[91mTrain Loss: 11.767\n",
      "\u001b[0m\u001b[1mEpoch: [2/70], Progress: 94.62 %, Steps: 182, Current Learning Rate: 0.0000452, \u001b[96mTrain Loss: 11.524\n",
      "\u001b[0m\u001b[1mEpoch: [2/70], Progress: 95.70 %, Steps: 183, Current Learning Rate: 0.0000455, \u001b[91mTrain Loss: 12.298\n",
      "\u001b[0m\u001b[1mEpoch: [2/70], Progress: 96.77 %, Steps: 184, Current Learning Rate: 0.0000457, \u001b[96mTrain Loss: 11.033\n",
      "\u001b[0m\u001b[1mEpoch: [2/70], Progress: 97.85 %, Steps: 185, Current Learning Rate: 0.0000460, \u001b[91mTrain Loss: 11.506\n",
      "\u001b[0m\u001b[1mEpoch: [2/70], Progress: 98.92 %, Steps: 186, Current Learning Rate: 0.0000462, \u001b[96mTrain Loss: 11.112\n",
      "\u001b[1m\u001b[93mComputing Validation Loss...\n",
      "\u001b[1mEpoch 2 Completed! Average Train Loss: 12.475, Average Validation Loss: 8.164\n",
      "Best Model saved in best_model/best_model.pt.\n",
      "\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m= Epoch [3/70] ==========================================\n",
      "\u001b[0m\u001b[1mEpoch: [3/70], Progress: 0.00 %, Steps: 187, Current Learning Rate: 0.0000464, \u001b[91mTrain Loss: 11.447\n",
      "\u001b[0m\u001b[1mEpoch: [3/70], Progress: 1.08 %, Steps: 188, Current Learning Rate: 0.0000467, \u001b[91mTrain Loss: 11.725\n",
      "\u001b[0m\u001b[1mEpoch: [3/70], Progress: 2.15 %, Steps: 189, Current Learning Rate: 0.0000469, \u001b[96mTrain Loss: 11.030\n",
      "\u001b[0m\u001b[1mEpoch: [3/70], Progress: 3.23 %, Steps: 190, Current Learning Rate: 0.0000472, \u001b[91mTrain Loss: 11.190\n",
      "\u001b[0m\u001b[1mEpoch: [3/70], Progress: 4.30 %, Steps: 191, Current Learning Rate: 0.0000474, \u001b[91mTrain Loss: 11.405\n",
      "\u001b[0m\u001b[1mEpoch: [3/70], Progress: 5.38 %, Steps: 192, Current Learning Rate: 0.0000477, \u001b[96mTrain Loss: 11.056\n",
      "\u001b[0m\u001b[1mEpoch: [3/70], Progress: 6.45 %, Steps: 193, Current Learning Rate: 0.0000479, \u001b[91mTrain Loss: 11.062\n",
      "\u001b[0m\u001b[1mEpoch: [3/70], Progress: 7.53 %, Steps: 194, Current Learning Rate: 0.0000482, \u001b[96mTrain Loss: 10.984\n",
      "\u001b[0m\u001b[1mEpoch: [3/70], Progress: 8.60 %, Steps: 195, Current Learning Rate: 0.0000484, \u001b[96mTrain Loss: 10.719\n",
      "\u001b[0m\u001b[1mEpoch: [3/70], Progress: 9.68 %, Steps: 196, Current Learning Rate: 0.0000487, \u001b[91mTrain Loss: 10.912\n",
      "\u001b[0m\u001b[1mEpoch: [3/70], Progress: 10.75 %, Steps: 197, Current Learning Rate: 0.0000489, \u001b[91mTrain Loss: 11.121\n",
      "\u001b[0m\u001b[1mEpoch: [3/70], Progress: 11.83 %, Steps: 198, Current Learning Rate: 0.0000492, \u001b[96mTrain Loss: 10.730\n",
      "\u001b[0m\u001b[1mEpoch: [3/70], Progress: 12.90 %, Steps: 199, Current Learning Rate: 0.0000494, \u001b[91mTrain Loss: 10.775\n",
      "\u001b[0m\u001b[1mEpoch: [3/70], Progress: 13.98 %, Steps: 200, Current Learning Rate: 0.0000497, \u001b[91mTrain Loss: 10.868\n",
      "\u001b[0m\u001b[1mEpoch: [3/70], Progress: 15.05 %, Steps: 201, Current Learning Rate: 0.0000499, \u001b[91mTrain Loss: 11.045\n",
      "\u001b[0m\u001b[1mEpoch: [3/70], Progress: 16.13 %, Steps: 202, Current Learning Rate: 0.0000502, \u001b[96mTrain Loss: 10.806\n",
      "\u001b[0m\u001b[1mEpoch: [3/70], Progress: 17.20 %, Steps: 203, Current Learning Rate: 0.0000504, \u001b[91mTrain Loss: 11.029\n",
      "\u001b[0m\u001b[1mEpoch: [3/70], Progress: 18.28 %, Steps: 204, Current Learning Rate: 0.0000506, \u001b[96mTrain Loss: 10.744\n",
      "\u001b[0m\u001b[1mEpoch: [3/70], Progress: 19.35 %, Steps: 205, Current Learning Rate: 0.0000509, \u001b[96mTrain Loss: 10.657\n",
      "\u001b[0m\u001b[1mEpoch: [3/70], Progress: 20.43 %, Steps: 206, Current Learning Rate: 0.0000511, \u001b[91mTrain Loss: 10.667\n",
      "\u001b[0m\u001b[1mEpoch: [3/70], Progress: 21.51 %, Steps: 207, Current Learning Rate: 0.0000514, \u001b[96mTrain Loss: 10.457\n",
      "\u001b[0m\u001b[1mEpoch: [3/70], Progress: 22.58 %, Steps: 208, Current Learning Rate: 0.0000516, \u001b[91mTrain Loss: 10.632\n",
      "\u001b[0m\u001b[1mEpoch: [3/70], Progress: 23.66 %, Steps: 209, Current Learning Rate: 0.0000519, \u001b[96mTrain Loss: 10.433\n",
      "\u001b[0m\u001b[1mEpoch: [3/70], Progress: 24.73 %, Steps: 210, Current Learning Rate: 0.0000521, \u001b[91mTrain Loss: 10.716\n",
      "\u001b[0m\u001b[1mEpoch: [3/70], Progress: 25.81 %, Steps: 211, Current Learning Rate: 0.0000524, \u001b[91mTrain Loss: 10.788\n",
      "\u001b[0m\u001b[1mEpoch: [3/70], Progress: 26.88 %, Steps: 212, Current Learning Rate: 0.0000526, \u001b[91mTrain Loss: 11.062\n",
      "\u001b[0m\u001b[1mEpoch: [3/70], Progress: 27.96 %, Steps: 213, Current Learning Rate: 0.0000529, \u001b[96mTrain Loss: 10.726\n",
      "\u001b[0m\u001b[1mEpoch: [3/70], Progress: 29.03 %, Steps: 214, Current Learning Rate: 0.0000531, \u001b[91mTrain Loss: 10.739\n",
      "\u001b[0m\u001b[1mEpoch: [3/70], Progress: 30.11 %, Steps: 215, Current Learning Rate: 0.0000534, \u001b[91mTrain Loss: 10.769\n",
      "\u001b[0m\u001b[1mEpoch: [3/70], Progress: 31.18 %, Steps: 216, Current Learning Rate: 0.0000536, \u001b[96mTrain Loss: 10.231\n",
      "\u001b[0m\u001b[1mEpoch: [3/70], Progress: 32.26 %, Steps: 217, Current Learning Rate: 0.0000539, \u001b[91mTrain Loss: 10.712\n",
      "\u001b[0m\u001b[1mEpoch: [3/70], Progress: 33.33 %, Steps: 218, Current Learning Rate: 0.0000541, \u001b[91mTrain Loss: 10.841\n",
      "\u001b[0m\u001b[1mEpoch: [3/70], Progress: 34.41 %, Steps: 219, Current Learning Rate: 0.0000544, \u001b[91mTrain Loss: 10.983\n",
      "\u001b[0m\u001b[1mEpoch: [3/70], Progress: 35.48 %, Steps: 220, Current Learning Rate: 0.0000546, \u001b[96mTrain Loss: 10.529\n",
      "\u001b[0m\u001b[1mEpoch: [3/70], Progress: 36.56 %, Steps: 221, Current Learning Rate: 0.0000548, \u001b[96mTrain Loss: 10.361\n",
      "\u001b[0m\u001b[1mEpoch: [3/70], Progress: 37.63 %, Steps: 222, Current Learning Rate: 0.0000551, \u001b[91mTrain Loss: 10.375\n",
      "\u001b[0m\u001b[1mEpoch: [3/70], Progress: 38.71 %, Steps: 223, Current Learning Rate: 0.0000553, \u001b[96mTrain Loss: 10.341\n",
      "\u001b[0m\u001b[1mEpoch: [3/70], Progress: 39.78 %, Steps: 224, Current Learning Rate: 0.0000556, \u001b[91mTrain Loss: 10.646\n",
      "\u001b[0m\u001b[1mEpoch: [3/70], Progress: 40.86 %, Steps: 225, Current Learning Rate: 0.0000558, \u001b[96mTrain Loss: 10.601\n",
      "\u001b[0m\u001b[1mEpoch: [3/70], Progress: 41.94 %, Steps: 226, Current Learning Rate: 0.0000561, \u001b[96mTrain Loss: 10.522\n",
      "\u001b[0m\u001b[1mEpoch: [3/70], Progress: 43.01 %, Steps: 227, Current Learning Rate: 0.0000563, \u001b[91mTrain Loss: 10.534\n",
      "\u001b[0m\u001b[1mEpoch: [3/70], Progress: 44.09 %, Steps: 228, Current Learning Rate: 0.0000566, \u001b[96mTrain Loss: 10.308\n",
      "\u001b[0m\u001b[1mEpoch: [3/70], Progress: 45.16 %, Steps: 229, Current Learning Rate: 0.0000568, \u001b[91mTrain Loss: 10.457\n",
      "\u001b[0m\u001b[1mEpoch: [3/70], Progress: 46.24 %, Steps: 230, Current Learning Rate: 0.0000571, \u001b[91mTrain Loss: 10.550\n",
      "\u001b[0m\u001b[1mEpoch: [3/70], Progress: 47.31 %, Steps: 231, Current Learning Rate: 0.0000573, \u001b[96mTrain Loss: 10.437\n",
      "\u001b[0m\u001b[1mEpoch: [3/70], Progress: 48.39 %, Steps: 232, Current Learning Rate: 0.0000576, \u001b[91mTrain Loss: 10.460\n",
      "\u001b[0m\u001b[1mEpoch: [3/70], Progress: 49.46 %, Steps: 233, Current Learning Rate: 0.0000578, \u001b[96mTrain Loss: 10.424\n",
      "\u001b[0m\u001b[1mEpoch: [3/70], Progress: 50.54 %, Steps: 234, Current Learning Rate: 0.0000581, \u001b[91mTrain Loss: 10.566\n",
      "\u001b[0m\u001b[1mEpoch: [3/70], Progress: 51.61 %, Steps: 235, Current Learning Rate: 0.0000583, \u001b[96mTrain Loss: 10.345\n",
      "\u001b[0m\u001b[1mEpoch: [3/70], Progress: 52.69 %, Steps: 236, Current Learning Rate: 0.0000586, \u001b[91mTrain Loss: 10.456\n",
      "\u001b[0m\u001b[1mEpoch: [3/70], Progress: 53.76 %, Steps: 237, Current Learning Rate: 0.0000588, \u001b[96mTrain Loss: 10.329\n",
      "\u001b[0m\u001b[1mEpoch: [3/70], Progress: 54.84 %, Steps: 238, Current Learning Rate: 0.0000590, \u001b[96mTrain Loss: 10.252\n",
      "\u001b[0m\u001b[1mEpoch: [3/70], Progress: 55.91 %, Steps: 239, Current Learning Rate: 0.0000593, \u001b[91mTrain Loss: 10.397\n",
      "\u001b[0m\u001b[1mEpoch: [3/70], Progress: 56.99 %, Steps: 240, Current Learning Rate: 0.0000595, \u001b[96mTrain Loss: 10.349\n",
      "\u001b[0m\u001b[1mEpoch: [3/70], Progress: 58.06 %, Steps: 241, Current Learning Rate: 0.0000598, \u001b[96mTrain Loss: 10.070\n",
      "\u001b[0m\u001b[1mEpoch: [3/70], Progress: 59.14 %, Steps: 242, Current Learning Rate: 0.0000600, \u001b[96mTrain Loss: 9.985\n",
      "\u001b[0m\u001b[1mEpoch: [3/70], Progress: 60.22 %, Steps: 243, Current Learning Rate: 0.0000603, \u001b[91mTrain Loss: 10.032\n",
      "\u001b[0m\u001b[1mEpoch: [3/70], Progress: 61.29 %, Steps: 244, Current Learning Rate: 0.0000605, \u001b[91mTrain Loss: 10.514\n",
      "\u001b[0m\u001b[1mEpoch: [3/70], Progress: 62.37 %, Steps: 245, Current Learning Rate: 0.0000608, \u001b[91mTrain Loss: 10.565\n",
      "\u001b[0m\u001b[1mEpoch: [3/70], Progress: 63.44 %, Steps: 246, Current Learning Rate: 0.0000610, \u001b[96mTrain Loss: 10.169\n",
      "\u001b[0m\u001b[1mEpoch: [3/70], Progress: 64.52 %, Steps: 247, Current Learning Rate: 0.0000613, \u001b[91mTrain Loss: 10.244\n",
      "\u001b[0m\u001b[1mEpoch: [3/70], Progress: 65.59 %, Steps: 248, Current Learning Rate: 0.0000615, \u001b[96mTrain Loss: 9.994\n",
      "\u001b[0m\u001b[1mEpoch: [3/70], Progress: 66.67 %, Steps: 249, Current Learning Rate: 0.0000618, \u001b[96mTrain Loss: 9.976\n",
      "\u001b[0m\u001b[1mEpoch: [3/70], Progress: 67.74 %, Steps: 250, Current Learning Rate: 0.0000620, \u001b[96mTrain Loss: 9.933\n",
      "\u001b[0m\u001b[1mEpoch: [3/70], Progress: 68.82 %, Steps: 251, Current Learning Rate: 0.0000623, \u001b[91mTrain Loss: 10.320\n",
      "\u001b[0m\u001b[1mEpoch: [3/70], Progress: 69.89 %, Steps: 252, Current Learning Rate: 0.0000625, \u001b[96mTrain Loss: 10.082\n",
      "\u001b[0m\u001b[1mEpoch: [3/70], Progress: 70.97 %, Steps: 253, Current Learning Rate: 0.0000628, \u001b[91mTrain Loss: 10.082\n",
      "\u001b[0m\u001b[1mEpoch: [3/70], Progress: 72.04 %, Steps: 254, Current Learning Rate: 0.0000630, \u001b[96mTrain Loss: 10.038\n",
      "\u001b[0m\u001b[1mEpoch: [3/70], Progress: 73.12 %, Steps: 255, Current Learning Rate: 0.0000632, \u001b[96mTrain Loss: 9.949\n",
      "\u001b[0m\u001b[1mEpoch: [3/70], Progress: 74.19 %, Steps: 256, Current Learning Rate: 0.0000635, \u001b[91mTrain Loss: 10.504\n",
      "\u001b[0m\u001b[1mEpoch: [3/70], Progress: 75.27 %, Steps: 257, Current Learning Rate: 0.0000637, \u001b[96mTrain Loss: 9.709\n",
      "\u001b[0m\u001b[1mEpoch: [3/70], Progress: 76.34 %, Steps: 258, Current Learning Rate: 0.0000640, \u001b[91mTrain Loss: 10.040\n",
      "\u001b[0m\u001b[1mEpoch: [3/70], Progress: 77.42 %, Steps: 259, Current Learning Rate: 0.0000642, \u001b[91mTrain Loss: 10.114\n",
      "\u001b[0m\u001b[1mEpoch: [3/70], Progress: 78.49 %, Steps: 260, Current Learning Rate: 0.0000645, \u001b[96mTrain Loss: 9.992\n",
      "\u001b[0m\u001b[1mEpoch: [3/70], Progress: 79.57 %, Steps: 261, Current Learning Rate: 0.0000647, \u001b[91mTrain Loss: 10.488\n",
      "\u001b[0m\u001b[1mEpoch: [3/70], Progress: 80.65 %, Steps: 262, Current Learning Rate: 0.0000650, \u001b[96mTrain Loss: 9.874\n",
      "\u001b[0m\u001b[1mEpoch: [3/70], Progress: 81.72 %, Steps: 263, Current Learning Rate: 0.0000652, \u001b[96mTrain Loss: 9.757\n",
      "\u001b[0m\u001b[1mEpoch: [3/70], Progress: 82.80 %, Steps: 264, Current Learning Rate: 0.0000655, \u001b[91mTrain Loss: 10.235\n",
      "\u001b[0m\u001b[1mEpoch: [3/70], Progress: 83.87 %, Steps: 265, Current Learning Rate: 0.0000657, \u001b[96mTrain Loss: 9.871\n",
      "\u001b[0m\u001b[1mEpoch: [3/70], Progress: 84.95 %, Steps: 266, Current Learning Rate: 0.0000660, \u001b[96mTrain Loss: 9.694\n",
      "\u001b[0m\u001b[1mEpoch: [3/70], Progress: 86.02 %, Steps: 267, Current Learning Rate: 0.0000662, \u001b[96mTrain Loss: 9.606\n",
      "\u001b[0m\u001b[1mEpoch: [3/70], Progress: 87.10 %, Steps: 268, Current Learning Rate: 0.0000665, \u001b[91mTrain Loss: 9.814\n",
      "\u001b[0m\u001b[1mEpoch: [3/70], Progress: 88.17 %, Steps: 269, Current Learning Rate: 0.0000667, \u001b[96mTrain Loss: 9.668\n",
      "\u001b[0m\u001b[1mEpoch: [3/70], Progress: 89.25 %, Steps: 270, Current Learning Rate: 0.0000670, \u001b[91mTrain Loss: 9.934\n",
      "\u001b[0m\u001b[1mEpoch: [3/70], Progress: 90.32 %, Steps: 271, Current Learning Rate: 0.0000672, \u001b[96mTrain Loss: 9.870\n",
      "\u001b[0m\u001b[1mEpoch: [3/70], Progress: 91.40 %, Steps: 272, Current Learning Rate: 0.0000674, \u001b[96mTrain Loss: 9.805\n",
      "\u001b[0m\u001b[1mEpoch: [3/70], Progress: 92.47 %, Steps: 273, Current Learning Rate: 0.0000677, \u001b[91mTrain Loss: 9.814\n",
      "\u001b[0m\u001b[1mEpoch: [3/70], Progress: 93.55 %, Steps: 274, Current Learning Rate: 0.0000679, \u001b[96mTrain Loss: 9.513\n",
      "\u001b[0m\u001b[1mEpoch: [3/70], Progress: 94.62 %, Steps: 275, Current Learning Rate: 0.0000682, \u001b[91mTrain Loss: 9.705\n",
      "\u001b[0m\u001b[1mEpoch: [3/70], Progress: 95.70 %, Steps: 276, Current Learning Rate: 0.0000684, \u001b[96mTrain Loss: 9.653\n",
      "\u001b[0m\u001b[1mEpoch: [3/70], Progress: 96.77 %, Steps: 277, Current Learning Rate: 0.0000687, \u001b[96mTrain Loss: 9.541\n",
      "\u001b[0m\u001b[1mEpoch: [3/70], Progress: 97.85 %, Steps: 278, Current Learning Rate: 0.0000689, \u001b[91mTrain Loss: 9.799\n",
      "\u001b[0m\u001b[1mEpoch: [3/70], Progress: 98.92 %, Steps: 279, Current Learning Rate: 0.0000692, \u001b[96mTrain Loss: 9.329\n",
      "\u001b[1m\u001b[93mComputing Validation Loss...\n",
      "\u001b[1mEpoch 3 Completed! Average Train Loss: 10.393, Average Validation Loss: 7.626\n",
      "Best Model saved in best_model/best_model.pt.\n",
      "\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m= Epoch [4/70] ==========================================\n",
      "\u001b[0m\u001b[1mEpoch: [4/70], Progress: 0.00 %, Steps: 280, Current Learning Rate: 0.0000694, \u001b[91mTrain Loss: 9.656\n",
      "\u001b[0m\u001b[1mEpoch: [4/70], Progress: 1.08 %, Steps: 281, Current Learning Rate: 0.0000697, \u001b[96mTrain Loss: 9.600\n",
      "\u001b[0m\u001b[1mEpoch: [4/70], Progress: 2.15 %, Steps: 282, Current Learning Rate: 0.0000699, \u001b[91mTrain Loss: 9.992\n",
      "\u001b[0m\u001b[1mEpoch: [4/70], Progress: 3.23 %, Steps: 283, Current Learning Rate: 0.0000702, \u001b[96mTrain Loss: 9.764\n",
      "\u001b[0m\u001b[1mEpoch: [4/70], Progress: 4.30 %, Steps: 284, Current Learning Rate: 0.0000704, \u001b[96mTrain Loss: 9.445\n",
      "\u001b[0m\u001b[1mEpoch: [4/70], Progress: 5.38 %, Steps: 285, Current Learning Rate: 0.0000707, \u001b[91mTrain Loss: 9.495\n",
      "\u001b[0m\u001b[1mEpoch: [4/70], Progress: 6.45 %, Steps: 286, Current Learning Rate: 0.0000709, \u001b[91mTrain Loss: 9.695\n",
      "\u001b[0m\u001b[1mEpoch: [4/70], Progress: 7.53 %, Steps: 287, Current Learning Rate: 0.0000712, \u001b[96mTrain Loss: 9.641\n",
      "\u001b[0m\u001b[1mEpoch: [4/70], Progress: 8.60 %, Steps: 288, Current Learning Rate: 0.0000714, \u001b[91mTrain Loss: 9.880\n",
      "\u001b[0m\u001b[1mEpoch: [4/70], Progress: 9.68 %, Steps: 289, Current Learning Rate: 0.0000716, \u001b[96mTrain Loss: 9.303\n",
      "\u001b[0m\u001b[1mEpoch: [4/70], Progress: 10.75 %, Steps: 290, Current Learning Rate: 0.0000719, \u001b[91mTrain Loss: 9.324\n",
      "\u001b[0m\u001b[1mEpoch: [4/70], Progress: 11.83 %, Steps: 291, Current Learning Rate: 0.0000721, \u001b[91mTrain Loss: 9.423\n",
      "\u001b[0m\u001b[1mEpoch: [4/70], Progress: 12.90 %, Steps: 292, Current Learning Rate: 0.0000724, \u001b[91mTrain Loss: 9.652\n",
      "\u001b[0m\u001b[1mEpoch: [4/70], Progress: 13.98 %, Steps: 293, Current Learning Rate: 0.0000726, \u001b[91mTrain Loss: 9.678\n",
      "\u001b[0m\u001b[1mEpoch: [4/70], Progress: 15.05 %, Steps: 294, Current Learning Rate: 0.0000729, \u001b[96mTrain Loss: 9.256\n",
      "\u001b[0m\u001b[1mEpoch: [4/70], Progress: 16.13 %, Steps: 295, Current Learning Rate: 0.0000731, \u001b[91mTrain Loss: 9.457\n",
      "\u001b[0m\u001b[1mEpoch: [4/70], Progress: 17.20 %, Steps: 296, Current Learning Rate: 0.0000734, \u001b[96mTrain Loss: 9.440\n",
      "\u001b[0m\u001b[1mEpoch: [4/70], Progress: 18.28 %, Steps: 297, Current Learning Rate: 0.0000736, \u001b[91mTrain Loss: 9.533\n",
      "\u001b[0m\u001b[1mEpoch: [4/70], Progress: 19.35 %, Steps: 298, Current Learning Rate: 0.0000739, \u001b[96mTrain Loss: 9.322\n",
      "\u001b[0m\u001b[1mEpoch: [4/70], Progress: 20.43 %, Steps: 299, Current Learning Rate: 0.0000741, \u001b[91mTrain Loss: 9.658\n",
      "\u001b[0m\u001b[1mEpoch: [4/70], Progress: 21.51 %, Steps: 300, Current Learning Rate: 0.0000744, \u001b[96mTrain Loss: 9.550\n",
      "\u001b[0m\u001b[1mEpoch: [4/70], Progress: 22.58 %, Steps: 301, Current Learning Rate: 0.0000746, \u001b[96mTrain Loss: 9.437\n",
      "\u001b[0m\u001b[1mEpoch: [4/70], Progress: 23.66 %, Steps: 302, Current Learning Rate: 0.0000749, \u001b[91mTrain Loss: 9.631\n",
      "\u001b[0m\u001b[1mEpoch: [4/70], Progress: 24.73 %, Steps: 303, Current Learning Rate: 0.0000751, \u001b[96mTrain Loss: 9.484\n",
      "\u001b[0m\u001b[1mEpoch: [4/70], Progress: 25.81 %, Steps: 304, Current Learning Rate: 0.0000754, \u001b[96mTrain Loss: 9.438\n",
      "\u001b[0m\u001b[1mEpoch: [4/70], Progress: 26.88 %, Steps: 305, Current Learning Rate: 0.0000756, \u001b[96mTrain Loss: 9.363\n",
      "\u001b[0m\u001b[1mEpoch: [4/70], Progress: 27.96 %, Steps: 306, Current Learning Rate: 0.0000758, \u001b[91mTrain Loss: 9.500\n",
      "\u001b[0m\u001b[1mEpoch: [4/70], Progress: 29.03 %, Steps: 307, Current Learning Rate: 0.0000761, \u001b[91mTrain Loss: 9.699\n",
      "\u001b[0m\u001b[1mEpoch: [4/70], Progress: 30.11 %, Steps: 308, Current Learning Rate: 0.0000763, \u001b[96mTrain Loss: 9.148\n",
      "\u001b[0m\u001b[1mEpoch: [4/70], Progress: 31.18 %, Steps: 309, Current Learning Rate: 0.0000766, \u001b[91mTrain Loss: 9.319\n",
      "\u001b[0m\u001b[1mEpoch: [4/70], Progress: 32.26 %, Steps: 310, Current Learning Rate: 0.0000768, \u001b[96mTrain Loss: 9.022\n",
      "\u001b[0m\u001b[1mEpoch: [4/70], Progress: 33.33 %, Steps: 311, Current Learning Rate: 0.0000771, \u001b[91mTrain Loss: 9.138\n",
      "\u001b[0m\u001b[1mEpoch: [4/70], Progress: 34.41 %, Steps: 312, Current Learning Rate: 0.0000773, \u001b[91mTrain Loss: 9.430\n",
      "\u001b[0m\u001b[1mEpoch: [4/70], Progress: 35.48 %, Steps: 313, Current Learning Rate: 0.0000776, \u001b[96mTrain Loss: 9.308\n",
      "\u001b[0m\u001b[1mEpoch: [4/70], Progress: 36.56 %, Steps: 314, Current Learning Rate: 0.0000778, \u001b[96mTrain Loss: 9.284\n",
      "\u001b[0m\u001b[1mEpoch: [4/70], Progress: 37.63 %, Steps: 315, Current Learning Rate: 0.0000781, \u001b[96mTrain Loss: 8.998\n",
      "\u001b[0m\u001b[1mEpoch: [4/70], Progress: 38.71 %, Steps: 316, Current Learning Rate: 0.0000783, \u001b[96mTrain Loss: 8.988\n",
      "\u001b[0m\u001b[1mEpoch: [4/70], Progress: 39.78 %, Steps: 317, Current Learning Rate: 0.0000786, \u001b[91mTrain Loss: 9.171\n",
      "\u001b[0m\u001b[1mEpoch: [4/70], Progress: 40.86 %, Steps: 318, Current Learning Rate: 0.0000788, \u001b[91mTrain Loss: 9.328\n",
      "\u001b[0m\u001b[1mEpoch: [4/70], Progress: 41.94 %, Steps: 319, Current Learning Rate: 0.0000791, \u001b[91mTrain Loss: 9.427\n",
      "\u001b[0m\u001b[1mEpoch: [4/70], Progress: 43.01 %, Steps: 320, Current Learning Rate: 0.0000793, \u001b[96mTrain Loss: 9.129\n",
      "\u001b[0m\u001b[1mEpoch: [4/70], Progress: 44.09 %, Steps: 321, Current Learning Rate: 0.0000796, \u001b[96mTrain Loss: 8.971\n",
      "\u001b[0m\u001b[1mEpoch: [4/70], Progress: 45.16 %, Steps: 322, Current Learning Rate: 0.0000798, \u001b[96mTrain Loss: 8.822\n",
      "\u001b[0m\u001b[1mEpoch: [4/70], Progress: 46.24 %, Steps: 323, Current Learning Rate: 0.0000800, \u001b[91mTrain Loss: 9.139\n",
      "\u001b[0m\u001b[1mEpoch: [4/70], Progress: 47.31 %, Steps: 324, Current Learning Rate: 0.0000803, \u001b[91mTrain Loss: 9.147\n",
      "\u001b[0m\u001b[1mEpoch: [4/70], Progress: 48.39 %, Steps: 325, Current Learning Rate: 0.0000805, \u001b[96mTrain Loss: 9.052\n",
      "\u001b[0m\u001b[1mEpoch: [4/70], Progress: 49.46 %, Steps: 326, Current Learning Rate: 0.0000808, \u001b[91mTrain Loss: 9.095\n",
      "\u001b[0m\u001b[1mEpoch: [4/70], Progress: 50.54 %, Steps: 327, Current Learning Rate: 0.0000810, \u001b[91mTrain Loss: 9.314\n",
      "\u001b[0m\u001b[1mEpoch: [4/70], Progress: 51.61 %, Steps: 328, Current Learning Rate: 0.0000813, \u001b[96mTrain Loss: 9.186\n",
      "\u001b[0m\u001b[1mEpoch: [4/70], Progress: 52.69 %, Steps: 329, Current Learning Rate: 0.0000815, \u001b[96mTrain Loss: 8.991\n",
      "\u001b[0m\u001b[1mEpoch: [4/70], Progress: 53.76 %, Steps: 330, Current Learning Rate: 0.0000818, \u001b[96mTrain Loss: 8.962\n",
      "\u001b[0m\u001b[1mEpoch: [4/70], Progress: 54.84 %, Steps: 331, Current Learning Rate: 0.0000820, \u001b[91mTrain Loss: 9.114\n",
      "\u001b[0m\u001b[1mEpoch: [4/70], Progress: 55.91 %, Steps: 332, Current Learning Rate: 0.0000823, \u001b[96mTrain Loss: 8.814\n",
      "\u001b[0m\u001b[1mEpoch: [4/70], Progress: 56.99 %, Steps: 333, Current Learning Rate: 0.0000825, \u001b[91mTrain Loss: 9.100\n",
      "\u001b[0m\u001b[1mEpoch: [4/70], Progress: 58.06 %, Steps: 334, Current Learning Rate: 0.0000828, \u001b[96mTrain Loss: 9.060\n",
      "\u001b[0m\u001b[1mEpoch: [4/70], Progress: 59.14 %, Steps: 335, Current Learning Rate: 0.0000830, \u001b[91mTrain Loss: 9.078\n",
      "\u001b[0m\u001b[1mEpoch: [4/70], Progress: 60.22 %, Steps: 336, Current Learning Rate: 0.0000833, \u001b[96mTrain Loss: 8.736\n",
      "\u001b[0m\u001b[1mEpoch: [4/70], Progress: 61.29 %, Steps: 337, Current Learning Rate: 0.0000835, \u001b[91mTrain Loss: 9.118\n",
      "\u001b[0m\u001b[1mEpoch: [4/70], Progress: 62.37 %, Steps: 338, Current Learning Rate: 0.0000838, \u001b[96mTrain Loss: 8.871\n",
      "\u001b[0m\u001b[1mEpoch: [4/70], Progress: 63.44 %, Steps: 339, Current Learning Rate: 0.0000840, \u001b[91mTrain Loss: 8.956\n",
      "\u001b[0m\u001b[1mEpoch: [4/70], Progress: 64.52 %, Steps: 340, Current Learning Rate: 0.0000842, \u001b[91mTrain Loss: 9.092\n",
      "\u001b[0m\u001b[1mEpoch: [4/70], Progress: 65.59 %, Steps: 341, Current Learning Rate: 0.0000845, \u001b[91mTrain Loss: 9.344\n",
      "\u001b[0m\u001b[1mEpoch: [4/70], Progress: 66.67 %, Steps: 342, Current Learning Rate: 0.0000847, \u001b[96mTrain Loss: 8.768\n",
      "\u001b[0m\u001b[1mEpoch: [4/70], Progress: 67.74 %, Steps: 343, Current Learning Rate: 0.0000850, \u001b[96mTrain Loss: 8.748\n",
      "\u001b[0m\u001b[1mEpoch: [4/70], Progress: 68.82 %, Steps: 344, Current Learning Rate: 0.0000852, \u001b[96mTrain Loss: 8.707\n",
      "\u001b[0m\u001b[1mEpoch: [4/70], Progress: 69.89 %, Steps: 345, Current Learning Rate: 0.0000855, \u001b[91mTrain Loss: 9.272\n",
      "\u001b[0m\u001b[1mEpoch: [4/70], Progress: 70.97 %, Steps: 346, Current Learning Rate: 0.0000857, \u001b[96mTrain Loss: 9.003\n",
      "\u001b[0m\u001b[1mEpoch: [4/70], Progress: 72.04 %, Steps: 347, Current Learning Rate: 0.0000860, \u001b[91mTrain Loss: 9.046\n",
      "\u001b[0m\u001b[1mEpoch: [4/70], Progress: 73.12 %, Steps: 348, Current Learning Rate: 0.0000862, \u001b[96mTrain Loss: 9.042\n",
      "\u001b[0m\u001b[1mEpoch: [4/70], Progress: 74.19 %, Steps: 349, Current Learning Rate: 0.0000865, \u001b[96mTrain Loss: 8.957\n",
      "\u001b[0m\u001b[1mEpoch: [4/70], Progress: 75.27 %, Steps: 350, Current Learning Rate: 0.0000867, \u001b[96mTrain Loss: 8.770\n",
      "\u001b[0m\u001b[1mEpoch: [4/70], Progress: 76.34 %, Steps: 351, Current Learning Rate: 0.0000870, \u001b[91mTrain Loss: 8.971\n",
      "\u001b[0m\u001b[1mEpoch: [4/70], Progress: 77.42 %, Steps: 352, Current Learning Rate: 0.0000872, \u001b[96mTrain Loss: 8.696\n",
      "\u001b[0m\u001b[1mEpoch: [4/70], Progress: 78.49 %, Steps: 353, Current Learning Rate: 0.0000875, \u001b[91mTrain Loss: 8.725\n",
      "\u001b[0m\u001b[1mEpoch: [4/70], Progress: 79.57 %, Steps: 354, Current Learning Rate: 0.0000877, \u001b[96mTrain Loss: 8.640\n",
      "\u001b[0m\u001b[1mEpoch: [4/70], Progress: 80.65 %, Steps: 355, Current Learning Rate: 0.0000880, \u001b[91mTrain Loss: 8.745\n",
      "\u001b[0m\u001b[1mEpoch: [4/70], Progress: 81.72 %, Steps: 356, Current Learning Rate: 0.0000882, \u001b[91mTrain Loss: 8.763\n",
      "\u001b[0m\u001b[1mEpoch: [4/70], Progress: 82.80 %, Steps: 357, Current Learning Rate: 0.0000884, \u001b[91mTrain Loss: 9.071\n",
      "\u001b[0m\u001b[1mEpoch: [4/70], Progress: 83.87 %, Steps: 358, Current Learning Rate: 0.0000887, \u001b[96mTrain Loss: 8.830\n",
      "\u001b[0m\u001b[1mEpoch: [4/70], Progress: 84.95 %, Steps: 359, Current Learning Rate: 0.0000889, \u001b[91mTrain Loss: 9.048\n",
      "\u001b[0m\u001b[1mEpoch: [4/70], Progress: 86.02 %, Steps: 360, Current Learning Rate: 0.0000892, \u001b[96mTrain Loss: 8.778\n",
      "\u001b[0m\u001b[1mEpoch: [4/70], Progress: 87.10 %, Steps: 361, Current Learning Rate: 0.0000894, \u001b[96mTrain Loss: 8.593\n",
      "\u001b[0m\u001b[1mEpoch: [4/70], Progress: 88.17 %, Steps: 362, Current Learning Rate: 0.0000897, \u001b[91mTrain Loss: 8.688\n",
      "\u001b[0m\u001b[1mEpoch: [4/70], Progress: 89.25 %, Steps: 363, Current Learning Rate: 0.0000899, \u001b[96mTrain Loss: 8.559\n",
      "\u001b[0m\u001b[1mEpoch: [4/70], Progress: 90.32 %, Steps: 364, Current Learning Rate: 0.0000902, \u001b[96mTrain Loss: 8.509\n",
      "\u001b[0m\u001b[1mEpoch: [4/70], Progress: 91.40 %, Steps: 365, Current Learning Rate: 0.0000904, \u001b[91mTrain Loss: 8.802\n",
      "\u001b[0m\u001b[1mEpoch: [4/70], Progress: 92.47 %, Steps: 366, Current Learning Rate: 0.0000907, \u001b[96mTrain Loss: 8.366\n",
      "\u001b[0m\u001b[1mEpoch: [4/70], Progress: 93.55 %, Steps: 367, Current Learning Rate: 0.0000909, \u001b[91mTrain Loss: 8.830\n",
      "\u001b[0m\u001b[1mEpoch: [4/70], Progress: 94.62 %, Steps: 368, Current Learning Rate: 0.0000912, \u001b[96mTrain Loss: 8.680\n",
      "\u001b[0m\u001b[1mEpoch: [4/70], Progress: 95.70 %, Steps: 369, Current Learning Rate: 0.0000914, \u001b[91mTrain Loss: 8.800\n",
      "\u001b[0m\u001b[1mEpoch: [4/70], Progress: 96.77 %, Steps: 370, Current Learning Rate: 0.0000917, \u001b[96mTrain Loss: 8.586\n",
      "\u001b[0m\u001b[1mEpoch: [4/70], Progress: 97.85 %, Steps: 371, Current Learning Rate: 0.0000919, \u001b[96mTrain Loss: 8.550\n",
      "\u001b[0m\u001b[1mEpoch: [4/70], Progress: 98.92 %, Steps: 372, Current Learning Rate: 0.0000922, \u001b[96mTrain Loss: 8.403\n",
      "\u001b[1m\u001b[93mComputing Validation Loss...\n",
      "\u001b[1mEpoch 4 Completed! Average Train Loss: 9.128, Average Validation Loss: 6.755\n",
      "Best Model saved in best_model/best_model.pt.\n",
      "\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m= Epoch [5/70] ==========================================\n",
      "\u001b[0m\u001b[1mEpoch: [5/70], Progress: 0.00 %, Steps: 373, Current Learning Rate: 0.0000924, \u001b[91mTrain Loss: 8.550\n",
      "\u001b[0m\u001b[1mEpoch: [5/70], Progress: 1.08 %, Steps: 374, Current Learning Rate: 0.0000926, \u001b[91mTrain Loss: 8.608\n",
      "\u001b[0m\u001b[1mEpoch: [5/70], Progress: 2.15 %, Steps: 375, Current Learning Rate: 0.0000929, \u001b[96mTrain Loss: 8.535\n",
      "\u001b[0m\u001b[1mEpoch: [5/70], Progress: 3.23 %, Steps: 376, Current Learning Rate: 0.0000931, \u001b[91mTrain Loss: 8.788\n",
      "\u001b[0m\u001b[1mEpoch: [5/70], Progress: 4.30 %, Steps: 377, Current Learning Rate: 0.0000934, \u001b[96mTrain Loss: 8.434\n",
      "\u001b[0m\u001b[1mEpoch: [5/70], Progress: 5.38 %, Steps: 378, Current Learning Rate: 0.0000936, \u001b[91mTrain Loss: 8.518\n",
      "\u001b[0m\u001b[1mEpoch: [5/70], Progress: 6.45 %, Steps: 379, Current Learning Rate: 0.0000939, \u001b[96mTrain Loss: 8.274\n",
      "\u001b[0m\u001b[1mEpoch: [5/70], Progress: 7.53 %, Steps: 380, Current Learning Rate: 0.0000941, \u001b[96mTrain Loss: 8.271\n",
      "\u001b[0m\u001b[1mEpoch: [5/70], Progress: 8.60 %, Steps: 381, Current Learning Rate: 0.0000944, \u001b[91mTrain Loss: 8.534\n",
      "\u001b[0m\u001b[1mEpoch: [5/70], Progress: 9.68 %, Steps: 382, Current Learning Rate: 0.0000946, \u001b[91mTrain Loss: 8.583\n",
      "\u001b[0m\u001b[1mEpoch: [5/70], Progress: 10.75 %, Steps: 383, Current Learning Rate: 0.0000949, \u001b[91mTrain Loss: 8.619\n",
      "\u001b[0m\u001b[1mEpoch: [5/70], Progress: 11.83 %, Steps: 384, Current Learning Rate: 0.0000951, \u001b[96mTrain Loss: 8.327\n",
      "\u001b[0m\u001b[1mEpoch: [5/70], Progress: 12.90 %, Steps: 385, Current Learning Rate: 0.0000954, \u001b[91mTrain Loss: 8.397\n",
      "\u001b[0m\u001b[1mEpoch: [5/70], Progress: 13.98 %, Steps: 386, Current Learning Rate: 0.0000956, \u001b[96mTrain Loss: 8.246\n",
      "\u001b[0m\u001b[1mEpoch: [5/70], Progress: 15.05 %, Steps: 387, Current Learning Rate: 0.0000959, \u001b[91mTrain Loss: 8.291\n",
      "\u001b[0m\u001b[1mEpoch: [5/70], Progress: 16.13 %, Steps: 388, Current Learning Rate: 0.0000961, \u001b[96mTrain Loss: 8.145\n",
      "\u001b[0m\u001b[1mEpoch: [5/70], Progress: 17.20 %, Steps: 389, Current Learning Rate: 0.0000964, \u001b[91mTrain Loss: 8.543\n",
      "\u001b[0m\u001b[1mEpoch: [5/70], Progress: 18.28 %, Steps: 390, Current Learning Rate: 0.0000966, \u001b[96mTrain Loss: 8.360\n",
      "\u001b[0m\u001b[1mEpoch: [5/70], Progress: 19.35 %, Steps: 391, Current Learning Rate: 0.0000968, \u001b[91mTrain Loss: 8.452\n",
      "\u001b[0m\u001b[1mEpoch: [5/70], Progress: 20.43 %, Steps: 392, Current Learning Rate: 0.0000971, \u001b[96mTrain Loss: 8.131\n",
      "\u001b[0m\u001b[1mEpoch: [5/70], Progress: 21.51 %, Steps: 393, Current Learning Rate: 0.0000973, \u001b[91mTrain Loss: 8.422\n",
      "\u001b[0m\u001b[1mEpoch: [5/70], Progress: 22.58 %, Steps: 394, Current Learning Rate: 0.0000976, \u001b[91mTrain Loss: 8.528\n",
      "\u001b[0m\u001b[1mEpoch: [5/70], Progress: 23.66 %, Steps: 395, Current Learning Rate: 0.0000978, \u001b[96mTrain Loss: 8.130\n",
      "\u001b[0m\u001b[1mEpoch: [5/70], Progress: 24.73 %, Steps: 396, Current Learning Rate: 0.0000981, \u001b[91mTrain Loss: 8.245\n",
      "\u001b[0m\u001b[1mEpoch: [5/70], Progress: 25.81 %, Steps: 397, Current Learning Rate: 0.0000983, \u001b[96mTrain Loss: 8.206\n",
      "\u001b[0m\u001b[1mEpoch: [5/70], Progress: 26.88 %, Steps: 398, Current Learning Rate: 0.0000986, \u001b[96mTrain Loss: 8.130\n",
      "\u001b[0m\u001b[1mEpoch: [5/70], Progress: 27.96 %, Steps: 399, Current Learning Rate: 0.0000988, \u001b[91mTrain Loss: 8.268\n",
      "\u001b[0m\u001b[1mEpoch: [5/70], Progress: 29.03 %, Steps: 400, Current Learning Rate: 0.0000991, \u001b[91mTrain Loss: 8.379\n",
      "\u001b[0m\u001b[1mEpoch: [5/70], Progress: 30.11 %, Steps: 401, Current Learning Rate: 0.0000993, \u001b[91mTrain Loss: 8.422\n",
      "\u001b[0m\u001b[1mEpoch: [5/70], Progress: 31.18 %, Steps: 402, Current Learning Rate: 0.0000996, \u001b[96mTrain Loss: 8.240\n",
      "\u001b[0m\u001b[1mEpoch: [5/70], Progress: 32.26 %, Steps: 403, Current Learning Rate: 0.0000998, \u001b[96mTrain Loss: 8.065\n",
      "\u001b[0m\u001b[1mEpoch: [5/70], Progress: 33.33 %, Steps: 404, Current Learning Rate: 0.0001001, \u001b[91mTrain Loss: 8.228\n",
      "\u001b[0m\u001b[1mEpoch: [5/70], Progress: 34.41 %, Steps: 405, Current Learning Rate: 0.0001003, \u001b[96mTrain Loss: 7.970\n",
      "\u001b[0m\u001b[1mEpoch: [5/70], Progress: 35.48 %, Steps: 406, Current Learning Rate: 0.0001006, \u001b[91mTrain Loss: 8.175\n",
      "\u001b[0m\u001b[1mEpoch: [5/70], Progress: 36.56 %, Steps: 407, Current Learning Rate: 0.0001008, \u001b[96mTrain Loss: 8.139\n",
      "\u001b[0m\u001b[1mEpoch: [5/70], Progress: 37.63 %, Steps: 408, Current Learning Rate: 0.0001010, \u001b[91mTrain Loss: 8.393\n",
      "\u001b[0m\u001b[1mEpoch: [5/70], Progress: 38.71 %, Steps: 409, Current Learning Rate: 0.0001013, \u001b[96mTrain Loss: 8.059\n",
      "\u001b[0m\u001b[1mEpoch: [5/70], Progress: 39.78 %, Steps: 410, Current Learning Rate: 0.0001015, \u001b[91mTrain Loss: 8.112\n",
      "\u001b[0m\u001b[1mEpoch: [5/70], Progress: 40.86 %, Steps: 411, Current Learning Rate: 0.0001018, \u001b[91mTrain Loss: 8.202\n",
      "\u001b[0m\u001b[1mEpoch: [5/70], Progress: 41.94 %, Steps: 412, Current Learning Rate: 0.0001020, \u001b[96mTrain Loss: 7.917\n",
      "\u001b[0m\u001b[1mEpoch: [5/70], Progress: 43.01 %, Steps: 413, Current Learning Rate: 0.0001023, \u001b[91mTrain Loss: 8.088\n",
      "\u001b[0m\u001b[1mEpoch: [5/70], Progress: 44.09 %, Steps: 414, Current Learning Rate: 0.0001025, \u001b[91mTrain Loss: 8.371\n",
      "\u001b[0m\u001b[1mEpoch: [5/70], Progress: 45.16 %, Steps: 415, Current Learning Rate: 0.0001028, \u001b[96mTrain Loss: 7.965\n",
      "\u001b[0m\u001b[1mEpoch: [5/70], Progress: 46.24 %, Steps: 416, Current Learning Rate: 0.0001030, \u001b[91mTrain Loss: 8.060\n",
      "\u001b[0m\u001b[1mEpoch: [5/70], Progress: 47.31 %, Steps: 417, Current Learning Rate: 0.0001033, \u001b[91mTrain Loss: 8.236\n",
      "\u001b[0m\u001b[1mEpoch: [5/70], Progress: 48.39 %, Steps: 418, Current Learning Rate: 0.0001035, \u001b[91mTrain Loss: 8.269\n",
      "\u001b[0m\u001b[1mEpoch: [5/70], Progress: 49.46 %, Steps: 419, Current Learning Rate: 0.0001038, \u001b[96mTrain Loss: 7.986\n",
      "\u001b[0m\u001b[1mEpoch: [5/70], Progress: 50.54 %, Steps: 420, Current Learning Rate: 0.0001040, \u001b[91mTrain Loss: 8.378\n",
      "\u001b[0m\u001b[1mEpoch: [5/70], Progress: 51.61 %, Steps: 421, Current Learning Rate: 0.0001043, \u001b[96mTrain Loss: 8.071\n",
      "\u001b[0m\u001b[1mEpoch: [5/70], Progress: 52.69 %, Steps: 422, Current Learning Rate: 0.0001045, \u001b[96mTrain Loss: 7.911\n",
      "\u001b[0m\u001b[1mEpoch: [5/70], Progress: 53.76 %, Steps: 423, Current Learning Rate: 0.0001048, \u001b[91mTrain Loss: 7.926\n",
      "\u001b[0m\u001b[1mEpoch: [5/70], Progress: 54.84 %, Steps: 424, Current Learning Rate: 0.0001050, \u001b[91mTrain Loss: 8.294\n",
      "\u001b[0m\u001b[1mEpoch: [5/70], Progress: 55.91 %, Steps: 425, Current Learning Rate: 0.0001052, \u001b[96mTrain Loss: 8.113\n",
      "\u001b[0m\u001b[1mEpoch: [5/70], Progress: 56.99 %, Steps: 426, Current Learning Rate: 0.0001055, \u001b[96mTrain Loss: 7.961\n",
      "\u001b[0m\u001b[1mEpoch: [5/70], Progress: 58.06 %, Steps: 427, Current Learning Rate: 0.0001057, \u001b[96mTrain Loss: 7.909\n",
      "\u001b[0m\u001b[1mEpoch: [5/70], Progress: 59.14 %, Steps: 428, Current Learning Rate: 0.0001060, \u001b[91mTrain Loss: 8.106\n",
      "\u001b[0m\u001b[1mEpoch: [5/70], Progress: 60.22 %, Steps: 429, Current Learning Rate: 0.0001062, \u001b[91mTrain Loss: 8.109\n",
      "\u001b[0m\u001b[1mEpoch: [5/70], Progress: 61.29 %, Steps: 430, Current Learning Rate: 0.0001065, \u001b[96mTrain Loss: 8.076\n",
      "\u001b[0m\u001b[1mEpoch: [5/70], Progress: 62.37 %, Steps: 431, Current Learning Rate: 0.0001067, \u001b[91mTrain Loss: 8.212\n",
      "\u001b[0m\u001b[1mEpoch: [5/70], Progress: 63.44 %, Steps: 432, Current Learning Rate: 0.0001070, \u001b[96mTrain Loss: 7.951\n",
      "\u001b[0m\u001b[1mEpoch: [5/70], Progress: 64.52 %, Steps: 433, Current Learning Rate: 0.0001072, \u001b[91mTrain Loss: 7.984\n",
      "\u001b[0m\u001b[1mEpoch: [5/70], Progress: 65.59 %, Steps: 434, Current Learning Rate: 0.0001075, \u001b[91mTrain Loss: 8.181\n",
      "\u001b[0m\u001b[1mEpoch: [5/70], Progress: 66.67 %, Steps: 435, Current Learning Rate: 0.0001077, \u001b[96mTrain Loss: 8.161\n",
      "\u001b[0m\u001b[1mEpoch: [5/70], Progress: 67.74 %, Steps: 436, Current Learning Rate: 0.0001080, \u001b[96mTrain Loss: 7.896\n",
      "\u001b[0m\u001b[1mEpoch: [5/70], Progress: 68.82 %, Steps: 437, Current Learning Rate: 0.0001082, \u001b[91mTrain Loss: 8.203\n",
      "\u001b[0m\u001b[1mEpoch: [5/70], Progress: 69.89 %, Steps: 438, Current Learning Rate: 0.0001085, \u001b[96mTrain Loss: 7.922\n",
      "\u001b[0m\u001b[1mEpoch: [5/70], Progress: 70.97 %, Steps: 439, Current Learning Rate: 0.0001087, \u001b[96mTrain Loss: 7.707\n",
      "\u001b[0m\u001b[1mEpoch: [5/70], Progress: 72.04 %, Steps: 440, Current Learning Rate: 0.0001090, \u001b[91mTrain Loss: 7.758\n",
      "\u001b[0m\u001b[1mEpoch: [5/70], Progress: 73.12 %, Steps: 441, Current Learning Rate: 0.0001092, \u001b[91mTrain Loss: 8.050\n",
      "\u001b[0m\u001b[1mEpoch: [5/70], Progress: 74.19 %, Steps: 442, Current Learning Rate: 0.0001094, \u001b[96mTrain Loss: 7.846\n",
      "\u001b[0m\u001b[1mEpoch: [5/70], Progress: 75.27 %, Steps: 443, Current Learning Rate: 0.0001097, \u001b[96mTrain Loss: 7.556\n",
      "\u001b[0m\u001b[1mEpoch: [5/70], Progress: 76.34 %, Steps: 444, Current Learning Rate: 0.0001099, \u001b[91mTrain Loss: 7.821\n",
      "\u001b[0m\u001b[1mEpoch: [5/70], Progress: 77.42 %, Steps: 445, Current Learning Rate: 0.0001102, \u001b[91mTrain Loss: 7.840\n",
      "\u001b[0m\u001b[1mEpoch: [5/70], Progress: 78.49 %, Steps: 446, Current Learning Rate: 0.0001104, \u001b[91mTrain Loss: 7.966\n",
      "\u001b[0m\u001b[1mEpoch: [5/70], Progress: 79.57 %, Steps: 447, Current Learning Rate: 0.0001107, \u001b[96mTrain Loss: 7.780\n",
      "\u001b[0m\u001b[1mEpoch: [5/70], Progress: 80.65 %, Steps: 448, Current Learning Rate: 0.0001109, \u001b[91mTrain Loss: 7.983\n",
      "\u001b[0m\u001b[1mEpoch: [5/70], Progress: 81.72 %, Steps: 449, Current Learning Rate: 0.0001112, \u001b[96mTrain Loss: 7.751\n",
      "\u001b[0m\u001b[1mEpoch: [5/70], Progress: 82.80 %, Steps: 450, Current Learning Rate: 0.0001114, \u001b[91mTrain Loss: 7.904\n",
      "\u001b[0m\u001b[1mEpoch: [5/70], Progress: 83.87 %, Steps: 451, Current Learning Rate: 0.0001117, \u001b[96mTrain Loss: 7.707\n",
      "\u001b[0m\u001b[1mEpoch: [5/70], Progress: 84.95 %, Steps: 452, Current Learning Rate: 0.0001119, \u001b[91mTrain Loss: 7.878\n",
      "\u001b[0m\u001b[1mEpoch: [5/70], Progress: 86.02 %, Steps: 453, Current Learning Rate: 0.0001122, \u001b[96mTrain Loss: 7.870\n",
      "\u001b[0m\u001b[1mEpoch: [5/70], Progress: 87.10 %, Steps: 454, Current Learning Rate: 0.0001124, \u001b[96mTrain Loss: 7.604\n",
      "\u001b[0m\u001b[1mEpoch: [5/70], Progress: 88.17 %, Steps: 455, Current Learning Rate: 0.0001127, \u001b[91mTrain Loss: 7.705\n",
      "\u001b[0m\u001b[1mEpoch: [5/70], Progress: 89.25 %, Steps: 456, Current Learning Rate: 0.0001129, \u001b[91mTrain Loss: 7.907\n",
      "\u001b[0m\u001b[1mEpoch: [5/70], Progress: 90.32 %, Steps: 457, Current Learning Rate: 0.0001132, \u001b[96mTrain Loss: 7.780\n",
      "\u001b[0m\u001b[1mEpoch: [5/70], Progress: 91.40 %, Steps: 458, Current Learning Rate: 0.0001134, \u001b[91mTrain Loss: 7.876\n",
      "\u001b[0m\u001b[1mEpoch: [5/70], Progress: 92.47 %, Steps: 459, Current Learning Rate: 0.0001136, \u001b[96mTrain Loss: 7.737\n",
      "\u001b[0m\u001b[1mEpoch: [5/70], Progress: 93.55 %, Steps: 460, Current Learning Rate: 0.0001139, \u001b[91mTrain Loss: 7.951\n",
      "\u001b[0m\u001b[1mEpoch: [5/70], Progress: 94.62 %, Steps: 461, Current Learning Rate: 0.0001141, \u001b[96mTrain Loss: 7.572\n",
      "\u001b[0m\u001b[1mEpoch: [5/70], Progress: 95.70 %, Steps: 462, Current Learning Rate: 0.0001144, \u001b[91mTrain Loss: 7.709\n",
      "\u001b[0m\u001b[1mEpoch: [5/70], Progress: 96.77 %, Steps: 463, Current Learning Rate: 0.0001146, \u001b[96mTrain Loss: 7.692\n",
      "\u001b[0m\u001b[1mEpoch: [5/70], Progress: 97.85 %, Steps: 464, Current Learning Rate: 0.0001149, \u001b[91mTrain Loss: 7.825\n",
      "\u001b[0m\u001b[1mEpoch: [5/70], Progress: 98.92 %, Steps: 465, Current Learning Rate: 0.0001151, \u001b[96mTrain Loss: 7.184\n",
      "\u001b[1m\u001b[93mComputing Validation Loss...\n",
      "\u001b[1mEpoch 5 Completed! Average Train Loss: 8.098, Average Validation Loss: 6.480\n",
      "Best Model saved in best_model/best_model.pt.\n",
      "\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m= Epoch [6/70] ==========================================\n",
      "\u001b[0m\u001b[1mEpoch: [6/70], Progress: 0.00 %, Steps: 466, Current Learning Rate: 0.0001154, \u001b[91mTrain Loss: 7.677\n",
      "\u001b[0m\u001b[1mEpoch: [6/70], Progress: 1.08 %, Steps: 467, Current Learning Rate: 0.0001156, \u001b[91mTrain Loss: 7.777\n",
      "\u001b[0m\u001b[1mEpoch: [6/70], Progress: 2.15 %, Steps: 468, Current Learning Rate: 0.0001159, \u001b[96mTrain Loss: 7.682\n",
      "\u001b[0m\u001b[1mEpoch: [6/70], Progress: 3.23 %, Steps: 469, Current Learning Rate: 0.0001161, \u001b[96mTrain Loss: 7.481\n",
      "\u001b[0m\u001b[1mEpoch: [6/70], Progress: 4.30 %, Steps: 470, Current Learning Rate: 0.0001164, \u001b[91mTrain Loss: 7.596\n",
      "\u001b[0m\u001b[1mEpoch: [6/70], Progress: 5.38 %, Steps: 471, Current Learning Rate: 0.0001166, \u001b[96mTrain Loss: 7.380\n",
      "\u001b[0m\u001b[1mEpoch: [6/70], Progress: 6.45 %, Steps: 472, Current Learning Rate: 0.0001169, \u001b[91mTrain Loss: 7.706\n",
      "\u001b[0m\u001b[1mEpoch: [6/70], Progress: 7.53 %, Steps: 473, Current Learning Rate: 0.0001171, \u001b[96mTrain Loss: 7.518\n",
      "\u001b[0m\u001b[1mEpoch: [6/70], Progress: 8.60 %, Steps: 474, Current Learning Rate: 0.0001174, \u001b[91mTrain Loss: 7.565\n",
      "\u001b[0m\u001b[1mEpoch: [6/70], Progress: 9.68 %, Steps: 475, Current Learning Rate: 0.0001176, \u001b[91mTrain Loss: 7.705\n",
      "\u001b[0m\u001b[1mEpoch: [6/70], Progress: 10.75 %, Steps: 476, Current Learning Rate: 0.0001178, \u001b[96mTrain Loss: 7.669\n",
      "\u001b[0m\u001b[1mEpoch: [6/70], Progress: 11.83 %, Steps: 477, Current Learning Rate: 0.0001181, \u001b[91mTrain Loss: 7.683\n",
      "\u001b[0m\u001b[1mEpoch: [6/70], Progress: 12.90 %, Steps: 478, Current Learning Rate: 0.0001183, \u001b[91mTrain Loss: 7.821\n",
      "\u001b[0m\u001b[1mEpoch: [6/70], Progress: 13.98 %, Steps: 479, Current Learning Rate: 0.0001186, \u001b[96mTrain Loss: 7.706\n",
      "\u001b[0m\u001b[1mEpoch: [6/70], Progress: 15.05 %, Steps: 480, Current Learning Rate: 0.0001188, \u001b[96mTrain Loss: 7.617\n",
      "\u001b[0m\u001b[1mEpoch: [6/70], Progress: 16.13 %, Steps: 481, Current Learning Rate: 0.0001191, \u001b[91mTrain Loss: 7.849\n",
      "\u001b[0m\u001b[1mEpoch: [6/70], Progress: 17.20 %, Steps: 482, Current Learning Rate: 0.0001193, \u001b[96mTrain Loss: 7.502\n",
      "\u001b[0m\u001b[1mEpoch: [6/70], Progress: 18.28 %, Steps: 483, Current Learning Rate: 0.0001196, \u001b[91mTrain Loss: 7.546\n",
      "\u001b[0m\u001b[1mEpoch: [6/70], Progress: 19.35 %, Steps: 484, Current Learning Rate: 0.0001198, \u001b[96mTrain Loss: 7.501\n",
      "\u001b[0m\u001b[1mEpoch: [6/70], Progress: 20.43 %, Steps: 485, Current Learning Rate: 0.0001201, \u001b[91mTrain Loss: 7.817\n",
      "\u001b[0m\u001b[1mEpoch: [6/70], Progress: 21.51 %, Steps: 486, Current Learning Rate: 0.0001203, \u001b[96mTrain Loss: 7.674\n",
      "\u001b[0m\u001b[1mEpoch: [6/70], Progress: 22.58 %, Steps: 487, Current Learning Rate: 0.0001206, \u001b[96mTrain Loss: 7.606\n",
      "\u001b[0m\u001b[1mEpoch: [6/70], Progress: 23.66 %, Steps: 488, Current Learning Rate: 0.0001208, \u001b[96mTrain Loss: 7.486\n",
      "\u001b[0m\u001b[1mEpoch: [6/70], Progress: 24.73 %, Steps: 489, Current Learning Rate: 0.0001211, \u001b[96mTrain Loss: 7.346\n",
      "\u001b[0m\u001b[1mEpoch: [6/70], Progress: 25.81 %, Steps: 490, Current Learning Rate: 0.0001213, \u001b[91mTrain Loss: 7.470\n",
      "\u001b[0m\u001b[1mEpoch: [6/70], Progress: 26.88 %, Steps: 491, Current Learning Rate: 0.0001216, \u001b[91mTrain Loss: 7.489\n",
      "\u001b[0m\u001b[1mEpoch: [6/70], Progress: 27.96 %, Steps: 492, Current Learning Rate: 0.0001218, \u001b[96mTrain Loss: 7.482\n",
      "\u001b[0m\u001b[1mEpoch: [6/70], Progress: 29.03 %, Steps: 493, Current Learning Rate: 0.0001220, \u001b[91mTrain Loss: 7.707\n",
      "\u001b[0m\u001b[1mEpoch: [6/70], Progress: 30.11 %, Steps: 494, Current Learning Rate: 0.0001223, \u001b[96mTrain Loss: 7.549\n",
      "\u001b[0m\u001b[1mEpoch: [6/70], Progress: 31.18 %, Steps: 495, Current Learning Rate: 0.0001225, \u001b[96mTrain Loss: 7.484\n",
      "\u001b[0m\u001b[1mEpoch: [6/70], Progress: 32.26 %, Steps: 496, Current Learning Rate: 0.0001228, \u001b[91mTrain Loss: 7.541\n",
      "\u001b[0m\u001b[1mEpoch: [6/70], Progress: 33.33 %, Steps: 497, Current Learning Rate: 0.0001230, \u001b[96mTrain Loss: 7.284\n",
      "\u001b[0m\u001b[1mEpoch: [6/70], Progress: 34.41 %, Steps: 498, Current Learning Rate: 0.0001233, \u001b[91mTrain Loss: 7.560\n",
      "\u001b[0m\u001b[1mEpoch: [6/70], Progress: 35.48 %, Steps: 499, Current Learning Rate: 0.0001235, \u001b[96mTrain Loss: 7.518\n",
      "\u001b[0m\u001b[1mEpoch: [6/70], Progress: 36.56 %, Steps: 500, Current Learning Rate: 0.0001238, \u001b[96mTrain Loss: 7.458\n",
      "\u001b[0m\u001b[1mEpoch: [6/70], Progress: 37.63 %, Steps: 501, Current Learning Rate: 0.0001240, \u001b[91mTrain Loss: 7.555\n",
      "\u001b[0m\u001b[1mEpoch: [6/70], Progress: 38.71 %, Steps: 502, Current Learning Rate: 0.0001243, \u001b[91mTrain Loss: 7.618\n",
      "\u001b[0m\u001b[1mEpoch: [6/70], Progress: 39.78 %, Steps: 503, Current Learning Rate: 0.0001245, \u001b[96mTrain Loss: 7.600\n",
      "\u001b[0m\u001b[1mEpoch: [6/70], Progress: 40.86 %, Steps: 504, Current Learning Rate: 0.0001248, \u001b[96mTrain Loss: 7.554\n",
      "\u001b[0m\u001b[1mEpoch: [6/70], Progress: 41.94 %, Steps: 505, Current Learning Rate: 0.0001250, \u001b[96mTrain Loss: 7.532\n",
      "\u001b[0m\u001b[1mEpoch: [6/70], Progress: 43.01 %, Steps: 506, Current Learning Rate: 0.0001253, \u001b[91mTrain Loss: 7.549\n",
      "\u001b[0m\u001b[1mEpoch: [6/70], Progress: 44.09 %, Steps: 507, Current Learning Rate: 0.0001255, \u001b[96mTrain Loss: 7.369\n",
      "\u001b[0m\u001b[1mEpoch: [6/70], Progress: 45.16 %, Steps: 508, Current Learning Rate: 0.0001257, \u001b[96mTrain Loss: 7.286\n",
      "\u001b[0m\u001b[1mEpoch: [6/70], Progress: 46.24 %, Steps: 509, Current Learning Rate: 0.0001260, \u001b[91mTrain Loss: 7.333\n",
      "\u001b[0m\u001b[1mEpoch: [6/70], Progress: 47.31 %, Steps: 510, Current Learning Rate: 0.0001262, \u001b[91mTrain Loss: 7.432\n",
      "\u001b[0m\u001b[1mEpoch: [6/70], Progress: 48.39 %, Steps: 511, Current Learning Rate: 0.0001265, \u001b[91mTrain Loss: 7.453\n",
      "\u001b[0m\u001b[1mEpoch: [6/70], Progress: 49.46 %, Steps: 512, Current Learning Rate: 0.0001267, \u001b[96mTrain Loss: 7.318\n",
      "\u001b[0m\u001b[1mEpoch: [6/70], Progress: 50.54 %, Steps: 513, Current Learning Rate: 0.0001270, \u001b[91mTrain Loss: 7.366\n",
      "\u001b[0m\u001b[1mEpoch: [6/70], Progress: 51.61 %, Steps: 514, Current Learning Rate: 0.0001272, \u001b[96mTrain Loss: 7.210\n",
      "\u001b[0m\u001b[1mEpoch: [6/70], Progress: 52.69 %, Steps: 515, Current Learning Rate: 0.0001275, \u001b[91mTrain Loss: 7.411\n",
      "\u001b[0m\u001b[1mEpoch: [6/70], Progress: 53.76 %, Steps: 516, Current Learning Rate: 0.0001277, \u001b[96mTrain Loss: 7.252\n",
      "\u001b[0m\u001b[1mEpoch: [6/70], Progress: 54.84 %, Steps: 517, Current Learning Rate: 0.0001280, \u001b[91mTrain Loss: 7.491\n",
      "\u001b[0m\u001b[1mEpoch: [6/70], Progress: 55.91 %, Steps: 518, Current Learning Rate: 0.0001282, \u001b[96mTrain Loss: 7.476\n",
      "\u001b[0m\u001b[1mEpoch: [6/70], Progress: 56.99 %, Steps: 519, Current Learning Rate: 0.0001285, \u001b[96mTrain Loss: 7.416\n",
      "\u001b[0m\u001b[1mEpoch: [6/70], Progress: 58.06 %, Steps: 520, Current Learning Rate: 0.0001287, \u001b[96mTrain Loss: 7.169\n",
      "\u001b[0m\u001b[1mEpoch: [6/70], Progress: 59.14 %, Steps: 521, Current Learning Rate: 0.0001290, \u001b[91mTrain Loss: 7.379\n",
      "\u001b[0m\u001b[1mEpoch: [6/70], Progress: 60.22 %, Steps: 522, Current Learning Rate: 0.0001292, \u001b[96mTrain Loss: 7.225\n",
      "\u001b[0m\u001b[1mEpoch: [6/70], Progress: 61.29 %, Steps: 523, Current Learning Rate: 0.0001295, \u001b[91mTrain Loss: 7.269\n",
      "\u001b[0m\u001b[1mEpoch: [6/70], Progress: 62.37 %, Steps: 524, Current Learning Rate: 0.0001297, \u001b[96mTrain Loss: 7.122\n",
      "\u001b[0m\u001b[1mEpoch: [6/70], Progress: 63.44 %, Steps: 525, Current Learning Rate: 0.0001299, \u001b[91mTrain Loss: 7.510\n",
      "\u001b[0m\u001b[1mEpoch: [6/70], Progress: 64.52 %, Steps: 526, Current Learning Rate: 0.0001302, \u001b[96mTrain Loss: 7.364\n",
      "\u001b[0m\u001b[1mEpoch: [6/70], Progress: 65.59 %, Steps: 527, Current Learning Rate: 0.0001304, \u001b[96mTrain Loss: 7.340\n",
      "\u001b[0m\u001b[1mEpoch: [6/70], Progress: 66.67 %, Steps: 528, Current Learning Rate: 0.0001307, \u001b[91mTrain Loss: 7.348\n",
      "\u001b[0m\u001b[1mEpoch: [6/70], Progress: 67.74 %, Steps: 529, Current Learning Rate: 0.0001309, \u001b[91mTrain Loss: 7.472\n",
      "\u001b[0m\u001b[1mEpoch: [6/70], Progress: 68.82 %, Steps: 530, Current Learning Rate: 0.0001312, \u001b[96mTrain Loss: 7.203\n",
      "\u001b[0m\u001b[1mEpoch: [6/70], Progress: 69.89 %, Steps: 531, Current Learning Rate: 0.0001314, \u001b[96mTrain Loss: 7.106\n",
      "\u001b[0m\u001b[1mEpoch: [6/70], Progress: 70.97 %, Steps: 532, Current Learning Rate: 0.0001317, \u001b[91mTrain Loss: 7.167\n",
      "\u001b[0m\u001b[1mEpoch: [6/70], Progress: 72.04 %, Steps: 533, Current Learning Rate: 0.0001319, \u001b[96mTrain Loss: 7.060\n",
      "\u001b[0m\u001b[1mEpoch: [6/70], Progress: 73.12 %, Steps: 534, Current Learning Rate: 0.0001322, \u001b[91mTrain Loss: 7.318\n",
      "\u001b[0m\u001b[1mEpoch: [6/70], Progress: 74.19 %, Steps: 535, Current Learning Rate: 0.0001324, \u001b[96mTrain Loss: 7.273\n",
      "\u001b[0m\u001b[1mEpoch: [6/70], Progress: 75.27 %, Steps: 536, Current Learning Rate: 0.0001327, \u001b[91mTrain Loss: 7.409\n",
      "\u001b[0m\u001b[1mEpoch: [6/70], Progress: 76.34 %, Steps: 537, Current Learning Rate: 0.0001329, \u001b[91mTrain Loss: 7.419\n",
      "\u001b[0m\u001b[1mEpoch: [6/70], Progress: 77.42 %, Steps: 538, Current Learning Rate: 0.0001332, \u001b[96mTrain Loss: 7.294\n",
      "\u001b[0m\u001b[1mEpoch: [6/70], Progress: 78.49 %, Steps: 539, Current Learning Rate: 0.0001334, \u001b[91mTrain Loss: 7.399\n",
      "\u001b[0m\u001b[1mEpoch: [6/70], Progress: 79.57 %, Steps: 540, Current Learning Rate: 0.0001337, \u001b[96mTrain Loss: 7.259\n",
      "\u001b[0m\u001b[1mEpoch: [6/70], Progress: 80.65 %, Steps: 541, Current Learning Rate: 0.0001339, \u001b[91mTrain Loss: 7.629\n",
      "\u001b[0m\u001b[1mEpoch: [6/70], Progress: 81.72 %, Steps: 542, Current Learning Rate: 0.0001341, \u001b[96mTrain Loss: 7.393\n",
      "\u001b[0m\u001b[1mEpoch: [6/70], Progress: 82.80 %, Steps: 543, Current Learning Rate: 0.0001344, \u001b[96mTrain Loss: 7.281\n",
      "\u001b[0m\u001b[1mEpoch: [6/70], Progress: 83.87 %, Steps: 544, Current Learning Rate: 0.0001346, \u001b[96mTrain Loss: 7.161\n",
      "\u001b[0m\u001b[1mEpoch: [6/70], Progress: 84.95 %, Steps: 545, Current Learning Rate: 0.0001349, \u001b[91mTrain Loss: 7.187\n",
      "\u001b[0m\u001b[1mEpoch: [6/70], Progress: 86.02 %, Steps: 546, Current Learning Rate: 0.0001351, \u001b[91mTrain Loss: 7.366\n",
      "\u001b[0m\u001b[1mEpoch: [6/70], Progress: 87.10 %, Steps: 547, Current Learning Rate: 0.0001354, \u001b[96mTrain Loss: 7.211\n",
      "\u001b[0m\u001b[1mEpoch: [6/70], Progress: 88.17 %, Steps: 548, Current Learning Rate: 0.0001356, \u001b[91mTrain Loss: 7.253\n",
      "\u001b[0m\u001b[1mEpoch: [6/70], Progress: 89.25 %, Steps: 549, Current Learning Rate: 0.0001359, \u001b[96mTrain Loss: 7.119\n",
      "\u001b[0m\u001b[1mEpoch: [6/70], Progress: 90.32 %, Steps: 550, Current Learning Rate: 0.0001361, \u001b[91mTrain Loss: 7.441\n",
      "\u001b[0m\u001b[1mEpoch: [6/70], Progress: 91.40 %, Steps: 551, Current Learning Rate: 0.0001364, \u001b[96mTrain Loss: 7.381\n",
      "\u001b[0m\u001b[1mEpoch: [6/70], Progress: 92.47 %, Steps: 552, Current Learning Rate: 0.0001366, \u001b[96mTrain Loss: 7.338\n",
      "\u001b[0m\u001b[1mEpoch: [6/70], Progress: 93.55 %, Steps: 553, Current Learning Rate: 0.0001369, \u001b[96mTrain Loss: 7.069\n",
      "\u001b[0m\u001b[1mEpoch: [6/70], Progress: 94.62 %, Steps: 554, Current Learning Rate: 0.0001371, \u001b[91mTrain Loss: 7.605\n",
      "\u001b[0m\u001b[1mEpoch: [6/70], Progress: 95.70 %, Steps: 555, Current Learning Rate: 0.0001374, \u001b[96mTrain Loss: 7.292\n",
      "\u001b[0m\u001b[1mEpoch: [6/70], Progress: 96.77 %, Steps: 556, Current Learning Rate: 0.0001376, \u001b[91mTrain Loss: 7.368\n",
      "\u001b[0m\u001b[1mEpoch: [6/70], Progress: 97.85 %, Steps: 557, Current Learning Rate: 0.0001379, \u001b[96mTrain Loss: 7.123\n",
      "\u001b[0m\u001b[1mEpoch: [6/70], Progress: 98.92 %, Steps: 558, Current Learning Rate: 0.0001381, \u001b[96mTrain Loss: 6.897\n",
      "\u001b[1m\u001b[93mComputing Validation Loss...\n",
      "\u001b[1mEpoch 6 Completed! Average Train Loss: 7.429, Average Validation Loss: 6.066\n",
      "Best Model saved in best_model/best_model.pt.\n",
      "\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m= Epoch [7/70] ==========================================\n",
      "\u001b[0m\u001b[1mEpoch: [7/70], Progress: 0.00 %, Steps: 559, Current Learning Rate: 0.0001383, \u001b[91mTrain Loss: 7.236\n",
      "\u001b[0m\u001b[1mEpoch: [7/70], Progress: 1.08 %, Steps: 560, Current Learning Rate: 0.0001386, \u001b[96mTrain Loss: 7.102\n",
      "\u001b[0m\u001b[1mEpoch: [7/70], Progress: 2.15 %, Steps: 561, Current Learning Rate: 0.0001388, \u001b[91mTrain Loss: 7.276\n",
      "\u001b[0m\u001b[1mEpoch: [7/70], Progress: 3.23 %, Steps: 562, Current Learning Rate: 0.0001391, \u001b[96mTrain Loss: 7.042\n",
      "\u001b[0m\u001b[1mEpoch: [7/70], Progress: 4.30 %, Steps: 563, Current Learning Rate: 0.0001393, \u001b[91mTrain Loss: 7.076\n",
      "\u001b[0m\u001b[1mEpoch: [7/70], Progress: 5.38 %, Steps: 564, Current Learning Rate: 0.0001396, \u001b[91mTrain Loss: 7.176\n",
      "\u001b[0m\u001b[1mEpoch: [7/70], Progress: 6.45 %, Steps: 565, Current Learning Rate: 0.0001398, \u001b[91mTrain Loss: 7.251\n",
      "\u001b[0m\u001b[1mEpoch: [7/70], Progress: 7.53 %, Steps: 566, Current Learning Rate: 0.0001401, \u001b[96mTrain Loss: 7.133\n",
      "\u001b[0m\u001b[1mEpoch: [7/70], Progress: 8.60 %, Steps: 567, Current Learning Rate: 0.0001403, \u001b[96mTrain Loss: 7.099\n",
      "\u001b[0m\u001b[1mEpoch: [7/70], Progress: 9.68 %, Steps: 568, Current Learning Rate: 0.0001406, \u001b[96mTrain Loss: 6.993\n",
      "\u001b[0m\u001b[1mEpoch: [7/70], Progress: 10.75 %, Steps: 569, Current Learning Rate: 0.0001408, \u001b[91mTrain Loss: 7.103\n",
      "\u001b[0m\u001b[1mEpoch: [7/70], Progress: 11.83 %, Steps: 570, Current Learning Rate: 0.0001411, \u001b[91mTrain Loss: 7.201\n",
      "\u001b[0m\u001b[1mEpoch: [7/70], Progress: 12.90 %, Steps: 571, Current Learning Rate: 0.0001413, \u001b[96mTrain Loss: 7.159\n",
      "\u001b[0m\u001b[1mEpoch: [7/70], Progress: 13.98 %, Steps: 572, Current Learning Rate: 0.0001416, \u001b[96mTrain Loss: 7.085\n",
      "\u001b[0m\u001b[1mEpoch: [7/70], Progress: 15.05 %, Steps: 573, Current Learning Rate: 0.0001418, \u001b[91mTrain Loss: 7.198\n",
      "\u001b[0m\u001b[1mEpoch: [7/70], Progress: 16.13 %, Steps: 574, Current Learning Rate: 0.0001421, \u001b[91mTrain Loss: 7.223\n",
      "\u001b[0m\u001b[1mEpoch: [7/70], Progress: 17.20 %, Steps: 575, Current Learning Rate: 0.0001423, \u001b[96mTrain Loss: 7.101\n",
      "\u001b[0m\u001b[1mEpoch: [7/70], Progress: 18.28 %, Steps: 576, Current Learning Rate: 0.0001425, \u001b[91mTrain Loss: 7.429\n",
      "\u001b[0m\u001b[1mEpoch: [7/70], Progress: 19.35 %, Steps: 577, Current Learning Rate: 0.0001428, \u001b[96mTrain Loss: 7.122\n",
      "\u001b[0m\u001b[1mEpoch: [7/70], Progress: 20.43 %, Steps: 578, Current Learning Rate: 0.0001430, \u001b[91mTrain Loss: 7.175\n",
      "\u001b[0m\u001b[1mEpoch: [7/70], Progress: 21.51 %, Steps: 579, Current Learning Rate: 0.0001433, \u001b[96mTrain Loss: 6.893\n",
      "\u001b[0m\u001b[1mEpoch: [7/70], Progress: 22.58 %, Steps: 580, Current Learning Rate: 0.0001435, \u001b[91mTrain Loss: 7.093\n",
      "\u001b[0m\u001b[1mEpoch: [7/70], Progress: 23.66 %, Steps: 581, Current Learning Rate: 0.0001438, \u001b[96mTrain Loss: 7.075\n",
      "\u001b[0m\u001b[1mEpoch: [7/70], Progress: 24.73 %, Steps: 582, Current Learning Rate: 0.0001440, \u001b[91mTrain Loss: 7.230\n",
      "\u001b[0m\u001b[1mEpoch: [7/70], Progress: 25.81 %, Steps: 583, Current Learning Rate: 0.0001443, \u001b[96mTrain Loss: 7.225\n",
      "\u001b[0m\u001b[1mEpoch: [7/70], Progress: 26.88 %, Steps: 584, Current Learning Rate: 0.0001445, \u001b[96mTrain Loss: 7.023\n",
      "\u001b[0m\u001b[1mEpoch: [7/70], Progress: 27.96 %, Steps: 585, Current Learning Rate: 0.0001448, \u001b[96mTrain Loss: 6.928\n",
      "\u001b[0m\u001b[1mEpoch: [7/70], Progress: 29.03 %, Steps: 586, Current Learning Rate: 0.0001450, \u001b[91mTrain Loss: 7.138\n",
      "\u001b[0m\u001b[1mEpoch: [7/70], Progress: 30.11 %, Steps: 587, Current Learning Rate: 0.0001453, \u001b[96mTrain Loss: 6.852\n",
      "\u001b[0m\u001b[1mEpoch: [7/70], Progress: 31.18 %, Steps: 588, Current Learning Rate: 0.0001455, \u001b[91mTrain Loss: 6.878\n",
      "\u001b[0m\u001b[1mEpoch: [7/70], Progress: 32.26 %, Steps: 589, Current Learning Rate: 0.0001458, \u001b[91mTrain Loss: 7.173\n",
      "\u001b[0m\u001b[1mEpoch: [7/70], Progress: 33.33 %, Steps: 590, Current Learning Rate: 0.0001460, \u001b[96mTrain Loss: 7.114\n",
      "\u001b[0m\u001b[1mEpoch: [7/70], Progress: 34.41 %, Steps: 591, Current Learning Rate: 0.0001463, \u001b[96mTrain Loss: 7.073\n",
      "\u001b[0m\u001b[1mEpoch: [7/70], Progress: 35.48 %, Steps: 592, Current Learning Rate: 0.0001465, \u001b[91mTrain Loss: 7.108\n",
      "\u001b[0m\u001b[1mEpoch: [7/70], Progress: 36.56 %, Steps: 593, Current Learning Rate: 0.0001467, \u001b[96mTrain Loss: 6.810\n",
      "\u001b[0m\u001b[1mEpoch: [7/70], Progress: 37.63 %, Steps: 594, Current Learning Rate: 0.0001470, \u001b[91mTrain Loss: 7.110\n",
      "\u001b[0m\u001b[1mEpoch: [7/70], Progress: 38.71 %, Steps: 595, Current Learning Rate: 0.0001472, \u001b[96mTrain Loss: 6.925\n",
      "\u001b[0m\u001b[1mEpoch: [7/70], Progress: 39.78 %, Steps: 596, Current Learning Rate: 0.0001475, \u001b[91mTrain Loss: 7.121\n",
      "\u001b[0m\u001b[1mEpoch: [7/70], Progress: 40.86 %, Steps: 597, Current Learning Rate: 0.0001477, \u001b[96mTrain Loss: 7.088\n",
      "\u001b[0m\u001b[1mEpoch: [7/70], Progress: 41.94 %, Steps: 598, Current Learning Rate: 0.0001480, \u001b[91mTrain Loss: 7.092\n",
      "\u001b[0m\u001b[1mEpoch: [7/70], Progress: 43.01 %, Steps: 599, Current Learning Rate: 0.0001482, \u001b[96mTrain Loss: 7.028\n",
      "\u001b[0m\u001b[1mEpoch: [7/70], Progress: 44.09 %, Steps: 600, Current Learning Rate: 0.0001485, \u001b[96mTrain Loss: 6.882\n",
      "\u001b[0m\u001b[1mEpoch: [7/70], Progress: 45.16 %, Steps: 601, Current Learning Rate: 0.0001487, \u001b[96mTrain Loss: 6.838\n",
      "\u001b[0m\u001b[1mEpoch: [7/70], Progress: 46.24 %, Steps: 602, Current Learning Rate: 0.0001490, \u001b[91mTrain Loss: 6.996\n",
      "\u001b[0m\u001b[1mEpoch: [7/70], Progress: 47.31 %, Steps: 603, Current Learning Rate: 0.0001492, \u001b[96mTrain Loss: 6.951\n",
      "\u001b[0m\u001b[1mEpoch: [7/70], Progress: 48.39 %, Steps: 604, Current Learning Rate: 0.0001495, \u001b[91mTrain Loss: 7.035\n",
      "\u001b[0m\u001b[1mEpoch: [7/70], Progress: 49.46 %, Steps: 605, Current Learning Rate: 0.0001497, \u001b[96mTrain Loss: 7.010\n",
      "\u001b[0m\u001b[1mEpoch: [7/70], Progress: 50.54 %, Steps: 606, Current Learning Rate: 0.0001500, \u001b[96mTrain Loss: 6.974\n",
      "\u001b[0m\u001b[1mEpoch: [7/70], Progress: 51.61 %, Steps: 607, Current Learning Rate: 0.0001502, \u001b[91mTrain Loss: 7.338\n",
      "\u001b[0m\u001b[1mEpoch: [7/70], Progress: 52.69 %, Steps: 608, Current Learning Rate: 0.0001505, \u001b[96mTrain Loss: 6.912\n",
      "\u001b[0m\u001b[1mEpoch: [7/70], Progress: 53.76 %, Steps: 609, Current Learning Rate: 0.0001507, \u001b[91mTrain Loss: 7.026\n",
      "\u001b[0m\u001b[1mEpoch: [7/70], Progress: 54.84 %, Steps: 610, Current Learning Rate: 0.0001509, \u001b[91mTrain Loss: 7.135\n",
      "\u001b[0m\u001b[1mEpoch: [7/70], Progress: 55.91 %, Steps: 611, Current Learning Rate: 0.0001512, \u001b[96mTrain Loss: 6.731\n",
      "\u001b[0m\u001b[1mEpoch: [7/70], Progress: 56.99 %, Steps: 612, Current Learning Rate: 0.0001514, \u001b[91mTrain Loss: 6.779\n",
      "\u001b[0m\u001b[1mEpoch: [7/70], Progress: 58.06 %, Steps: 613, Current Learning Rate: 0.0001517, \u001b[91mTrain Loss: 6.945\n",
      "\u001b[0m\u001b[1mEpoch: [7/70], Progress: 59.14 %, Steps: 614, Current Learning Rate: 0.0001519, \u001b[91mTrain Loss: 7.316\n",
      "\u001b[0m\u001b[1mEpoch: [7/70], Progress: 60.22 %, Steps: 615, Current Learning Rate: 0.0001522, \u001b[96mTrain Loss: 7.265\n",
      "\u001b[0m\u001b[1mEpoch: [7/70], Progress: 61.29 %, Steps: 616, Current Learning Rate: 0.0001524, \u001b[91mTrain Loss: 7.267\n",
      "\u001b[0m\u001b[1mEpoch: [7/70], Progress: 62.37 %, Steps: 617, Current Learning Rate: 0.0001527, \u001b[96mTrain Loss: 6.986\n",
      "\u001b[0m\u001b[1mEpoch: [7/70], Progress: 63.44 %, Steps: 618, Current Learning Rate: 0.0001529, \u001b[96mTrain Loss: 6.674\n",
      "\u001b[0m\u001b[1mEpoch: [7/70], Progress: 64.52 %, Steps: 619, Current Learning Rate: 0.0001532, \u001b[91mTrain Loss: 6.942\n",
      "\u001b[0m\u001b[1mEpoch: [7/70], Progress: 65.59 %, Steps: 620, Current Learning Rate: 0.0001534, \u001b[91mTrain Loss: 7.093\n",
      "\u001b[0m\u001b[1mEpoch: [7/70], Progress: 66.67 %, Steps: 621, Current Learning Rate: 0.0001537, \u001b[96mTrain Loss: 7.053\n",
      "\u001b[0m\u001b[1mEpoch: [7/70], Progress: 67.74 %, Steps: 622, Current Learning Rate: 0.0001539, \u001b[96mTrain Loss: 7.051\n",
      "\u001b[0m\u001b[1mEpoch: [7/70], Progress: 68.82 %, Steps: 623, Current Learning Rate: 0.0001542, \u001b[96mTrain Loss: 6.892\n",
      "\u001b[0m\u001b[1mEpoch: [7/70], Progress: 69.89 %, Steps: 624, Current Learning Rate: 0.0001544, \u001b[96mTrain Loss: 6.778\n",
      "\u001b[0m\u001b[1mEpoch: [7/70], Progress: 70.97 %, Steps: 625, Current Learning Rate: 0.0001547, \u001b[91mTrain Loss: 6.952\n",
      "\u001b[0m\u001b[1mEpoch: [7/70], Progress: 72.04 %, Steps: 626, Current Learning Rate: 0.0001549, \u001b[91mTrain Loss: 7.101\n",
      "\u001b[0m\u001b[1mEpoch: [7/70], Progress: 73.12 %, Steps: 627, Current Learning Rate: 0.0001551, \u001b[96mTrain Loss: 7.084\n",
      "\u001b[0m\u001b[1mEpoch: [7/70], Progress: 74.19 %, Steps: 628, Current Learning Rate: 0.0001554, \u001b[96mTrain Loss: 6.872\n",
      "\u001b[0m\u001b[1mEpoch: [7/70], Progress: 75.27 %, Steps: 629, Current Learning Rate: 0.0001556, \u001b[96mTrain Loss: 6.821\n",
      "\u001b[0m\u001b[1mEpoch: [7/70], Progress: 76.34 %, Steps: 630, Current Learning Rate: 0.0001559, \u001b[91mTrain Loss: 6.965\n",
      "\u001b[0m\u001b[1mEpoch: [7/70], Progress: 77.42 %, Steps: 631, Current Learning Rate: 0.0001561, \u001b[91mTrain Loss: 7.000\n",
      "\u001b[0m\u001b[1mEpoch: [7/70], Progress: 78.49 %, Steps: 632, Current Learning Rate: 0.0001564, \u001b[96mTrain Loss: 6.769\n",
      "\u001b[0m\u001b[1mEpoch: [7/70], Progress: 79.57 %, Steps: 633, Current Learning Rate: 0.0001566, \u001b[91mTrain Loss: 6.965\n",
      "\u001b[0m\u001b[1mEpoch: [7/70], Progress: 80.65 %, Steps: 634, Current Learning Rate: 0.0001569, \u001b[91mTrain Loss: 6.992\n",
      "\u001b[0m\u001b[1mEpoch: [7/70], Progress: 81.72 %, Steps: 635, Current Learning Rate: 0.0001571, \u001b[96mTrain Loss: 6.614\n",
      "\u001b[0m\u001b[1mEpoch: [7/70], Progress: 82.80 %, Steps: 636, Current Learning Rate: 0.0001574, \u001b[91mTrain Loss: 6.881\n",
      "\u001b[0m\u001b[1mEpoch: [7/70], Progress: 83.87 %, Steps: 637, Current Learning Rate: 0.0001576, \u001b[96mTrain Loss: 6.811\n",
      "\u001b[0m\u001b[1mEpoch: [7/70], Progress: 84.95 %, Steps: 638, Current Learning Rate: 0.0001579, \u001b[96mTrain Loss: 6.764\n",
      "\u001b[0m\u001b[1mEpoch: [7/70], Progress: 86.02 %, Steps: 639, Current Learning Rate: 0.0001581, \u001b[91mTrain Loss: 6.853\n",
      "\u001b[0m\u001b[1mEpoch: [7/70], Progress: 87.10 %, Steps: 640, Current Learning Rate: 0.0001584, \u001b[91mTrain Loss: 6.964\n",
      "\u001b[0m\u001b[1mEpoch: [7/70], Progress: 88.17 %, Steps: 641, Current Learning Rate: 0.0001586, \u001b[91mTrain Loss: 7.213\n",
      "\u001b[0m\u001b[1mEpoch: [7/70], Progress: 89.25 %, Steps: 642, Current Learning Rate: 0.0001589, \u001b[96mTrain Loss: 7.025\n",
      "\u001b[0m\u001b[1mEpoch: [7/70], Progress: 90.32 %, Steps: 643, Current Learning Rate: 0.0001591, \u001b[96mTrain Loss: 6.944\n",
      "\u001b[0m\u001b[1mEpoch: [7/70], Progress: 91.40 %, Steps: 644, Current Learning Rate: 0.0001593, \u001b[91mTrain Loss: 7.073\n",
      "\u001b[0m\u001b[1mEpoch: [7/70], Progress: 92.47 %, Steps: 645, Current Learning Rate: 0.0001596, \u001b[96mTrain Loss: 6.979\n",
      "\u001b[0m\u001b[1mEpoch: [7/70], Progress: 93.55 %, Steps: 646, Current Learning Rate: 0.0001598, \u001b[96mTrain Loss: 6.942\n",
      "\u001b[0m\u001b[1mEpoch: [7/70], Progress: 94.62 %, Steps: 647, Current Learning Rate: 0.0001601, \u001b[91mTrain Loss: 6.960\n",
      "\u001b[0m\u001b[1mEpoch: [7/70], Progress: 95.70 %, Steps: 648, Current Learning Rate: 0.0001603, \u001b[91mTrain Loss: 6.988\n",
      "\u001b[0m\u001b[1mEpoch: [7/70], Progress: 96.77 %, Steps: 649, Current Learning Rate: 0.0001606, \u001b[96mTrain Loss: 6.681\n",
      "\u001b[0m\u001b[1mEpoch: [7/70], Progress: 97.85 %, Steps: 650, Current Learning Rate: 0.0001608, \u001b[91mTrain Loss: 6.717\n",
      "\u001b[0m\u001b[1mEpoch: [7/70], Progress: 98.92 %, Steps: 651, Current Learning Rate: 0.0001611, \u001b[91mTrain Loss: 6.887\n",
      "\u001b[1m\u001b[93mComputing Validation Loss...\n",
      "\u001b[1mEpoch 7 Completed! Average Train Loss: 7.020, Average Validation Loss: 5.872\n",
      "Best Model saved in best_model/best_model.pt.\n",
      "\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m= Epoch [8/70] ==========================================\n",
      "\u001b[0m\u001b[1mEpoch: [8/70], Progress: 0.00 %, Steps: 652, Current Learning Rate: 0.0001613, \u001b[91mTrain Loss: 6.599\n",
      "\u001b[0m\u001b[1mEpoch: [8/70], Progress: 1.08 %, Steps: 653, Current Learning Rate: 0.0001616, \u001b[91mTrain Loss: 6.915\n",
      "\u001b[0m\u001b[1mEpoch: [8/70], Progress: 2.15 %, Steps: 654, Current Learning Rate: 0.0001618, \u001b[96mTrain Loss: 6.902\n",
      "\u001b[0m\u001b[1mEpoch: [8/70], Progress: 3.23 %, Steps: 655, Current Learning Rate: 0.0001621, \u001b[96mTrain Loss: 6.748\n",
      "\u001b[0m\u001b[1mEpoch: [8/70], Progress: 4.30 %, Steps: 656, Current Learning Rate: 0.0001623, \u001b[91mTrain Loss: 6.794\n",
      "\u001b[0m\u001b[1mEpoch: [8/70], Progress: 5.38 %, Steps: 657, Current Learning Rate: 0.0001626, \u001b[96mTrain Loss: 6.775\n",
      "\u001b[0m\u001b[1mEpoch: [8/70], Progress: 6.45 %, Steps: 658, Current Learning Rate: 0.0001628, \u001b[91mTrain Loss: 6.960\n",
      "\u001b[0m\u001b[1mEpoch: [8/70], Progress: 7.53 %, Steps: 659, Current Learning Rate: 0.0001631, \u001b[96mTrain Loss: 6.792\n",
      "\u001b[0m\u001b[1mEpoch: [8/70], Progress: 8.60 %, Steps: 660, Current Learning Rate: 0.0001633, \u001b[91mTrain Loss: 6.888\n",
      "\u001b[0m\u001b[1mEpoch: [8/70], Progress: 9.68 %, Steps: 661, Current Learning Rate: 0.0001635, \u001b[96mTrain Loss: 6.592\n",
      "\u001b[0m\u001b[1mEpoch: [8/70], Progress: 10.75 %, Steps: 662, Current Learning Rate: 0.0001638, \u001b[91mTrain Loss: 6.922\n",
      "\u001b[0m\u001b[1mEpoch: [8/70], Progress: 11.83 %, Steps: 663, Current Learning Rate: 0.0001640, \u001b[96mTrain Loss: 6.719\n",
      "\u001b[0m\u001b[1mEpoch: [8/70], Progress: 12.90 %, Steps: 664, Current Learning Rate: 0.0001643, \u001b[91mTrain Loss: 6.937\n",
      "\u001b[0m\u001b[1mEpoch: [8/70], Progress: 13.98 %, Steps: 665, Current Learning Rate: 0.0001645, \u001b[96mTrain Loss: 6.790\n",
      "\u001b[0m\u001b[1mEpoch: [8/70], Progress: 15.05 %, Steps: 666, Current Learning Rate: 0.0001648, \u001b[91mTrain Loss: 6.859\n",
      "\u001b[0m\u001b[1mEpoch: [8/70], Progress: 16.13 %, Steps: 667, Current Learning Rate: 0.0001650, \u001b[96mTrain Loss: 6.794\n",
      "\u001b[0m\u001b[1mEpoch: [8/70], Progress: 17.20 %, Steps: 668, Current Learning Rate: 0.0001653, \u001b[96mTrain Loss: 6.712\n",
      "\u001b[0m\u001b[1mEpoch: [8/70], Progress: 18.28 %, Steps: 669, Current Learning Rate: 0.0001655, \u001b[91mTrain Loss: 6.801\n",
      "\u001b[0m\u001b[1mEpoch: [8/70], Progress: 19.35 %, Steps: 670, Current Learning Rate: 0.0001658, \u001b[96mTrain Loss: 6.688\n",
      "\u001b[0m\u001b[1mEpoch: [8/70], Progress: 20.43 %, Steps: 671, Current Learning Rate: 0.0001660, \u001b[91mTrain Loss: 6.796\n",
      "\u001b[0m\u001b[1mEpoch: [8/70], Progress: 21.51 %, Steps: 672, Current Learning Rate: 0.0001663, \u001b[91mTrain Loss: 6.812\n",
      "\u001b[0m\u001b[1mEpoch: [8/70], Progress: 22.58 %, Steps: 673, Current Learning Rate: 0.0001665, \u001b[96mTrain Loss: 6.665\n",
      "\u001b[0m\u001b[1mEpoch: [8/70], Progress: 23.66 %, Steps: 674, Current Learning Rate: 0.0001668, \u001b[91mTrain Loss: 6.953\n",
      "\u001b[0m\u001b[1mEpoch: [8/70], Progress: 24.73 %, Steps: 675, Current Learning Rate: 0.0001670, \u001b[96mTrain Loss: 6.715\n",
      "\u001b[0m\u001b[1mEpoch: [8/70], Progress: 25.81 %, Steps: 676, Current Learning Rate: 0.0001673, \u001b[91mTrain Loss: 6.751\n",
      "\u001b[0m\u001b[1mEpoch: [8/70], Progress: 26.88 %, Steps: 677, Current Learning Rate: 0.0001675, \u001b[96mTrain Loss: 6.704\n",
      "\u001b[0m\u001b[1mEpoch: [8/70], Progress: 27.96 %, Steps: 678, Current Learning Rate: 0.0001677, \u001b[96mTrain Loss: 6.601\n",
      "\u001b[0m\u001b[1mEpoch: [8/70], Progress: 29.03 %, Steps: 679, Current Learning Rate: 0.0001680, \u001b[96mTrain Loss: 6.434\n",
      "\u001b[0m\u001b[1mEpoch: [8/70], Progress: 30.11 %, Steps: 680, Current Learning Rate: 0.0001682, \u001b[91mTrain Loss: 6.592\n",
      "\u001b[0m\u001b[1mEpoch: [8/70], Progress: 31.18 %, Steps: 681, Current Learning Rate: 0.0001685, \u001b[91mTrain Loss: 6.660\n",
      "\u001b[0m\u001b[1mEpoch: [8/70], Progress: 32.26 %, Steps: 682, Current Learning Rate: 0.0001687, \u001b[96mTrain Loss: 6.592\n",
      "\u001b[0m\u001b[1mEpoch: [8/70], Progress: 33.33 %, Steps: 683, Current Learning Rate: 0.0001690, \u001b[91mTrain Loss: 6.726\n",
      "\u001b[0m\u001b[1mEpoch: [8/70], Progress: 34.41 %, Steps: 684, Current Learning Rate: 0.0001692, \u001b[91mTrain Loss: 6.883\n",
      "\u001b[0m\u001b[1mEpoch: [8/70], Progress: 35.48 %, Steps: 685, Current Learning Rate: 0.0001695, \u001b[96mTrain Loss: 6.579\n",
      "\u001b[0m\u001b[1mEpoch: [8/70], Progress: 36.56 %, Steps: 686, Current Learning Rate: 0.0001697, \u001b[91mTrain Loss: 6.866\n",
      "\u001b[0m\u001b[1mEpoch: [8/70], Progress: 37.63 %, Steps: 687, Current Learning Rate: 0.0001700, \u001b[96mTrain Loss: 6.801\n",
      "\u001b[0m\u001b[1mEpoch: [8/70], Progress: 38.71 %, Steps: 688, Current Learning Rate: 0.0001702, \u001b[91mTrain Loss: 6.870\n",
      "\u001b[0m\u001b[1mEpoch: [8/70], Progress: 39.78 %, Steps: 689, Current Learning Rate: 0.0001705, \u001b[96mTrain Loss: 6.599\n",
      "\u001b[0m\u001b[1mEpoch: [8/70], Progress: 40.86 %, Steps: 690, Current Learning Rate: 0.0001707, \u001b[91mTrain Loss: 6.697\n",
      "\u001b[0m\u001b[1mEpoch: [8/70], Progress: 41.94 %, Steps: 691, Current Learning Rate: 0.0001710, \u001b[96mTrain Loss: 6.625\n",
      "\u001b[0m\u001b[1mEpoch: [8/70], Progress: 43.01 %, Steps: 692, Current Learning Rate: 0.0001712, \u001b[96mTrain Loss: 6.531\n",
      "\u001b[0m\u001b[1mEpoch: [8/70], Progress: 44.09 %, Steps: 693, Current Learning Rate: 0.0001715, \u001b[91mTrain Loss: 6.657\n",
      "\u001b[0m\u001b[1mEpoch: [8/70], Progress: 45.16 %, Steps: 694, Current Learning Rate: 0.0001717, \u001b[96mTrain Loss: 6.638\n",
      "\u001b[0m\u001b[1mEpoch: [8/70], Progress: 46.24 %, Steps: 695, Current Learning Rate: 0.0001719, \u001b[96mTrain Loss: 6.584\n",
      "\u001b[0m\u001b[1mEpoch: [8/70], Progress: 47.31 %, Steps: 696, Current Learning Rate: 0.0001722, \u001b[91mTrain Loss: 6.963\n",
      "\u001b[0m\u001b[1mEpoch: [8/70], Progress: 48.39 %, Steps: 697, Current Learning Rate: 0.0001724, \u001b[96mTrain Loss: 6.662\n",
      "\u001b[0m\u001b[1mEpoch: [8/70], Progress: 49.46 %, Steps: 698, Current Learning Rate: 0.0001727, \u001b[91mTrain Loss: 7.013\n",
      "\u001b[0m\u001b[1mEpoch: [8/70], Progress: 50.54 %, Steps: 699, Current Learning Rate: 0.0001729, \u001b[96mTrain Loss: 6.785\n",
      "\u001b[0m\u001b[1mEpoch: [8/70], Progress: 51.61 %, Steps: 700, Current Learning Rate: 0.0001732, \u001b[96mTrain Loss: 6.590\n",
      "\u001b[0m\u001b[1mEpoch: [8/70], Progress: 52.69 %, Steps: 701, Current Learning Rate: 0.0001734, \u001b[91mTrain Loss: 6.636\n",
      "\u001b[0m\u001b[1mEpoch: [8/70], Progress: 53.76 %, Steps: 702, Current Learning Rate: 0.0001737, \u001b[91mTrain Loss: 6.687\n",
      "\u001b[0m\u001b[1mEpoch: [8/70], Progress: 54.84 %, Steps: 703, Current Learning Rate: 0.0001739, \u001b[96mTrain Loss: 6.613\n",
      "\u001b[0m\u001b[1mEpoch: [8/70], Progress: 55.91 %, Steps: 704, Current Learning Rate: 0.0001742, \u001b[96mTrain Loss: 6.419\n",
      "\u001b[0m\u001b[1mEpoch: [8/70], Progress: 56.99 %, Steps: 705, Current Learning Rate: 0.0001744, \u001b[91mTrain Loss: 6.788\n",
      "\u001b[0m\u001b[1mEpoch: [8/70], Progress: 58.06 %, Steps: 706, Current Learning Rate: 0.0001747, \u001b[96mTrain Loss: 6.526\n",
      "\u001b[0m\u001b[1mEpoch: [8/70], Progress: 59.14 %, Steps: 707, Current Learning Rate: 0.0001749, \u001b[91mTrain Loss: 6.700\n",
      "\u001b[0m\u001b[1mEpoch: [8/70], Progress: 60.22 %, Steps: 708, Current Learning Rate: 0.0001752, \u001b[96mTrain Loss: 6.631\n",
      "\u001b[0m\u001b[1mEpoch: [8/70], Progress: 61.29 %, Steps: 709, Current Learning Rate: 0.0001754, \u001b[91mTrain Loss: 6.684\n",
      "\u001b[0m\u001b[1mEpoch: [8/70], Progress: 62.37 %, Steps: 710, Current Learning Rate: 0.0001757, \u001b[91mTrain Loss: 6.710\n",
      "\u001b[0m\u001b[1mEpoch: [8/70], Progress: 63.44 %, Steps: 711, Current Learning Rate: 0.0001759, \u001b[96mTrain Loss: 6.679\n",
      "\u001b[0m\u001b[1mEpoch: [8/70], Progress: 64.52 %, Steps: 712, Current Learning Rate: 0.0001761, \u001b[91mTrain Loss: 6.810\n",
      "\u001b[0m\u001b[1mEpoch: [8/70], Progress: 65.59 %, Steps: 713, Current Learning Rate: 0.0001764, \u001b[96mTrain Loss: 6.641\n",
      "\u001b[0m\u001b[1mEpoch: [8/70], Progress: 66.67 %, Steps: 714, Current Learning Rate: 0.0001766, \u001b[91mTrain Loss: 6.758\n",
      "\u001b[0m\u001b[1mEpoch: [8/70], Progress: 67.74 %, Steps: 715, Current Learning Rate: 0.0001769, \u001b[96mTrain Loss: 6.596\n",
      "\u001b[0m\u001b[1mEpoch: [8/70], Progress: 68.82 %, Steps: 716, Current Learning Rate: 0.0001771, \u001b[91mTrain Loss: 6.629\n",
      "\u001b[0m\u001b[1mEpoch: [8/70], Progress: 69.89 %, Steps: 717, Current Learning Rate: 0.0001774, \u001b[96mTrain Loss: 6.521\n",
      "\u001b[0m\u001b[1mEpoch: [8/70], Progress: 70.97 %, Steps: 718, Current Learning Rate: 0.0001776, \u001b[91mTrain Loss: 6.652\n",
      "\u001b[0m\u001b[1mEpoch: [8/70], Progress: 72.04 %, Steps: 719, Current Learning Rate: 0.0001779, \u001b[91mTrain Loss: 6.688\n",
      "\u001b[0m\u001b[1mEpoch: [8/70], Progress: 73.12 %, Steps: 720, Current Learning Rate: 0.0001781, \u001b[96mTrain Loss: 6.646\n",
      "\u001b[0m\u001b[1mEpoch: [8/70], Progress: 74.19 %, Steps: 721, Current Learning Rate: 0.0001784, \u001b[91mTrain Loss: 6.748\n",
      "\u001b[0m\u001b[1mEpoch: [8/70], Progress: 75.27 %, Steps: 722, Current Learning Rate: 0.0001786, \u001b[96mTrain Loss: 6.621\n",
      "\u001b[0m\u001b[1mEpoch: [8/70], Progress: 76.34 %, Steps: 723, Current Learning Rate: 0.0001789, \u001b[96mTrain Loss: 6.611\n",
      "\u001b[0m\u001b[1mEpoch: [8/70], Progress: 77.42 %, Steps: 724, Current Learning Rate: 0.0001791, \u001b[91mTrain Loss: 6.685\n",
      "\u001b[0m\u001b[1mEpoch: [8/70], Progress: 78.49 %, Steps: 725, Current Learning Rate: 0.0001794, \u001b[96mTrain Loss: 6.371\n",
      "\u001b[0m\u001b[1mEpoch: [8/70], Progress: 79.57 %, Steps: 726, Current Learning Rate: 0.0001796, \u001b[91mTrain Loss: 6.443\n",
      "\u001b[0m\u001b[1mEpoch: [8/70], Progress: 80.65 %, Steps: 727, Current Learning Rate: 0.0001799, \u001b[91mTrain Loss: 6.795\n",
      "\u001b[0m\u001b[1mEpoch: [8/70], Progress: 81.72 %, Steps: 728, Current Learning Rate: 0.0001801, \u001b[91mTrain Loss: 6.886\n",
      "\u001b[0m\u001b[1mEpoch: [8/70], Progress: 82.80 %, Steps: 729, Current Learning Rate: 0.0001803, \u001b[96mTrain Loss: 6.752\n",
      "\u001b[0m\u001b[1mEpoch: [8/70], Progress: 83.87 %, Steps: 730, Current Learning Rate: 0.0001806, \u001b[96mTrain Loss: 6.576\n",
      "\u001b[0m\u001b[1mEpoch: [8/70], Progress: 84.95 %, Steps: 731, Current Learning Rate: 0.0001808, \u001b[91mTrain Loss: 6.779\n",
      "\u001b[0m\u001b[1mEpoch: [8/70], Progress: 86.02 %, Steps: 732, Current Learning Rate: 0.0001811, \u001b[96mTrain Loss: 6.522\n",
      "\u001b[0m\u001b[1mEpoch: [8/70], Progress: 87.10 %, Steps: 733, Current Learning Rate: 0.0001813, \u001b[91mTrain Loss: 6.827\n",
      "\u001b[0m\u001b[1mEpoch: [8/70], Progress: 88.17 %, Steps: 734, Current Learning Rate: 0.0001816, \u001b[96mTrain Loss: 6.587\n",
      "\u001b[0m\u001b[1mEpoch: [8/70], Progress: 89.25 %, Steps: 735, Current Learning Rate: 0.0001818, \u001b[96mTrain Loss: 6.567\n",
      "\u001b[0m\u001b[1mEpoch: [8/70], Progress: 90.32 %, Steps: 736, Current Learning Rate: 0.0001821, \u001b[91mTrain Loss: 6.713\n",
      "\u001b[0m\u001b[1mEpoch: [8/70], Progress: 91.40 %, Steps: 737, Current Learning Rate: 0.0001823, \u001b[96mTrain Loss: 6.583\n",
      "\u001b[0m\u001b[1mEpoch: [8/70], Progress: 92.47 %, Steps: 738, Current Learning Rate: 0.0001826, \u001b[91mTrain Loss: 6.763\n",
      "\u001b[0m\u001b[1mEpoch: [8/70], Progress: 93.55 %, Steps: 739, Current Learning Rate: 0.0001828, \u001b[91mTrain Loss: 6.778\n",
      "\u001b[0m\u001b[1mEpoch: [8/70], Progress: 94.62 %, Steps: 740, Current Learning Rate: 0.0001831, \u001b[96mTrain Loss: 6.549\n",
      "\u001b[0m\u001b[1mEpoch: [8/70], Progress: 95.70 %, Steps: 741, Current Learning Rate: 0.0001833, \u001b[96mTrain Loss: 6.469\n",
      "\u001b[0m\u001b[1mEpoch: [8/70], Progress: 96.77 %, Steps: 742, Current Learning Rate: 0.0001836, \u001b[96mTrain Loss: 6.408\n",
      "\u001b[0m\u001b[1mEpoch: [8/70], Progress: 97.85 %, Steps: 743, Current Learning Rate: 0.0001838, \u001b[91mTrain Loss: 6.601\n",
      "\u001b[0m\u001b[1mEpoch: [8/70], Progress: 98.92 %, Steps: 744, Current Learning Rate: 0.0001841, \u001b[96mTrain Loss: 6.562\n",
      "\u001b[1m\u001b[93mComputing Validation Loss...\n",
      "\u001b[1mEpoch 8 Completed! Average Train Loss: 6.696, Average Validation Loss: 5.753\n",
      "Best Model saved in best_model/best_model.pt.\n",
      "\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m= Epoch [9/70] ==========================================\n",
      "\u001b[0m\u001b[1mEpoch: [9/70], Progress: 0.00 %, Steps: 745, Current Learning Rate: 0.0001843, \u001b[91mTrain Loss: 6.557\n",
      "\u001b[0m\u001b[1mEpoch: [9/70], Progress: 1.08 %, Steps: 746, Current Learning Rate: 0.0001845, \u001b[91mTrain Loss: 6.732\n",
      "\u001b[0m\u001b[1mEpoch: [9/70], Progress: 2.15 %, Steps: 747, Current Learning Rate: 0.0001848, \u001b[96mTrain Loss: 6.550\n",
      "\u001b[0m\u001b[1mEpoch: [9/70], Progress: 3.23 %, Steps: 748, Current Learning Rate: 0.0001850, \u001b[96mTrain Loss: 6.467\n",
      "\u001b[0m\u001b[1mEpoch: [9/70], Progress: 4.30 %, Steps: 749, Current Learning Rate: 0.0001853, \u001b[96mTrain Loss: 6.424\n",
      "\u001b[0m\u001b[1mEpoch: [9/70], Progress: 5.38 %, Steps: 750, Current Learning Rate: 0.0001855, \u001b[91mTrain Loss: 6.508\n",
      "\u001b[0m\u001b[1mEpoch: [9/70], Progress: 6.45 %, Steps: 751, Current Learning Rate: 0.0001858, \u001b[96mTrain Loss: 6.379\n",
      "\u001b[0m\u001b[1mEpoch: [9/70], Progress: 7.53 %, Steps: 752, Current Learning Rate: 0.0001860, \u001b[91mTrain Loss: 6.404\n",
      "\u001b[0m\u001b[1mEpoch: [9/70], Progress: 8.60 %, Steps: 753, Current Learning Rate: 0.0001863, \u001b[96mTrain Loss: 6.389\n",
      "\u001b[0m\u001b[1mEpoch: [9/70], Progress: 9.68 %, Steps: 754, Current Learning Rate: 0.0001865, \u001b[91mTrain Loss: 6.544\n",
      "\u001b[0m\u001b[1mEpoch: [9/70], Progress: 10.75 %, Steps: 755, Current Learning Rate: 0.0001868, \u001b[96mTrain Loss: 6.473\n",
      "\u001b[0m\u001b[1mEpoch: [9/70], Progress: 11.83 %, Steps: 756, Current Learning Rate: 0.0001870, \u001b[96mTrain Loss: 6.414\n",
      "\u001b[0m\u001b[1mEpoch: [9/70], Progress: 12.90 %, Steps: 757, Current Learning Rate: 0.0001873, \u001b[91mTrain Loss: 6.517\n",
      "\u001b[0m\u001b[1mEpoch: [9/70], Progress: 13.98 %, Steps: 758, Current Learning Rate: 0.0001875, \u001b[91mTrain Loss: 6.522\n",
      "\u001b[0m\u001b[1mEpoch: [9/70], Progress: 15.05 %, Steps: 759, Current Learning Rate: 0.0001878, \u001b[96mTrain Loss: 6.514\n",
      "\u001b[0m\u001b[1mEpoch: [9/70], Progress: 16.13 %, Steps: 760, Current Learning Rate: 0.0001880, \u001b[96mTrain Loss: 6.333\n",
      "\u001b[0m\u001b[1mEpoch: [9/70], Progress: 17.20 %, Steps: 761, Current Learning Rate: 0.0001883, \u001b[91mTrain Loss: 6.441\n",
      "\u001b[0m\u001b[1mEpoch: [9/70], Progress: 18.28 %, Steps: 762, Current Learning Rate: 0.0001885, \u001b[91mTrain Loss: 6.555\n",
      "\u001b[0m\u001b[1mEpoch: [9/70], Progress: 19.35 %, Steps: 763, Current Learning Rate: 0.0001887, \u001b[91mTrain Loss: 6.606\n",
      "\u001b[0m\u001b[1mEpoch: [9/70], Progress: 20.43 %, Steps: 764, Current Learning Rate: 0.0001890, \u001b[91mTrain Loss: 6.631\n",
      "\u001b[0m\u001b[1mEpoch: [9/70], Progress: 21.51 %, Steps: 765, Current Learning Rate: 0.0001892, \u001b[96mTrain Loss: 6.151\n",
      "\u001b[0m\u001b[1mEpoch: [9/70], Progress: 22.58 %, Steps: 766, Current Learning Rate: 0.0001895, \u001b[91mTrain Loss: 6.525\n",
      "\u001b[0m\u001b[1mEpoch: [9/70], Progress: 23.66 %, Steps: 767, Current Learning Rate: 0.0001897, \u001b[91mTrain Loss: 6.536\n",
      "\u001b[0m\u001b[1mEpoch: [9/70], Progress: 24.73 %, Steps: 768, Current Learning Rate: 0.0001900, \u001b[96mTrain Loss: 6.337\n",
      "\u001b[0m\u001b[1mEpoch: [9/70], Progress: 25.81 %, Steps: 769, Current Learning Rate: 0.0001902, \u001b[91mTrain Loss: 6.532\n",
      "\u001b[0m\u001b[1mEpoch: [9/70], Progress: 26.88 %, Steps: 770, Current Learning Rate: 0.0001905, \u001b[91mTrain Loss: 6.546\n",
      "\u001b[0m\u001b[1mEpoch: [9/70], Progress: 27.96 %, Steps: 771, Current Learning Rate: 0.0001907, \u001b[96mTrain Loss: 6.460\n",
      "\u001b[0m\u001b[1mEpoch: [9/70], Progress: 29.03 %, Steps: 772, Current Learning Rate: 0.0001910, \u001b[96mTrain Loss: 6.376\n",
      "\u001b[0m\u001b[1mEpoch: [9/70], Progress: 30.11 %, Steps: 773, Current Learning Rate: 0.0001912, \u001b[91mTrain Loss: 6.647\n",
      "\u001b[0m\u001b[1mEpoch: [9/70], Progress: 31.18 %, Steps: 774, Current Learning Rate: 0.0001915, \u001b[96mTrain Loss: 6.197\n",
      "\u001b[0m\u001b[1mEpoch: [9/70], Progress: 32.26 %, Steps: 775, Current Learning Rate: 0.0001917, \u001b[91mTrain Loss: 6.473\n",
      "\u001b[0m\u001b[1mEpoch: [9/70], Progress: 33.33 %, Steps: 776, Current Learning Rate: 0.0001920, \u001b[91mTrain Loss: 6.528\n",
      "\u001b[0m\u001b[1mEpoch: [9/70], Progress: 34.41 %, Steps: 777, Current Learning Rate: 0.0001922, \u001b[91mTrain Loss: 6.664\n",
      "\u001b[0m\u001b[1mEpoch: [9/70], Progress: 35.48 %, Steps: 778, Current Learning Rate: 0.0001925, \u001b[96mTrain Loss: 6.468\n",
      "\u001b[0m\u001b[1mEpoch: [9/70], Progress: 36.56 %, Steps: 779, Current Learning Rate: 0.0001927, \u001b[91mTrain Loss: 6.689\n",
      "\u001b[0m\u001b[1mEpoch: [9/70], Progress: 37.63 %, Steps: 780, Current Learning Rate: 0.0001929, \u001b[96mTrain Loss: 6.498\n",
      "\u001b[0m\u001b[1mEpoch: [9/70], Progress: 38.71 %, Steps: 781, Current Learning Rate: 0.0001932, \u001b[96mTrain Loss: 6.378\n",
      "\u001b[0m\u001b[1mEpoch: [9/70], Progress: 39.78 %, Steps: 782, Current Learning Rate: 0.0001934, \u001b[91mTrain Loss: 6.386\n",
      "\u001b[0m\u001b[1mEpoch: [9/70], Progress: 40.86 %, Steps: 783, Current Learning Rate: 0.0001937, \u001b[91mTrain Loss: 6.453\n",
      "\u001b[0m\u001b[1mEpoch: [9/70], Progress: 41.94 %, Steps: 784, Current Learning Rate: 0.0001939, \u001b[91mTrain Loss: 6.525\n",
      "\u001b[0m\u001b[1mEpoch: [9/70], Progress: 43.01 %, Steps: 785, Current Learning Rate: 0.0001942, \u001b[96mTrain Loss: 6.349\n",
      "\u001b[0m\u001b[1mEpoch: [9/70], Progress: 44.09 %, Steps: 786, Current Learning Rate: 0.0001944, \u001b[96mTrain Loss: 6.337\n",
      "\u001b[0m\u001b[1mEpoch: [9/70], Progress: 45.16 %, Steps: 787, Current Learning Rate: 0.0001947, \u001b[91mTrain Loss: 6.548\n",
      "\u001b[0m\u001b[1mEpoch: [9/70], Progress: 46.24 %, Steps: 788, Current Learning Rate: 0.0001949, \u001b[96mTrain Loss: 6.371\n",
      "\u001b[0m\u001b[1mEpoch: [9/70], Progress: 47.31 %, Steps: 789, Current Learning Rate: 0.0001952, \u001b[96mTrain Loss: 6.308\n",
      "\u001b[0m\u001b[1mEpoch: [9/70], Progress: 48.39 %, Steps: 790, Current Learning Rate: 0.0001954, \u001b[96mTrain Loss: 6.299\n",
      "\u001b[0m\u001b[1mEpoch: [9/70], Progress: 49.46 %, Steps: 791, Current Learning Rate: 0.0001957, \u001b[91mTrain Loss: 6.386\n",
      "\u001b[0m\u001b[1mEpoch: [9/70], Progress: 50.54 %, Steps: 792, Current Learning Rate: 0.0001959, \u001b[91mTrain Loss: 6.585\n",
      "\u001b[0m\u001b[1mEpoch: [9/70], Progress: 51.61 %, Steps: 793, Current Learning Rate: 0.0001962, \u001b[96mTrain Loss: 6.441\n",
      "\u001b[0m\u001b[1mEpoch: [9/70], Progress: 52.69 %, Steps: 794, Current Learning Rate: 0.0001964, \u001b[91mTrain Loss: 6.704\n",
      "\u001b[0m\u001b[1mEpoch: [9/70], Progress: 53.76 %, Steps: 795, Current Learning Rate: 0.0001967, \u001b[96mTrain Loss: 6.639\n",
      "\u001b[0m\u001b[1mEpoch: [9/70], Progress: 54.84 %, Steps: 796, Current Learning Rate: 0.0001969, \u001b[96mTrain Loss: 6.588\n",
      "\u001b[0m\u001b[1mEpoch: [9/70], Progress: 55.91 %, Steps: 797, Current Learning Rate: 0.0001971, \u001b[96mTrain Loss: 6.449\n",
      "\u001b[0m\u001b[1mEpoch: [9/70], Progress: 56.99 %, Steps: 798, Current Learning Rate: 0.0001974, \u001b[96mTrain Loss: 6.325\n",
      "\u001b[0m\u001b[1mEpoch: [9/70], Progress: 58.06 %, Steps: 799, Current Learning Rate: 0.0001976, \u001b[91mTrain Loss: 6.492\n",
      "\u001b[0m\u001b[1mEpoch: [9/70], Progress: 59.14 %, Steps: 800, Current Learning Rate: 0.0001979, \u001b[96mTrain Loss: 6.447\n",
      "\u001b[0m\u001b[1mEpoch: [9/70], Progress: 60.22 %, Steps: 801, Current Learning Rate: 0.0001981, \u001b[91mTrain Loss: 6.490\n",
      "\u001b[0m\u001b[1mEpoch: [9/70], Progress: 61.29 %, Steps: 802, Current Learning Rate: 0.0001984, \u001b[91mTrain Loss: 6.545\n",
      "\u001b[0m\u001b[1mEpoch: [9/70], Progress: 62.37 %, Steps: 803, Current Learning Rate: 0.0001986, \u001b[96mTrain Loss: 6.501\n",
      "\u001b[0m\u001b[1mEpoch: [9/70], Progress: 63.44 %, Steps: 804, Current Learning Rate: 0.0001989, \u001b[96mTrain Loss: 6.413\n",
      "\u001b[0m\u001b[1mEpoch: [9/70], Progress: 64.52 %, Steps: 805, Current Learning Rate: 0.0001991, \u001b[91mTrain Loss: 6.530\n",
      "\u001b[0m\u001b[1mEpoch: [9/70], Progress: 65.59 %, Steps: 806, Current Learning Rate: 0.0001994, \u001b[96mTrain Loss: 6.306\n",
      "\u001b[0m\u001b[1mEpoch: [9/70], Progress: 66.67 %, Steps: 807, Current Learning Rate: 0.0001996, \u001b[91mTrain Loss: 6.536\n",
      "\u001b[0m\u001b[1mEpoch: [9/70], Progress: 67.74 %, Steps: 808, Current Learning Rate: 0.0001999, \u001b[91mTrain Loss: 6.605\n",
      "\u001b[0m\u001b[1mEpoch: [9/70], Progress: 68.82 %, Steps: 809, Current Learning Rate: 0.0002001, \u001b[96mTrain Loss: 6.246\n",
      "\u001b[0m\u001b[1mEpoch: [9/70], Progress: 69.89 %, Steps: 810, Current Learning Rate: 0.0002004, \u001b[91mTrain Loss: 6.391\n",
      "\u001b[0m\u001b[1mEpoch: [9/70], Progress: 70.97 %, Steps: 811, Current Learning Rate: 0.0002006, \u001b[91mTrain Loss: 6.537\n",
      "\u001b[0m\u001b[1mEpoch: [9/70], Progress: 72.04 %, Steps: 812, Current Learning Rate: 0.0002009, \u001b[96mTrain Loss: 6.481\n",
      "\u001b[0m\u001b[1mEpoch: [9/70], Progress: 73.12 %, Steps: 813, Current Learning Rate: 0.0002011, \u001b[91mTrain Loss: 6.577\n",
      "\u001b[0m\u001b[1mEpoch: [9/70], Progress: 74.19 %, Steps: 814, Current Learning Rate: 0.0002013, \u001b[96mTrain Loss: 6.493\n",
      "\u001b[0m\u001b[1mEpoch: [9/70], Progress: 75.27 %, Steps: 815, Current Learning Rate: 0.0002016, \u001b[96mTrain Loss: 6.405\n",
      "\u001b[0m\u001b[1mEpoch: [9/70], Progress: 76.34 %, Steps: 816, Current Learning Rate: 0.0002018, \u001b[91mTrain Loss: 6.463\n",
      "\u001b[0m\u001b[1mEpoch: [9/70], Progress: 77.42 %, Steps: 817, Current Learning Rate: 0.0002021, \u001b[91mTrain Loss: 6.593\n",
      "\u001b[0m\u001b[1mEpoch: [9/70], Progress: 78.49 %, Steps: 818, Current Learning Rate: 0.0002023, \u001b[96mTrain Loss: 6.568\n",
      "\u001b[0m\u001b[1mEpoch: [9/70], Progress: 79.57 %, Steps: 819, Current Learning Rate: 0.0002026, \u001b[96mTrain Loss: 6.270\n",
      "\u001b[0m\u001b[1mEpoch: [9/70], Progress: 80.65 %, Steps: 820, Current Learning Rate: 0.0002028, \u001b[91mTrain Loss: 6.531\n",
      "\u001b[0m\u001b[1mEpoch: [9/70], Progress: 81.72 %, Steps: 821, Current Learning Rate: 0.0002031, \u001b[91mTrain Loss: 6.657\n",
      "\u001b[0m\u001b[1mEpoch: [9/70], Progress: 82.80 %, Steps: 822, Current Learning Rate: 0.0002033, \u001b[96mTrain Loss: 6.502\n",
      "\u001b[0m\u001b[1mEpoch: [9/70], Progress: 83.87 %, Steps: 823, Current Learning Rate: 0.0002036, \u001b[96mTrain Loss: 6.409\n",
      "\u001b[0m\u001b[1mEpoch: [9/70], Progress: 84.95 %, Steps: 824, Current Learning Rate: 0.0002038, \u001b[91mTrain Loss: 6.475\n",
      "\u001b[0m\u001b[1mEpoch: [9/70], Progress: 86.02 %, Steps: 825, Current Learning Rate: 0.0002041, \u001b[96mTrain Loss: 6.307\n",
      "\u001b[0m\u001b[1mEpoch: [9/70], Progress: 87.10 %, Steps: 826, Current Learning Rate: 0.0002043, \u001b[91mTrain Loss: 6.479\n",
      "\u001b[0m\u001b[1mEpoch: [9/70], Progress: 88.17 %, Steps: 827, Current Learning Rate: 0.0002046, \u001b[96mTrain Loss: 6.349\n",
      "\u001b[0m\u001b[1mEpoch: [9/70], Progress: 89.25 %, Steps: 828, Current Learning Rate: 0.0002048, \u001b[96mTrain Loss: 6.279\n",
      "\u001b[0m\u001b[1mEpoch: [9/70], Progress: 90.32 %, Steps: 829, Current Learning Rate: 0.0002051, \u001b[91mTrain Loss: 6.290\n",
      "\u001b[0m\u001b[1mEpoch: [9/70], Progress: 91.40 %, Steps: 830, Current Learning Rate: 0.0002053, \u001b[91mTrain Loss: 6.310\n",
      "\u001b[0m\u001b[1mEpoch: [9/70], Progress: 92.47 %, Steps: 831, Current Learning Rate: 0.0002055, \u001b[91mTrain Loss: 6.566\n",
      "\u001b[0m\u001b[1mEpoch: [9/70], Progress: 93.55 %, Steps: 832, Current Learning Rate: 0.0002058, \u001b[96mTrain Loss: 6.366\n",
      "\u001b[0m\u001b[1mEpoch: [9/70], Progress: 94.62 %, Steps: 833, Current Learning Rate: 0.0002060, \u001b[96mTrain Loss: 6.169\n",
      "\u001b[0m\u001b[1mEpoch: [9/70], Progress: 95.70 %, Steps: 834, Current Learning Rate: 0.0002063, \u001b[91mTrain Loss: 6.538\n",
      "\u001b[0m\u001b[1mEpoch: [9/70], Progress: 96.77 %, Steps: 835, Current Learning Rate: 0.0002065, \u001b[96mTrain Loss: 6.469\n",
      "\u001b[0m\u001b[1mEpoch: [9/70], Progress: 97.85 %, Steps: 836, Current Learning Rate: 0.0002068, \u001b[91mTrain Loss: 6.595\n",
      "\u001b[0m\u001b[1mEpoch: [9/70], Progress: 98.92 %, Steps: 837, Current Learning Rate: 0.0002070, \u001b[91mTrain Loss: 6.906\n",
      "\u001b[1m\u001b[93mComputing Validation Loss...\n",
      "\u001b[1mEpoch 9 Completed! Average Train Loss: 6.470, Average Validation Loss: 5.689\n",
      "Best Model saved in best_model/best_model.pt.\n",
      "\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m= Epoch [10/70] ==========================================\n",
      "\u001b[0m\u001b[1mEpoch: [10/70], Progress: 0.00 %, Steps: 838, Current Learning Rate: 0.0002073, \u001b[91mTrain Loss: 6.216\n",
      "\u001b[0m\u001b[1mEpoch: [10/70], Progress: 1.08 %, Steps: 839, Current Learning Rate: 0.0002075, \u001b[91mTrain Loss: 6.466\n",
      "\u001b[0m\u001b[1mEpoch: [10/70], Progress: 2.15 %, Steps: 840, Current Learning Rate: 0.0002078, \u001b[96mTrain Loss: 6.297\n",
      "\u001b[0m\u001b[1mEpoch: [10/70], Progress: 3.23 %, Steps: 841, Current Learning Rate: 0.0002080, \u001b[91mTrain Loss: 6.450\n",
      "\u001b[0m\u001b[1mEpoch: [10/70], Progress: 4.30 %, Steps: 842, Current Learning Rate: 0.0002083, \u001b[96mTrain Loss: 6.341\n",
      "\u001b[0m\u001b[1mEpoch: [10/70], Progress: 5.38 %, Steps: 843, Current Learning Rate: 0.0002085, \u001b[91mTrain Loss: 6.458\n",
      "\u001b[0m\u001b[1mEpoch: [10/70], Progress: 6.45 %, Steps: 844, Current Learning Rate: 0.0002088, \u001b[96mTrain Loss: 6.420\n",
      "\u001b[0m\u001b[1mEpoch: [10/70], Progress: 7.53 %, Steps: 845, Current Learning Rate: 0.0002090, \u001b[96mTrain Loss: 6.136\n",
      "\u001b[0m\u001b[1mEpoch: [10/70], Progress: 8.60 %, Steps: 846, Current Learning Rate: 0.0002093, \u001b[91mTrain Loss: 6.341\n",
      "\u001b[0m\u001b[1mEpoch: [10/70], Progress: 9.68 %, Steps: 847, Current Learning Rate: 0.0002095, \u001b[96mTrain Loss: 6.323\n",
      "\u001b[0m\u001b[1mEpoch: [10/70], Progress: 10.75 %, Steps: 848, Current Learning Rate: 0.0002097, \u001b[91mTrain Loss: 6.382\n",
      "\u001b[0m\u001b[1mEpoch: [10/70], Progress: 11.83 %, Steps: 849, Current Learning Rate: 0.0002100, \u001b[91mTrain Loss: 6.398\n",
      "\u001b[0m\u001b[1mEpoch: [10/70], Progress: 12.90 %, Steps: 850, Current Learning Rate: 0.0002102, \u001b[91mTrain Loss: 6.545\n",
      "\u001b[0m\u001b[1mEpoch: [10/70], Progress: 13.98 %, Steps: 851, Current Learning Rate: 0.0002105, \u001b[96mTrain Loss: 6.399\n",
      "\u001b[0m\u001b[1mEpoch: [10/70], Progress: 15.05 %, Steps: 852, Current Learning Rate: 0.0002107, \u001b[96mTrain Loss: 6.254\n",
      "\u001b[0m\u001b[1mEpoch: [10/70], Progress: 16.13 %, Steps: 853, Current Learning Rate: 0.0002110, \u001b[91mTrain Loss: 6.267\n",
      "\u001b[0m\u001b[1mEpoch: [10/70], Progress: 17.20 %, Steps: 854, Current Learning Rate: 0.0002112, \u001b[91mTrain Loss: 6.292\n",
      "\u001b[0m\u001b[1mEpoch: [10/70], Progress: 18.28 %, Steps: 855, Current Learning Rate: 0.0002115, \u001b[91mTrain Loss: 6.437\n",
      "\u001b[0m\u001b[1mEpoch: [10/70], Progress: 19.35 %, Steps: 856, Current Learning Rate: 0.0002117, \u001b[91mTrain Loss: 6.553\n",
      "\u001b[0m\u001b[1mEpoch: [10/70], Progress: 20.43 %, Steps: 857, Current Learning Rate: 0.0002120, \u001b[96mTrain Loss: 6.237\n",
      "\u001b[0m\u001b[1mEpoch: [10/70], Progress: 21.51 %, Steps: 858, Current Learning Rate: 0.0002122, \u001b[91mTrain Loss: 6.326\n",
      "\u001b[0m\u001b[1mEpoch: [10/70], Progress: 22.58 %, Steps: 859, Current Learning Rate: 0.0002125, \u001b[91mTrain Loss: 6.586\n",
      "\u001b[0m\u001b[1mEpoch: [10/70], Progress: 23.66 %, Steps: 860, Current Learning Rate: 0.0002127, \u001b[96mTrain Loss: 6.253\n",
      "\u001b[0m\u001b[1mEpoch: [10/70], Progress: 24.73 %, Steps: 861, Current Learning Rate: 0.0002130, \u001b[96mTrain Loss: 5.987\n",
      "\u001b[0m\u001b[1mEpoch: [10/70], Progress: 25.81 %, Steps: 862, Current Learning Rate: 0.0002132, \u001b[91mTrain Loss: 6.324\n",
      "\u001b[0m\u001b[1mEpoch: [10/70], Progress: 26.88 %, Steps: 863, Current Learning Rate: 0.0002135, \u001b[96mTrain Loss: 6.120\n",
      "\u001b[0m\u001b[1mEpoch: [10/70], Progress: 27.96 %, Steps: 864, Current Learning Rate: 0.0002137, \u001b[91mTrain Loss: 6.264\n",
      "\u001b[0m\u001b[1mEpoch: [10/70], Progress: 29.03 %, Steps: 865, Current Learning Rate: 0.0002139, \u001b[91mTrain Loss: 6.393\n",
      "\u001b[0m\u001b[1mEpoch: [10/70], Progress: 30.11 %, Steps: 866, Current Learning Rate: 0.0002142, \u001b[91mTrain Loss: 6.491\n",
      "\u001b[0m\u001b[1mEpoch: [10/70], Progress: 31.18 %, Steps: 867, Current Learning Rate: 0.0002144, \u001b[96mTrain Loss: 6.118\n",
      "\u001b[0m\u001b[1mEpoch: [10/70], Progress: 32.26 %, Steps: 868, Current Learning Rate: 0.0002147, \u001b[91mTrain Loss: 6.316\n",
      "\u001b[0m\u001b[1mEpoch: [10/70], Progress: 33.33 %, Steps: 869, Current Learning Rate: 0.0002149, \u001b[91mTrain Loss: 6.323\n",
      "\u001b[0m\u001b[1mEpoch: [10/70], Progress: 34.41 %, Steps: 870, Current Learning Rate: 0.0002152, \u001b[96mTrain Loss: 6.034\n",
      "\u001b[0m\u001b[1mEpoch: [10/70], Progress: 35.48 %, Steps: 871, Current Learning Rate: 0.0002154, \u001b[91mTrain Loss: 6.188\n",
      "\u001b[0m\u001b[1mEpoch: [10/70], Progress: 36.56 %, Steps: 872, Current Learning Rate: 0.0002157, \u001b[91mTrain Loss: 6.412\n",
      "\u001b[0m\u001b[1mEpoch: [10/70], Progress: 37.63 %, Steps: 873, Current Learning Rate: 0.0002159, \u001b[96mTrain Loss: 6.242\n",
      "\u001b[0m\u001b[1mEpoch: [10/70], Progress: 38.71 %, Steps: 874, Current Learning Rate: 0.0002162, \u001b[91mTrain Loss: 6.531\n",
      "\u001b[0m\u001b[1mEpoch: [10/70], Progress: 39.78 %, Steps: 875, Current Learning Rate: 0.0002164, \u001b[96mTrain Loss: 6.385\n",
      "\u001b[0m\u001b[1mEpoch: [10/70], Progress: 40.86 %, Steps: 876, Current Learning Rate: 0.0002167, \u001b[96mTrain Loss: 6.212\n",
      "\u001b[0m\u001b[1mEpoch: [10/70], Progress: 41.94 %, Steps: 877, Current Learning Rate: 0.0002169, \u001b[91mTrain Loss: 6.364\n",
      "\u001b[0m\u001b[1mEpoch: [10/70], Progress: 43.01 %, Steps: 878, Current Learning Rate: 0.0002172, \u001b[96mTrain Loss: 6.190\n",
      "\u001b[0m\u001b[1mEpoch: [10/70], Progress: 44.09 %, Steps: 879, Current Learning Rate: 0.0002174, \u001b[91mTrain Loss: 6.266\n",
      "\u001b[0m\u001b[1mEpoch: [10/70], Progress: 45.16 %, Steps: 880, Current Learning Rate: 0.0002177, \u001b[91mTrain Loss: 6.357\n",
      "\u001b[0m\u001b[1mEpoch: [10/70], Progress: 46.24 %, Steps: 881, Current Learning Rate: 0.0002179, \u001b[96mTrain Loss: 6.245\n",
      "\u001b[0m\u001b[1mEpoch: [10/70], Progress: 47.31 %, Steps: 882, Current Learning Rate: 0.0002181, \u001b[91mTrain Loss: 6.367\n",
      "\u001b[0m\u001b[1mEpoch: [10/70], Progress: 48.39 %, Steps: 883, Current Learning Rate: 0.0002184, \u001b[96mTrain Loss: 6.275\n",
      "\u001b[0m\u001b[1mEpoch: [10/70], Progress: 49.46 %, Steps: 884, Current Learning Rate: 0.0002186, \u001b[91mTrain Loss: 6.370\n",
      "\u001b[0m\u001b[1mEpoch: [10/70], Progress: 50.54 %, Steps: 885, Current Learning Rate: 0.0002189, \u001b[96mTrain Loss: 6.178\n",
      "\u001b[0m\u001b[1mEpoch: [10/70], Progress: 51.61 %, Steps: 886, Current Learning Rate: 0.0002191, \u001b[91mTrain Loss: 6.480\n",
      "\u001b[0m\u001b[1mEpoch: [10/70], Progress: 52.69 %, Steps: 887, Current Learning Rate: 0.0002194, \u001b[96mTrain Loss: 6.198\n",
      "\u001b[0m\u001b[1mEpoch: [10/70], Progress: 53.76 %, Steps: 888, Current Learning Rate: 0.0002196, \u001b[91mTrain Loss: 6.313\n",
      "\u001b[0m\u001b[1mEpoch: [10/70], Progress: 54.84 %, Steps: 889, Current Learning Rate: 0.0002199, \u001b[96mTrain Loss: 6.055\n",
      "\u001b[0m\u001b[1mEpoch: [10/70], Progress: 55.91 %, Steps: 890, Current Learning Rate: 0.0002201, \u001b[91mTrain Loss: 6.089\n",
      "\u001b[0m\u001b[1mEpoch: [10/70], Progress: 56.99 %, Steps: 891, Current Learning Rate: 0.0002204, \u001b[91mTrain Loss: 6.221\n",
      "\u001b[0m\u001b[1mEpoch: [10/70], Progress: 58.06 %, Steps: 892, Current Learning Rate: 0.0002206, \u001b[91mTrain Loss: 6.336\n",
      "\u001b[0m\u001b[1mEpoch: [10/70], Progress: 59.14 %, Steps: 893, Current Learning Rate: 0.0002209, \u001b[96mTrain Loss: 6.229\n",
      "\u001b[0m\u001b[1mEpoch: [10/70], Progress: 60.22 %, Steps: 894, Current Learning Rate: 0.0002211, \u001b[91mTrain Loss: 6.270\n",
      "\u001b[0m\u001b[1mEpoch: [10/70], Progress: 61.29 %, Steps: 895, Current Learning Rate: 0.0002214, \u001b[96mTrain Loss: 6.242\n",
      "\u001b[0m\u001b[1mEpoch: [10/70], Progress: 62.37 %, Steps: 896, Current Learning Rate: 0.0002216, \u001b[96mTrain Loss: 6.129\n",
      "\u001b[0m\u001b[1mEpoch: [10/70], Progress: 63.44 %, Steps: 897, Current Learning Rate: 0.0002219, \u001b[91mTrain Loss: 6.456\n",
      "\u001b[0m\u001b[1mEpoch: [10/70], Progress: 64.52 %, Steps: 898, Current Learning Rate: 0.0002221, \u001b[96mTrain Loss: 6.397\n",
      "\u001b[0m\u001b[1mEpoch: [10/70], Progress: 65.59 %, Steps: 899, Current Learning Rate: 0.0002223, \u001b[96mTrain Loss: 6.255\n",
      "\u001b[0m\u001b[1mEpoch: [10/70], Progress: 66.67 %, Steps: 900, Current Learning Rate: 0.0002226, \u001b[96mTrain Loss: 6.230\n",
      "\u001b[0m\u001b[1mEpoch: [10/70], Progress: 67.74 %, Steps: 901, Current Learning Rate: 0.0002228, \u001b[91mTrain Loss: 6.276\n",
      "\u001b[0m\u001b[1mEpoch: [10/70], Progress: 68.82 %, Steps: 902, Current Learning Rate: 0.0002231, \u001b[91mTrain Loss: 6.464\n",
      "\u001b[0m\u001b[1mEpoch: [10/70], Progress: 69.89 %, Steps: 903, Current Learning Rate: 0.0002233, \u001b[96mTrain Loss: 6.247\n",
      "\u001b[0m\u001b[1mEpoch: [10/70], Progress: 70.97 %, Steps: 904, Current Learning Rate: 0.0002236, \u001b[96mTrain Loss: 6.143\n",
      "\u001b[0m\u001b[1mEpoch: [10/70], Progress: 72.04 %, Steps: 905, Current Learning Rate: 0.0002238, \u001b[91mTrain Loss: 6.393\n",
      "\u001b[0m\u001b[1mEpoch: [10/70], Progress: 73.12 %, Steps: 906, Current Learning Rate: 0.0002241, \u001b[91mTrain Loss: 6.412\n",
      "\u001b[0m\u001b[1mEpoch: [10/70], Progress: 74.19 %, Steps: 907, Current Learning Rate: 0.0002243, \u001b[96mTrain Loss: 6.148\n",
      "\u001b[0m\u001b[1mEpoch: [10/70], Progress: 75.27 %, Steps: 908, Current Learning Rate: 0.0002246, \u001b[91mTrain Loss: 6.282\n",
      "\u001b[0m\u001b[1mEpoch: [10/70], Progress: 76.34 %, Steps: 909, Current Learning Rate: 0.0002248, \u001b[96mTrain Loss: 6.227\n",
      "\u001b[0m\u001b[1mEpoch: [10/70], Progress: 77.42 %, Steps: 910, Current Learning Rate: 0.0002251, \u001b[96mTrain Loss: 6.217\n",
      "\u001b[0m\u001b[1mEpoch: [10/70], Progress: 78.49 %, Steps: 911, Current Learning Rate: 0.0002253, \u001b[91mTrain Loss: 6.262\n",
      "\u001b[0m\u001b[1mEpoch: [10/70], Progress: 79.57 %, Steps: 912, Current Learning Rate: 0.0002256, \u001b[96mTrain Loss: 6.060\n",
      "\u001b[0m\u001b[1mEpoch: [10/70], Progress: 80.65 %, Steps: 913, Current Learning Rate: 0.0002258, \u001b[91mTrain Loss: 6.281\n",
      "\u001b[0m\u001b[1mEpoch: [10/70], Progress: 81.72 %, Steps: 914, Current Learning Rate: 0.0002261, \u001b[96mTrain Loss: 6.257\n",
      "\u001b[0m\u001b[1mEpoch: [10/70], Progress: 82.80 %, Steps: 915, Current Learning Rate: 0.0002263, \u001b[91mTrain Loss: 6.281\n",
      "\u001b[0m\u001b[1mEpoch: [10/70], Progress: 83.87 %, Steps: 916, Current Learning Rate: 0.0002265, \u001b[91mTrain Loss: 6.303\n",
      "\u001b[0m\u001b[1mEpoch: [10/70], Progress: 84.95 %, Steps: 917, Current Learning Rate: 0.0002268, \u001b[96mTrain Loss: 6.141\n",
      "\u001b[0m\u001b[1mEpoch: [10/70], Progress: 86.02 %, Steps: 918, Current Learning Rate: 0.0002270, \u001b[96mTrain Loss: 6.076\n",
      "\u001b[0m\u001b[1mEpoch: [10/70], Progress: 87.10 %, Steps: 919, Current Learning Rate: 0.0002273, \u001b[91mTrain Loss: 6.308\n",
      "\u001b[0m\u001b[1mEpoch: [10/70], Progress: 88.17 %, Steps: 920, Current Learning Rate: 0.0002275, \u001b[96mTrain Loss: 6.066\n",
      "\u001b[0m\u001b[1mEpoch: [10/70], Progress: 89.25 %, Steps: 921, Current Learning Rate: 0.0002278, \u001b[91mTrain Loss: 6.329\n",
      "\u001b[0m\u001b[1mEpoch: [10/70], Progress: 90.32 %, Steps: 922, Current Learning Rate: 0.0002280, \u001b[96mTrain Loss: 6.201\n",
      "\u001b[0m\u001b[1mEpoch: [10/70], Progress: 91.40 %, Steps: 923, Current Learning Rate: 0.0002283, \u001b[96mTrain Loss: 6.175\n",
      "\u001b[0m\u001b[1mEpoch: [10/70], Progress: 92.47 %, Steps: 924, Current Learning Rate: 0.0002285, \u001b[91mTrain Loss: 6.377\n",
      "\u001b[0m\u001b[1mEpoch: [10/70], Progress: 93.55 %, Steps: 925, Current Learning Rate: 0.0002288, \u001b[96mTrain Loss: 6.148\n",
      "\u001b[0m\u001b[1mEpoch: [10/70], Progress: 94.62 %, Steps: 926, Current Learning Rate: 0.0002290, \u001b[91mTrain Loss: 6.227\n",
      "\u001b[0m\u001b[1mEpoch: [10/70], Progress: 95.70 %, Steps: 927, Current Learning Rate: 0.0002293, \u001b[96mTrain Loss: 6.103\n",
      "\u001b[0m\u001b[1mEpoch: [10/70], Progress: 96.77 %, Steps: 928, Current Learning Rate: 0.0002295, \u001b[91mTrain Loss: 6.380\n",
      "\u001b[0m\u001b[1mEpoch: [10/70], Progress: 97.85 %, Steps: 929, Current Learning Rate: 0.0002298, \u001b[96mTrain Loss: 6.147\n",
      "\u001b[0m\u001b[1mEpoch: [10/70], Progress: 98.92 %, Steps: 930, Current Learning Rate: 0.0002300, \u001b[91mTrain Loss: 6.321\n",
      "\u001b[1m\u001b[93mComputing Validation Loss...\n",
      "\u001b[1mEpoch 10 Completed! Average Train Loss: 6.285, Average Validation Loss: 5.542\n",
      "Best Model saved in best_model/best_model.pt.\n",
      "\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m= Epoch [11/70] ==========================================\n",
      "\u001b[0m\u001b[1mEpoch: [11/70], Progress: 0.00 %, Steps: 931, Current Learning Rate: 0.0002303, \u001b[91mTrain Loss: 6.379\n",
      "\u001b[0m\u001b[1mEpoch: [11/70], Progress: 1.08 %, Steps: 932, Current Learning Rate: 0.0002305, \u001b[96mTrain Loss: 5.989\n",
      "\u001b[0m\u001b[1mEpoch: [11/70], Progress: 2.15 %, Steps: 933, Current Learning Rate: 0.0002307, \u001b[91mTrain Loss: 6.384\n",
      "\u001b[0m\u001b[1mEpoch: [11/70], Progress: 3.23 %, Steps: 934, Current Learning Rate: 0.0002310, \u001b[96mTrain Loss: 5.742\n",
      "\u001b[0m\u001b[1mEpoch: [11/70], Progress: 4.30 %, Steps: 935, Current Learning Rate: 0.0002312, \u001b[91mTrain Loss: 5.956\n",
      "\u001b[0m\u001b[1mEpoch: [11/70], Progress: 5.38 %, Steps: 936, Current Learning Rate: 0.0002315, \u001b[91mTrain Loss: 6.388\n",
      "\u001b[0m\u001b[1mEpoch: [11/70], Progress: 6.45 %, Steps: 937, Current Learning Rate: 0.0002317, \u001b[96mTrain Loss: 5.830\n",
      "\u001b[0m\u001b[1mEpoch: [11/70], Progress: 7.53 %, Steps: 938, Current Learning Rate: 0.0002320, \u001b[91mTrain Loss: 6.016\n",
      "\u001b[0m\u001b[1mEpoch: [11/70], Progress: 8.60 %, Steps: 939, Current Learning Rate: 0.0002322, \u001b[91mTrain Loss: 6.160\n",
      "\u001b[0m\u001b[1mEpoch: [11/70], Progress: 9.68 %, Steps: 940, Current Learning Rate: 0.0002325, \u001b[96mTrain Loss: 6.140\n",
      "\u001b[0m\u001b[1mEpoch: [11/70], Progress: 10.75 %, Steps: 941, Current Learning Rate: 0.0002327, \u001b[91mTrain Loss: 6.363\n",
      "\u001b[0m\u001b[1mEpoch: [11/70], Progress: 11.83 %, Steps: 942, Current Learning Rate: 0.0002330, \u001b[96mTrain Loss: 6.183\n",
      "\u001b[0m\u001b[1mEpoch: [11/70], Progress: 12.90 %, Steps: 943, Current Learning Rate: 0.0002332, \u001b[91mTrain Loss: 6.338\n",
      "\u001b[0m\u001b[1mEpoch: [11/70], Progress: 13.98 %, Steps: 944, Current Learning Rate: 0.0002335, \u001b[96mTrain Loss: 6.211\n",
      "\u001b[0m\u001b[1mEpoch: [11/70], Progress: 15.05 %, Steps: 945, Current Learning Rate: 0.0002337, \u001b[96mTrain Loss: 6.039\n",
      "\u001b[0m\u001b[1mEpoch: [11/70], Progress: 16.13 %, Steps: 946, Current Learning Rate: 0.0002340, \u001b[91mTrain Loss: 6.414\n",
      "\u001b[0m\u001b[1mEpoch: [11/70], Progress: 17.20 %, Steps: 947, Current Learning Rate: 0.0002342, \u001b[96mTrain Loss: 5.997\n",
      "\u001b[0m\u001b[1mEpoch: [11/70], Progress: 18.28 %, Steps: 948, Current Learning Rate: 0.0002345, \u001b[91mTrain Loss: 6.149\n",
      "\u001b[0m\u001b[1mEpoch: [11/70], Progress: 19.35 %, Steps: 949, Current Learning Rate: 0.0002347, \u001b[91mTrain Loss: 6.185\n",
      "\u001b[0m\u001b[1mEpoch: [11/70], Progress: 20.43 %, Steps: 950, Current Learning Rate: 0.0002349, \u001b[96mTrain Loss: 6.070\n",
      "\u001b[0m\u001b[1mEpoch: [11/70], Progress: 21.51 %, Steps: 951, Current Learning Rate: 0.0002352, \u001b[96mTrain Loss: 6.048\n",
      "\u001b[0m\u001b[1mEpoch: [11/70], Progress: 22.58 %, Steps: 952, Current Learning Rate: 0.0002354, \u001b[91mTrain Loss: 6.380\n",
      "\u001b[0m\u001b[1mEpoch: [11/70], Progress: 23.66 %, Steps: 953, Current Learning Rate: 0.0002357, \u001b[96mTrain Loss: 6.114\n",
      "\u001b[0m\u001b[1mEpoch: [11/70], Progress: 24.73 %, Steps: 954, Current Learning Rate: 0.0002359, \u001b[91mTrain Loss: 6.161\n",
      "\u001b[0m\u001b[1mEpoch: [11/70], Progress: 25.81 %, Steps: 955, Current Learning Rate: 0.0002362, \u001b[96mTrain Loss: 5.923\n",
      "\u001b[0m\u001b[1mEpoch: [11/70], Progress: 26.88 %, Steps: 956, Current Learning Rate: 0.0002364, \u001b[91mTrain Loss: 6.160\n",
      "\u001b[0m\u001b[1mEpoch: [11/70], Progress: 27.96 %, Steps: 957, Current Learning Rate: 0.0002367, \u001b[96mTrain Loss: 6.140\n",
      "\u001b[0m\u001b[1mEpoch: [11/70], Progress: 29.03 %, Steps: 958, Current Learning Rate: 0.0002369, \u001b[91mTrain Loss: 6.322\n",
      "\u001b[0m\u001b[1mEpoch: [11/70], Progress: 30.11 %, Steps: 959, Current Learning Rate: 0.0002372, \u001b[96mTrain Loss: 5.923\n",
      "\u001b[0m\u001b[1mEpoch: [11/70], Progress: 31.18 %, Steps: 960, Current Learning Rate: 0.0002374, \u001b[91mTrain Loss: 6.054\n",
      "\u001b[0m\u001b[1mEpoch: [11/70], Progress: 32.26 %, Steps: 961, Current Learning Rate: 0.0002377, \u001b[91mTrain Loss: 6.196\n",
      "\u001b[0m\u001b[1mEpoch: [11/70], Progress: 33.33 %, Steps: 962, Current Learning Rate: 0.0002379, \u001b[96mTrain Loss: 6.153\n",
      "\u001b[0m\u001b[1mEpoch: [11/70], Progress: 34.41 %, Steps: 963, Current Learning Rate: 0.0002382, \u001b[96mTrain Loss: 6.041\n",
      "\u001b[0m\u001b[1mEpoch: [11/70], Progress: 35.48 %, Steps: 964, Current Learning Rate: 0.0002384, \u001b[91mTrain Loss: 6.203\n",
      "\u001b[0m\u001b[1mEpoch: [11/70], Progress: 36.56 %, Steps: 965, Current Learning Rate: 0.0002387, \u001b[96mTrain Loss: 6.070\n",
      "\u001b[0m\u001b[1mEpoch: [11/70], Progress: 37.63 %, Steps: 966, Current Learning Rate: 0.0002389, \u001b[91mTrain Loss: 6.257\n",
      "\u001b[0m\u001b[1mEpoch: [11/70], Progress: 38.71 %, Steps: 967, Current Learning Rate: 0.0002391, \u001b[96mTrain Loss: 6.204\n",
      "\u001b[0m\u001b[1mEpoch: [11/70], Progress: 39.78 %, Steps: 968, Current Learning Rate: 0.0002394, \u001b[96mTrain Loss: 6.112\n",
      "\u001b[0m\u001b[1mEpoch: [11/70], Progress: 40.86 %, Steps: 969, Current Learning Rate: 0.0002396, \u001b[91mTrain Loss: 6.155\n",
      "\u001b[0m\u001b[1mEpoch: [11/70], Progress: 41.94 %, Steps: 970, Current Learning Rate: 0.0002399, \u001b[96mTrain Loss: 6.092\n",
      "\u001b[0m\u001b[1mEpoch: [11/70], Progress: 43.01 %, Steps: 971, Current Learning Rate: 0.0002401, \u001b[91mTrain Loss: 6.284\n",
      "\u001b[0m\u001b[1mEpoch: [11/70], Progress: 44.09 %, Steps: 972, Current Learning Rate: 0.0002404, \u001b[96mTrain Loss: 6.066\n",
      "\u001b[0m\u001b[1mEpoch: [11/70], Progress: 45.16 %, Steps: 973, Current Learning Rate: 0.0002406, \u001b[91mTrain Loss: 6.133\n",
      "\u001b[0m\u001b[1mEpoch: [11/70], Progress: 46.24 %, Steps: 974, Current Learning Rate: 0.0002409, \u001b[96mTrain Loss: 5.992\n",
      "\u001b[0m\u001b[1mEpoch: [11/70], Progress: 47.31 %, Steps: 975, Current Learning Rate: 0.0002411, \u001b[96mTrain Loss: 5.940\n",
      "\u001b[0m\u001b[1mEpoch: [11/70], Progress: 48.39 %, Steps: 976, Current Learning Rate: 0.0002414, \u001b[91mTrain Loss: 6.113\n",
      "\u001b[0m\u001b[1mEpoch: [11/70], Progress: 49.46 %, Steps: 977, Current Learning Rate: 0.0002416, \u001b[96mTrain Loss: 6.026\n",
      "\u001b[0m\u001b[1mEpoch: [11/70], Progress: 50.54 %, Steps: 978, Current Learning Rate: 0.0002419, \u001b[91mTrain Loss: 6.204\n",
      "\u001b[0m\u001b[1mEpoch: [11/70], Progress: 51.61 %, Steps: 979, Current Learning Rate: 0.0002421, \u001b[96mTrain Loss: 5.951\n",
      "\u001b[0m\u001b[1mEpoch: [11/70], Progress: 52.69 %, Steps: 980, Current Learning Rate: 0.0002424, \u001b[91mTrain Loss: 6.181\n",
      "\u001b[0m\u001b[1mEpoch: [11/70], Progress: 53.76 %, Steps: 981, Current Learning Rate: 0.0002426, \u001b[96mTrain Loss: 6.049\n",
      "\u001b[0m\u001b[1mEpoch: [11/70], Progress: 54.84 %, Steps: 982, Current Learning Rate: 0.0002429, \u001b[91mTrain Loss: 6.118\n",
      "\u001b[0m\u001b[1mEpoch: [11/70], Progress: 55.91 %, Steps: 983, Current Learning Rate: 0.0002431, \u001b[91mTrain Loss: 6.223\n",
      "\u001b[0m\u001b[1mEpoch: [11/70], Progress: 56.99 %, Steps: 984, Current Learning Rate: 0.0002433, \u001b[96mTrain Loss: 6.212\n",
      "\u001b[0m\u001b[1mEpoch: [11/70], Progress: 58.06 %, Steps: 985, Current Learning Rate: 0.0002436, \u001b[96mTrain Loss: 6.174\n",
      "\u001b[0m\u001b[1mEpoch: [11/70], Progress: 59.14 %, Steps: 986, Current Learning Rate: 0.0002438, \u001b[91mTrain Loss: 6.220\n",
      "\u001b[0m\u001b[1mEpoch: [11/70], Progress: 60.22 %, Steps: 987, Current Learning Rate: 0.0002441, \u001b[96mTrain Loss: 6.105\n",
      "\u001b[0m\u001b[1mEpoch: [11/70], Progress: 61.29 %, Steps: 988, Current Learning Rate: 0.0002443, \u001b[96mTrain Loss: 6.060\n",
      "\u001b[0m\u001b[1mEpoch: [11/70], Progress: 62.37 %, Steps: 989, Current Learning Rate: 0.0002446, \u001b[91mTrain Loss: 6.221\n",
      "\u001b[0m\u001b[1mEpoch: [11/70], Progress: 63.44 %, Steps: 990, Current Learning Rate: 0.0002448, \u001b[96mTrain Loss: 5.952\n",
      "\u001b[0m\u001b[1mEpoch: [11/70], Progress: 64.52 %, Steps: 991, Current Learning Rate: 0.0002451, \u001b[91mTrain Loss: 6.014\n",
      "\u001b[0m\u001b[1mEpoch: [11/70], Progress: 65.59 %, Steps: 992, Current Learning Rate: 0.0002453, \u001b[91mTrain Loss: 6.147\n",
      "\u001b[0m\u001b[1mEpoch: [11/70], Progress: 66.67 %, Steps: 993, Current Learning Rate: 0.0002456, \u001b[96mTrain Loss: 5.869\n",
      "\u001b[0m\u001b[1mEpoch: [11/70], Progress: 67.74 %, Steps: 994, Current Learning Rate: 0.0002458, \u001b[91mTrain Loss: 6.237\n",
      "\u001b[0m\u001b[1mEpoch: [11/70], Progress: 68.82 %, Steps: 995, Current Learning Rate: 0.0002461, \u001b[96mTrain Loss: 6.069\n",
      "\u001b[0m\u001b[1mEpoch: [11/70], Progress: 69.89 %, Steps: 996, Current Learning Rate: 0.0002463, \u001b[91mTrain Loss: 6.229\n",
      "\u001b[0m\u001b[1mEpoch: [11/70], Progress: 70.97 %, Steps: 997, Current Learning Rate: 0.0002466, \u001b[96mTrain Loss: 6.062\n",
      "\u001b[0m\u001b[1mEpoch: [11/70], Progress: 72.04 %, Steps: 998, Current Learning Rate: 0.0002468, \u001b[91mTrain Loss: 6.072\n",
      "\u001b[0m\u001b[1mEpoch: [11/70], Progress: 73.12 %, Steps: 999, Current Learning Rate: 0.0002471, \u001b[91mTrain Loss: 6.107\n",
      "\u001b[0m\u001b[1mEpoch: [11/70], Progress: 74.19 %, Steps: 1000, Current Learning Rate: 0.0002473, \u001b[96mTrain Loss: 6.033\n",
      "\u001b[0m\u001b[1mEpoch: [11/70], Progress: 75.27 %, Steps: 1001, Current Learning Rate: 0.0002475, \u001b[91mTrain Loss: 6.130\n",
      "\u001b[0m\u001b[1mEpoch: [11/70], Progress: 76.34 %, Steps: 1002, Current Learning Rate: 0.0002478, \u001b[96mTrain Loss: 5.886\n",
      "\u001b[0m\u001b[1mEpoch: [11/70], Progress: 77.42 %, Steps: 1003, Current Learning Rate: 0.0002480, \u001b[91mTrain Loss: 5.949\n",
      "\u001b[0m\u001b[1mEpoch: [11/70], Progress: 78.49 %, Steps: 1004, Current Learning Rate: 0.0002483, \u001b[91mTrain Loss: 6.096\n",
      "\u001b[0m\u001b[1mEpoch: [11/70], Progress: 79.57 %, Steps: 1005, Current Learning Rate: 0.0002485, \u001b[91mTrain Loss: 6.269\n",
      "\u001b[0m\u001b[1mEpoch: [11/70], Progress: 80.65 %, Steps: 1006, Current Learning Rate: 0.0002488, \u001b[91mTrain Loss: 6.364\n",
      "\u001b[0m\u001b[1mEpoch: [11/70], Progress: 81.72 %, Steps: 1007, Current Learning Rate: 0.0002490, \u001b[96mTrain Loss: 6.063\n",
      "\u001b[0m\u001b[1mEpoch: [11/70], Progress: 82.80 %, Steps: 1008, Current Learning Rate: 0.0002493, \u001b[91mTrain Loss: 6.247\n",
      "\u001b[0m\u001b[1mEpoch: [11/70], Progress: 83.87 %, Steps: 1009, Current Learning Rate: 0.0002495, \u001b[96mTrain Loss: 6.175\n",
      "\u001b[0m\u001b[1mEpoch: [11/70], Progress: 84.95 %, Steps: 1010, Current Learning Rate: 0.0002498, \u001b[96mTrain Loss: 6.032\n",
      "\u001b[0m\u001b[1mEpoch: [11/70], Progress: 86.02 %, Steps: 1011, Current Learning Rate: 0.0002500, \u001b[96mTrain Loss: 5.900\n",
      "\u001b[0m\u001b[1mEpoch: [11/70], Progress: 87.10 %, Steps: 1012, Current Learning Rate: 0.0002503, \u001b[91mTrain Loss: 6.211\n",
      "\u001b[0m\u001b[1mEpoch: [11/70], Progress: 88.17 %, Steps: 1013, Current Learning Rate: 0.0002505, \u001b[96mTrain Loss: 6.056\n",
      "\u001b[0m\u001b[1mEpoch: [11/70], Progress: 89.25 %, Steps: 1014, Current Learning Rate: 0.0002508, \u001b[91mTrain Loss: 6.084\n",
      "\u001b[0m\u001b[1mEpoch: [11/70], Progress: 90.32 %, Steps: 1015, Current Learning Rate: 0.0002510, \u001b[91mTrain Loss: 6.095\n",
      "\u001b[0m\u001b[1mEpoch: [11/70], Progress: 91.40 %, Steps: 1016, Current Learning Rate: 0.0002513, \u001b[91mTrain Loss: 6.149\n",
      "\u001b[0m\u001b[1mEpoch: [11/70], Progress: 92.47 %, Steps: 1017, Current Learning Rate: 0.0002515, \u001b[91mTrain Loss: 6.326\n",
      "\u001b[0m\u001b[1mEpoch: [11/70], Progress: 93.55 %, Steps: 1018, Current Learning Rate: 0.0002517, \u001b[96mTrain Loss: 6.225\n",
      "\u001b[0m\u001b[1mEpoch: [11/70], Progress: 94.62 %, Steps: 1019, Current Learning Rate: 0.0002520, \u001b[96mTrain Loss: 6.005\n",
      "\u001b[0m\u001b[1mEpoch: [11/70], Progress: 95.70 %, Steps: 1020, Current Learning Rate: 0.0002522, \u001b[91mTrain Loss: 6.078\n",
      "\u001b[0m\u001b[1mEpoch: [11/70], Progress: 96.77 %, Steps: 1021, Current Learning Rate: 0.0002525, \u001b[91mTrain Loss: 6.101\n",
      "\u001b[0m\u001b[1mEpoch: [11/70], Progress: 97.85 %, Steps: 1022, Current Learning Rate: 0.0002527, \u001b[91mTrain Loss: 6.121\n",
      "\u001b[0m\u001b[1mEpoch: [11/70], Progress: 98.92 %, Steps: 1023, Current Learning Rate: 0.0002530, \u001b[96mTrain Loss: 6.079\n",
      "\u001b[1m\u001b[93mComputing Validation Loss...\n",
      "\u001b[1mEpoch 11 Completed! Average Train Loss: 6.122, Average Validation Loss: 5.392\n",
      "Best Model saved in best_model/best_model.pt.\n",
      "\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m= Epoch [12/70] ==========================================\n",
      "\u001b[0m\u001b[1mEpoch: [12/70], Progress: 0.00 %, Steps: 1024, Current Learning Rate: 0.0002532, \u001b[91mTrain Loss: 5.900\n",
      "\u001b[0m\u001b[1mEpoch: [12/70], Progress: 1.08 %, Steps: 1025, Current Learning Rate: 0.0002535, \u001b[91mTrain Loss: 6.142\n",
      "\u001b[0m\u001b[1mEpoch: [12/70], Progress: 2.15 %, Steps: 1026, Current Learning Rate: 0.0002537, \u001b[96mTrain Loss: 5.968\n",
      "\u001b[0m\u001b[1mEpoch: [12/70], Progress: 3.23 %, Steps: 1027, Current Learning Rate: 0.0002540, \u001b[96mTrain Loss: 5.895\n",
      "\u001b[0m\u001b[1mEpoch: [12/70], Progress: 4.30 %, Steps: 1028, Current Learning Rate: 0.0002542, \u001b[91mTrain Loss: 6.163\n",
      "\u001b[0m\u001b[1mEpoch: [12/70], Progress: 5.38 %, Steps: 1029, Current Learning Rate: 0.0002545, \u001b[96mTrain Loss: 5.881\n",
      "\u001b[0m\u001b[1mEpoch: [12/70], Progress: 6.45 %, Steps: 1030, Current Learning Rate: 0.0002547, \u001b[91mTrain Loss: 6.003\n",
      "\u001b[0m\u001b[1mEpoch: [12/70], Progress: 7.53 %, Steps: 1031, Current Learning Rate: 0.0002550, \u001b[96mTrain Loss: 5.985\n",
      "\u001b[0m\u001b[1mEpoch: [12/70], Progress: 8.60 %, Steps: 1032, Current Learning Rate: 0.0002552, \u001b[91mTrain Loss: 6.070\n",
      "\u001b[0m\u001b[1mEpoch: [12/70], Progress: 9.68 %, Steps: 1033, Current Learning Rate: 0.0002555, \u001b[91mTrain Loss: 6.157\n",
      "\u001b[0m\u001b[1mEpoch: [12/70], Progress: 10.75 %, Steps: 1034, Current Learning Rate: 0.0002557, \u001b[96mTrain Loss: 5.954\n",
      "\u001b[0m\u001b[1mEpoch: [12/70], Progress: 11.83 %, Steps: 1035, Current Learning Rate: 0.0002559, \u001b[96mTrain Loss: 5.901\n",
      "\u001b[0m\u001b[1mEpoch: [12/70], Progress: 12.90 %, Steps: 1036, Current Learning Rate: 0.0002562, \u001b[91mTrain Loss: 5.947\n",
      "\u001b[0m\u001b[1mEpoch: [12/70], Progress: 13.98 %, Steps: 1037, Current Learning Rate: 0.0002564, \u001b[96mTrain Loss: 5.837\n",
      "\u001b[0m\u001b[1mEpoch: [12/70], Progress: 15.05 %, Steps: 1038, Current Learning Rate: 0.0002567, \u001b[91mTrain Loss: 6.062\n",
      "\u001b[0m\u001b[1mEpoch: [12/70], Progress: 16.13 %, Steps: 1039, Current Learning Rate: 0.0002569, \u001b[96mTrain Loss: 5.760\n",
      "\u001b[0m\u001b[1mEpoch: [12/70], Progress: 17.20 %, Steps: 1040, Current Learning Rate: 0.0002572, \u001b[91mTrain Loss: 6.144\n",
      "\u001b[0m\u001b[1mEpoch: [12/70], Progress: 18.28 %, Steps: 1041, Current Learning Rate: 0.0002574, \u001b[96mTrain Loss: 5.894\n",
      "\u001b[0m\u001b[1mEpoch: [12/70], Progress: 19.35 %, Steps: 1042, Current Learning Rate: 0.0002577, \u001b[96mTrain Loss: 5.838\n",
      "\u001b[0m\u001b[1mEpoch: [12/70], Progress: 20.43 %, Steps: 1043, Current Learning Rate: 0.0002579, \u001b[91mTrain Loss: 5.933\n",
      "\u001b[0m\u001b[1mEpoch: [12/70], Progress: 21.51 %, Steps: 1044, Current Learning Rate: 0.0002582, \u001b[96mTrain Loss: 5.820\n",
      "\u001b[0m\u001b[1mEpoch: [12/70], Progress: 22.58 %, Steps: 1045, Current Learning Rate: 0.0002584, \u001b[96mTrain Loss: 5.806\n",
      "\u001b[0m\u001b[1mEpoch: [12/70], Progress: 23.66 %, Steps: 1046, Current Learning Rate: 0.0002587, \u001b[91mTrain Loss: 6.084\n",
      "\u001b[0m\u001b[1mEpoch: [12/70], Progress: 24.73 %, Steps: 1047, Current Learning Rate: 0.0002589, \u001b[91mTrain Loss: 6.149\n",
      "\u001b[0m\u001b[1mEpoch: [12/70], Progress: 25.81 %, Steps: 1048, Current Learning Rate: 0.0002592, \u001b[96mTrain Loss: 6.077\n",
      "\u001b[0m\u001b[1mEpoch: [12/70], Progress: 26.88 %, Steps: 1049, Current Learning Rate: 0.0002594, \u001b[96mTrain Loss: 6.004\n",
      "\u001b[0m\u001b[1mEpoch: [12/70], Progress: 27.96 %, Steps: 1050, Current Learning Rate: 0.0002597, \u001b[91mTrain Loss: 6.046\n",
      "\u001b[0m\u001b[1mEpoch: [12/70], Progress: 29.03 %, Steps: 1051, Current Learning Rate: 0.0002599, \u001b[96mTrain Loss: 5.786\n",
      "\u001b[0m\u001b[1mEpoch: [12/70], Progress: 30.11 %, Steps: 1052, Current Learning Rate: 0.0002601, \u001b[91mTrain Loss: 5.976\n",
      "\u001b[0m\u001b[1mEpoch: [12/70], Progress: 31.18 %, Steps: 1053, Current Learning Rate: 0.0002604, \u001b[91mTrain Loss: 6.059\n",
      "\u001b[0m\u001b[1mEpoch: [12/70], Progress: 32.26 %, Steps: 1054, Current Learning Rate: 0.0002606, \u001b[96mTrain Loss: 5.959\n",
      "\u001b[0m\u001b[1mEpoch: [12/70], Progress: 33.33 %, Steps: 1055, Current Learning Rate: 0.0002609, \u001b[96mTrain Loss: 5.665\n",
      "\u001b[0m\u001b[1mEpoch: [12/70], Progress: 34.41 %, Steps: 1056, Current Learning Rate: 0.0002611, \u001b[91mTrain Loss: 6.007\n",
      "\u001b[0m\u001b[1mEpoch: [12/70], Progress: 35.48 %, Steps: 1057, Current Learning Rate: 0.0002614, \u001b[91mTrain Loss: 6.092\n",
      "\u001b[0m\u001b[1mEpoch: [12/70], Progress: 36.56 %, Steps: 1058, Current Learning Rate: 0.0002616, \u001b[96mTrain Loss: 5.897\n",
      "\u001b[0m\u001b[1mEpoch: [12/70], Progress: 37.63 %, Steps: 1059, Current Learning Rate: 0.0002619, \u001b[91mTrain Loss: 5.931\n",
      "\u001b[0m\u001b[1mEpoch: [12/70], Progress: 38.71 %, Steps: 1060, Current Learning Rate: 0.0002621, \u001b[91mTrain Loss: 5.997\n",
      "\u001b[0m\u001b[1mEpoch: [12/70], Progress: 39.78 %, Steps: 1061, Current Learning Rate: 0.0002624, \u001b[96mTrain Loss: 5.968\n",
      "\u001b[0m\u001b[1mEpoch: [12/70], Progress: 40.86 %, Steps: 1062, Current Learning Rate: 0.0002626, \u001b[96mTrain Loss: 5.956\n",
      "\u001b[0m\u001b[1mEpoch: [12/70], Progress: 41.94 %, Steps: 1063, Current Learning Rate: 0.0002629, \u001b[91mTrain Loss: 6.104\n",
      "\u001b[0m\u001b[1mEpoch: [12/70], Progress: 43.01 %, Steps: 1064, Current Learning Rate: 0.0002631, \u001b[96mTrain Loss: 6.046\n",
      "\u001b[0m\u001b[1mEpoch: [12/70], Progress: 44.09 %, Steps: 1065, Current Learning Rate: 0.0002634, \u001b[96mTrain Loss: 5.934\n",
      "\u001b[0m\u001b[1mEpoch: [12/70], Progress: 45.16 %, Steps: 1066, Current Learning Rate: 0.0002636, \u001b[91mTrain Loss: 6.133\n",
      "\u001b[0m\u001b[1mEpoch: [12/70], Progress: 46.24 %, Steps: 1067, Current Learning Rate: 0.0002639, \u001b[96mTrain Loss: 6.026\n",
      "\u001b[0m\u001b[1mEpoch: [12/70], Progress: 47.31 %, Steps: 1068, Current Learning Rate: 0.0002641, \u001b[96mTrain Loss: 5.907\n",
      "\u001b[0m\u001b[1mEpoch: [12/70], Progress: 48.39 %, Steps: 1069, Current Learning Rate: 0.0002643, \u001b[91mTrain Loss: 5.971\n",
      "\u001b[0m\u001b[1mEpoch: [12/70], Progress: 49.46 %, Steps: 1070, Current Learning Rate: 0.0002646, \u001b[91mTrain Loss: 6.066\n",
      "\u001b[0m\u001b[1mEpoch: [12/70], Progress: 50.54 %, Steps: 1071, Current Learning Rate: 0.0002648, \u001b[96mTrain Loss: 5.781\n",
      "\u001b[0m\u001b[1mEpoch: [12/70], Progress: 51.61 %, Steps: 1072, Current Learning Rate: 0.0002651, \u001b[91mTrain Loss: 5.989\n",
      "\u001b[0m\u001b[1mEpoch: [12/70], Progress: 52.69 %, Steps: 1073, Current Learning Rate: 0.0002653, \u001b[91mTrain Loss: 6.005\n",
      "\u001b[0m\u001b[1mEpoch: [12/70], Progress: 53.76 %, Steps: 1074, Current Learning Rate: 0.0002656, \u001b[96mTrain Loss: 5.834\n",
      "\u001b[0m\u001b[1mEpoch: [12/70], Progress: 54.84 %, Steps: 1075, Current Learning Rate: 0.0002658, \u001b[91mTrain Loss: 6.023\n",
      "\u001b[0m\u001b[1mEpoch: [12/70], Progress: 55.91 %, Steps: 1076, Current Learning Rate: 0.0002661, \u001b[96mTrain Loss: 5.921\n",
      "\u001b[0m\u001b[1mEpoch: [12/70], Progress: 56.99 %, Steps: 1077, Current Learning Rate: 0.0002663, \u001b[91mTrain Loss: 6.116\n",
      "\u001b[0m\u001b[1mEpoch: [12/70], Progress: 58.06 %, Steps: 1078, Current Learning Rate: 0.0002666, \u001b[96mTrain Loss: 5.850\n",
      "\u001b[0m\u001b[1mEpoch: [12/70], Progress: 59.14 %, Steps: 1079, Current Learning Rate: 0.0002668, \u001b[91mTrain Loss: 5.971\n",
      "\u001b[0m\u001b[1mEpoch: [12/70], Progress: 60.22 %, Steps: 1080, Current Learning Rate: 0.0002671, \u001b[91mTrain Loss: 6.037\n",
      "\u001b[0m\u001b[1mEpoch: [12/70], Progress: 61.29 %, Steps: 1081, Current Learning Rate: 0.0002673, \u001b[96mTrain Loss: 5.877\n",
      "\u001b[0m\u001b[1mEpoch: [12/70], Progress: 62.37 %, Steps: 1082, Current Learning Rate: 0.0002676, \u001b[96mTrain Loss: 5.873\n",
      "\u001b[0m\u001b[1mEpoch: [12/70], Progress: 63.44 %, Steps: 1083, Current Learning Rate: 0.0002678, \u001b[96mTrain Loss: 5.804\n",
      "\u001b[0m\u001b[1mEpoch: [12/70], Progress: 64.52 %, Steps: 1084, Current Learning Rate: 0.0002681, \u001b[91mTrain Loss: 6.016\n",
      "\u001b[0m\u001b[1mEpoch: [12/70], Progress: 65.59 %, Steps: 1085, Current Learning Rate: 0.0002683, \u001b[91mTrain Loss: 6.156\n",
      "\u001b[0m\u001b[1mEpoch: [12/70], Progress: 66.67 %, Steps: 1086, Current Learning Rate: 0.0002685, \u001b[96mTrain Loss: 6.040\n",
      "\u001b[0m\u001b[1mEpoch: [12/70], Progress: 67.74 %, Steps: 1087, Current Learning Rate: 0.0002688, \u001b[96mTrain Loss: 6.015\n",
      "\u001b[0m\u001b[1mEpoch: [12/70], Progress: 68.82 %, Steps: 1088, Current Learning Rate: 0.0002690, \u001b[96mTrain Loss: 5.890\n",
      "\u001b[0m\u001b[1mEpoch: [12/70], Progress: 69.89 %, Steps: 1089, Current Learning Rate: 0.0002693, \u001b[91mTrain Loss: 6.063\n",
      "\u001b[0m\u001b[1mEpoch: [12/70], Progress: 70.97 %, Steps: 1090, Current Learning Rate: 0.0002695, \u001b[96mTrain Loss: 5.920\n",
      "\u001b[0m\u001b[1mEpoch: [12/70], Progress: 72.04 %, Steps: 1091, Current Learning Rate: 0.0002698, \u001b[96mTrain Loss: 5.858\n",
      "\u001b[0m\u001b[1mEpoch: [12/70], Progress: 73.12 %, Steps: 1092, Current Learning Rate: 0.0002700, \u001b[91mTrain Loss: 5.888\n",
      "\u001b[0m\u001b[1mEpoch: [12/70], Progress: 74.19 %, Steps: 1093, Current Learning Rate: 0.0002703, \u001b[91mTrain Loss: 5.900\n",
      "\u001b[0m\u001b[1mEpoch: [12/70], Progress: 75.27 %, Steps: 1094, Current Learning Rate: 0.0002705, \u001b[91mTrain Loss: 6.009\n",
      "\u001b[0m\u001b[1mEpoch: [12/70], Progress: 76.34 %, Steps: 1095, Current Learning Rate: 0.0002708, \u001b[96mTrain Loss: 5.951\n",
      "\u001b[0m\u001b[1mEpoch: [12/70], Progress: 77.42 %, Steps: 1096, Current Learning Rate: 0.0002710, \u001b[96mTrain Loss: 5.724\n",
      "\u001b[0m\u001b[1mEpoch: [12/70], Progress: 78.49 %, Steps: 1097, Current Learning Rate: 0.0002713, \u001b[91mTrain Loss: 5.903\n",
      "\u001b[0m\u001b[1mEpoch: [12/70], Progress: 79.57 %, Steps: 1098, Current Learning Rate: 0.0002715, \u001b[91mTrain Loss: 5.982\n",
      "\u001b[0m\u001b[1mEpoch: [12/70], Progress: 80.65 %, Steps: 1099, Current Learning Rate: 0.0002718, \u001b[91mTrain Loss: 6.125\n",
      "\u001b[0m\u001b[1mEpoch: [12/70], Progress: 81.72 %, Steps: 1100, Current Learning Rate: 0.0002720, \u001b[96mTrain Loss: 5.994\n",
      "\u001b[0m\u001b[1mEpoch: [12/70], Progress: 82.80 %, Steps: 1101, Current Learning Rate: 0.0002723, \u001b[96mTrain Loss: 5.873\n",
      "\u001b[0m\u001b[1mEpoch: [12/70], Progress: 83.87 %, Steps: 1102, Current Learning Rate: 0.0002725, \u001b[91mTrain Loss: 5.879\n",
      "\u001b[0m\u001b[1mEpoch: [12/70], Progress: 84.95 %, Steps: 1103, Current Learning Rate: 0.0002727, \u001b[91mTrain Loss: 5.965\n",
      "\u001b[0m\u001b[1mEpoch: [12/70], Progress: 86.02 %, Steps: 1104, Current Learning Rate: 0.0002730, \u001b[96mTrain Loss: 5.889\n",
      "\u001b[0m\u001b[1mEpoch: [12/70], Progress: 87.10 %, Steps: 1105, Current Learning Rate: 0.0002732, \u001b[91mTrain Loss: 5.953\n",
      "\u001b[0m\u001b[1mEpoch: [12/70], Progress: 88.17 %, Steps: 1106, Current Learning Rate: 0.0002735, \u001b[96mTrain Loss: 5.855\n",
      "\u001b[0m\u001b[1mEpoch: [12/70], Progress: 89.25 %, Steps: 1107, Current Learning Rate: 0.0002737, \u001b[96mTrain Loss: 5.852\n",
      "\u001b[0m\u001b[1mEpoch: [12/70], Progress: 90.32 %, Steps: 1108, Current Learning Rate: 0.0002740, \u001b[96mTrain Loss: 5.775\n",
      "\u001b[0m\u001b[1mEpoch: [12/70], Progress: 91.40 %, Steps: 1109, Current Learning Rate: 0.0002742, \u001b[91mTrain Loss: 5.908\n",
      "\u001b[0m\u001b[1mEpoch: [12/70], Progress: 92.47 %, Steps: 1110, Current Learning Rate: 0.0002745, \u001b[91mTrain Loss: 5.920\n",
      "\u001b[0m\u001b[1mEpoch: [12/70], Progress: 93.55 %, Steps: 1111, Current Learning Rate: 0.0002747, \u001b[96mTrain Loss: 5.813\n",
      "\u001b[0m\u001b[1mEpoch: [12/70], Progress: 94.62 %, Steps: 1112, Current Learning Rate: 0.0002750, \u001b[91mTrain Loss: 5.855\n",
      "\u001b[0m\u001b[1mEpoch: [12/70], Progress: 95.70 %, Steps: 1113, Current Learning Rate: 0.0002752, \u001b[91mTrain Loss: 6.191\n",
      "\u001b[0m\u001b[1mEpoch: [12/70], Progress: 96.77 %, Steps: 1114, Current Learning Rate: 0.0002755, \u001b[96mTrain Loss: 5.692\n",
      "\u001b[0m\u001b[1mEpoch: [12/70], Progress: 97.85 %, Steps: 1115, Current Learning Rate: 0.0002757, \u001b[91mTrain Loss: 6.111\n",
      "\u001b[0m\u001b[1mEpoch: [12/70], Progress: 98.92 %, Steps: 1116, Current Learning Rate: 0.0002760, \u001b[91mTrain Loss: 6.277\n",
      "\u001b[1m\u001b[93mComputing Validation Loss...\n",
      "\u001b[1mEpoch 12 Completed! Average Train Loss: 5.959, Average Validation Loss: 5.275\n",
      "Best Model saved in best_model/best_model.pt.\n",
      "\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m= Epoch [13/70] ==========================================\n",
      "\u001b[0m\u001b[1mEpoch: [13/70], Progress: 0.00 %, Steps: 1117, Current Learning Rate: 0.0002762, \u001b[91mTrain Loss: 5.915\n",
      "\u001b[0m\u001b[1mEpoch: [13/70], Progress: 1.08 %, Steps: 1118, Current Learning Rate: 0.0002765, \u001b[91mTrain Loss: 5.943\n",
      "\u001b[0m\u001b[1mEpoch: [13/70], Progress: 2.15 %, Steps: 1119, Current Learning Rate: 0.0002767, \u001b[96mTrain Loss: 5.784\n",
      "\u001b[0m\u001b[1mEpoch: [13/70], Progress: 3.23 %, Steps: 1120, Current Learning Rate: 0.0002769, \u001b[91mTrain Loss: 5.922\n",
      "\u001b[0m\u001b[1mEpoch: [13/70], Progress: 4.30 %, Steps: 1121, Current Learning Rate: 0.0002772, \u001b[96mTrain Loss: 5.660\n",
      "\u001b[0m\u001b[1mEpoch: [13/70], Progress: 5.38 %, Steps: 1122, Current Learning Rate: 0.0002774, \u001b[91mTrain Loss: 5.867\n",
      "\u001b[0m\u001b[1mEpoch: [13/70], Progress: 6.45 %, Steps: 1123, Current Learning Rate: 0.0002777, \u001b[91mTrain Loss: 6.125\n",
      "\u001b[0m\u001b[1mEpoch: [13/70], Progress: 7.53 %, Steps: 1124, Current Learning Rate: 0.0002779, \u001b[96mTrain Loss: 5.872\n",
      "\u001b[0m\u001b[1mEpoch: [13/70], Progress: 8.60 %, Steps: 1125, Current Learning Rate: 0.0002782, \u001b[91mTrain Loss: 5.891\n",
      "\u001b[0m\u001b[1mEpoch: [13/70], Progress: 9.68 %, Steps: 1126, Current Learning Rate: 0.0002784, \u001b[96mTrain Loss: 5.652\n",
      "\u001b[0m\u001b[1mEpoch: [13/70], Progress: 10.75 %, Steps: 1127, Current Learning Rate: 0.0002787, \u001b[91mTrain Loss: 5.743\n",
      "\u001b[0m\u001b[1mEpoch: [13/70], Progress: 11.83 %, Steps: 1128, Current Learning Rate: 0.0002789, \u001b[91mTrain Loss: 6.014\n",
      "\u001b[0m\u001b[1mEpoch: [13/70], Progress: 12.90 %, Steps: 1129, Current Learning Rate: 0.0002792, \u001b[96mTrain Loss: 5.817\n",
      "\u001b[0m\u001b[1mEpoch: [13/70], Progress: 13.98 %, Steps: 1130, Current Learning Rate: 0.0002794, \u001b[91mTrain Loss: 5.897\n",
      "\u001b[0m\u001b[1mEpoch: [13/70], Progress: 15.05 %, Steps: 1131, Current Learning Rate: 0.0002797, \u001b[96mTrain Loss: 5.857\n",
      "\u001b[0m\u001b[1mEpoch: [13/70], Progress: 16.13 %, Steps: 1132, Current Learning Rate: 0.0002799, \u001b[91mTrain Loss: 5.951\n",
      "\u001b[0m\u001b[1mEpoch: [13/70], Progress: 17.20 %, Steps: 1133, Current Learning Rate: 0.0002802, \u001b[91mTrain Loss: 5.990\n",
      "\u001b[0m\u001b[1mEpoch: [13/70], Progress: 18.28 %, Steps: 1134, Current Learning Rate: 0.0002804, \u001b[96mTrain Loss: 5.854\n",
      "\u001b[0m\u001b[1mEpoch: [13/70], Progress: 19.35 %, Steps: 1135, Current Learning Rate: 0.0002807, \u001b[96mTrain Loss: 5.843\n",
      "\u001b[0m\u001b[1mEpoch: [13/70], Progress: 20.43 %, Steps: 1136, Current Learning Rate: 0.0002809, \u001b[96mTrain Loss: 5.790\n",
      "\u001b[0m\u001b[1mEpoch: [13/70], Progress: 21.51 %, Steps: 1137, Current Learning Rate: 0.0002811, \u001b[91mTrain Loss: 5.816\n",
      "\u001b[0m\u001b[1mEpoch: [13/70], Progress: 22.58 %, Steps: 1138, Current Learning Rate: 0.0002814, \u001b[96mTrain Loss: 5.690\n",
      "\u001b[0m\u001b[1mEpoch: [13/70], Progress: 23.66 %, Steps: 1139, Current Learning Rate: 0.0002816, \u001b[96mTrain Loss: 5.661\n",
      "\u001b[0m\u001b[1mEpoch: [13/70], Progress: 24.73 %, Steps: 1140, Current Learning Rate: 0.0002819, \u001b[91mTrain Loss: 5.682\n",
      "\u001b[0m\u001b[1mEpoch: [13/70], Progress: 25.81 %, Steps: 1141, Current Learning Rate: 0.0002821, \u001b[91mTrain Loss: 6.011\n",
      "\u001b[0m\u001b[1mEpoch: [13/70], Progress: 26.88 %, Steps: 1142, Current Learning Rate: 0.0002824, \u001b[96mTrain Loss: 5.998\n",
      "\u001b[0m\u001b[1mEpoch: [13/70], Progress: 27.96 %, Steps: 1143, Current Learning Rate: 0.0002826, \u001b[96mTrain Loss: 5.726\n",
      "\u001b[0m\u001b[1mEpoch: [13/70], Progress: 29.03 %, Steps: 1144, Current Learning Rate: 0.0002829, \u001b[91mTrain Loss: 5.734\n",
      "\u001b[0m\u001b[1mEpoch: [13/70], Progress: 30.11 %, Steps: 1145, Current Learning Rate: 0.0002831, \u001b[91mTrain Loss: 6.115\n",
      "\u001b[0m\u001b[1mEpoch: [13/70], Progress: 31.18 %, Steps: 1146, Current Learning Rate: 0.0002834, \u001b[96mTrain Loss: 5.923\n",
      "\u001b[0m\u001b[1mEpoch: [13/70], Progress: 32.26 %, Steps: 1147, Current Learning Rate: 0.0002836, \u001b[96mTrain Loss: 5.676\n",
      "\u001b[0m\u001b[1mEpoch: [13/70], Progress: 33.33 %, Steps: 1148, Current Learning Rate: 0.0002839, \u001b[91mTrain Loss: 5.729\n",
      "\u001b[0m\u001b[1mEpoch: [13/70], Progress: 34.41 %, Steps: 1149, Current Learning Rate: 0.0002841, \u001b[91mTrain Loss: 5.926\n",
      "\u001b[0m\u001b[1mEpoch: [13/70], Progress: 35.48 %, Steps: 1150, Current Learning Rate: 0.0002844, \u001b[91mTrain Loss: 6.059\n",
      "\u001b[0m\u001b[1mEpoch: [13/70], Progress: 36.56 %, Steps: 1151, Current Learning Rate: 0.0002846, \u001b[96mTrain Loss: 5.824\n",
      "\u001b[0m\u001b[1mEpoch: [13/70], Progress: 37.63 %, Steps: 1152, Current Learning Rate: 0.0002849, \u001b[96mTrain Loss: 5.734\n",
      "\u001b[0m\u001b[1mEpoch: [13/70], Progress: 38.71 %, Steps: 1153, Current Learning Rate: 0.0002851, \u001b[91mTrain Loss: 5.931\n",
      "\u001b[0m\u001b[1mEpoch: [13/70], Progress: 39.78 %, Steps: 1154, Current Learning Rate: 0.0002853, \u001b[96mTrain Loss: 5.558\n",
      "\u001b[0m\u001b[1mEpoch: [13/70], Progress: 40.86 %, Steps: 1155, Current Learning Rate: 0.0002856, \u001b[91mTrain Loss: 5.928\n",
      "\u001b[0m\u001b[1mEpoch: [13/70], Progress: 41.94 %, Steps: 1156, Current Learning Rate: 0.0002858, \u001b[91mTrain Loss: 5.942\n",
      "\u001b[0m\u001b[1mEpoch: [13/70], Progress: 43.01 %, Steps: 1157, Current Learning Rate: 0.0002861, \u001b[96mTrain Loss: 5.724\n",
      "\u001b[0m\u001b[1mEpoch: [13/70], Progress: 44.09 %, Steps: 1158, Current Learning Rate: 0.0002863, \u001b[91mTrain Loss: 5.743\n",
      "\u001b[0m\u001b[1mEpoch: [13/70], Progress: 45.16 %, Steps: 1159, Current Learning Rate: 0.0002866, \u001b[96mTrain Loss: 5.712\n",
      "\u001b[0m\u001b[1mEpoch: [13/70], Progress: 46.24 %, Steps: 1160, Current Learning Rate: 0.0002868, \u001b[91mTrain Loss: 5.819\n",
      "\u001b[0m\u001b[1mEpoch: [13/70], Progress: 47.31 %, Steps: 1161, Current Learning Rate: 0.0002871, \u001b[91mTrain Loss: 5.866\n",
      "\u001b[0m\u001b[1mEpoch: [13/70], Progress: 48.39 %, Steps: 1162, Current Learning Rate: 0.0002873, \u001b[96mTrain Loss: 5.830\n",
      "\u001b[0m\u001b[1mEpoch: [13/70], Progress: 49.46 %, Steps: 1163, Current Learning Rate: 0.0002876, \u001b[96mTrain Loss: 5.716\n",
      "\u001b[0m\u001b[1mEpoch: [13/70], Progress: 50.54 %, Steps: 1164, Current Learning Rate: 0.0002878, \u001b[91mTrain Loss: 5.878\n",
      "\u001b[0m\u001b[1mEpoch: [13/70], Progress: 51.61 %, Steps: 1165, Current Learning Rate: 0.0002881, \u001b[96mTrain Loss: 5.701\n",
      "\u001b[0m\u001b[1mEpoch: [13/70], Progress: 52.69 %, Steps: 1166, Current Learning Rate: 0.0002883, \u001b[96mTrain Loss: 5.469\n",
      "\u001b[0m\u001b[1mEpoch: [13/70], Progress: 53.76 %, Steps: 1167, Current Learning Rate: 0.0002886, \u001b[91mTrain Loss: 5.696\n",
      "\u001b[0m\u001b[1mEpoch: [13/70], Progress: 54.84 %, Steps: 1168, Current Learning Rate: 0.0002888, \u001b[96mTrain Loss: 5.616\n",
      "\u001b[0m\u001b[1mEpoch: [13/70], Progress: 55.91 %, Steps: 1169, Current Learning Rate: 0.0002891, \u001b[91mTrain Loss: 5.825\n",
      "\u001b[0m\u001b[1mEpoch: [13/70], Progress: 56.99 %, Steps: 1170, Current Learning Rate: 0.0002893, \u001b[91mTrain Loss: 5.849\n",
      "\u001b[0m\u001b[1mEpoch: [13/70], Progress: 58.06 %, Steps: 1171, Current Learning Rate: 0.0002895, \u001b[96mTrain Loss: 5.804\n",
      "\u001b[0m\u001b[1mEpoch: [13/70], Progress: 59.14 %, Steps: 1172, Current Learning Rate: 0.0002898, \u001b[91mTrain Loss: 6.011\n",
      "\u001b[0m\u001b[1mEpoch: [13/70], Progress: 60.22 %, Steps: 1173, Current Learning Rate: 0.0002900, \u001b[96mTrain Loss: 5.868\n",
      "\u001b[0m\u001b[1mEpoch: [13/70], Progress: 61.29 %, Steps: 1174, Current Learning Rate: 0.0002903, \u001b[91mTrain Loss: 6.036\n",
      "\u001b[0m\u001b[1mEpoch: [13/70], Progress: 62.37 %, Steps: 1175, Current Learning Rate: 0.0002905, \u001b[96mTrain Loss: 5.664\n",
      "\u001b[0m\u001b[1mEpoch: [13/70], Progress: 63.44 %, Steps: 1176, Current Learning Rate: 0.0002908, \u001b[91mTrain Loss: 5.850\n",
      "\u001b[0m\u001b[1mEpoch: [13/70], Progress: 64.52 %, Steps: 1177, Current Learning Rate: 0.0002910, \u001b[91mTrain Loss: 5.929\n",
      "\u001b[0m\u001b[1mEpoch: [13/70], Progress: 65.59 %, Steps: 1178, Current Learning Rate: 0.0002913, \u001b[96mTrain Loss: 5.895\n",
      "\u001b[0m\u001b[1mEpoch: [13/70], Progress: 66.67 %, Steps: 1179, Current Learning Rate: 0.0002915, \u001b[96mTrain Loss: 5.623\n",
      "\u001b[0m\u001b[1mEpoch: [13/70], Progress: 67.74 %, Steps: 1180, Current Learning Rate: 0.0002918, \u001b[91mTrain Loss: 5.724\n",
      "\u001b[0m\u001b[1mEpoch: [13/70], Progress: 68.82 %, Steps: 1181, Current Learning Rate: 0.0002920, \u001b[91mTrain Loss: 5.752\n",
      "\u001b[0m\u001b[1mEpoch: [13/70], Progress: 69.89 %, Steps: 1182, Current Learning Rate: 0.0002923, \u001b[91mTrain Loss: 6.002\n",
      "\u001b[0m\u001b[1mEpoch: [13/70], Progress: 70.97 %, Steps: 1183, Current Learning Rate: 0.0002925, \u001b[96mTrain Loss: 5.872\n",
      "\u001b[0m\u001b[1mEpoch: [13/70], Progress: 72.04 %, Steps: 1184, Current Learning Rate: 0.0002928, \u001b[91mTrain Loss: 5.899\n",
      "\u001b[0m\u001b[1mEpoch: [13/70], Progress: 73.12 %, Steps: 1185, Current Learning Rate: 0.0002930, \u001b[96mTrain Loss: 5.715\n",
      "\u001b[0m\u001b[1mEpoch: [13/70], Progress: 74.19 %, Steps: 1186, Current Learning Rate: 0.0002933, \u001b[96mTrain Loss: 5.712\n",
      "\u001b[0m\u001b[1mEpoch: [13/70], Progress: 75.27 %, Steps: 1187, Current Learning Rate: 0.0002935, \u001b[96mTrain Loss: 5.630\n",
      "\u001b[0m\u001b[1mEpoch: [13/70], Progress: 76.34 %, Steps: 1188, Current Learning Rate: 0.0002937, \u001b[91mTrain Loss: 5.900\n",
      "\u001b[0m\u001b[1mEpoch: [13/70], Progress: 77.42 %, Steps: 1189, Current Learning Rate: 0.0002940, \u001b[96mTrain Loss: 5.772\n",
      "\u001b[0m\u001b[1mEpoch: [13/70], Progress: 78.49 %, Steps: 1190, Current Learning Rate: 0.0002942, \u001b[91mTrain Loss: 5.829\n",
      "\u001b[0m\u001b[1mEpoch: [13/70], Progress: 79.57 %, Steps: 1191, Current Learning Rate: 0.0002945, \u001b[96mTrain Loss: 5.715\n",
      "\u001b[0m\u001b[1mEpoch: [13/70], Progress: 80.65 %, Steps: 1192, Current Learning Rate: 0.0002947, \u001b[91mTrain Loss: 5.744\n",
      "\u001b[0m\u001b[1mEpoch: [13/70], Progress: 81.72 %, Steps: 1193, Current Learning Rate: 0.0002950, \u001b[91mTrain Loss: 5.867\n",
      "\u001b[0m\u001b[1mEpoch: [13/70], Progress: 82.80 %, Steps: 1194, Current Learning Rate: 0.0002952, \u001b[96mTrain Loss: 5.709\n",
      "\u001b[0m\u001b[1mEpoch: [13/70], Progress: 83.87 %, Steps: 1195, Current Learning Rate: 0.0002955, \u001b[96mTrain Loss: 5.677\n",
      "\u001b[0m\u001b[1mEpoch: [13/70], Progress: 84.95 %, Steps: 1196, Current Learning Rate: 0.0002957, \u001b[96mTrain Loss: 5.593\n",
      "\u001b[0m\u001b[1mEpoch: [13/70], Progress: 86.02 %, Steps: 1197, Current Learning Rate: 0.0002960, \u001b[91mTrain Loss: 5.788\n",
      "\u001b[0m\u001b[1mEpoch: [13/70], Progress: 87.10 %, Steps: 1198, Current Learning Rate: 0.0002962, \u001b[96mTrain Loss: 5.740\n",
      "\u001b[0m\u001b[1mEpoch: [13/70], Progress: 88.17 %, Steps: 1199, Current Learning Rate: 0.0002965, \u001b[96mTrain Loss: 5.628\n",
      "\u001b[0m\u001b[1mEpoch: [13/70], Progress: 89.25 %, Steps: 1200, Current Learning Rate: 0.0002967, \u001b[91mTrain Loss: 5.820\n",
      "\u001b[0m\u001b[1mEpoch: [13/70], Progress: 90.32 %, Steps: 1201, Current Learning Rate: 0.0002970, \u001b[96mTrain Loss: 5.809\n",
      "\u001b[0m\u001b[1mEpoch: [13/70], Progress: 91.40 %, Steps: 1202, Current Learning Rate: 0.0002972, \u001b[96mTrain Loss: 5.724\n",
      "\u001b[0m\u001b[1mEpoch: [13/70], Progress: 92.47 %, Steps: 1203, Current Learning Rate: 0.0002975, \u001b[91mTrain Loss: 5.838\n",
      "\u001b[0m\u001b[1mEpoch: [13/70], Progress: 93.55 %, Steps: 1204, Current Learning Rate: 0.0002977, \u001b[96mTrain Loss: 5.745\n",
      "\u001b[0m\u001b[1mEpoch: [13/70], Progress: 94.62 %, Steps: 1205, Current Learning Rate: 0.0002979, \u001b[91mTrain Loss: 5.887\n",
      "\u001b[0m\u001b[1mEpoch: [13/70], Progress: 95.70 %, Steps: 1206, Current Learning Rate: 0.0002982, \u001b[96mTrain Loss: 5.728\n",
      "\u001b[0m\u001b[1mEpoch: [13/70], Progress: 96.77 %, Steps: 1207, Current Learning Rate: 0.0002984, \u001b[96mTrain Loss: 5.700\n",
      "\u001b[0m\u001b[1mEpoch: [13/70], Progress: 97.85 %, Steps: 1208, Current Learning Rate: 0.0002987, \u001b[91mTrain Loss: 5.807\n",
      "\u001b[0m\u001b[1mEpoch: [13/70], Progress: 98.92 %, Steps: 1209, Current Learning Rate: 0.0002989, \u001b[91mTrain Loss: 5.836\n",
      "\u001b[1m\u001b[93mComputing Validation Loss...\n",
      "\u001b[1mEpoch 13 Completed! Average Train Loss: 5.810, Average Validation Loss: 5.203\n",
      "Best Model saved in best_model/best_model.pt.\n",
      "\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m= Epoch [14/70] ==========================================\n",
      "\u001b[0m\u001b[1mEpoch: [14/70], Progress: 0.00 %, Steps: 1210, Current Learning Rate: 0.0002992, \u001b[91mTrain Loss: 5.646\n",
      "\u001b[0m\u001b[1mEpoch: [14/70], Progress: 1.08 %, Steps: 1211, Current Learning Rate: 0.0002994, \u001b[91mTrain Loss: 5.725\n",
      "\u001b[0m\u001b[1mEpoch: [14/70], Progress: 2.15 %, Steps: 1212, Current Learning Rate: 0.0002997, \u001b[96mTrain Loss: 5.598\n",
      "\u001b[0m\u001b[1mEpoch: [14/70], Progress: 3.23 %, Steps: 1213, Current Learning Rate: 0.0002999, \u001b[91mTrain Loss: 5.671\n",
      "\u001b[0m\u001b[1mEpoch: [14/70], Progress: 4.30 %, Steps: 1214, Current Learning Rate: 0.0003002, \u001b[96mTrain Loss: 5.650\n",
      "\u001b[0m\u001b[1mEpoch: [14/70], Progress: 5.38 %, Steps: 1215, Current Learning Rate: 0.0003004, \u001b[96mTrain Loss: 5.626\n",
      "\u001b[0m\u001b[1mEpoch: [14/70], Progress: 6.45 %, Steps: 1216, Current Learning Rate: 0.0003007, \u001b[96mTrain Loss: 5.601\n",
      "\u001b[0m\u001b[1mEpoch: [14/70], Progress: 7.53 %, Steps: 1217, Current Learning Rate: 0.0003009, \u001b[91mTrain Loss: 5.674\n",
      "\u001b[0m\u001b[1mEpoch: [14/70], Progress: 8.60 %, Steps: 1218, Current Learning Rate: 0.0003012, \u001b[96mTrain Loss: 5.566\n",
      "\u001b[0m\u001b[1mEpoch: [14/70], Progress: 9.68 %, Steps: 1219, Current Learning Rate: 0.0003014, \u001b[91mTrain Loss: 5.792\n",
      "\u001b[0m\u001b[1mEpoch: [14/70], Progress: 10.75 %, Steps: 1220, Current Learning Rate: 0.0003017, \u001b[91mTrain Loss: 5.876\n",
      "\u001b[0m\u001b[1mEpoch: [14/70], Progress: 11.83 %, Steps: 1221, Current Learning Rate: 0.0003019, \u001b[96mTrain Loss: 5.714\n",
      "\u001b[0m\u001b[1mEpoch: [14/70], Progress: 12.90 %, Steps: 1222, Current Learning Rate: 0.0003021, \u001b[96mTrain Loss: 5.487\n",
      "\u001b[0m\u001b[1mEpoch: [14/70], Progress: 13.98 %, Steps: 1223, Current Learning Rate: 0.0003024, \u001b[91mTrain Loss: 5.552\n",
      "\u001b[0m\u001b[1mEpoch: [14/70], Progress: 15.05 %, Steps: 1224, Current Learning Rate: 0.0003026, \u001b[91mTrain Loss: 5.743\n",
      "\u001b[0m\u001b[1mEpoch: [14/70], Progress: 16.13 %, Steps: 1225, Current Learning Rate: 0.0003029, \u001b[91mTrain Loss: 5.825\n",
      "\u001b[0m\u001b[1mEpoch: [14/70], Progress: 17.20 %, Steps: 1226, Current Learning Rate: 0.0003031, \u001b[96mTrain Loss: 5.602\n",
      "\u001b[0m\u001b[1mEpoch: [14/70], Progress: 18.28 %, Steps: 1227, Current Learning Rate: 0.0003034, \u001b[91mTrain Loss: 5.789\n",
      "\u001b[0m\u001b[1mEpoch: [14/70], Progress: 19.35 %, Steps: 1228, Current Learning Rate: 0.0003036, \u001b[96mTrain Loss: 5.646\n",
      "\u001b[0m\u001b[1mEpoch: [14/70], Progress: 20.43 %, Steps: 1229, Current Learning Rate: 0.0003039, \u001b[91mTrain Loss: 5.655\n",
      "\u001b[0m\u001b[1mEpoch: [14/70], Progress: 21.51 %, Steps: 1230, Current Learning Rate: 0.0003041, \u001b[96mTrain Loss: 5.550\n",
      "\u001b[0m\u001b[1mEpoch: [14/70], Progress: 22.58 %, Steps: 1231, Current Learning Rate: 0.0003044, \u001b[91mTrain Loss: 5.609\n",
      "\u001b[0m\u001b[1mEpoch: [14/70], Progress: 23.66 %, Steps: 1232, Current Learning Rate: 0.0003046, \u001b[91mTrain Loss: 5.738\n",
      "\u001b[0m\u001b[1mEpoch: [14/70], Progress: 24.73 %, Steps: 1233, Current Learning Rate: 0.0003049, \u001b[96mTrain Loss: 5.605\n",
      "\u001b[0m\u001b[1mEpoch: [14/70], Progress: 25.81 %, Steps: 1234, Current Learning Rate: 0.0003051, \u001b[91mTrain Loss: 5.709\n",
      "\u001b[0m\u001b[1mEpoch: [14/70], Progress: 26.88 %, Steps: 1235, Current Learning Rate: 0.0003054, \u001b[91mTrain Loss: 5.786\n",
      "\u001b[0m\u001b[1mEpoch: [14/70], Progress: 27.96 %, Steps: 1236, Current Learning Rate: 0.0003056, \u001b[96mTrain Loss: 5.722\n",
      "\u001b[0m\u001b[1mEpoch: [14/70], Progress: 29.03 %, Steps: 1237, Current Learning Rate: 0.0003059, \u001b[96mTrain Loss: 5.626\n",
      "\u001b[0m\u001b[1mEpoch: [14/70], Progress: 30.11 %, Steps: 1238, Current Learning Rate: 0.0003061, \u001b[96mTrain Loss: 5.512\n",
      "\u001b[0m\u001b[1mEpoch: [14/70], Progress: 31.18 %, Steps: 1239, Current Learning Rate: 0.0003063, \u001b[91mTrain Loss: 5.616\n",
      "\u001b[0m\u001b[1mEpoch: [14/70], Progress: 32.26 %, Steps: 1240, Current Learning Rate: 0.0003066, \u001b[91mTrain Loss: 5.639\n",
      "\u001b[0m\u001b[1mEpoch: [14/70], Progress: 33.33 %, Steps: 1241, Current Learning Rate: 0.0003068, \u001b[91mTrain Loss: 5.688\n",
      "\u001b[0m\u001b[1mEpoch: [14/70], Progress: 34.41 %, Steps: 1242, Current Learning Rate: 0.0003071, \u001b[96mTrain Loss: 5.516\n",
      "\u001b[0m\u001b[1mEpoch: [14/70], Progress: 35.48 %, Steps: 1243, Current Learning Rate: 0.0003073, \u001b[91mTrain Loss: 5.628\n",
      "\u001b[0m\u001b[1mEpoch: [14/70], Progress: 36.56 %, Steps: 1244, Current Learning Rate: 0.0003076, \u001b[91mTrain Loss: 5.657\n",
      "\u001b[0m\u001b[1mEpoch: [14/70], Progress: 37.63 %, Steps: 1245, Current Learning Rate: 0.0003078, \u001b[96mTrain Loss: 5.449\n",
      "\u001b[0m\u001b[1mEpoch: [14/70], Progress: 38.71 %, Steps: 1246, Current Learning Rate: 0.0003081, \u001b[91mTrain Loss: 5.863\n",
      "\u001b[0m\u001b[1mEpoch: [14/70], Progress: 39.78 %, Steps: 1247, Current Learning Rate: 0.0003083, \u001b[91mTrain Loss: 5.933\n",
      "\u001b[0m\u001b[1mEpoch: [14/70], Progress: 40.86 %, Steps: 1248, Current Learning Rate: 0.0003086, \u001b[96mTrain Loss: 5.715\n",
      "\u001b[0m\u001b[1mEpoch: [14/70], Progress: 41.94 %, Steps: 1249, Current Learning Rate: 0.0003088, \u001b[91mTrain Loss: 5.874\n",
      "\u001b[0m\u001b[1mEpoch: [14/70], Progress: 43.01 %, Steps: 1250, Current Learning Rate: 0.0003091, \u001b[96mTrain Loss: 5.449\n",
      "\u001b[0m\u001b[1mEpoch: [14/70], Progress: 44.09 %, Steps: 1251, Current Learning Rate: 0.0003093, \u001b[91mTrain Loss: 5.861\n",
      "\u001b[0m\u001b[1mEpoch: [14/70], Progress: 45.16 %, Steps: 1252, Current Learning Rate: 0.0003096, \u001b[96mTrain Loss: 5.768\n",
      "\u001b[0m\u001b[1mEpoch: [14/70], Progress: 46.24 %, Steps: 1253, Current Learning Rate: 0.0003098, \u001b[96mTrain Loss: 5.586\n",
      "\u001b[0m\u001b[1mEpoch: [14/70], Progress: 47.31 %, Steps: 1254, Current Learning Rate: 0.0003101, \u001b[91mTrain Loss: 5.659\n",
      "\u001b[0m\u001b[1mEpoch: [14/70], Progress: 48.39 %, Steps: 1255, Current Learning Rate: 0.0003103, \u001b[91mTrain Loss: 5.667\n",
      "\u001b[0m\u001b[1mEpoch: [14/70], Progress: 49.46 %, Steps: 1256, Current Learning Rate: 0.0003105, \u001b[91mTrain Loss: 5.805\n",
      "\u001b[0m\u001b[1mEpoch: [14/70], Progress: 50.54 %, Steps: 1257, Current Learning Rate: 0.0003108, \u001b[96mTrain Loss: 5.615\n",
      "\u001b[0m\u001b[1mEpoch: [14/70], Progress: 51.61 %, Steps: 1258, Current Learning Rate: 0.0003110, \u001b[91mTrain Loss: 5.665\n",
      "\u001b[0m\u001b[1mEpoch: [14/70], Progress: 52.69 %, Steps: 1259, Current Learning Rate: 0.0003113, \u001b[96mTrain Loss: 5.545\n",
      "\u001b[0m\u001b[1mEpoch: [14/70], Progress: 53.76 %, Steps: 1260, Current Learning Rate: 0.0003115, \u001b[91mTrain Loss: 5.784\n",
      "\u001b[0m\u001b[1mEpoch: [14/70], Progress: 54.84 %, Steps: 1261, Current Learning Rate: 0.0003118, \u001b[96mTrain Loss: 5.736\n",
      "\u001b[0m\u001b[1mEpoch: [14/70], Progress: 55.91 %, Steps: 1262, Current Learning Rate: 0.0003120, \u001b[96mTrain Loss: 5.629\n",
      "\u001b[0m\u001b[1mEpoch: [14/70], Progress: 56.99 %, Steps: 1263, Current Learning Rate: 0.0003123, \u001b[91mTrain Loss: 5.707\n",
      "\u001b[0m\u001b[1mEpoch: [14/70], Progress: 58.06 %, Steps: 1264, Current Learning Rate: 0.0003125, \u001b[96mTrain Loss: 5.538\n",
      "\u001b[0m\u001b[1mEpoch: [14/70], Progress: 59.14 %, Steps: 1265, Current Learning Rate: 0.0003128, \u001b[91mTrain Loss: 5.840\n",
      "\u001b[0m\u001b[1mEpoch: [14/70], Progress: 60.22 %, Steps: 1266, Current Learning Rate: 0.0003130, \u001b[96mTrain Loss: 5.655\n",
      "\u001b[0m\u001b[1mEpoch: [14/70], Progress: 61.29 %, Steps: 1267, Current Learning Rate: 0.0003133, \u001b[91mTrain Loss: 5.665\n",
      "\u001b[0m\u001b[1mEpoch: [14/70], Progress: 62.37 %, Steps: 1268, Current Learning Rate: 0.0003135, \u001b[91mTrain Loss: 5.678\n",
      "\u001b[0m\u001b[1mEpoch: [14/70], Progress: 63.44 %, Steps: 1269, Current Learning Rate: 0.0003138, \u001b[96mTrain Loss: 5.540\n",
      "\u001b[0m\u001b[1mEpoch: [14/70], Progress: 64.52 %, Steps: 1270, Current Learning Rate: 0.0003140, \u001b[96mTrain Loss: 5.489\n",
      "\u001b[0m\u001b[1mEpoch: [14/70], Progress: 65.59 %, Steps: 1271, Current Learning Rate: 0.0003143, \u001b[96mTrain Loss: 5.472\n",
      "\u001b[0m\u001b[1mEpoch: [14/70], Progress: 66.67 %, Steps: 1272, Current Learning Rate: 0.0003145, \u001b[91mTrain Loss: 5.660\n",
      "\u001b[0m\u001b[1mEpoch: [14/70], Progress: 67.74 %, Steps: 1273, Current Learning Rate: 0.0003147, \u001b[96mTrain Loss: 5.525\n",
      "\u001b[0m\u001b[1mEpoch: [14/70], Progress: 68.82 %, Steps: 1274, Current Learning Rate: 0.0003150, \u001b[91mTrain Loss: 5.554\n",
      "\u001b[0m\u001b[1mEpoch: [14/70], Progress: 69.89 %, Steps: 1275, Current Learning Rate: 0.0003152, \u001b[91mTrain Loss: 5.798\n",
      "\u001b[0m\u001b[1mEpoch: [14/70], Progress: 70.97 %, Steps: 1276, Current Learning Rate: 0.0003155, \u001b[96mTrain Loss: 5.746\n",
      "\u001b[0m\u001b[1mEpoch: [14/70], Progress: 72.04 %, Steps: 1277, Current Learning Rate: 0.0003157, \u001b[96mTrain Loss: 5.646\n",
      "\u001b[0m\u001b[1mEpoch: [14/70], Progress: 73.12 %, Steps: 1278, Current Learning Rate: 0.0003160, \u001b[91mTrain Loss: 5.692\n",
      "\u001b[0m\u001b[1mEpoch: [14/70], Progress: 74.19 %, Steps: 1279, Current Learning Rate: 0.0003162, \u001b[96mTrain Loss: 5.567\n",
      "\u001b[0m\u001b[1mEpoch: [14/70], Progress: 75.27 %, Steps: 1280, Current Learning Rate: 0.0003165, \u001b[91mTrain Loss: 5.591\n",
      "\u001b[0m\u001b[1mEpoch: [14/70], Progress: 76.34 %, Steps: 1281, Current Learning Rate: 0.0003167, \u001b[91mTrain Loss: 5.630\n",
      "\u001b[0m\u001b[1mEpoch: [14/70], Progress: 77.42 %, Steps: 1282, Current Learning Rate: 0.0003170, \u001b[91mTrain Loss: 5.756\n",
      "\u001b[0m\u001b[1mEpoch: [14/70], Progress: 78.49 %, Steps: 1283, Current Learning Rate: 0.0003172, \u001b[96mTrain Loss: 5.640\n",
      "\u001b[0m\u001b[1mEpoch: [14/70], Progress: 79.57 %, Steps: 1284, Current Learning Rate: 0.0003175, \u001b[96mTrain Loss: 5.630\n",
      "\u001b[0m\u001b[1mEpoch: [14/70], Progress: 80.65 %, Steps: 1285, Current Learning Rate: 0.0003177, \u001b[96mTrain Loss: 5.591\n",
      "\u001b[0m\u001b[1mEpoch: [14/70], Progress: 81.72 %, Steps: 1286, Current Learning Rate: 0.0003180, \u001b[91mTrain Loss: 5.620\n",
      "\u001b[0m\u001b[1mEpoch: [14/70], Progress: 82.80 %, Steps: 1287, Current Learning Rate: 0.0003182, \u001b[91mTrain Loss: 5.739\n",
      "\u001b[0m\u001b[1mEpoch: [14/70], Progress: 83.87 %, Steps: 1288, Current Learning Rate: 0.0003185, \u001b[96mTrain Loss: 5.551\n",
      "\u001b[0m\u001b[1mEpoch: [14/70], Progress: 84.95 %, Steps: 1289, Current Learning Rate: 0.0003187, \u001b[91mTrain Loss: 5.755\n",
      "\u001b[0m\u001b[1mEpoch: [14/70], Progress: 86.02 %, Steps: 1290, Current Learning Rate: 0.0003189, \u001b[96mTrain Loss: 5.569\n",
      "\u001b[0m\u001b[1mEpoch: [14/70], Progress: 87.10 %, Steps: 1291, Current Learning Rate: 0.0003192, \u001b[91mTrain Loss: 5.589\n",
      "\u001b[0m\u001b[1mEpoch: [14/70], Progress: 88.17 %, Steps: 1292, Current Learning Rate: 0.0003194, \u001b[96mTrain Loss: 5.511\n",
      "\u001b[0m\u001b[1mEpoch: [14/70], Progress: 89.25 %, Steps: 1293, Current Learning Rate: 0.0003197, \u001b[91mTrain Loss: 5.601\n",
      "\u001b[0m\u001b[1mEpoch: [14/70], Progress: 90.32 %, Steps: 1294, Current Learning Rate: 0.0003199, \u001b[96mTrain Loss: 5.565\n",
      "\u001b[0m\u001b[1mEpoch: [14/70], Progress: 91.40 %, Steps: 1295, Current Learning Rate: 0.0003202, \u001b[91mTrain Loss: 5.921\n",
      "\u001b[0m\u001b[1mEpoch: [14/70], Progress: 92.47 %, Steps: 1296, Current Learning Rate: 0.0003204, \u001b[96mTrain Loss: 5.779\n",
      "\u001b[0m\u001b[1mEpoch: [14/70], Progress: 93.55 %, Steps: 1297, Current Learning Rate: 0.0003207, \u001b[96mTrain Loss: 5.500\n",
      "\u001b[0m\u001b[1mEpoch: [14/70], Progress: 94.62 %, Steps: 1298, Current Learning Rate: 0.0003209, \u001b[96mTrain Loss: 5.431\n",
      "\u001b[0m\u001b[1mEpoch: [14/70], Progress: 95.70 %, Steps: 1299, Current Learning Rate: 0.0003212, \u001b[91mTrain Loss: 5.765\n",
      "\u001b[0m\u001b[1mEpoch: [14/70], Progress: 96.77 %, Steps: 1300, Current Learning Rate: 0.0003214, \u001b[96mTrain Loss: 5.492\n",
      "\u001b[0m\u001b[1mEpoch: [14/70], Progress: 97.85 %, Steps: 1301, Current Learning Rate: 0.0003217, \u001b[91mTrain Loss: 5.579\n",
      "\u001b[0m\u001b[1mEpoch: [14/70], Progress: 98.92 %, Steps: 1302, Current Learning Rate: 0.0003219, \u001b[96mTrain Loss: 5.519\n",
      "\u001b[1m\u001b[93mComputing Validation Loss...\n",
      "\u001b[1mEpoch 14 Completed! Average Train Loss: 5.653, Average Validation Loss: 5.030\n",
      "Best Model saved in best_model/best_model.pt.\n",
      "\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m= Epoch [15/70] ==========================================\n",
      "\u001b[0m\u001b[1mEpoch: [15/70], Progress: 0.00 %, Steps: 1303, Current Learning Rate: 0.0003222, \u001b[91mTrain Loss: 5.560\n",
      "\u001b[0m\u001b[1mEpoch: [15/70], Progress: 1.08 %, Steps: 1304, Current Learning Rate: 0.0003224, \u001b[91mTrain Loss: 5.596\n",
      "\u001b[0m\u001b[1mEpoch: [15/70], Progress: 2.15 %, Steps: 1305, Current Learning Rate: 0.0003227, \u001b[91mTrain Loss: 5.644\n",
      "\u001b[0m\u001b[1mEpoch: [15/70], Progress: 3.23 %, Steps: 1306, Current Learning Rate: 0.0003229, \u001b[96mTrain Loss: 5.536\n",
      "\u001b[0m\u001b[1mEpoch: [15/70], Progress: 4.30 %, Steps: 1307, Current Learning Rate: 0.0003231, \u001b[96mTrain Loss: 5.501\n",
      "\u001b[0m\u001b[1mEpoch: [15/70], Progress: 5.38 %, Steps: 1308, Current Learning Rate: 0.0003234, \u001b[91mTrain Loss: 5.717\n",
      "\u001b[0m\u001b[1mEpoch: [15/70], Progress: 6.45 %, Steps: 1309, Current Learning Rate: 0.0003236, \u001b[96mTrain Loss: 5.448\n",
      "\u001b[0m\u001b[1mEpoch: [15/70], Progress: 7.53 %, Steps: 1310, Current Learning Rate: 0.0003239, \u001b[91mTrain Loss: 5.464\n",
      "\u001b[0m\u001b[1mEpoch: [15/70], Progress: 8.60 %, Steps: 1311, Current Learning Rate: 0.0003241, \u001b[91mTrain Loss: 5.541\n",
      "\u001b[0m\u001b[1mEpoch: [15/70], Progress: 9.68 %, Steps: 1312, Current Learning Rate: 0.0003244, \u001b[96mTrain Loss: 5.468\n",
      "\u001b[0m\u001b[1mEpoch: [15/70], Progress: 10.75 %, Steps: 1313, Current Learning Rate: 0.0003246, \u001b[96mTrain Loss: 5.407\n",
      "\u001b[0m\u001b[1mEpoch: [15/70], Progress: 11.83 %, Steps: 1314, Current Learning Rate: 0.0003249, \u001b[91mTrain Loss: 5.606\n",
      "\u001b[0m\u001b[1mEpoch: [15/70], Progress: 12.90 %, Steps: 1315, Current Learning Rate: 0.0003251, \u001b[96mTrain Loss: 5.428\n",
      "\u001b[0m\u001b[1mEpoch: [15/70], Progress: 13.98 %, Steps: 1316, Current Learning Rate: 0.0003254, \u001b[91mTrain Loss: 5.613\n",
      "\u001b[0m\u001b[1mEpoch: [15/70], Progress: 15.05 %, Steps: 1317, Current Learning Rate: 0.0003256, \u001b[96mTrain Loss: 5.399\n",
      "\u001b[0m\u001b[1mEpoch: [15/70], Progress: 16.13 %, Steps: 1318, Current Learning Rate: 0.0003259, \u001b[91mTrain Loss: 5.585\n",
      "\u001b[0m\u001b[1mEpoch: [15/70], Progress: 17.20 %, Steps: 1319, Current Learning Rate: 0.0003261, \u001b[96mTrain Loss: 5.380\n",
      "\u001b[0m\u001b[1mEpoch: [15/70], Progress: 18.28 %, Steps: 1320, Current Learning Rate: 0.0003264, \u001b[91mTrain Loss: 5.586\n",
      "\u001b[0m\u001b[1mEpoch: [15/70], Progress: 19.35 %, Steps: 1321, Current Learning Rate: 0.0003266, \u001b[96mTrain Loss: 5.443\n",
      "\u001b[0m\u001b[1mEpoch: [15/70], Progress: 20.43 %, Steps: 1322, Current Learning Rate: 0.0003269, \u001b[91mTrain Loss: 5.639\n",
      "\u001b[0m\u001b[1mEpoch: [15/70], Progress: 21.51 %, Steps: 1323, Current Learning Rate: 0.0003271, \u001b[91mTrain Loss: 5.713\n",
      "\u001b[0m\u001b[1mEpoch: [15/70], Progress: 22.58 %, Steps: 1324, Current Learning Rate: 0.0003273, \u001b[96mTrain Loss: 5.444\n",
      "\u001b[0m\u001b[1mEpoch: [15/70], Progress: 23.66 %, Steps: 1325, Current Learning Rate: 0.0003276, \u001b[91mTrain Loss: 5.466\n",
      "\u001b[0m\u001b[1mEpoch: [15/70], Progress: 24.73 %, Steps: 1326, Current Learning Rate: 0.0003278, \u001b[91mTrain Loss: 5.489\n",
      "\u001b[0m\u001b[1mEpoch: [15/70], Progress: 25.81 %, Steps: 1327, Current Learning Rate: 0.0003281, \u001b[96mTrain Loss: 5.399\n",
      "\u001b[0m\u001b[1mEpoch: [15/70], Progress: 26.88 %, Steps: 1328, Current Learning Rate: 0.0003283, \u001b[91mTrain Loss: 5.628\n",
      "\u001b[0m\u001b[1mEpoch: [15/70], Progress: 27.96 %, Steps: 1329, Current Learning Rate: 0.0003286, \u001b[96mTrain Loss: 5.379\n",
      "\u001b[0m\u001b[1mEpoch: [15/70], Progress: 29.03 %, Steps: 1330, Current Learning Rate: 0.0003288, \u001b[91mTrain Loss: 5.484\n",
      "\u001b[0m\u001b[1mEpoch: [15/70], Progress: 30.11 %, Steps: 1331, Current Learning Rate: 0.0003291, \u001b[96mTrain Loss: 5.480\n",
      "\u001b[0m\u001b[1mEpoch: [15/70], Progress: 31.18 %, Steps: 1332, Current Learning Rate: 0.0003293, \u001b[96mTrain Loss: 5.395\n",
      "\u001b[0m\u001b[1mEpoch: [15/70], Progress: 32.26 %, Steps: 1333, Current Learning Rate: 0.0003296, \u001b[91mTrain Loss: 5.540\n",
      "\u001b[0m\u001b[1mEpoch: [15/70], Progress: 33.33 %, Steps: 1334, Current Learning Rate: 0.0003298, \u001b[91mTrain Loss: 5.592\n",
      "\u001b[0m\u001b[1mEpoch: [15/70], Progress: 34.41 %, Steps: 1335, Current Learning Rate: 0.0003301, \u001b[91mTrain Loss: 5.748\n",
      "\u001b[0m\u001b[1mEpoch: [15/70], Progress: 35.48 %, Steps: 1336, Current Learning Rate: 0.0003303, \u001b[96mTrain Loss: 5.425\n",
      "\u001b[0m\u001b[1mEpoch: [15/70], Progress: 36.56 %, Steps: 1337, Current Learning Rate: 0.0003306, \u001b[96mTrain Loss: 5.265\n",
      "\u001b[0m\u001b[1mEpoch: [15/70], Progress: 37.63 %, Steps: 1338, Current Learning Rate: 0.0003308, \u001b[91mTrain Loss: 5.584\n",
      "\u001b[0m\u001b[1mEpoch: [15/70], Progress: 38.71 %, Steps: 1339, Current Learning Rate: 0.0003311, \u001b[96mTrain Loss: 5.583\n",
      "\u001b[0m\u001b[1mEpoch: [15/70], Progress: 39.78 %, Steps: 1340, Current Learning Rate: 0.0003313, \u001b[96mTrain Loss: 5.485\n",
      "\u001b[0m\u001b[1mEpoch: [15/70], Progress: 40.86 %, Steps: 1341, Current Learning Rate: 0.0003315, \u001b[91mTrain Loss: 5.561\n",
      "\u001b[0m\u001b[1mEpoch: [15/70], Progress: 41.94 %, Steps: 1342, Current Learning Rate: 0.0003318, \u001b[91mTrain Loss: 5.649\n",
      "\u001b[0m\u001b[1mEpoch: [15/70], Progress: 43.01 %, Steps: 1343, Current Learning Rate: 0.0003320, \u001b[96mTrain Loss: 5.405\n",
      "\u001b[0m\u001b[1mEpoch: [15/70], Progress: 44.09 %, Steps: 1344, Current Learning Rate: 0.0003323, \u001b[96mTrain Loss: 5.350\n",
      "\u001b[0m\u001b[1mEpoch: [15/70], Progress: 45.16 %, Steps: 1345, Current Learning Rate: 0.0003325, \u001b[91mTrain Loss: 5.649\n",
      "\u001b[0m\u001b[1mEpoch: [15/70], Progress: 46.24 %, Steps: 1346, Current Learning Rate: 0.0003328, \u001b[96mTrain Loss: 5.559\n",
      "\u001b[0m\u001b[1mEpoch: [15/70], Progress: 47.31 %, Steps: 1347, Current Learning Rate: 0.0003330, \u001b[96mTrain Loss: 5.547\n",
      "\u001b[0m\u001b[1mEpoch: [15/70], Progress: 48.39 %, Steps: 1348, Current Learning Rate: 0.0003333, \u001b[91mTrain Loss: 5.705\n",
      "\u001b[0m\u001b[1mEpoch: [15/70], Progress: 49.46 %, Steps: 1349, Current Learning Rate: 0.0003335, \u001b[96mTrain Loss: 5.432\n",
      "\u001b[0m\u001b[1mEpoch: [15/70], Progress: 50.54 %, Steps: 1350, Current Learning Rate: 0.0003338, \u001b[91mTrain Loss: 5.546\n",
      "\u001b[0m\u001b[1mEpoch: [15/70], Progress: 51.61 %, Steps: 1351, Current Learning Rate: 0.0003340, \u001b[96mTrain Loss: 5.495\n",
      "\u001b[0m\u001b[1mEpoch: [15/70], Progress: 52.69 %, Steps: 1352, Current Learning Rate: 0.0003343, \u001b[91mTrain Loss: 5.545\n",
      "\u001b[0m\u001b[1mEpoch: [15/70], Progress: 53.76 %, Steps: 1353, Current Learning Rate: 0.0003345, \u001b[91mTrain Loss: 5.563\n",
      "\u001b[0m\u001b[1mEpoch: [15/70], Progress: 54.84 %, Steps: 1354, Current Learning Rate: 0.0003348, \u001b[96mTrain Loss: 5.529\n",
      "\u001b[0m\u001b[1mEpoch: [15/70], Progress: 55.91 %, Steps: 1355, Current Learning Rate: 0.0003350, \u001b[91mTrain Loss: 5.664\n",
      "\u001b[0m\u001b[1mEpoch: [15/70], Progress: 56.99 %, Steps: 1356, Current Learning Rate: 0.0003353, \u001b[91mTrain Loss: 5.801\n",
      "\u001b[0m\u001b[1mEpoch: [15/70], Progress: 58.06 %, Steps: 1357, Current Learning Rate: 0.0003355, \u001b[96mTrain Loss: 5.599\n",
      "\u001b[0m\u001b[1mEpoch: [15/70], Progress: 59.14 %, Steps: 1358, Current Learning Rate: 0.0003357, \u001b[96mTrain Loss: 5.437\n",
      "\u001b[0m\u001b[1mEpoch: [15/70], Progress: 60.22 %, Steps: 1359, Current Learning Rate: 0.0003360, \u001b[91mTrain Loss: 5.588\n",
      "\u001b[0m\u001b[1mEpoch: [15/70], Progress: 61.29 %, Steps: 1360, Current Learning Rate: 0.0003362, \u001b[91mTrain Loss: 5.694\n",
      "\u001b[0m\u001b[1mEpoch: [15/70], Progress: 62.37 %, Steps: 1361, Current Learning Rate: 0.0003365, \u001b[96mTrain Loss: 5.585\n",
      "\u001b[0m\u001b[1mEpoch: [15/70], Progress: 63.44 %, Steps: 1362, Current Learning Rate: 0.0003367, \u001b[96mTrain Loss: 5.506\n",
      "\u001b[0m\u001b[1mEpoch: [15/70], Progress: 64.52 %, Steps: 1363, Current Learning Rate: 0.0003370, \u001b[96mTrain Loss: 5.228\n",
      "\u001b[0m\u001b[1mEpoch: [15/70], Progress: 65.59 %, Steps: 1364, Current Learning Rate: 0.0003372, \u001b[91mTrain Loss: 5.480\n",
      "\u001b[0m\u001b[1mEpoch: [15/70], Progress: 66.67 %, Steps: 1365, Current Learning Rate: 0.0003375, \u001b[91mTrain Loss: 5.505\n",
      "\u001b[0m\u001b[1mEpoch: [15/70], Progress: 67.74 %, Steps: 1366, Current Learning Rate: 0.0003377, \u001b[91mTrain Loss: 5.533\n",
      "\u001b[0m\u001b[1mEpoch: [15/70], Progress: 68.82 %, Steps: 1367, Current Learning Rate: 0.0003380, \u001b[91mTrain Loss: 5.566\n",
      "\u001b[0m\u001b[1mEpoch: [15/70], Progress: 69.89 %, Steps: 1368, Current Learning Rate: 0.0003382, \u001b[96mTrain Loss: 5.550\n",
      "\u001b[0m\u001b[1mEpoch: [15/70], Progress: 70.97 %, Steps: 1369, Current Learning Rate: 0.0003385, \u001b[91mTrain Loss: 5.800\n",
      "\u001b[0m\u001b[1mEpoch: [15/70], Progress: 72.04 %, Steps: 1370, Current Learning Rate: 0.0003387, \u001b[96mTrain Loss: 5.256\n",
      "\u001b[0m\u001b[1mEpoch: [15/70], Progress: 73.12 %, Steps: 1371, Current Learning Rate: 0.0003390, \u001b[91mTrain Loss: 5.464\n",
      "\u001b[0m\u001b[1mEpoch: [15/70], Progress: 74.19 %, Steps: 1372, Current Learning Rate: 0.0003392, \u001b[91mTrain Loss: 5.517\n",
      "\u001b[0m\u001b[1mEpoch: [15/70], Progress: 75.27 %, Steps: 1373, Current Learning Rate: 0.0003395, \u001b[96mTrain Loss: 5.498\n",
      "\u001b[0m\u001b[1mEpoch: [15/70], Progress: 76.34 %, Steps: 1374, Current Learning Rate: 0.0003397, \u001b[96mTrain Loss: 5.460\n",
      "\u001b[0m\u001b[1mEpoch: [15/70], Progress: 77.42 %, Steps: 1375, Current Learning Rate: 0.0003399, \u001b[96mTrain Loss: 5.455\n",
      "\u001b[0m\u001b[1mEpoch: [15/70], Progress: 78.49 %, Steps: 1376, Current Learning Rate: 0.0003402, \u001b[91mTrain Loss: 5.486\n",
      "\u001b[0m\u001b[1mEpoch: [15/70], Progress: 79.57 %, Steps: 1377, Current Learning Rate: 0.0003404, \u001b[96mTrain Loss: 5.255\n",
      "\u001b[0m\u001b[1mEpoch: [15/70], Progress: 80.65 %, Steps: 1378, Current Learning Rate: 0.0003407, \u001b[91mTrain Loss: 5.505\n",
      "\u001b[0m\u001b[1mEpoch: [15/70], Progress: 81.72 %, Steps: 1379, Current Learning Rate: 0.0003409, \u001b[96mTrain Loss: 5.235\n",
      "\u001b[0m\u001b[1mEpoch: [15/70], Progress: 82.80 %, Steps: 1380, Current Learning Rate: 0.0003412, \u001b[91mTrain Loss: 5.681\n",
      "\u001b[0m\u001b[1mEpoch: [15/70], Progress: 83.87 %, Steps: 1381, Current Learning Rate: 0.0003414, \u001b[96mTrain Loss: 5.310\n",
      "\u001b[0m\u001b[1mEpoch: [15/70], Progress: 84.95 %, Steps: 1382, Current Learning Rate: 0.0003417, \u001b[91mTrain Loss: 5.566\n",
      "\u001b[0m\u001b[1mEpoch: [15/70], Progress: 86.02 %, Steps: 1383, Current Learning Rate: 0.0003419, \u001b[91mTrain Loss: 5.634\n",
      "\u001b[0m\u001b[1mEpoch: [15/70], Progress: 87.10 %, Steps: 1384, Current Learning Rate: 0.0003422, \u001b[96mTrain Loss: 5.628\n",
      "\u001b[0m\u001b[1mEpoch: [15/70], Progress: 88.17 %, Steps: 1385, Current Learning Rate: 0.0003424, \u001b[96mTrain Loss: 5.542\n",
      "\u001b[0m\u001b[1mEpoch: [15/70], Progress: 89.25 %, Steps: 1386, Current Learning Rate: 0.0003427, \u001b[96mTrain Loss: 5.508\n",
      "\u001b[0m\u001b[1mEpoch: [15/70], Progress: 90.32 %, Steps: 1387, Current Learning Rate: 0.0003429, \u001b[96mTrain Loss: 5.392\n",
      "\u001b[0m\u001b[1mEpoch: [15/70], Progress: 91.40 %, Steps: 1388, Current Learning Rate: 0.0003432, \u001b[91mTrain Loss: 5.750\n",
      "\u001b[0m\u001b[1mEpoch: [15/70], Progress: 92.47 %, Steps: 1389, Current Learning Rate: 0.0003434, \u001b[96mTrain Loss: 5.432\n",
      "\u001b[0m\u001b[1mEpoch: [15/70], Progress: 93.55 %, Steps: 1390, Current Learning Rate: 0.0003437, \u001b[91mTrain Loss: 5.518\n",
      "\u001b[0m\u001b[1mEpoch: [15/70], Progress: 94.62 %, Steps: 1391, Current Learning Rate: 0.0003439, \u001b[96mTrain Loss: 5.370\n",
      "\u001b[0m\u001b[1mEpoch: [15/70], Progress: 95.70 %, Steps: 1392, Current Learning Rate: 0.0003441, \u001b[91mTrain Loss: 5.390\n",
      "\u001b[0m\u001b[1mEpoch: [15/70], Progress: 96.77 %, Steps: 1393, Current Learning Rate: 0.0003444, \u001b[91mTrain Loss: 5.524\n",
      "\u001b[0m\u001b[1mEpoch: [15/70], Progress: 97.85 %, Steps: 1394, Current Learning Rate: 0.0003446, \u001b[91mTrain Loss: 5.586\n",
      "\u001b[0m\u001b[1mEpoch: [15/70], Progress: 98.92 %, Steps: 1395, Current Learning Rate: 0.0003449, \u001b[96mTrain Loss: 5.428\n",
      "\u001b[1m\u001b[93mComputing Validation Loss...\n",
      "\u001b[1mEpoch 15 Completed! Average Train Loss: 5.517, Average Validation Loss: 4.953\n",
      "Best Model saved in best_model/best_model.pt.\n",
      "\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m= Epoch [16/70] ==========================================\n",
      "\u001b[0m\u001b[1mEpoch: [16/70], Progress: 0.00 %, Steps: 1396, Current Learning Rate: 0.0003451, \u001b[91mTrain Loss: 5.513\n",
      "\u001b[0m\u001b[1mEpoch: [16/70], Progress: 1.08 %, Steps: 1397, Current Learning Rate: 0.0003454, \u001b[96mTrain Loss: 5.470\n",
      "\u001b[0m\u001b[1mEpoch: [16/70], Progress: 2.15 %, Steps: 1398, Current Learning Rate: 0.0003456, \u001b[96mTrain Loss: 5.444\n",
      "\u001b[0m\u001b[1mEpoch: [16/70], Progress: 3.23 %, Steps: 1399, Current Learning Rate: 0.0003459, \u001b[96mTrain Loss: 5.305\n",
      "\u001b[0m\u001b[1mEpoch: [16/70], Progress: 4.30 %, Steps: 1400, Current Learning Rate: 0.0003461, \u001b[91mTrain Loss: 5.545\n",
      "\u001b[0m\u001b[1mEpoch: [16/70], Progress: 5.38 %, Steps: 1401, Current Learning Rate: 0.0003464, \u001b[91mTrain Loss: 5.576\n",
      "\u001b[0m\u001b[1mEpoch: [16/70], Progress: 6.45 %, Steps: 1402, Current Learning Rate: 0.0003466, \u001b[96mTrain Loss: 5.305\n",
      "\u001b[0m\u001b[1mEpoch: [16/70], Progress: 7.53 %, Steps: 1403, Current Learning Rate: 0.0003469, \u001b[96mTrain Loss: 5.257\n",
      "\u001b[0m\u001b[1mEpoch: [16/70], Progress: 8.60 %, Steps: 1404, Current Learning Rate: 0.0003471, \u001b[91mTrain Loss: 5.312\n",
      "\u001b[0m\u001b[1mEpoch: [16/70], Progress: 9.68 %, Steps: 1405, Current Learning Rate: 0.0003474, \u001b[96mTrain Loss: 5.290\n",
      "\u001b[0m\u001b[1mEpoch: [16/70], Progress: 10.75 %, Steps: 1406, Current Learning Rate: 0.0003476, \u001b[91mTrain Loss: 5.459\n",
      "\u001b[0m\u001b[1mEpoch: [16/70], Progress: 11.83 %, Steps: 1407, Current Learning Rate: 0.0003479, \u001b[96mTrain Loss: 5.436\n",
      "\u001b[0m\u001b[1mEpoch: [16/70], Progress: 12.90 %, Steps: 1408, Current Learning Rate: 0.0003481, \u001b[96mTrain Loss: 5.309\n",
      "\u001b[0m\u001b[1mEpoch: [16/70], Progress: 13.98 %, Steps: 1409, Current Learning Rate: 0.0003483, \u001b[96mTrain Loss: 5.268\n",
      "\u001b[0m\u001b[1mEpoch: [16/70], Progress: 15.05 %, Steps: 1410, Current Learning Rate: 0.0003486, \u001b[91mTrain Loss: 5.568\n",
      "\u001b[0m\u001b[1mEpoch: [16/70], Progress: 16.13 %, Steps: 1411, Current Learning Rate: 0.0003488, \u001b[96mTrain Loss: 5.216\n",
      "\u001b[0m\u001b[1mEpoch: [16/70], Progress: 17.20 %, Steps: 1412, Current Learning Rate: 0.0003491, \u001b[91mTrain Loss: 5.474\n",
      "\u001b[0m\u001b[1mEpoch: [16/70], Progress: 18.28 %, Steps: 1413, Current Learning Rate: 0.0003493, \u001b[96mTrain Loss: 5.377\n",
      "\u001b[0m\u001b[1mEpoch: [16/70], Progress: 19.35 %, Steps: 1414, Current Learning Rate: 0.0003496, \u001b[96mTrain Loss: 5.187\n",
      "\u001b[0m\u001b[1mEpoch: [16/70], Progress: 20.43 %, Steps: 1415, Current Learning Rate: 0.0003498, \u001b[91mTrain Loss: 5.262\n",
      "\u001b[0m\u001b[1mEpoch: [16/70], Progress: 21.51 %, Steps: 1416, Current Learning Rate: 0.0003501, \u001b[91mTrain Loss: 5.265\n",
      "\u001b[0m\u001b[1mEpoch: [16/70], Progress: 22.58 %, Steps: 1417, Current Learning Rate: 0.0003503, \u001b[91mTrain Loss: 5.371\n",
      "\u001b[0m\u001b[1mEpoch: [16/70], Progress: 23.66 %, Steps: 1418, Current Learning Rate: 0.0003506, \u001b[91mTrain Loss: 5.386\n",
      "\u001b[0m\u001b[1mEpoch: [16/70], Progress: 24.73 %, Steps: 1419, Current Learning Rate: 0.0003508, \u001b[96mTrain Loss: 5.385\n",
      "\u001b[0m\u001b[1mEpoch: [16/70], Progress: 25.81 %, Steps: 1420, Current Learning Rate: 0.0003511, \u001b[96mTrain Loss: 5.322\n",
      "\u001b[0m\u001b[1mEpoch: [16/70], Progress: 26.88 %, Steps: 1421, Current Learning Rate: 0.0003513, \u001b[91mTrain Loss: 5.362\n",
      "\u001b[0m\u001b[1mEpoch: [16/70], Progress: 27.96 %, Steps: 1422, Current Learning Rate: 0.0003516, \u001b[96mTrain Loss: 5.301\n",
      "\u001b[0m\u001b[1mEpoch: [16/70], Progress: 29.03 %, Steps: 1423, Current Learning Rate: 0.0003518, \u001b[91mTrain Loss: 5.310\n",
      "\u001b[0m\u001b[1mEpoch: [16/70], Progress: 30.11 %, Steps: 1424, Current Learning Rate: 0.0003521, \u001b[91mTrain Loss: 5.415\n",
      "\u001b[0m\u001b[1mEpoch: [16/70], Progress: 31.18 %, Steps: 1425, Current Learning Rate: 0.0003523, \u001b[96mTrain Loss: 5.364\n",
      "\u001b[0m\u001b[1mEpoch: [16/70], Progress: 32.26 %, Steps: 1426, Current Learning Rate: 0.0003525, \u001b[96mTrain Loss: 5.338\n",
      "\u001b[0m\u001b[1mEpoch: [16/70], Progress: 33.33 %, Steps: 1427, Current Learning Rate: 0.0003528, \u001b[96mTrain Loss: 5.220\n",
      "\u001b[0m\u001b[1mEpoch: [16/70], Progress: 34.41 %, Steps: 1428, Current Learning Rate: 0.0003530, \u001b[91mTrain Loss: 5.499\n",
      "\u001b[0m\u001b[1mEpoch: [16/70], Progress: 35.48 %, Steps: 1429, Current Learning Rate: 0.0003533, \u001b[91mTrain Loss: 5.501\n",
      "\u001b[0m\u001b[1mEpoch: [16/70], Progress: 36.56 %, Steps: 1430, Current Learning Rate: 0.0003535, \u001b[96mTrain Loss: 5.446\n",
      "\u001b[0m\u001b[1mEpoch: [16/70], Progress: 37.63 %, Steps: 1431, Current Learning Rate: 0.0003538, \u001b[91mTrain Loss: 5.559\n",
      "\u001b[0m\u001b[1mEpoch: [16/70], Progress: 38.71 %, Steps: 1432, Current Learning Rate: 0.0003540, \u001b[96mTrain Loss: 5.396\n",
      "\u001b[0m\u001b[1mEpoch: [16/70], Progress: 39.78 %, Steps: 1433, Current Learning Rate: 0.0003543, \u001b[96mTrain Loss: 5.281\n",
      "\u001b[0m\u001b[1mEpoch: [16/70], Progress: 40.86 %, Steps: 1434, Current Learning Rate: 0.0003545, \u001b[91mTrain Loss: 5.388\n",
      "\u001b[0m\u001b[1mEpoch: [16/70], Progress: 41.94 %, Steps: 1435, Current Learning Rate: 0.0003548, \u001b[91mTrain Loss: 5.401\n",
      "\u001b[0m\u001b[1mEpoch: [16/70], Progress: 43.01 %, Steps: 1436, Current Learning Rate: 0.0003550, \u001b[91mTrain Loss: 5.408\n",
      "\u001b[0m\u001b[1mEpoch: [16/70], Progress: 44.09 %, Steps: 1437, Current Learning Rate: 0.0003553, \u001b[96mTrain Loss: 5.275\n",
      "\u001b[0m\u001b[1mEpoch: [16/70], Progress: 45.16 %, Steps: 1438, Current Learning Rate: 0.0003555, \u001b[91mTrain Loss: 5.351\n",
      "\u001b[0m\u001b[1mEpoch: [16/70], Progress: 46.24 %, Steps: 1439, Current Learning Rate: 0.0003558, \u001b[91mTrain Loss: 5.606\n",
      "\u001b[0m\u001b[1mEpoch: [16/70], Progress: 47.31 %, Steps: 1440, Current Learning Rate: 0.0003560, \u001b[96mTrain Loss: 5.360\n",
      "\u001b[0m\u001b[1mEpoch: [16/70], Progress: 48.39 %, Steps: 1441, Current Learning Rate: 0.0003563, \u001b[91mTrain Loss: 5.367\n",
      "\u001b[0m\u001b[1mEpoch: [16/70], Progress: 49.46 %, Steps: 1442, Current Learning Rate: 0.0003565, \u001b[96mTrain Loss: 5.317\n",
      "\u001b[0m\u001b[1mEpoch: [16/70], Progress: 50.54 %, Steps: 1443, Current Learning Rate: 0.0003567, \u001b[91mTrain Loss: 5.355\n",
      "\u001b[0m\u001b[1mEpoch: [16/70], Progress: 51.61 %, Steps: 1444, Current Learning Rate: 0.0003570, \u001b[96mTrain Loss: 5.283\n",
      "\u001b[0m\u001b[1mEpoch: [16/70], Progress: 52.69 %, Steps: 1445, Current Learning Rate: 0.0003572, \u001b[91mTrain Loss: 5.328\n",
      "\u001b[0m\u001b[1mEpoch: [16/70], Progress: 53.76 %, Steps: 1446, Current Learning Rate: 0.0003575, \u001b[91mTrain Loss: 5.364\n",
      "\u001b[0m\u001b[1mEpoch: [16/70], Progress: 54.84 %, Steps: 1447, Current Learning Rate: 0.0003577, \u001b[96mTrain Loss: 5.272\n",
      "\u001b[0m\u001b[1mEpoch: [16/70], Progress: 55.91 %, Steps: 1448, Current Learning Rate: 0.0003580, \u001b[91mTrain Loss: 5.489\n",
      "\u001b[0m\u001b[1mEpoch: [16/70], Progress: 56.99 %, Steps: 1449, Current Learning Rate: 0.0003582, \u001b[96mTrain Loss: 5.381\n",
      "\u001b[0m\u001b[1mEpoch: [16/70], Progress: 58.06 %, Steps: 1450, Current Learning Rate: 0.0003585, \u001b[96mTrain Loss: 5.194\n",
      "\u001b[0m\u001b[1mEpoch: [16/70], Progress: 59.14 %, Steps: 1451, Current Learning Rate: 0.0003587, \u001b[91mTrain Loss: 5.208\n",
      "\u001b[0m\u001b[1mEpoch: [16/70], Progress: 60.22 %, Steps: 1452, Current Learning Rate: 0.0003590, \u001b[91mTrain Loss: 5.349\n",
      "\u001b[0m\u001b[1mEpoch: [16/70], Progress: 61.29 %, Steps: 1453, Current Learning Rate: 0.0003592, \u001b[91mTrain Loss: 5.441\n",
      "\u001b[0m\u001b[1mEpoch: [16/70], Progress: 62.37 %, Steps: 1454, Current Learning Rate: 0.0003595, \u001b[96mTrain Loss: 5.303\n",
      "\u001b[0m\u001b[1mEpoch: [16/70], Progress: 63.44 %, Steps: 1455, Current Learning Rate: 0.0003597, \u001b[91mTrain Loss: 5.451\n",
      "\u001b[0m\u001b[1mEpoch: [16/70], Progress: 64.52 %, Steps: 1456, Current Learning Rate: 0.0003600, \u001b[96mTrain Loss: 5.373\n",
      "\u001b[0m\u001b[1mEpoch: [16/70], Progress: 65.59 %, Steps: 1457, Current Learning Rate: 0.0003602, \u001b[91mTrain Loss: 5.422\n",
      "\u001b[0m\u001b[1mEpoch: [16/70], Progress: 66.67 %, Steps: 1458, Current Learning Rate: 0.0003605, \u001b[96mTrain Loss: 5.272\n",
      "\u001b[0m\u001b[1mEpoch: [16/70], Progress: 67.74 %, Steps: 1459, Current Learning Rate: 0.0003607, \u001b[91mTrain Loss: 5.573\n",
      "\u001b[0m\u001b[1mEpoch: [16/70], Progress: 68.82 %, Steps: 1460, Current Learning Rate: 0.0003609, \u001b[96mTrain Loss: 5.326\n",
      "\u001b[0m\u001b[1mEpoch: [16/70], Progress: 69.89 %, Steps: 1461, Current Learning Rate: 0.0003612, \u001b[91mTrain Loss: 5.391\n",
      "\u001b[0m\u001b[1mEpoch: [16/70], Progress: 70.97 %, Steps: 1462, Current Learning Rate: 0.0003614, \u001b[96mTrain Loss: 5.338\n",
      "\u001b[0m\u001b[1mEpoch: [16/70], Progress: 72.04 %, Steps: 1463, Current Learning Rate: 0.0003617, \u001b[91mTrain Loss: 5.398\n",
      "\u001b[0m\u001b[1mEpoch: [16/70], Progress: 73.12 %, Steps: 1464, Current Learning Rate: 0.0003619, \u001b[96mTrain Loss: 5.376\n",
      "\u001b[0m\u001b[1mEpoch: [16/70], Progress: 74.19 %, Steps: 1465, Current Learning Rate: 0.0003622, \u001b[91mTrain Loss: 5.443\n",
      "\u001b[0m\u001b[1mEpoch: [16/70], Progress: 75.27 %, Steps: 1466, Current Learning Rate: 0.0003624, \u001b[96mTrain Loss: 5.416\n",
      "\u001b[0m\u001b[1mEpoch: [16/70], Progress: 76.34 %, Steps: 1467, Current Learning Rate: 0.0003627, \u001b[91mTrain Loss: 5.556\n",
      "\u001b[0m\u001b[1mEpoch: [16/70], Progress: 77.42 %, Steps: 1468, Current Learning Rate: 0.0003629, \u001b[96mTrain Loss: 5.436\n",
      "\u001b[0m\u001b[1mEpoch: [16/70], Progress: 78.49 %, Steps: 1469, Current Learning Rate: 0.0003632, \u001b[96mTrain Loss: 5.320\n",
      "\u001b[0m\u001b[1mEpoch: [16/70], Progress: 79.57 %, Steps: 1470, Current Learning Rate: 0.0003634, \u001b[91mTrain Loss: 5.438\n",
      "\u001b[0m\u001b[1mEpoch: [16/70], Progress: 80.65 %, Steps: 1471, Current Learning Rate: 0.0003637, \u001b[96mTrain Loss: 5.350\n",
      "\u001b[0m\u001b[1mEpoch: [16/70], Progress: 81.72 %, Steps: 1472, Current Learning Rate: 0.0003639, \u001b[96mTrain Loss: 5.296\n",
      "\u001b[0m\u001b[1mEpoch: [16/70], Progress: 82.80 %, Steps: 1473, Current Learning Rate: 0.0003642, \u001b[91mTrain Loss: 5.358\n",
      "\u001b[0m\u001b[1mEpoch: [16/70], Progress: 83.87 %, Steps: 1474, Current Learning Rate: 0.0003644, \u001b[96mTrain Loss: 5.253\n",
      "\u001b[0m\u001b[1mEpoch: [16/70], Progress: 84.95 %, Steps: 1475, Current Learning Rate: 0.0003647, \u001b[91mTrain Loss: 5.316\n",
      "\u001b[0m\u001b[1mEpoch: [16/70], Progress: 86.02 %, Steps: 1476, Current Learning Rate: 0.0003649, \u001b[91mTrain Loss: 5.514\n",
      "\u001b[0m\u001b[1mEpoch: [16/70], Progress: 87.10 %, Steps: 1477, Current Learning Rate: 0.0003651, \u001b[96mTrain Loss: 5.382\n",
      "\u001b[0m\u001b[1mEpoch: [16/70], Progress: 88.17 %, Steps: 1478, Current Learning Rate: 0.0003654, \u001b[96mTrain Loss: 5.296\n",
      "\u001b[0m\u001b[1mEpoch: [16/70], Progress: 89.25 %, Steps: 1479, Current Learning Rate: 0.0003656, \u001b[91mTrain Loss: 5.468\n",
      "\u001b[0m\u001b[1mEpoch: [16/70], Progress: 90.32 %, Steps: 1480, Current Learning Rate: 0.0003659, \u001b[96mTrain Loss: 5.365\n",
      "\u001b[0m\u001b[1mEpoch: [16/70], Progress: 91.40 %, Steps: 1481, Current Learning Rate: 0.0003661, \u001b[91mTrain Loss: 5.563\n",
      "\u001b[0m\u001b[1mEpoch: [16/70], Progress: 92.47 %, Steps: 1482, Current Learning Rate: 0.0003664, \u001b[96mTrain Loss: 5.286\n",
      "\u001b[0m\u001b[1mEpoch: [16/70], Progress: 93.55 %, Steps: 1483, Current Learning Rate: 0.0003666, \u001b[91mTrain Loss: 5.444\n",
      "\u001b[0m\u001b[1mEpoch: [16/70], Progress: 94.62 %, Steps: 1484, Current Learning Rate: 0.0003669, \u001b[96mTrain Loss: 5.291\n",
      "\u001b[0m\u001b[1mEpoch: [16/70], Progress: 95.70 %, Steps: 1485, Current Learning Rate: 0.0003671, \u001b[91mTrain Loss: 5.403\n",
      "\u001b[0m\u001b[1mEpoch: [16/70], Progress: 96.77 %, Steps: 1486, Current Learning Rate: 0.0003674, \u001b[96mTrain Loss: 5.308\n",
      "\u001b[0m\u001b[1mEpoch: [16/70], Progress: 97.85 %, Steps: 1487, Current Learning Rate: 0.0003676, \u001b[91mTrain Loss: 5.416\n",
      "\u001b[0m\u001b[1mEpoch: [16/70], Progress: 98.92 %, Steps: 1488, Current Learning Rate: 0.0003679, \u001b[91mTrain Loss: 5.864\n",
      "\u001b[1m\u001b[93mComputing Validation Loss...\n",
      "\u001b[1mEpoch 16 Completed! Average Train Loss: 5.381, Average Validation Loss: 4.782\n",
      "Best Model saved in best_model/best_model.pt.\n",
      "\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m= Epoch [17/70] ==========================================\n",
      "\u001b[0m\u001b[1mEpoch: [17/70], Progress: 0.00 %, Steps: 1489, Current Learning Rate: 0.0003681, \u001b[91mTrain Loss: 5.116\n",
      "\u001b[0m\u001b[1mEpoch: [17/70], Progress: 1.08 %, Steps: 1490, Current Learning Rate: 0.0003684, \u001b[91mTrain Loss: 5.378\n",
      "\u001b[0m\u001b[1mEpoch: [17/70], Progress: 2.15 %, Steps: 1491, Current Learning Rate: 0.0003686, \u001b[96mTrain Loss: 4.970\n",
      "\u001b[0m\u001b[1mEpoch: [17/70], Progress: 3.23 %, Steps: 1492, Current Learning Rate: 0.0003689, \u001b[91mTrain Loss: 5.438\n",
      "\u001b[0m\u001b[1mEpoch: [17/70], Progress: 4.30 %, Steps: 1493, Current Learning Rate: 0.0003691, \u001b[96mTrain Loss: 5.242\n",
      "\u001b[0m\u001b[1mEpoch: [17/70], Progress: 5.38 %, Steps: 1494, Current Learning Rate: 0.0003693, \u001b[96mTrain Loss: 5.167\n",
      "\u001b[0m\u001b[1mEpoch: [17/70], Progress: 6.45 %, Steps: 1495, Current Learning Rate: 0.0003696, \u001b[96mTrain Loss: 5.133\n",
      "\u001b[0m\u001b[1mEpoch: [17/70], Progress: 7.53 %, Steps: 1496, Current Learning Rate: 0.0003698, \u001b[91mTrain Loss: 5.336\n",
      "\u001b[0m\u001b[1mEpoch: [17/70], Progress: 8.60 %, Steps: 1497, Current Learning Rate: 0.0003701, \u001b[96mTrain Loss: 5.286\n",
      "\u001b[0m\u001b[1mEpoch: [17/70], Progress: 9.68 %, Steps: 1498, Current Learning Rate: 0.0003703, \u001b[91mTrain Loss: 5.325\n",
      "\u001b[0m\u001b[1mEpoch: [17/70], Progress: 10.75 %, Steps: 1499, Current Learning Rate: 0.0003706, \u001b[96mTrain Loss: 5.154\n",
      "\u001b[0m\u001b[1mEpoch: [17/70], Progress: 11.83 %, Steps: 1500, Current Learning Rate: 0.0003708, \u001b[91mTrain Loss: 5.372\n",
      "\u001b[0m\u001b[1mEpoch: [17/70], Progress: 12.90 %, Steps: 1501, Current Learning Rate: 0.0003711, \u001b[96mTrain Loss: 5.109\n",
      "\u001b[0m\u001b[1mEpoch: [17/70], Progress: 13.98 %, Steps: 1502, Current Learning Rate: 0.0003713, \u001b[96mTrain Loss: 5.048\n",
      "\u001b[0m\u001b[1mEpoch: [17/70], Progress: 15.05 %, Steps: 1503, Current Learning Rate: 0.0003716, \u001b[91mTrain Loss: 5.580\n",
      "\u001b[0m\u001b[1mEpoch: [17/70], Progress: 16.13 %, Steps: 1504, Current Learning Rate: 0.0003718, \u001b[96mTrain Loss: 5.288\n",
      "\u001b[0m\u001b[1mEpoch: [17/70], Progress: 17.20 %, Steps: 1505, Current Learning Rate: 0.0003721, \u001b[96mTrain Loss: 5.194\n",
      "\u001b[0m\u001b[1mEpoch: [17/70], Progress: 18.28 %, Steps: 1506, Current Learning Rate: 0.0003723, \u001b[96mTrain Loss: 5.174\n",
      "\u001b[0m\u001b[1mEpoch: [17/70], Progress: 19.35 %, Steps: 1507, Current Learning Rate: 0.0003726, \u001b[96mTrain Loss: 5.097\n",
      "\u001b[0m\u001b[1mEpoch: [17/70], Progress: 20.43 %, Steps: 1508, Current Learning Rate: 0.0003728, \u001b[91mTrain Loss: 5.507\n",
      "\u001b[0m\u001b[1mEpoch: [17/70], Progress: 21.51 %, Steps: 1509, Current Learning Rate: 0.0003730, \u001b[96mTrain Loss: 5.074\n",
      "\u001b[0m\u001b[1mEpoch: [17/70], Progress: 22.58 %, Steps: 1510, Current Learning Rate: 0.0003733, \u001b[91mTrain Loss: 5.360\n",
      "\u001b[0m\u001b[1mEpoch: [17/70], Progress: 23.66 %, Steps: 1511, Current Learning Rate: 0.0003735, \u001b[96mTrain Loss: 5.052\n",
      "\u001b[0m\u001b[1mEpoch: [17/70], Progress: 24.73 %, Steps: 1512, Current Learning Rate: 0.0003738, \u001b[91mTrain Loss: 5.141\n",
      "\u001b[0m\u001b[1mEpoch: [17/70], Progress: 25.81 %, Steps: 1513, Current Learning Rate: 0.0003740, \u001b[91mTrain Loss: 5.265\n",
      "\u001b[0m\u001b[1mEpoch: [17/70], Progress: 26.88 %, Steps: 1514, Current Learning Rate: 0.0003743, \u001b[96mTrain Loss: 5.223\n",
      "\u001b[0m\u001b[1mEpoch: [17/70], Progress: 27.96 %, Steps: 1515, Current Learning Rate: 0.0003745, \u001b[91mTrain Loss: 5.357\n",
      "\u001b[0m\u001b[1mEpoch: [17/70], Progress: 29.03 %, Steps: 1516, Current Learning Rate: 0.0003748, \u001b[96mTrain Loss: 5.207\n",
      "\u001b[0m\u001b[1mEpoch: [17/70], Progress: 30.11 %, Steps: 1517, Current Learning Rate: 0.0003750, \u001b[96mTrain Loss: 4.996\n",
      "\u001b[0m\u001b[1mEpoch: [17/70], Progress: 31.18 %, Steps: 1518, Current Learning Rate: 0.0003753, \u001b[91mTrain Loss: 5.357\n",
      "\u001b[0m\u001b[1mEpoch: [17/70], Progress: 32.26 %, Steps: 1519, Current Learning Rate: 0.0003755, \u001b[96mTrain Loss: 5.092\n",
      "\u001b[0m\u001b[1mEpoch: [17/70], Progress: 33.33 %, Steps: 1520, Current Learning Rate: 0.0003758, \u001b[91mTrain Loss: 5.268\n",
      "\u001b[0m\u001b[1mEpoch: [17/70], Progress: 34.41 %, Steps: 1521, Current Learning Rate: 0.0003760, \u001b[96mTrain Loss: 5.228\n",
      "\u001b[0m\u001b[1mEpoch: [17/70], Progress: 35.48 %, Steps: 1522, Current Learning Rate: 0.0003763, \u001b[96mTrain Loss: 5.150\n",
      "\u001b[0m\u001b[1mEpoch: [17/70], Progress: 36.56 %, Steps: 1523, Current Learning Rate: 0.0003765, \u001b[91mTrain Loss: 5.275\n",
      "\u001b[0m\u001b[1mEpoch: [17/70], Progress: 37.63 %, Steps: 1524, Current Learning Rate: 0.0003768, \u001b[91mTrain Loss: 5.288\n",
      "\u001b[0m\u001b[1mEpoch: [17/70], Progress: 38.71 %, Steps: 1525, Current Learning Rate: 0.0003770, \u001b[91mTrain Loss: 5.346\n",
      "\u001b[0m\u001b[1mEpoch: [17/70], Progress: 39.78 %, Steps: 1526, Current Learning Rate: 0.0003772, \u001b[91mTrain Loss: 5.364\n",
      "\u001b[0m\u001b[1mEpoch: [17/70], Progress: 40.86 %, Steps: 1527, Current Learning Rate: 0.0003775, \u001b[96mTrain Loss: 5.324\n",
      "\u001b[0m\u001b[1mEpoch: [17/70], Progress: 41.94 %, Steps: 1528, Current Learning Rate: 0.0003777, \u001b[96mTrain Loss: 5.186\n",
      "\u001b[0m\u001b[1mEpoch: [17/70], Progress: 43.01 %, Steps: 1529, Current Learning Rate: 0.0003780, \u001b[96mTrain Loss: 5.033\n",
      "\u001b[0m\u001b[1mEpoch: [17/70], Progress: 44.09 %, Steps: 1530, Current Learning Rate: 0.0003782, \u001b[91mTrain Loss: 5.439\n",
      "\u001b[0m\u001b[1mEpoch: [17/70], Progress: 45.16 %, Steps: 1531, Current Learning Rate: 0.0003785, \u001b[96mTrain Loss: 5.277\n",
      "\u001b[0m\u001b[1mEpoch: [17/70], Progress: 46.24 %, Steps: 1532, Current Learning Rate: 0.0003787, \u001b[91mTrain Loss: 5.296\n",
      "\u001b[0m\u001b[1mEpoch: [17/70], Progress: 47.31 %, Steps: 1533, Current Learning Rate: 0.0003790, \u001b[91mTrain Loss: 5.343\n",
      "\u001b[0m\u001b[1mEpoch: [17/70], Progress: 48.39 %, Steps: 1534, Current Learning Rate: 0.0003792, \u001b[96mTrain Loss: 5.234\n",
      "\u001b[0m\u001b[1mEpoch: [17/70], Progress: 49.46 %, Steps: 1535, Current Learning Rate: 0.0003795, \u001b[96mTrain Loss: 5.232\n",
      "\u001b[0m\u001b[1mEpoch: [17/70], Progress: 50.54 %, Steps: 1536, Current Learning Rate: 0.0003797, \u001b[91mTrain Loss: 5.233\n",
      "\u001b[0m\u001b[1mEpoch: [17/70], Progress: 51.61 %, Steps: 1537, Current Learning Rate: 0.0003800, \u001b[91mTrain Loss: 5.266\n",
      "\u001b[0m\u001b[1mEpoch: [17/70], Progress: 52.69 %, Steps: 1538, Current Learning Rate: 0.0003802, \u001b[91mTrain Loss: 5.342\n",
      "\u001b[0m\u001b[1mEpoch: [17/70], Progress: 53.76 %, Steps: 1539, Current Learning Rate: 0.0003805, \u001b[96mTrain Loss: 5.133\n",
      "\u001b[0m\u001b[1mEpoch: [17/70], Progress: 54.84 %, Steps: 1540, Current Learning Rate: 0.0003807, \u001b[91mTrain Loss: 5.244\n",
      "\u001b[0m\u001b[1mEpoch: [17/70], Progress: 55.91 %, Steps: 1541, Current Learning Rate: 0.0003810, \u001b[96mTrain Loss: 5.066\n",
      "\u001b[0m\u001b[1mEpoch: [17/70], Progress: 56.99 %, Steps: 1542, Current Learning Rate: 0.0003812, \u001b[91mTrain Loss: 5.136\n",
      "\u001b[0m\u001b[1mEpoch: [17/70], Progress: 58.06 %, Steps: 1543, Current Learning Rate: 0.0003814, \u001b[91mTrain Loss: 5.329\n",
      "\u001b[0m\u001b[1mEpoch: [17/70], Progress: 59.14 %, Steps: 1544, Current Learning Rate: 0.0003817, \u001b[96mTrain Loss: 5.151\n",
      "\u001b[0m\u001b[1mEpoch: [17/70], Progress: 60.22 %, Steps: 1545, Current Learning Rate: 0.0003819, \u001b[91mTrain Loss: 5.319\n",
      "\u001b[0m\u001b[1mEpoch: [17/70], Progress: 61.29 %, Steps: 1546, Current Learning Rate: 0.0003822, \u001b[96mTrain Loss: 5.228\n",
      "\u001b[0m\u001b[1mEpoch: [17/70], Progress: 62.37 %, Steps: 1547, Current Learning Rate: 0.0003824, \u001b[96mTrain Loss: 5.158\n",
      "\u001b[0m\u001b[1mEpoch: [17/70], Progress: 63.44 %, Steps: 1548, Current Learning Rate: 0.0003827, \u001b[91mTrain Loss: 5.217\n",
      "\u001b[0m\u001b[1mEpoch: [17/70], Progress: 64.52 %, Steps: 1549, Current Learning Rate: 0.0003829, \u001b[96mTrain Loss: 5.130\n",
      "\u001b[0m\u001b[1mEpoch: [17/70], Progress: 65.59 %, Steps: 1550, Current Learning Rate: 0.0003832, \u001b[91mTrain Loss: 5.342\n",
      "\u001b[0m\u001b[1mEpoch: [17/70], Progress: 66.67 %, Steps: 1551, Current Learning Rate: 0.0003834, \u001b[96mTrain Loss: 5.191\n",
      "\u001b[0m\u001b[1mEpoch: [17/70], Progress: 67.74 %, Steps: 1552, Current Learning Rate: 0.0003837, \u001b[96mTrain Loss: 5.139\n",
      "\u001b[0m\u001b[1mEpoch: [17/70], Progress: 68.82 %, Steps: 1553, Current Learning Rate: 0.0003839, \u001b[91mTrain Loss: 5.232\n",
      "\u001b[0m\u001b[1mEpoch: [17/70], Progress: 69.89 %, Steps: 1554, Current Learning Rate: 0.0003842, \u001b[91mTrain Loss: 5.429\n",
      "\u001b[0m\u001b[1mEpoch: [17/70], Progress: 70.97 %, Steps: 1555, Current Learning Rate: 0.0003844, \u001b[96mTrain Loss: 5.331\n",
      "\u001b[0m\u001b[1mEpoch: [17/70], Progress: 72.04 %, Steps: 1556, Current Learning Rate: 0.0003847, \u001b[91mTrain Loss: 5.500\n",
      "\u001b[0m\u001b[1mEpoch: [17/70], Progress: 73.12 %, Steps: 1557, Current Learning Rate: 0.0003849, \u001b[96mTrain Loss: 5.396\n",
      "\u001b[0m\u001b[1mEpoch: [17/70], Progress: 74.19 %, Steps: 1558, Current Learning Rate: 0.0003852, \u001b[96mTrain Loss: 5.123\n",
      "\u001b[0m\u001b[1mEpoch: [17/70], Progress: 75.27 %, Steps: 1559, Current Learning Rate: 0.0003854, \u001b[91mTrain Loss: 5.233\n",
      "\u001b[0m\u001b[1mEpoch: [17/70], Progress: 76.34 %, Steps: 1560, Current Learning Rate: 0.0003856, \u001b[91mTrain Loss: 5.262\n",
      "\u001b[0m\u001b[1mEpoch: [17/70], Progress: 77.42 %, Steps: 1561, Current Learning Rate: 0.0003859, \u001b[96mTrain Loss: 5.241\n",
      "\u001b[0m\u001b[1mEpoch: [17/70], Progress: 78.49 %, Steps: 1562, Current Learning Rate: 0.0003861, \u001b[91mTrain Loss: 5.271\n",
      "\u001b[0m\u001b[1mEpoch: [17/70], Progress: 79.57 %, Steps: 1563, Current Learning Rate: 0.0003864, \u001b[91mTrain Loss: 5.285\n",
      "\u001b[0m\u001b[1mEpoch: [17/70], Progress: 80.65 %, Steps: 1564, Current Learning Rate: 0.0003866, \u001b[96mTrain Loss: 5.263\n",
      "\u001b[0m\u001b[1mEpoch: [17/70], Progress: 81.72 %, Steps: 1565, Current Learning Rate: 0.0003869, \u001b[96mTrain Loss: 5.144\n",
      "\u001b[0m\u001b[1mEpoch: [17/70], Progress: 82.80 %, Steps: 1566, Current Learning Rate: 0.0003871, \u001b[91mTrain Loss: 5.311\n",
      "\u001b[0m\u001b[1mEpoch: [17/70], Progress: 83.87 %, Steps: 1567, Current Learning Rate: 0.0003874, \u001b[96mTrain Loss: 5.210\n",
      "\u001b[0m\u001b[1mEpoch: [17/70], Progress: 84.95 %, Steps: 1568, Current Learning Rate: 0.0003876, \u001b[91mTrain Loss: 5.355\n",
      "\u001b[0m\u001b[1mEpoch: [17/70], Progress: 86.02 %, Steps: 1569, Current Learning Rate: 0.0003879, \u001b[96mTrain Loss: 5.263\n",
      "\u001b[0m\u001b[1mEpoch: [17/70], Progress: 87.10 %, Steps: 1570, Current Learning Rate: 0.0003881, \u001b[91mTrain Loss: 5.322\n",
      "\u001b[0m\u001b[1mEpoch: [17/70], Progress: 88.17 %, Steps: 1571, Current Learning Rate: 0.0003884, \u001b[96mTrain Loss: 5.236\n",
      "\u001b[0m\u001b[1mEpoch: [17/70], Progress: 89.25 %, Steps: 1572, Current Learning Rate: 0.0003886, \u001b[96mTrain Loss: 5.162\n",
      "\u001b[0m\u001b[1mEpoch: [17/70], Progress: 90.32 %, Steps: 1573, Current Learning Rate: 0.0003889, \u001b[91mTrain Loss: 5.284\n",
      "\u001b[0m\u001b[1mEpoch: [17/70], Progress: 91.40 %, Steps: 1574, Current Learning Rate: 0.0003891, \u001b[96mTrain Loss: 5.208\n",
      "\u001b[0m\u001b[1mEpoch: [17/70], Progress: 92.47 %, Steps: 1575, Current Learning Rate: 0.0003894, \u001b[91mTrain Loss: 5.414\n",
      "\u001b[0m\u001b[1mEpoch: [17/70], Progress: 93.55 %, Steps: 1576, Current Learning Rate: 0.0003896, \u001b[96mTrain Loss: 5.158\n",
      "\u001b[0m\u001b[1mEpoch: [17/70], Progress: 94.62 %, Steps: 1577, Current Learning Rate: 0.0003898, \u001b[91mTrain Loss: 5.353\n",
      "\u001b[0m\u001b[1mEpoch: [17/70], Progress: 95.70 %, Steps: 1578, Current Learning Rate: 0.0003901, \u001b[96mTrain Loss: 5.183\n",
      "\u001b[0m\u001b[1mEpoch: [17/70], Progress: 96.77 %, Steps: 1579, Current Learning Rate: 0.0003903, \u001b[96mTrain Loss: 5.064\n",
      "\u001b[0m\u001b[1mEpoch: [17/70], Progress: 97.85 %, Steps: 1580, Current Learning Rate: 0.0003906, \u001b[91mTrain Loss: 5.300\n",
      "\u001b[0m\u001b[1mEpoch: [17/70], Progress: 98.92 %, Steps: 1581, Current Learning Rate: 0.0003908, \u001b[91mTrain Loss: 5.413\n",
      "\u001b[1m\u001b[93mComputing Validation Loss...\n",
      "\u001b[1mEpoch 17 Completed! Average Train Loss: 5.246, Average Validation Loss: 4.697\n",
      "Best Model saved in best_model/best_model.pt.\n",
      "\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m= Epoch [18/70] ==========================================\n",
      "\u001b[0m\u001b[1mEpoch: [18/70], Progress: 0.00 %, Steps: 1582, Current Learning Rate: 0.0003911, \u001b[91mTrain Loss: 5.052\n",
      "\u001b[0m\u001b[1mEpoch: [18/70], Progress: 1.08 %, Steps: 1583, Current Learning Rate: 0.0003913, \u001b[91mTrain Loss: 5.151\n",
      "\u001b[0m\u001b[1mEpoch: [18/70], Progress: 2.15 %, Steps: 1584, Current Learning Rate: 0.0003916, \u001b[96mTrain Loss: 5.097\n",
      "\u001b[0m\u001b[1mEpoch: [18/70], Progress: 3.23 %, Steps: 1585, Current Learning Rate: 0.0003918, \u001b[91mTrain Loss: 5.212\n",
      "\u001b[0m\u001b[1mEpoch: [18/70], Progress: 4.30 %, Steps: 1586, Current Learning Rate: 0.0003921, \u001b[91mTrain Loss: 5.232\n",
      "\u001b[0m\u001b[1mEpoch: [18/70], Progress: 5.38 %, Steps: 1587, Current Learning Rate: 0.0003923, \u001b[96mTrain Loss: 5.202\n",
      "\u001b[0m\u001b[1mEpoch: [18/70], Progress: 6.45 %, Steps: 1588, Current Learning Rate: 0.0003926, \u001b[96mTrain Loss: 5.064\n",
      "\u001b[0m\u001b[1mEpoch: [18/70], Progress: 7.53 %, Steps: 1589, Current Learning Rate: 0.0003928, \u001b[91mTrain Loss: 5.104\n",
      "\u001b[0m\u001b[1mEpoch: [18/70], Progress: 8.60 %, Steps: 1590, Current Learning Rate: 0.0003931, \u001b[96mTrain Loss: 5.068\n",
      "\u001b[0m\u001b[1mEpoch: [18/70], Progress: 9.68 %, Steps: 1591, Current Learning Rate: 0.0003933, \u001b[96mTrain Loss: 4.956\n",
      "\u001b[0m\u001b[1mEpoch: [18/70], Progress: 10.75 %, Steps: 1592, Current Learning Rate: 0.0003936, \u001b[91mTrain Loss: 5.063\n",
      "\u001b[0m\u001b[1mEpoch: [18/70], Progress: 11.83 %, Steps: 1593, Current Learning Rate: 0.0003938, \u001b[96mTrain Loss: 4.959\n",
      "\u001b[0m\u001b[1mEpoch: [18/70], Progress: 12.90 %, Steps: 1594, Current Learning Rate: 0.0003940, \u001b[91mTrain Loss: 5.171\n",
      "\u001b[0m\u001b[1mEpoch: [18/70], Progress: 13.98 %, Steps: 1595, Current Learning Rate: 0.0003943, \u001b[91mTrain Loss: 5.289\n",
      "\u001b[0m\u001b[1mEpoch: [18/70], Progress: 15.05 %, Steps: 1596, Current Learning Rate: 0.0003945, \u001b[96mTrain Loss: 5.006\n",
      "\u001b[0m\u001b[1mEpoch: [18/70], Progress: 16.13 %, Steps: 1597, Current Learning Rate: 0.0003948, \u001b[91mTrain Loss: 5.305\n",
      "\u001b[0m\u001b[1mEpoch: [18/70], Progress: 17.20 %, Steps: 1598, Current Learning Rate: 0.0003950, \u001b[96mTrain Loss: 5.198\n",
      "\u001b[0m\u001b[1mEpoch: [18/70], Progress: 18.28 %, Steps: 1599, Current Learning Rate: 0.0003953, \u001b[91mTrain Loss: 5.207\n",
      "\u001b[0m\u001b[1mEpoch: [18/70], Progress: 19.35 %, Steps: 1600, Current Learning Rate: 0.0003955, \u001b[96mTrain Loss: 5.180\n",
      "\u001b[0m\u001b[1mEpoch: [18/70], Progress: 20.43 %, Steps: 1601, Current Learning Rate: 0.0003958, \u001b[96mTrain Loss: 5.013\n",
      "\u001b[0m\u001b[1mEpoch: [18/70], Progress: 21.51 %, Steps: 1602, Current Learning Rate: 0.0003960, \u001b[91mTrain Loss: 5.229\n",
      "\u001b[0m\u001b[1mEpoch: [18/70], Progress: 22.58 %, Steps: 1603, Current Learning Rate: 0.0003963, \u001b[91mTrain Loss: 5.317\n",
      "\u001b[0m\u001b[1mEpoch: [18/70], Progress: 23.66 %, Steps: 1604, Current Learning Rate: 0.0003965, \u001b[96mTrain Loss: 5.178\n",
      "\u001b[0m\u001b[1mEpoch: [18/70], Progress: 24.73 %, Steps: 1605, Current Learning Rate: 0.0003968, \u001b[96mTrain Loss: 5.098\n",
      "\u001b[0m\u001b[1mEpoch: [18/70], Progress: 25.81 %, Steps: 1606, Current Learning Rate: 0.0003970, \u001b[91mTrain Loss: 5.286\n",
      "\u001b[0m\u001b[1mEpoch: [18/70], Progress: 26.88 %, Steps: 1607, Current Learning Rate: 0.0003973, \u001b[96mTrain Loss: 5.260\n",
      "\u001b[0m\u001b[1mEpoch: [18/70], Progress: 27.96 %, Steps: 1608, Current Learning Rate: 0.0003975, \u001b[96mTrain Loss: 5.040\n",
      "\u001b[0m\u001b[1mEpoch: [18/70], Progress: 29.03 %, Steps: 1609, Current Learning Rate: 0.0003978, \u001b[91mTrain Loss: 5.085\n",
      "\u001b[0m\u001b[1mEpoch: [18/70], Progress: 30.11 %, Steps: 1610, Current Learning Rate: 0.0003980, \u001b[96mTrain Loss: 5.074\n",
      "\u001b[0m\u001b[1mEpoch: [18/70], Progress: 31.18 %, Steps: 1611, Current Learning Rate: 0.0003982, \u001b[91mTrain Loss: 5.090\n",
      "\u001b[0m\u001b[1mEpoch: [18/70], Progress: 32.26 %, Steps: 1612, Current Learning Rate: 0.0003985, \u001b[91mTrain Loss: 5.289\n",
      "\u001b[0m\u001b[1mEpoch: [18/70], Progress: 33.33 %, Steps: 1613, Current Learning Rate: 0.0003987, \u001b[96mTrain Loss: 5.148\n",
      "\u001b[0m\u001b[1mEpoch: [18/70], Progress: 34.41 %, Steps: 1614, Current Learning Rate: 0.0003990, \u001b[96mTrain Loss: 5.092\n",
      "\u001b[0m\u001b[1mEpoch: [18/70], Progress: 35.48 %, Steps: 1615, Current Learning Rate: 0.0003992, \u001b[96mTrain Loss: 5.084\n",
      "\u001b[0m\u001b[1mEpoch: [18/70], Progress: 36.56 %, Steps: 1616, Current Learning Rate: 0.0003995, \u001b[96mTrain Loss: 5.060\n",
      "\u001b[0m\u001b[1mEpoch: [18/70], Progress: 37.63 %, Steps: 1617, Current Learning Rate: 0.0003997, \u001b[91mTrain Loss: 5.182\n",
      "\u001b[0m\u001b[1mEpoch: [18/70], Progress: 38.71 %, Steps: 1618, Current Learning Rate: 0.0004000, \u001b[96mTrain Loss: 5.094\n",
      "\u001b[0m\u001b[1mEpoch: [18/70], Progress: 39.78 %, Steps: 1619, Current Learning Rate: 0.0004002, \u001b[91mTrain Loss: 5.119\n",
      "\u001b[0m\u001b[1mEpoch: [18/70], Progress: 40.86 %, Steps: 1620, Current Learning Rate: 0.0004005, \u001b[91mTrain Loss: 5.287\n",
      "\u001b[0m\u001b[1mEpoch: [18/70], Progress: 41.94 %, Steps: 1621, Current Learning Rate: 0.0004007, \u001b[96mTrain Loss: 5.068\n",
      "\u001b[0m\u001b[1mEpoch: [18/70], Progress: 43.01 %, Steps: 1622, Current Learning Rate: 0.0004010, \u001b[91mTrain Loss: 5.140\n",
      "\u001b[0m\u001b[1mEpoch: [18/70], Progress: 44.09 %, Steps: 1623, Current Learning Rate: 0.0004012, \u001b[96mTrain Loss: 4.905\n",
      "\u001b[0m\u001b[1mEpoch: [18/70], Progress: 45.16 %, Steps: 1624, Current Learning Rate: 0.0004015, \u001b[91mTrain Loss: 5.095\n",
      "\u001b[0m\u001b[1mEpoch: [18/70], Progress: 46.24 %, Steps: 1625, Current Learning Rate: 0.0004017, \u001b[96mTrain Loss: 5.081\n",
      "\u001b[0m\u001b[1mEpoch: [18/70], Progress: 47.31 %, Steps: 1626, Current Learning Rate: 0.0004020, \u001b[91mTrain Loss: 5.113\n",
      "\u001b[0m\u001b[1mEpoch: [18/70], Progress: 48.39 %, Steps: 1627, Current Learning Rate: 0.0004022, \u001b[91mTrain Loss: 5.242\n",
      "\u001b[0m\u001b[1mEpoch: [18/70], Progress: 49.46 %, Steps: 1628, Current Learning Rate: 0.0004024, \u001b[96mTrain Loss: 4.890\n",
      "\u001b[0m\u001b[1mEpoch: [18/70], Progress: 50.54 %, Steps: 1629, Current Learning Rate: 0.0004027, \u001b[91mTrain Loss: 5.149\n",
      "\u001b[0m\u001b[1mEpoch: [18/70], Progress: 51.61 %, Steps: 1630, Current Learning Rate: 0.0004029, \u001b[96mTrain Loss: 5.104\n",
      "\u001b[0m\u001b[1mEpoch: [18/70], Progress: 52.69 %, Steps: 1631, Current Learning Rate: 0.0004032, \u001b[96mTrain Loss: 4.931\n",
      "\u001b[0m\u001b[1mEpoch: [18/70], Progress: 53.76 %, Steps: 1632, Current Learning Rate: 0.0004034, \u001b[91mTrain Loss: 5.142\n",
      "\u001b[0m\u001b[1mEpoch: [18/70], Progress: 54.84 %, Steps: 1633, Current Learning Rate: 0.0004037, \u001b[91mTrain Loss: 5.143\n",
      "\u001b[0m\u001b[1mEpoch: [18/70], Progress: 55.91 %, Steps: 1634, Current Learning Rate: 0.0004039, \u001b[91mTrain Loss: 5.168\n",
      "\u001b[0m\u001b[1mEpoch: [18/70], Progress: 56.99 %, Steps: 1635, Current Learning Rate: 0.0004042, \u001b[96mTrain Loss: 5.152\n",
      "\u001b[0m\u001b[1mEpoch: [18/70], Progress: 58.06 %, Steps: 1636, Current Learning Rate: 0.0004044, \u001b[96mTrain Loss: 4.975\n",
      "\u001b[0m\u001b[1mEpoch: [18/70], Progress: 59.14 %, Steps: 1637, Current Learning Rate: 0.0004047, \u001b[91mTrain Loss: 5.094\n",
      "\u001b[0m\u001b[1mEpoch: [18/70], Progress: 60.22 %, Steps: 1638, Current Learning Rate: 0.0004049, \u001b[96mTrain Loss: 4.880\n",
      "\u001b[0m\u001b[1mEpoch: [18/70], Progress: 61.29 %, Steps: 1639, Current Learning Rate: 0.0004052, \u001b[91mTrain Loss: 5.066\n",
      "\u001b[0m\u001b[1mEpoch: [18/70], Progress: 62.37 %, Steps: 1640, Current Learning Rate: 0.0004054, \u001b[91mTrain Loss: 5.099\n",
      "\u001b[0m\u001b[1mEpoch: [18/70], Progress: 63.44 %, Steps: 1641, Current Learning Rate: 0.0004057, \u001b[96mTrain Loss: 4.960\n",
      "\u001b[0m\u001b[1mEpoch: [18/70], Progress: 64.52 %, Steps: 1642, Current Learning Rate: 0.0004059, \u001b[91mTrain Loss: 5.189\n",
      "\u001b[0m\u001b[1mEpoch: [18/70], Progress: 65.59 %, Steps: 1643, Current Learning Rate: 0.0004062, \u001b[91mTrain Loss: 5.247\n",
      "\u001b[0m\u001b[1mEpoch: [18/70], Progress: 66.67 %, Steps: 1644, Current Learning Rate: 0.0004064, \u001b[96mTrain Loss: 5.187\n",
      "\u001b[0m\u001b[1mEpoch: [18/70], Progress: 67.74 %, Steps: 1645, Current Learning Rate: 0.0004066, \u001b[96mTrain Loss: 5.016\n",
      "\u001b[0m\u001b[1mEpoch: [18/70], Progress: 68.82 %, Steps: 1646, Current Learning Rate: 0.0004069, \u001b[91mTrain Loss: 5.248\n",
      "\u001b[0m\u001b[1mEpoch: [18/70], Progress: 69.89 %, Steps: 1647, Current Learning Rate: 0.0004071, \u001b[91mTrain Loss: 5.278\n",
      "\u001b[0m\u001b[1mEpoch: [18/70], Progress: 70.97 %, Steps: 1648, Current Learning Rate: 0.0004074, \u001b[96mTrain Loss: 4.974\n",
      "\u001b[0m\u001b[1mEpoch: [18/70], Progress: 72.04 %, Steps: 1649, Current Learning Rate: 0.0004076, \u001b[91mTrain Loss: 5.175\n",
      "\u001b[0m\u001b[1mEpoch: [18/70], Progress: 73.12 %, Steps: 1650, Current Learning Rate: 0.0004079, \u001b[91mTrain Loss: 5.274\n",
      "\u001b[0m\u001b[1mEpoch: [18/70], Progress: 74.19 %, Steps: 1651, Current Learning Rate: 0.0004081, \u001b[96mTrain Loss: 5.100\n",
      "\u001b[0m\u001b[1mEpoch: [18/70], Progress: 75.27 %, Steps: 1652, Current Learning Rate: 0.0004084, \u001b[96mTrain Loss: 4.983\n",
      "\u001b[0m\u001b[1mEpoch: [18/70], Progress: 76.34 %, Steps: 1653, Current Learning Rate: 0.0004086, \u001b[91mTrain Loss: 5.169\n",
      "\u001b[0m\u001b[1mEpoch: [18/70], Progress: 77.42 %, Steps: 1654, Current Learning Rate: 0.0004089, \u001b[91mTrain Loss: 5.173\n",
      "\u001b[0m\u001b[1mEpoch: [18/70], Progress: 78.49 %, Steps: 1655, Current Learning Rate: 0.0004091, \u001b[96mTrain Loss: 5.163\n",
      "\u001b[0m\u001b[1mEpoch: [18/70], Progress: 79.57 %, Steps: 1656, Current Learning Rate: 0.0004094, \u001b[96mTrain Loss: 5.137\n",
      "\u001b[0m\u001b[1mEpoch: [18/70], Progress: 80.65 %, Steps: 1657, Current Learning Rate: 0.0004096, \u001b[96mTrain Loss: 4.995\n",
      "\u001b[0m\u001b[1mEpoch: [18/70], Progress: 81.72 %, Steps: 1658, Current Learning Rate: 0.0004099, \u001b[91mTrain Loss: 5.157\n",
      "\u001b[0m\u001b[1mEpoch: [18/70], Progress: 82.80 %, Steps: 1659, Current Learning Rate: 0.0004101, \u001b[96mTrain Loss: 5.108\n",
      "\u001b[0m\u001b[1mEpoch: [18/70], Progress: 83.87 %, Steps: 1660, Current Learning Rate: 0.0004104, \u001b[91mTrain Loss: 5.252\n",
      "\u001b[0m\u001b[1mEpoch: [18/70], Progress: 84.95 %, Steps: 1661, Current Learning Rate: 0.0004106, \u001b[96mTrain Loss: 4.930\n",
      "\u001b[0m\u001b[1mEpoch: [18/70], Progress: 86.02 %, Steps: 1662, Current Learning Rate: 0.0004108, \u001b[91mTrain Loss: 5.219\n",
      "\u001b[0m\u001b[1mEpoch: [18/70], Progress: 87.10 %, Steps: 1663, Current Learning Rate: 0.0004111, \u001b[96mTrain Loss: 4.988\n",
      "\u001b[0m\u001b[1mEpoch: [18/70], Progress: 88.17 %, Steps: 1664, Current Learning Rate: 0.0004113, \u001b[91mTrain Loss: 5.121\n",
      "\u001b[0m\u001b[1mEpoch: [18/70], Progress: 89.25 %, Steps: 1665, Current Learning Rate: 0.0004116, \u001b[96mTrain Loss: 4.925\n",
      "\u001b[0m\u001b[1mEpoch: [18/70], Progress: 90.32 %, Steps: 1666, Current Learning Rate: 0.0004118, \u001b[91mTrain Loss: 5.367\n",
      "\u001b[0m\u001b[1mEpoch: [18/70], Progress: 91.40 %, Steps: 1667, Current Learning Rate: 0.0004121, \u001b[96mTrain Loss: 4.915\n",
      "\u001b[0m\u001b[1mEpoch: [18/70], Progress: 92.47 %, Steps: 1668, Current Learning Rate: 0.0004123, \u001b[91mTrain Loss: 5.118\n",
      "\u001b[0m\u001b[1mEpoch: [18/70], Progress: 93.55 %, Steps: 1669, Current Learning Rate: 0.0004126, \u001b[96mTrain Loss: 4.984\n",
      "\u001b[0m\u001b[1mEpoch: [18/70], Progress: 94.62 %, Steps: 1670, Current Learning Rate: 0.0004128, \u001b[91mTrain Loss: 5.380\n",
      "\u001b[0m\u001b[1mEpoch: [18/70], Progress: 95.70 %, Steps: 1671, Current Learning Rate: 0.0004131, \u001b[96mTrain Loss: 5.173\n",
      "\u001b[0m\u001b[1mEpoch: [18/70], Progress: 96.77 %, Steps: 1672, Current Learning Rate: 0.0004133, \u001b[96mTrain Loss: 5.111\n",
      "\u001b[0m\u001b[1mEpoch: [18/70], Progress: 97.85 %, Steps: 1673, Current Learning Rate: 0.0004136, \u001b[91mTrain Loss: 5.148\n",
      "\u001b[0m\u001b[1mEpoch: [18/70], Progress: 98.92 %, Steps: 1674, Current Learning Rate: 0.0004138, \u001b[96mTrain Loss: 5.136\n",
      "\u001b[1m\u001b[93mComputing Validation Loss...\n",
      "\u001b[1mEpoch 18 Completed! Average Train Loss: 5.121, Average Validation Loss: 4.593\n",
      "Best Model saved in best_model/best_model.pt.\n",
      "\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m= Epoch [19/70] ==========================================\n",
      "\u001b[0m\u001b[1mEpoch: [19/70], Progress: 0.00 %, Steps: 1675, Current Learning Rate: 0.0004141, \u001b[91mTrain Loss: 5.004\n",
      "\u001b[0m\u001b[1mEpoch: [19/70], Progress: 1.08 %, Steps: 1676, Current Learning Rate: 0.0004143, \u001b[96mTrain Loss: 4.999\n",
      "\u001b[0m\u001b[1mEpoch: [19/70], Progress: 2.15 %, Steps: 1677, Current Learning Rate: 0.0004146, \u001b[96mTrain Loss: 4.921\n",
      "\u001b[0m\u001b[1mEpoch: [19/70], Progress: 3.23 %, Steps: 1678, Current Learning Rate: 0.0004148, \u001b[91mTrain Loss: 5.173\n",
      "\u001b[0m\u001b[1mEpoch: [19/70], Progress: 4.30 %, Steps: 1679, Current Learning Rate: 0.0004150, \u001b[96mTrain Loss: 4.933\n",
      "\u001b[0m\u001b[1mEpoch: [19/70], Progress: 5.38 %, Steps: 1680, Current Learning Rate: 0.0004153, \u001b[96mTrain Loss: 4.784\n",
      "\u001b[0m\u001b[1mEpoch: [19/70], Progress: 6.45 %, Steps: 1681, Current Learning Rate: 0.0004155, \u001b[91mTrain Loss: 4.898\n",
      "\u001b[0m\u001b[1mEpoch: [19/70], Progress: 7.53 %, Steps: 1682, Current Learning Rate: 0.0004158, \u001b[91mTrain Loss: 4.941\n",
      "\u001b[0m\u001b[1mEpoch: [19/70], Progress: 8.60 %, Steps: 1683, Current Learning Rate: 0.0004160, \u001b[96mTrain Loss: 4.836\n",
      "\u001b[0m\u001b[1mEpoch: [19/70], Progress: 9.68 %, Steps: 1684, Current Learning Rate: 0.0004163, \u001b[91mTrain Loss: 5.154\n",
      "\u001b[0m\u001b[1mEpoch: [19/70], Progress: 10.75 %, Steps: 1685, Current Learning Rate: 0.0004165, \u001b[96mTrain Loss: 5.044\n",
      "\u001b[0m\u001b[1mEpoch: [19/70], Progress: 11.83 %, Steps: 1686, Current Learning Rate: 0.0004168, \u001b[91mTrain Loss: 5.205\n",
      "\u001b[0m\u001b[1mEpoch: [19/70], Progress: 12.90 %, Steps: 1687, Current Learning Rate: 0.0004170, \u001b[96mTrain Loss: 5.148\n",
      "\u001b[0m\u001b[1mEpoch: [19/70], Progress: 13.98 %, Steps: 1688, Current Learning Rate: 0.0004173, \u001b[96mTrain Loss: 5.029\n",
      "\u001b[0m\u001b[1mEpoch: [19/70], Progress: 15.05 %, Steps: 1689, Current Learning Rate: 0.0004175, \u001b[91mTrain Loss: 5.073\n",
      "\u001b[0m\u001b[1mEpoch: [19/70], Progress: 16.13 %, Steps: 1690, Current Learning Rate: 0.0004178, \u001b[91mTrain Loss: 5.196\n",
      "\u001b[0m\u001b[1mEpoch: [19/70], Progress: 17.20 %, Steps: 1691, Current Learning Rate: 0.0004180, \u001b[96mTrain Loss: 4.837\n",
      "\u001b[0m\u001b[1mEpoch: [19/70], Progress: 18.28 %, Steps: 1692, Current Learning Rate: 0.0004183, \u001b[91mTrain Loss: 5.164\n",
      "\u001b[0m\u001b[1mEpoch: [19/70], Progress: 19.35 %, Steps: 1693, Current Learning Rate: 0.0004185, \u001b[96mTrain Loss: 4.876\n",
      "\u001b[0m\u001b[1mEpoch: [19/70], Progress: 20.43 %, Steps: 1694, Current Learning Rate: 0.0004188, \u001b[96mTrain Loss: 4.863\n",
      "\u001b[0m\u001b[1mEpoch: [19/70], Progress: 21.51 %, Steps: 1695, Current Learning Rate: 0.0004190, \u001b[96mTrain Loss: 4.818\n",
      "\u001b[0m\u001b[1mEpoch: [19/70], Progress: 22.58 %, Steps: 1696, Current Learning Rate: 0.0004192, \u001b[91mTrain Loss: 4.972\n",
      "\u001b[0m\u001b[1mEpoch: [19/70], Progress: 23.66 %, Steps: 1697, Current Learning Rate: 0.0004195, \u001b[96mTrain Loss: 4.886\n",
      "\u001b[0m\u001b[1mEpoch: [19/70], Progress: 24.73 %, Steps: 1698, Current Learning Rate: 0.0004197, \u001b[91mTrain Loss: 4.915\n",
      "\u001b[0m\u001b[1mEpoch: [19/70], Progress: 25.81 %, Steps: 1699, Current Learning Rate: 0.0004200, \u001b[96mTrain Loss: 4.747\n",
      "\u001b[0m\u001b[1mEpoch: [19/70], Progress: 26.88 %, Steps: 1700, Current Learning Rate: 0.0004202, \u001b[96mTrain Loss: 4.643\n",
      "\u001b[0m\u001b[1mEpoch: [19/70], Progress: 27.96 %, Steps: 1701, Current Learning Rate: 0.0004205, \u001b[91mTrain Loss: 5.026\n",
      "\u001b[0m\u001b[1mEpoch: [19/70], Progress: 29.03 %, Steps: 1702, Current Learning Rate: 0.0004207, \u001b[91mTrain Loss: 5.039\n",
      "\u001b[0m\u001b[1mEpoch: [19/70], Progress: 30.11 %, Steps: 1703, Current Learning Rate: 0.0004210, \u001b[96mTrain Loss: 4.931\n",
      "\u001b[0m\u001b[1mEpoch: [19/70], Progress: 31.18 %, Steps: 1704, Current Learning Rate: 0.0004212, \u001b[91mTrain Loss: 5.021\n",
      "\u001b[0m\u001b[1mEpoch: [19/70], Progress: 32.26 %, Steps: 1705, Current Learning Rate: 0.0004215, \u001b[96mTrain Loss: 4.939\n",
      "\u001b[0m\u001b[1mEpoch: [19/70], Progress: 33.33 %, Steps: 1706, Current Learning Rate: 0.0004217, \u001b[91mTrain Loss: 4.992\n",
      "\u001b[0m\u001b[1mEpoch: [19/70], Progress: 34.41 %, Steps: 1707, Current Learning Rate: 0.0004220, \u001b[96mTrain Loss: 4.984\n",
      "\u001b[0m\u001b[1mEpoch: [19/70], Progress: 35.48 %, Steps: 1708, Current Learning Rate: 0.0004222, \u001b[91mTrain Loss: 5.057\n",
      "\u001b[0m\u001b[1mEpoch: [19/70], Progress: 36.56 %, Steps: 1709, Current Learning Rate: 0.0004225, \u001b[96mTrain Loss: 4.694\n",
      "\u001b[0m\u001b[1mEpoch: [19/70], Progress: 37.63 %, Steps: 1710, Current Learning Rate: 0.0004227, \u001b[91mTrain Loss: 4.924\n",
      "\u001b[0m\u001b[1mEpoch: [19/70], Progress: 38.71 %, Steps: 1711, Current Learning Rate: 0.0004230, \u001b[91mTrain Loss: 5.057\n",
      "\u001b[0m\u001b[1mEpoch: [19/70], Progress: 39.78 %, Steps: 1712, Current Learning Rate: 0.0004232, \u001b[96mTrain Loss: 4.880\n",
      "\u001b[0m\u001b[1mEpoch: [19/70], Progress: 40.86 %, Steps: 1713, Current Learning Rate: 0.0004234, \u001b[96mTrain Loss: 4.671\n",
      "\u001b[0m\u001b[1mEpoch: [19/70], Progress: 41.94 %, Steps: 1714, Current Learning Rate: 0.0004237, \u001b[91mTrain Loss: 4.971\n",
      "\u001b[0m\u001b[1mEpoch: [19/70], Progress: 43.01 %, Steps: 1715, Current Learning Rate: 0.0004239, \u001b[91mTrain Loss: 5.049\n",
      "\u001b[0m\u001b[1mEpoch: [19/70], Progress: 44.09 %, Steps: 1716, Current Learning Rate: 0.0004242, \u001b[96mTrain Loss: 4.939\n",
      "\u001b[0m\u001b[1mEpoch: [19/70], Progress: 45.16 %, Steps: 1717, Current Learning Rate: 0.0004244, \u001b[91mTrain Loss: 4.977\n",
      "\u001b[0m\u001b[1mEpoch: [19/70], Progress: 46.24 %, Steps: 1718, Current Learning Rate: 0.0004247, \u001b[91mTrain Loss: 4.981\n",
      "\u001b[0m\u001b[1mEpoch: [19/70], Progress: 47.31 %, Steps: 1719, Current Learning Rate: 0.0004249, \u001b[91mTrain Loss: 5.259\n",
      "\u001b[0m\u001b[1mEpoch: [19/70], Progress: 48.39 %, Steps: 1720, Current Learning Rate: 0.0004252, \u001b[96mTrain Loss: 5.039\n",
      "\u001b[0m\u001b[1mEpoch: [19/70], Progress: 49.46 %, Steps: 1721, Current Learning Rate: 0.0004254, \u001b[91mTrain Loss: 5.100\n",
      "\u001b[0m\u001b[1mEpoch: [19/70], Progress: 50.54 %, Steps: 1722, Current Learning Rate: 0.0004257, \u001b[96mTrain Loss: 4.954\n",
      "\u001b[0m\u001b[1mEpoch: [19/70], Progress: 51.61 %, Steps: 1723, Current Learning Rate: 0.0004259, \u001b[91mTrain Loss: 5.013\n",
      "\u001b[0m\u001b[1mEpoch: [19/70], Progress: 52.69 %, Steps: 1724, Current Learning Rate: 0.0004262, \u001b[91mTrain Loss: 5.116\n",
      "\u001b[0m\u001b[1mEpoch: [19/70], Progress: 53.76 %, Steps: 1725, Current Learning Rate: 0.0004264, \u001b[96mTrain Loss: 4.991\n",
      "\u001b[0m\u001b[1mEpoch: [19/70], Progress: 54.84 %, Steps: 1726, Current Learning Rate: 0.0004267, \u001b[96mTrain Loss: 4.877\n",
      "\u001b[0m\u001b[1mEpoch: [19/70], Progress: 55.91 %, Steps: 1727, Current Learning Rate: 0.0004269, \u001b[91mTrain Loss: 5.056\n",
      "\u001b[0m\u001b[1mEpoch: [19/70], Progress: 56.99 %, Steps: 1728, Current Learning Rate: 0.0004272, \u001b[96mTrain Loss: 4.978\n",
      "\u001b[0m\u001b[1mEpoch: [19/70], Progress: 58.06 %, Steps: 1729, Current Learning Rate: 0.0004274, \u001b[96mTrain Loss: 4.950\n",
      "\u001b[0m\u001b[1mEpoch: [19/70], Progress: 59.14 %, Steps: 1730, Current Learning Rate: 0.0004276, \u001b[91mTrain Loss: 4.979\n",
      "\u001b[0m\u001b[1mEpoch: [19/70], Progress: 60.22 %, Steps: 1731, Current Learning Rate: 0.0004279, \u001b[91mTrain Loss: 5.041\n",
      "\u001b[0m\u001b[1mEpoch: [19/70], Progress: 61.29 %, Steps: 1732, Current Learning Rate: 0.0004281, \u001b[96mTrain Loss: 4.824\n",
      "\u001b[0m\u001b[1mEpoch: [19/70], Progress: 62.37 %, Steps: 1733, Current Learning Rate: 0.0004284, \u001b[91mTrain Loss: 4.978\n",
      "\u001b[0m\u001b[1mEpoch: [19/70], Progress: 63.44 %, Steps: 1734, Current Learning Rate: 0.0004286, \u001b[96mTrain Loss: 4.906\n",
      "\u001b[0m\u001b[1mEpoch: [19/70], Progress: 64.52 %, Steps: 1735, Current Learning Rate: 0.0004289, \u001b[91mTrain Loss: 5.259\n",
      "\u001b[0m\u001b[1mEpoch: [19/70], Progress: 65.59 %, Steps: 1736, Current Learning Rate: 0.0004291, \u001b[96mTrain Loss: 5.024\n",
      "\u001b[0m\u001b[1mEpoch: [19/70], Progress: 66.67 %, Steps: 1737, Current Learning Rate: 0.0004294, \u001b[91mTrain Loss: 5.146\n",
      "\u001b[0m\u001b[1mEpoch: [19/70], Progress: 67.74 %, Steps: 1738, Current Learning Rate: 0.0004296, \u001b[96mTrain Loss: 4.948\n",
      "\u001b[0m\u001b[1mEpoch: [19/70], Progress: 68.82 %, Steps: 1739, Current Learning Rate: 0.0004299, \u001b[96mTrain Loss: 4.925\n",
      "\u001b[0m\u001b[1mEpoch: [19/70], Progress: 69.89 %, Steps: 1740, Current Learning Rate: 0.0004301, \u001b[96mTrain Loss: 4.883\n",
      "\u001b[0m\u001b[1mEpoch: [19/70], Progress: 70.97 %, Steps: 1741, Current Learning Rate: 0.0004304, \u001b[91mTrain Loss: 5.123\n",
      "\u001b[0m\u001b[1mEpoch: [19/70], Progress: 72.04 %, Steps: 1742, Current Learning Rate: 0.0004306, \u001b[91mTrain Loss: 5.143\n",
      "\u001b[0m\u001b[1mEpoch: [19/70], Progress: 73.12 %, Steps: 1743, Current Learning Rate: 0.0004309, \u001b[96mTrain Loss: 4.965\n",
      "\u001b[0m\u001b[1mEpoch: [19/70], Progress: 74.19 %, Steps: 1744, Current Learning Rate: 0.0004311, \u001b[91mTrain Loss: 5.053\n",
      "\u001b[0m\u001b[1mEpoch: [19/70], Progress: 75.27 %, Steps: 1745, Current Learning Rate: 0.0004314, \u001b[91mTrain Loss: 5.062\n",
      "\u001b[0m\u001b[1mEpoch: [19/70], Progress: 76.34 %, Steps: 1746, Current Learning Rate: 0.0004316, \u001b[96mTrain Loss: 4.828\n",
      "\u001b[0m\u001b[1mEpoch: [19/70], Progress: 77.42 %, Steps: 1747, Current Learning Rate: 0.0004318, \u001b[91mTrain Loss: 5.133\n",
      "\u001b[0m\u001b[1mEpoch: [19/70], Progress: 78.49 %, Steps: 1748, Current Learning Rate: 0.0004321, \u001b[96mTrain Loss: 5.069\n",
      "\u001b[0m\u001b[1mEpoch: [19/70], Progress: 79.57 %, Steps: 1749, Current Learning Rate: 0.0004323, \u001b[91mTrain Loss: 5.072\n",
      "\u001b[0m\u001b[1mEpoch: [19/70], Progress: 80.65 %, Steps: 1750, Current Learning Rate: 0.0004326, \u001b[96mTrain Loss: 4.872\n",
      "\u001b[0m\u001b[1mEpoch: [19/70], Progress: 81.72 %, Steps: 1751, Current Learning Rate: 0.0004328, \u001b[96mTrain Loss: 4.859\n",
      "\u001b[0m\u001b[1mEpoch: [19/70], Progress: 82.80 %, Steps: 1752, Current Learning Rate: 0.0004331, \u001b[91mTrain Loss: 4.965\n",
      "\u001b[0m\u001b[1mEpoch: [19/70], Progress: 83.87 %, Steps: 1753, Current Learning Rate: 0.0004333, \u001b[91mTrain Loss: 4.985\n",
      "\u001b[0m\u001b[1mEpoch: [19/70], Progress: 84.95 %, Steps: 1754, Current Learning Rate: 0.0004336, \u001b[91mTrain Loss: 4.987\n",
      "\u001b[0m\u001b[1mEpoch: [19/70], Progress: 86.02 %, Steps: 1755, Current Learning Rate: 0.0004338, \u001b[91mTrain Loss: 5.197\n",
      "\u001b[0m\u001b[1mEpoch: [19/70], Progress: 87.10 %, Steps: 1756, Current Learning Rate: 0.0004341, \u001b[96mTrain Loss: 4.750\n",
      "\u001b[0m\u001b[1mEpoch: [19/70], Progress: 88.17 %, Steps: 1757, Current Learning Rate: 0.0004343, \u001b[91mTrain Loss: 5.158\n",
      "\u001b[0m\u001b[1mEpoch: [19/70], Progress: 89.25 %, Steps: 1758, Current Learning Rate: 0.0004346, \u001b[96mTrain Loss: 4.884\n",
      "\u001b[0m\u001b[1mEpoch: [19/70], Progress: 90.32 %, Steps: 1759, Current Learning Rate: 0.0004348, \u001b[91mTrain Loss: 4.892\n",
      "\u001b[0m\u001b[1mEpoch: [19/70], Progress: 91.40 %, Steps: 1760, Current Learning Rate: 0.0004351, \u001b[91mTrain Loss: 4.949\n",
      "\u001b[0m\u001b[1mEpoch: [19/70], Progress: 92.47 %, Steps: 1761, Current Learning Rate: 0.0004353, \u001b[96mTrain Loss: 4.870\n",
      "\u001b[0m\u001b[1mEpoch: [19/70], Progress: 93.55 %, Steps: 1762, Current Learning Rate: 0.0004356, \u001b[91mTrain Loss: 5.219\n",
      "\u001b[0m\u001b[1mEpoch: [19/70], Progress: 94.62 %, Steps: 1763, Current Learning Rate: 0.0004358, \u001b[96mTrain Loss: 5.218\n",
      "\u001b[0m\u001b[1mEpoch: [19/70], Progress: 95.70 %, Steps: 1764, Current Learning Rate: 0.0004360, \u001b[96mTrain Loss: 5.204\n",
      "\u001b[0m\u001b[1mEpoch: [19/70], Progress: 96.77 %, Steps: 1765, Current Learning Rate: 0.0004363, \u001b[96mTrain Loss: 4.937\n",
      "\u001b[0m\u001b[1mEpoch: [19/70], Progress: 97.85 %, Steps: 1766, Current Learning Rate: 0.0004365, \u001b[91mTrain Loss: 5.003\n",
      "\u001b[0m\u001b[1mEpoch: [19/70], Progress: 98.92 %, Steps: 1767, Current Learning Rate: 0.0004368, \u001b[96mTrain Loss: 4.946\n",
      "\u001b[1m\u001b[93mComputing Validation Loss...\n",
      "\u001b[1mEpoch 19 Completed! Average Train Loss: 4.987, Average Validation Loss: 4.437\n",
      "Best Model saved in best_model/best_model.pt.\n",
      "\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m= Epoch [20/70] ==========================================\n",
      "\u001b[0m\u001b[1mEpoch: [20/70], Progress: 0.00 %, Steps: 1768, Current Learning Rate: 0.0004370, \u001b[91mTrain Loss: 4.923\n",
      "\u001b[0m\u001b[1mEpoch: [20/70], Progress: 1.08 %, Steps: 1769, Current Learning Rate: 0.0004373, \u001b[96mTrain Loss: 4.834\n",
      "\u001b[0m\u001b[1mEpoch: [20/70], Progress: 2.15 %, Steps: 1770, Current Learning Rate: 0.0004375, \u001b[91mTrain Loss: 5.048\n",
      "\u001b[0m\u001b[1mEpoch: [20/70], Progress: 3.23 %, Steps: 1771, Current Learning Rate: 0.0004378, \u001b[96mTrain Loss: 4.795\n",
      "\u001b[0m\u001b[1mEpoch: [20/70], Progress: 4.30 %, Steps: 1772, Current Learning Rate: 0.0004380, \u001b[96mTrain Loss: 4.614\n",
      "\u001b[0m\u001b[1mEpoch: [20/70], Progress: 5.38 %, Steps: 1773, Current Learning Rate: 0.0004383, \u001b[91mTrain Loss: 4.807\n",
      "\u001b[0m\u001b[1mEpoch: [20/70], Progress: 6.45 %, Steps: 1774, Current Learning Rate: 0.0004385, \u001b[91mTrain Loss: 4.929\n",
      "\u001b[0m\u001b[1mEpoch: [20/70], Progress: 7.53 %, Steps: 1775, Current Learning Rate: 0.0004388, \u001b[96mTrain Loss: 4.882\n",
      "\u001b[0m\u001b[1mEpoch: [20/70], Progress: 8.60 %, Steps: 1776, Current Learning Rate: 0.0004390, \u001b[91mTrain Loss: 4.981\n",
      "\u001b[0m\u001b[1mEpoch: [20/70], Progress: 9.68 %, Steps: 1777, Current Learning Rate: 0.0004393, \u001b[96mTrain Loss: 4.804\n",
      "\u001b[0m\u001b[1mEpoch: [20/70], Progress: 10.75 %, Steps: 1778, Current Learning Rate: 0.0004395, \u001b[91mTrain Loss: 4.868\n",
      "\u001b[0m\u001b[1mEpoch: [20/70], Progress: 11.83 %, Steps: 1779, Current Learning Rate: 0.0004398, \u001b[96mTrain Loss: 4.826\n",
      "\u001b[0m\u001b[1mEpoch: [20/70], Progress: 12.90 %, Steps: 1780, Current Learning Rate: 0.0004400, \u001b[91mTrain Loss: 4.901\n",
      "\u001b[0m\u001b[1mEpoch: [20/70], Progress: 13.98 %, Steps: 1781, Current Learning Rate: 0.0004402, \u001b[91mTrain Loss: 4.905\n",
      "\u001b[0m\u001b[1mEpoch: [20/70], Progress: 15.05 %, Steps: 1782, Current Learning Rate: 0.0004405, \u001b[96mTrain Loss: 4.854\n",
      "\u001b[0m\u001b[1mEpoch: [20/70], Progress: 16.13 %, Steps: 1783, Current Learning Rate: 0.0004407, \u001b[91mTrain Loss: 5.033\n",
      "\u001b[0m\u001b[1mEpoch: [20/70], Progress: 17.20 %, Steps: 1784, Current Learning Rate: 0.0004410, \u001b[96mTrain Loss: 4.825\n",
      "\u001b[0m\u001b[1mEpoch: [20/70], Progress: 18.28 %, Steps: 1785, Current Learning Rate: 0.0004412, \u001b[96mTrain Loss: 4.731\n",
      "\u001b[0m\u001b[1mEpoch: [20/70], Progress: 19.35 %, Steps: 1786, Current Learning Rate: 0.0004415, \u001b[91mTrain Loss: 4.931\n",
      "\u001b[0m\u001b[1mEpoch: [20/70], Progress: 20.43 %, Steps: 1787, Current Learning Rate: 0.0004417, \u001b[96mTrain Loss: 4.709\n",
      "\u001b[0m\u001b[1mEpoch: [20/70], Progress: 21.51 %, Steps: 1788, Current Learning Rate: 0.0004420, \u001b[91mTrain Loss: 4.840\n",
      "\u001b[0m\u001b[1mEpoch: [20/70], Progress: 22.58 %, Steps: 1789, Current Learning Rate: 0.0004422, \u001b[96mTrain Loss: 4.735\n",
      "\u001b[0m\u001b[1mEpoch: [20/70], Progress: 23.66 %, Steps: 1790, Current Learning Rate: 0.0004425, \u001b[91mTrain Loss: 4.812\n",
      "\u001b[0m\u001b[1mEpoch: [20/70], Progress: 24.73 %, Steps: 1791, Current Learning Rate: 0.0004427, \u001b[96mTrain Loss: 4.691\n",
      "\u001b[0m\u001b[1mEpoch: [20/70], Progress: 25.81 %, Steps: 1792, Current Learning Rate: 0.0004430, \u001b[91mTrain Loss: 4.806\n",
      "\u001b[0m\u001b[1mEpoch: [20/70], Progress: 26.88 %, Steps: 1793, Current Learning Rate: 0.0004432, \u001b[96mTrain Loss: 4.744\n",
      "\u001b[0m\u001b[1mEpoch: [20/70], Progress: 27.96 %, Steps: 1794, Current Learning Rate: 0.0004435, \u001b[91mTrain Loss: 4.883\n",
      "\u001b[0m\u001b[1mEpoch: [20/70], Progress: 29.03 %, Steps: 1795, Current Learning Rate: 0.0004437, \u001b[96mTrain Loss: 4.767\n",
      "\u001b[0m\u001b[1mEpoch: [20/70], Progress: 30.11 %, Steps: 1796, Current Learning Rate: 0.0004440, \u001b[91mTrain Loss: 4.851\n",
      "\u001b[0m\u001b[1mEpoch: [20/70], Progress: 31.18 %, Steps: 1797, Current Learning Rate: 0.0004442, \u001b[96mTrain Loss: 4.718\n",
      "\u001b[0m\u001b[1mEpoch: [20/70], Progress: 32.26 %, Steps: 1798, Current Learning Rate: 0.0004444, \u001b[91mTrain Loss: 4.942\n",
      "\u001b[0m\u001b[1mEpoch: [20/70], Progress: 33.33 %, Steps: 1799, Current Learning Rate: 0.0004447, \u001b[96mTrain Loss: 4.626\n",
      "\u001b[0m\u001b[1mEpoch: [20/70], Progress: 34.41 %, Steps: 1800, Current Learning Rate: 0.0004449, \u001b[91mTrain Loss: 4.826\n",
      "\u001b[0m\u001b[1mEpoch: [20/70], Progress: 35.48 %, Steps: 1801, Current Learning Rate: 0.0004452, \u001b[91mTrain Loss: 4.827\n",
      "\u001b[0m\u001b[1mEpoch: [20/70], Progress: 36.56 %, Steps: 1802, Current Learning Rate: 0.0004454, \u001b[96mTrain Loss: 4.790\n",
      "\u001b[0m\u001b[1mEpoch: [20/70], Progress: 37.63 %, Steps: 1803, Current Learning Rate: 0.0004457, \u001b[91mTrain Loss: 4.948\n",
      "\u001b[0m\u001b[1mEpoch: [20/70], Progress: 38.71 %, Steps: 1804, Current Learning Rate: 0.0004459, \u001b[96mTrain Loss: 4.947\n",
      "\u001b[0m\u001b[1mEpoch: [20/70], Progress: 39.78 %, Steps: 1805, Current Learning Rate: 0.0004462, \u001b[96mTrain Loss: 4.912\n",
      "\u001b[0m\u001b[1mEpoch: [20/70], Progress: 40.86 %, Steps: 1806, Current Learning Rate: 0.0004464, \u001b[96mTrain Loss: 4.795\n",
      "\u001b[0m\u001b[1mEpoch: [20/70], Progress: 41.94 %, Steps: 1807, Current Learning Rate: 0.0004467, \u001b[91mTrain Loss: 4.890\n",
      "\u001b[0m\u001b[1mEpoch: [20/70], Progress: 43.01 %, Steps: 1808, Current Learning Rate: 0.0004469, \u001b[96mTrain Loss: 4.769\n",
      "\u001b[0m\u001b[1mEpoch: [20/70], Progress: 44.09 %, Steps: 1809, Current Learning Rate: 0.0004472, \u001b[91mTrain Loss: 4.858\n",
      "\u001b[0m\u001b[1mEpoch: [20/70], Progress: 45.16 %, Steps: 1810, Current Learning Rate: 0.0004474, \u001b[96mTrain Loss: 4.848\n",
      "\u001b[0m\u001b[1mEpoch: [20/70], Progress: 46.24 %, Steps: 1811, Current Learning Rate: 0.0004477, \u001b[96mTrain Loss: 4.807\n",
      "\u001b[0m\u001b[1mEpoch: [20/70], Progress: 47.31 %, Steps: 1812, Current Learning Rate: 0.0004479, \u001b[96mTrain Loss: 4.637\n",
      "\u001b[0m\u001b[1mEpoch: [20/70], Progress: 48.39 %, Steps: 1813, Current Learning Rate: 0.0004482, \u001b[91mTrain Loss: 4.896\n",
      "\u001b[0m\u001b[1mEpoch: [20/70], Progress: 49.46 %, Steps: 1814, Current Learning Rate: 0.0004484, \u001b[91mTrain Loss: 4.903\n",
      "\u001b[0m\u001b[1mEpoch: [20/70], Progress: 50.54 %, Steps: 1815, Current Learning Rate: 0.0004486, \u001b[96mTrain Loss: 4.895\n",
      "\u001b[0m\u001b[1mEpoch: [20/70], Progress: 51.61 %, Steps: 1816, Current Learning Rate: 0.0004489, \u001b[96mTrain Loss: 4.852\n",
      "\u001b[0m\u001b[1mEpoch: [20/70], Progress: 52.69 %, Steps: 1817, Current Learning Rate: 0.0004491, \u001b[96mTrain Loss: 4.749\n",
      "\u001b[0m\u001b[1mEpoch: [20/70], Progress: 53.76 %, Steps: 1818, Current Learning Rate: 0.0004494, \u001b[91mTrain Loss: 4.937\n",
      "\u001b[0m\u001b[1mEpoch: [20/70], Progress: 54.84 %, Steps: 1819, Current Learning Rate: 0.0004496, \u001b[96mTrain Loss: 4.682\n",
      "\u001b[0m\u001b[1mEpoch: [20/70], Progress: 55.91 %, Steps: 1820, Current Learning Rate: 0.0004499, \u001b[91mTrain Loss: 4.863\n",
      "\u001b[0m\u001b[1mEpoch: [20/70], Progress: 56.99 %, Steps: 1821, Current Learning Rate: 0.0004501, \u001b[91mTrain Loss: 4.874\n",
      "\u001b[0m\u001b[1mEpoch: [20/70], Progress: 58.06 %, Steps: 1822, Current Learning Rate: 0.0004504, \u001b[96mTrain Loss: 4.871\n",
      "\u001b[0m\u001b[1mEpoch: [20/70], Progress: 59.14 %, Steps: 1823, Current Learning Rate: 0.0004506, \u001b[91mTrain Loss: 4.924\n",
      "\u001b[0m\u001b[1mEpoch: [20/70], Progress: 60.22 %, Steps: 1824, Current Learning Rate: 0.0004509, \u001b[96mTrain Loss: 4.779\n",
      "\u001b[0m\u001b[1mEpoch: [20/70], Progress: 61.29 %, Steps: 1825, Current Learning Rate: 0.0004511, \u001b[91mTrain Loss: 4.925\n",
      "\u001b[0m\u001b[1mEpoch: [20/70], Progress: 62.37 %, Steps: 1826, Current Learning Rate: 0.0004514, \u001b[96mTrain Loss: 4.921\n",
      "\u001b[0m\u001b[1mEpoch: [20/70], Progress: 63.44 %, Steps: 1827, Current Learning Rate: 0.0004516, \u001b[91mTrain Loss: 4.975\n",
      "\u001b[0m\u001b[1mEpoch: [20/70], Progress: 64.52 %, Steps: 1828, Current Learning Rate: 0.0004519, \u001b[96mTrain Loss: 4.925\n",
      "\u001b[0m\u001b[1mEpoch: [20/70], Progress: 65.59 %, Steps: 1829, Current Learning Rate: 0.0004521, \u001b[96mTrain Loss: 4.802\n",
      "\u001b[0m\u001b[1mEpoch: [20/70], Progress: 66.67 %, Steps: 1830, Current Learning Rate: 0.0004524, \u001b[91mTrain Loss: 4.915\n",
      "\u001b[0m\u001b[1mEpoch: [20/70], Progress: 67.74 %, Steps: 1831, Current Learning Rate: 0.0004526, \u001b[96mTrain Loss: 4.737\n",
      "\u001b[0m\u001b[1mEpoch: [20/70], Progress: 68.82 %, Steps: 1832, Current Learning Rate: 0.0004528, \u001b[91mTrain Loss: 4.908\n",
      "\u001b[0m\u001b[1mEpoch: [20/70], Progress: 69.89 %, Steps: 1833, Current Learning Rate: 0.0004531, \u001b[91mTrain Loss: 4.926\n",
      "\u001b[0m\u001b[1mEpoch: [20/70], Progress: 70.97 %, Steps: 1834, Current Learning Rate: 0.0004533, \u001b[96mTrain Loss: 4.659\n",
      "\u001b[0m\u001b[1mEpoch: [20/70], Progress: 72.04 %, Steps: 1835, Current Learning Rate: 0.0004536, \u001b[91mTrain Loss: 4.941\n",
      "\u001b[0m\u001b[1mEpoch: [20/70], Progress: 73.12 %, Steps: 1836, Current Learning Rate: 0.0004538, \u001b[96mTrain Loss: 4.837\n",
      "\u001b[0m\u001b[1mEpoch: [20/70], Progress: 74.19 %, Steps: 1837, Current Learning Rate: 0.0004541, \u001b[96mTrain Loss: 4.726\n",
      "\u001b[0m\u001b[1mEpoch: [20/70], Progress: 75.27 %, Steps: 1838, Current Learning Rate: 0.0004543, \u001b[91mTrain Loss: 4.864\n",
      "\u001b[0m\u001b[1mEpoch: [20/70], Progress: 76.34 %, Steps: 1839, Current Learning Rate: 0.0004546, \u001b[91mTrain Loss: 4.913\n",
      "\u001b[0m\u001b[1mEpoch: [20/70], Progress: 77.42 %, Steps: 1840, Current Learning Rate: 0.0004548, \u001b[96mTrain Loss: 4.771\n",
      "\u001b[0m\u001b[1mEpoch: [20/70], Progress: 78.49 %, Steps: 1841, Current Learning Rate: 0.0004551, \u001b[91mTrain Loss: 5.015\n",
      "\u001b[0m\u001b[1mEpoch: [20/70], Progress: 79.57 %, Steps: 1842, Current Learning Rate: 0.0004553, \u001b[96mTrain Loss: 4.929\n",
      "\u001b[0m\u001b[1mEpoch: [20/70], Progress: 80.65 %, Steps: 1843, Current Learning Rate: 0.0004556, \u001b[96mTrain Loss: 4.786\n",
      "\u001b[0m\u001b[1mEpoch: [20/70], Progress: 81.72 %, Steps: 1844, Current Learning Rate: 0.0004558, \u001b[91mTrain Loss: 4.836\n",
      "\u001b[0m\u001b[1mEpoch: [20/70], Progress: 82.80 %, Steps: 1845, Current Learning Rate: 0.0004561, \u001b[96mTrain Loss: 4.711\n",
      "\u001b[0m\u001b[1mEpoch: [20/70], Progress: 83.87 %, Steps: 1846, Current Learning Rate: 0.0004563, \u001b[91mTrain Loss: 4.907\n",
      "\u001b[0m\u001b[1mEpoch: [20/70], Progress: 84.95 %, Steps: 1847, Current Learning Rate: 0.0004566, \u001b[96mTrain Loss: 4.742\n",
      "\u001b[0m\u001b[1mEpoch: [20/70], Progress: 86.02 %, Steps: 1848, Current Learning Rate: 0.0004568, \u001b[91mTrain Loss: 5.021\n",
      "\u001b[0m\u001b[1mEpoch: [20/70], Progress: 87.10 %, Steps: 1849, Current Learning Rate: 0.0004570, \u001b[96mTrain Loss: 4.851\n",
      "\u001b[0m\u001b[1mEpoch: [20/70], Progress: 88.17 %, Steps: 1850, Current Learning Rate: 0.0004573, \u001b[91mTrain Loss: 4.901\n",
      "\u001b[0m\u001b[1mEpoch: [20/70], Progress: 89.25 %, Steps: 1851, Current Learning Rate: 0.0004575, \u001b[91mTrain Loss: 5.002\n",
      "\u001b[0m\u001b[1mEpoch: [20/70], Progress: 90.32 %, Steps: 1852, Current Learning Rate: 0.0004578, \u001b[96mTrain Loss: 4.970\n",
      "\u001b[0m\u001b[1mEpoch: [20/70], Progress: 91.40 %, Steps: 1853, Current Learning Rate: 0.0004580, \u001b[96mTrain Loss: 4.802\n",
      "\u001b[0m\u001b[1mEpoch: [20/70], Progress: 92.47 %, Steps: 1854, Current Learning Rate: 0.0004583, \u001b[91mTrain Loss: 4.838\n",
      "\u001b[0m\u001b[1mEpoch: [20/70], Progress: 93.55 %, Steps: 1855, Current Learning Rate: 0.0004585, \u001b[96mTrain Loss: 4.648\n",
      "\u001b[0m\u001b[1mEpoch: [20/70], Progress: 94.62 %, Steps: 1856, Current Learning Rate: 0.0004588, \u001b[91mTrain Loss: 4.823\n",
      "\u001b[0m\u001b[1mEpoch: [20/70], Progress: 95.70 %, Steps: 1857, Current Learning Rate: 0.0004590, \u001b[91mTrain Loss: 5.083\n",
      "\u001b[0m\u001b[1mEpoch: [20/70], Progress: 96.77 %, Steps: 1858, Current Learning Rate: 0.0004593, \u001b[96mTrain Loss: 4.829\n",
      "\u001b[0m\u001b[1mEpoch: [20/70], Progress: 97.85 %, Steps: 1859, Current Learning Rate: 0.0004595, \u001b[91mTrain Loss: 4.896\n",
      "\u001b[0m\u001b[1mEpoch: [20/70], Progress: 98.92 %, Steps: 1860, Current Learning Rate: 0.0004598, \u001b[96mTrain Loss: 4.781\n",
      "\u001b[1m\u001b[93mComputing Validation Loss...\n",
      "\u001b[1mEpoch 20 Completed! Average Train Loss: 4.847, Average Validation Loss: 4.238\n",
      "Best Model saved in best_model/best_model.pt.\n",
      "\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m= Epoch [21/70] ==========================================\n",
      "\u001b[0m\u001b[1mEpoch: [21/70], Progress: 0.00 %, Steps: 1861, Current Learning Rate: 0.0004600, \u001b[91mTrain Loss: 4.627\n",
      "\u001b[0m\u001b[1mEpoch: [21/70], Progress: 1.08 %, Steps: 1862, Current Learning Rate: 0.0004603, \u001b[96mTrain Loss: 4.617\n",
      "\u001b[0m\u001b[1mEpoch: [21/70], Progress: 2.15 %, Steps: 1863, Current Learning Rate: 0.0004605, \u001b[91mTrain Loss: 4.814\n",
      "\u001b[0m\u001b[1mEpoch: [21/70], Progress: 3.23 %, Steps: 1864, Current Learning Rate: 0.0004608, \u001b[91mTrain Loss: 4.816\n",
      "\u001b[0m\u001b[1mEpoch: [21/70], Progress: 4.30 %, Steps: 1865, Current Learning Rate: 0.0004610, \u001b[96mTrain Loss: 4.581\n",
      "\u001b[0m\u001b[1mEpoch: [21/70], Progress: 5.38 %, Steps: 1866, Current Learning Rate: 0.0004612, \u001b[91mTrain Loss: 4.712\n",
      "\u001b[0m\u001b[1mEpoch: [21/70], Progress: 6.45 %, Steps: 1867, Current Learning Rate: 0.0004615, \u001b[96mTrain Loss: 4.711\n",
      "\u001b[0m\u001b[1mEpoch: [21/70], Progress: 7.53 %, Steps: 1868, Current Learning Rate: 0.0004617, \u001b[96mTrain Loss: 4.697\n",
      "\u001b[0m\u001b[1mEpoch: [21/70], Progress: 8.60 %, Steps: 1869, Current Learning Rate: 0.0004620, \u001b[96mTrain Loss: 4.631\n",
      "\u001b[0m\u001b[1mEpoch: [21/70], Progress: 9.68 %, Steps: 1870, Current Learning Rate: 0.0004622, \u001b[91mTrain Loss: 4.697\n",
      "\u001b[0m\u001b[1mEpoch: [21/70], Progress: 10.75 %, Steps: 1871, Current Learning Rate: 0.0004625, \u001b[91mTrain Loss: 4.733\n",
      "\u001b[0m\u001b[1mEpoch: [21/70], Progress: 11.83 %, Steps: 1872, Current Learning Rate: 0.0004627, \u001b[96mTrain Loss: 4.699\n",
      "\u001b[0m\u001b[1mEpoch: [21/70], Progress: 12.90 %, Steps: 1873, Current Learning Rate: 0.0004630, \u001b[96mTrain Loss: 4.512\n",
      "\u001b[0m\u001b[1mEpoch: [21/70], Progress: 13.98 %, Steps: 1874, Current Learning Rate: 0.0004632, \u001b[91mTrain Loss: 4.733\n",
      "\u001b[0m\u001b[1mEpoch: [21/70], Progress: 15.05 %, Steps: 1875, Current Learning Rate: 0.0004635, \u001b[91mTrain Loss: 4.788\n",
      "\u001b[0m\u001b[1mEpoch: [21/70], Progress: 16.13 %, Steps: 1876, Current Learning Rate: 0.0004637, \u001b[96mTrain Loss: 4.610\n",
      "\u001b[0m\u001b[1mEpoch: [21/70], Progress: 17.20 %, Steps: 1877, Current Learning Rate: 0.0004640, \u001b[91mTrain Loss: 4.731\n",
      "\u001b[0m\u001b[1mEpoch: [21/70], Progress: 18.28 %, Steps: 1878, Current Learning Rate: 0.0004642, \u001b[96mTrain Loss: 4.516\n",
      "\u001b[0m\u001b[1mEpoch: [21/70], Progress: 19.35 %, Steps: 1879, Current Learning Rate: 0.0004645, \u001b[91mTrain Loss: 4.653\n",
      "\u001b[0m\u001b[1mEpoch: [21/70], Progress: 20.43 %, Steps: 1880, Current Learning Rate: 0.0004647, \u001b[96mTrain Loss: 4.459\n",
      "\u001b[0m\u001b[1mEpoch: [21/70], Progress: 21.51 %, Steps: 1881, Current Learning Rate: 0.0004650, \u001b[91mTrain Loss: 4.630\n",
      "\u001b[0m\u001b[1mEpoch: [21/70], Progress: 22.58 %, Steps: 1882, Current Learning Rate: 0.0004652, \u001b[96mTrain Loss: 4.590\n",
      "\u001b[0m\u001b[1mEpoch: [21/70], Progress: 23.66 %, Steps: 1883, Current Learning Rate: 0.0004654, \u001b[91mTrain Loss: 4.851\n",
      "\u001b[0m\u001b[1mEpoch: [21/70], Progress: 24.73 %, Steps: 1884, Current Learning Rate: 0.0004657, \u001b[96mTrain Loss: 4.716\n",
      "\u001b[0m\u001b[1mEpoch: [21/70], Progress: 25.81 %, Steps: 1885, Current Learning Rate: 0.0004659, \u001b[96mTrain Loss: 4.518\n",
      "\u001b[0m\u001b[1mEpoch: [21/70], Progress: 26.88 %, Steps: 1886, Current Learning Rate: 0.0004662, \u001b[91mTrain Loss: 4.754\n",
      "\u001b[0m\u001b[1mEpoch: [21/70], Progress: 27.96 %, Steps: 1887, Current Learning Rate: 0.0004664, \u001b[96mTrain Loss: 4.672\n",
      "\u001b[0m\u001b[1mEpoch: [21/70], Progress: 29.03 %, Steps: 1888, Current Learning Rate: 0.0004667, \u001b[96mTrain Loss: 4.644\n",
      "\u001b[0m\u001b[1mEpoch: [21/70], Progress: 30.11 %, Steps: 1889, Current Learning Rate: 0.0004669, \u001b[96mTrain Loss: 4.530\n",
      "\u001b[0m\u001b[1mEpoch: [21/70], Progress: 31.18 %, Steps: 1890, Current Learning Rate: 0.0004672, \u001b[91mTrain Loss: 4.615\n",
      "\u001b[0m\u001b[1mEpoch: [21/70], Progress: 32.26 %, Steps: 1891, Current Learning Rate: 0.0004674, \u001b[91mTrain Loss: 4.702\n",
      "\u001b[0m\u001b[1mEpoch: [21/70], Progress: 33.33 %, Steps: 1892, Current Learning Rate: 0.0004677, \u001b[96mTrain Loss: 4.632\n",
      "\u001b[0m\u001b[1mEpoch: [21/70], Progress: 34.41 %, Steps: 1893, Current Learning Rate: 0.0004679, \u001b[96mTrain Loss: 4.592\n",
      "\u001b[0m\u001b[1mEpoch: [21/70], Progress: 35.48 %, Steps: 1894, Current Learning Rate: 0.0004682, \u001b[91mTrain Loss: 4.612\n",
      "\u001b[0m\u001b[1mEpoch: [21/70], Progress: 36.56 %, Steps: 1895, Current Learning Rate: 0.0004684, \u001b[91mTrain Loss: 4.733\n",
      "\u001b[0m\u001b[1mEpoch: [21/70], Progress: 37.63 %, Steps: 1896, Current Learning Rate: 0.0004687, \u001b[91mTrain Loss: 4.742\n",
      "\u001b[0m\u001b[1mEpoch: [21/70], Progress: 38.71 %, Steps: 1897, Current Learning Rate: 0.0004689, \u001b[96mTrain Loss: 4.711\n",
      "\u001b[0m\u001b[1mEpoch: [21/70], Progress: 39.78 %, Steps: 1898, Current Learning Rate: 0.0004692, \u001b[91mTrain Loss: 4.749\n",
      "\u001b[0m\u001b[1mEpoch: [21/70], Progress: 40.86 %, Steps: 1899, Current Learning Rate: 0.0004694, \u001b[96mTrain Loss: 4.739\n",
      "\u001b[0m\u001b[1mEpoch: [21/70], Progress: 41.94 %, Steps: 1900, Current Learning Rate: 0.0004696, \u001b[96mTrain Loss: 4.699\n",
      "\u001b[0m\u001b[1mEpoch: [21/70], Progress: 43.01 %, Steps: 1901, Current Learning Rate: 0.0004699, \u001b[96mTrain Loss: 4.683\n",
      "\u001b[0m\u001b[1mEpoch: [21/70], Progress: 44.09 %, Steps: 1902, Current Learning Rate: 0.0004701, \u001b[91mTrain Loss: 4.794\n",
      "\u001b[0m\u001b[1mEpoch: [21/70], Progress: 45.16 %, Steps: 1903, Current Learning Rate: 0.0004704, \u001b[96mTrain Loss: 4.728\n",
      "\u001b[0m\u001b[1mEpoch: [21/70], Progress: 46.24 %, Steps: 1904, Current Learning Rate: 0.0004706, \u001b[96mTrain Loss: 4.622\n",
      "\u001b[0m\u001b[1mEpoch: [21/70], Progress: 47.31 %, Steps: 1905, Current Learning Rate: 0.0004709, \u001b[91mTrain Loss: 4.788\n",
      "\u001b[0m\u001b[1mEpoch: [21/70], Progress: 48.39 %, Steps: 1906, Current Learning Rate: 0.0004711, \u001b[96mTrain Loss: 4.516\n",
      "\u001b[0m\u001b[1mEpoch: [21/70], Progress: 49.46 %, Steps: 1907, Current Learning Rate: 0.0004714, \u001b[91mTrain Loss: 4.713\n",
      "\u001b[0m\u001b[1mEpoch: [21/70], Progress: 50.54 %, Steps: 1908, Current Learning Rate: 0.0004716, \u001b[91mTrain Loss: 4.752\n",
      "\u001b[0m\u001b[1mEpoch: [21/70], Progress: 51.61 %, Steps: 1909, Current Learning Rate: 0.0004719, \u001b[96mTrain Loss: 4.715\n",
      "\u001b[0m\u001b[1mEpoch: [21/70], Progress: 52.69 %, Steps: 1910, Current Learning Rate: 0.0004721, \u001b[91mTrain Loss: 4.717\n",
      "\u001b[0m\u001b[1mEpoch: [21/70], Progress: 53.76 %, Steps: 1911, Current Learning Rate: 0.0004724, \u001b[96mTrain Loss: 4.603\n",
      "\u001b[0m\u001b[1mEpoch: [21/70], Progress: 54.84 %, Steps: 1912, Current Learning Rate: 0.0004726, \u001b[91mTrain Loss: 4.661\n",
      "\u001b[0m\u001b[1mEpoch: [21/70], Progress: 55.91 %, Steps: 1913, Current Learning Rate: 0.0004729, \u001b[91mTrain Loss: 4.816\n",
      "\u001b[0m\u001b[1mEpoch: [21/70], Progress: 56.99 %, Steps: 1914, Current Learning Rate: 0.0004731, \u001b[96mTrain Loss: 4.747\n",
      "\u001b[0m\u001b[1mEpoch: [21/70], Progress: 58.06 %, Steps: 1915, Current Learning Rate: 0.0004734, \u001b[96mTrain Loss: 4.512\n",
      "\u001b[0m\u001b[1mEpoch: [21/70], Progress: 59.14 %, Steps: 1916, Current Learning Rate: 0.0004736, \u001b[91mTrain Loss: 4.809\n",
      "\u001b[0m\u001b[1mEpoch: [21/70], Progress: 60.22 %, Steps: 1917, Current Learning Rate: 0.0004738, \u001b[96mTrain Loss: 4.717\n",
      "\u001b[0m\u001b[1mEpoch: [21/70], Progress: 61.29 %, Steps: 1918, Current Learning Rate: 0.0004741, \u001b[91mTrain Loss: 4.815\n",
      "\u001b[0m\u001b[1mEpoch: [21/70], Progress: 62.37 %, Steps: 1919, Current Learning Rate: 0.0004743, \u001b[96mTrain Loss: 4.810\n",
      "\u001b[0m\u001b[1mEpoch: [21/70], Progress: 63.44 %, Steps: 1920, Current Learning Rate: 0.0004746, \u001b[96mTrain Loss: 4.653\n",
      "\u001b[0m\u001b[1mEpoch: [21/70], Progress: 64.52 %, Steps: 1921, Current Learning Rate: 0.0004748, \u001b[96mTrain Loss: 4.644\n",
      "\u001b[0m\u001b[1mEpoch: [21/70], Progress: 65.59 %, Steps: 1922, Current Learning Rate: 0.0004751, \u001b[91mTrain Loss: 4.727\n",
      "\u001b[0m\u001b[1mEpoch: [21/70], Progress: 66.67 %, Steps: 1923, Current Learning Rate: 0.0004753, \u001b[96mTrain Loss: 4.688\n",
      "\u001b[0m\u001b[1mEpoch: [21/70], Progress: 67.74 %, Steps: 1924, Current Learning Rate: 0.0004756, \u001b[91mTrain Loss: 4.723\n",
      "\u001b[0m\u001b[1mEpoch: [21/70], Progress: 68.82 %, Steps: 1925, Current Learning Rate: 0.0004758, \u001b[96mTrain Loss: 4.608\n",
      "\u001b[0m\u001b[1mEpoch: [21/70], Progress: 69.89 %, Steps: 1926, Current Learning Rate: 0.0004761, \u001b[91mTrain Loss: 4.746\n",
      "\u001b[0m\u001b[1mEpoch: [21/70], Progress: 70.97 %, Steps: 1927, Current Learning Rate: 0.0004763, \u001b[96mTrain Loss: 4.677\n",
      "\u001b[0m\u001b[1mEpoch: [21/70], Progress: 72.04 %, Steps: 1928, Current Learning Rate: 0.0004766, \u001b[91mTrain Loss: 4.822\n",
      "\u001b[0m\u001b[1mEpoch: [21/70], Progress: 73.12 %, Steps: 1929, Current Learning Rate: 0.0004768, \u001b[91mTrain Loss: 4.823\n",
      "\u001b[0m\u001b[1mEpoch: [21/70], Progress: 74.19 %, Steps: 1930, Current Learning Rate: 0.0004771, \u001b[96mTrain Loss: 4.530\n",
      "\u001b[0m\u001b[1mEpoch: [21/70], Progress: 75.27 %, Steps: 1931, Current Learning Rate: 0.0004773, \u001b[91mTrain Loss: 4.622\n",
      "\u001b[0m\u001b[1mEpoch: [21/70], Progress: 76.34 %, Steps: 1932, Current Learning Rate: 0.0004776, \u001b[91mTrain Loss: 4.712\n",
      "\u001b[0m\u001b[1mEpoch: [21/70], Progress: 77.42 %, Steps: 1933, Current Learning Rate: 0.0004778, \u001b[91mTrain Loss: 4.799\n",
      "\u001b[0m\u001b[1mEpoch: [21/70], Progress: 78.49 %, Steps: 1934, Current Learning Rate: 0.0004780, \u001b[96mTrain Loss: 4.569\n",
      "\u001b[0m\u001b[1mEpoch: [21/70], Progress: 79.57 %, Steps: 1935, Current Learning Rate: 0.0004783, \u001b[91mTrain Loss: 4.788\n",
      "\u001b[0m\u001b[1mEpoch: [21/70], Progress: 80.65 %, Steps: 1936, Current Learning Rate: 0.0004785, \u001b[96mTrain Loss: 4.560\n",
      "\u001b[0m\u001b[1mEpoch: [21/70], Progress: 81.72 %, Steps: 1937, Current Learning Rate: 0.0004788, \u001b[91mTrain Loss: 4.832\n",
      "\u001b[0m\u001b[1mEpoch: [21/70], Progress: 82.80 %, Steps: 1938, Current Learning Rate: 0.0004790, \u001b[96mTrain Loss: 4.600\n",
      "\u001b[0m\u001b[1mEpoch: [21/70], Progress: 83.87 %, Steps: 1939, Current Learning Rate: 0.0004793, \u001b[91mTrain Loss: 4.634\n",
      "\u001b[0m\u001b[1mEpoch: [21/70], Progress: 84.95 %, Steps: 1940, Current Learning Rate: 0.0004795, \u001b[96mTrain Loss: 4.615\n",
      "\u001b[0m\u001b[1mEpoch: [21/70], Progress: 86.02 %, Steps: 1941, Current Learning Rate: 0.0004798, \u001b[91mTrain Loss: 4.624\n",
      "\u001b[0m\u001b[1mEpoch: [21/70], Progress: 87.10 %, Steps: 1942, Current Learning Rate: 0.0004800, \u001b[96mTrain Loss: 4.471\n",
      "\u001b[0m\u001b[1mEpoch: [21/70], Progress: 88.17 %, Steps: 1943, Current Learning Rate: 0.0004803, \u001b[91mTrain Loss: 4.793\n",
      "\u001b[0m\u001b[1mEpoch: [21/70], Progress: 89.25 %, Steps: 1944, Current Learning Rate: 0.0004805, \u001b[96mTrain Loss: 4.678\n",
      "\u001b[0m\u001b[1mEpoch: [21/70], Progress: 90.32 %, Steps: 1945, Current Learning Rate: 0.0004808, \u001b[91mTrain Loss: 4.908\n",
      "\u001b[0m\u001b[1mEpoch: [21/70], Progress: 91.40 %, Steps: 1946, Current Learning Rate: 0.0004810, \u001b[96mTrain Loss: 4.628\n",
      "\u001b[0m\u001b[1mEpoch: [21/70], Progress: 92.47 %, Steps: 1947, Current Learning Rate: 0.0004813, \u001b[91mTrain Loss: 4.912\n",
      "\u001b[0m\u001b[1mEpoch: [21/70], Progress: 93.55 %, Steps: 1948, Current Learning Rate: 0.0004815, \u001b[96mTrain Loss: 4.633\n",
      "\u001b[0m\u001b[1mEpoch: [21/70], Progress: 94.62 %, Steps: 1949, Current Learning Rate: 0.0004818, \u001b[91mTrain Loss: 4.695\n",
      "\u001b[0m\u001b[1mEpoch: [21/70], Progress: 95.70 %, Steps: 1950, Current Learning Rate: 0.0004820, \u001b[96mTrain Loss: 4.551\n",
      "\u001b[0m\u001b[1mEpoch: [21/70], Progress: 96.77 %, Steps: 1951, Current Learning Rate: 0.0004822, \u001b[91mTrain Loss: 4.663\n",
      "\u001b[0m\u001b[1mEpoch: [21/70], Progress: 97.85 %, Steps: 1952, Current Learning Rate: 0.0004825, \u001b[96mTrain Loss: 4.631\n",
      "\u001b[0m\u001b[1mEpoch: [21/70], Progress: 98.92 %, Steps: 1953, Current Learning Rate: 0.0004827, \u001b[91mTrain Loss: 4.898\n",
      "\u001b[1m\u001b[93mComputing Validation Loss...\n",
      "\u001b[1mEpoch 21 Completed! Average Train Loss: 4.685, Average Validation Loss: 4.054\n",
      "Best Model saved in best_model/best_model.pt.\n",
      "\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m= Epoch [22/70] ==========================================\n",
      "\u001b[0m\u001b[1mEpoch: [22/70], Progress: 0.00 %, Steps: 1954, Current Learning Rate: 0.0004830, \u001b[91mTrain Loss: 4.563\n",
      "\u001b[0m\u001b[1mEpoch: [22/70], Progress: 1.08 %, Steps: 1955, Current Learning Rate: 0.0004832, \u001b[96mTrain Loss: 4.419\n",
      "\u001b[0m\u001b[1mEpoch: [22/70], Progress: 2.15 %, Steps: 1956, Current Learning Rate: 0.0004835, \u001b[91mTrain Loss: 4.653\n",
      "\u001b[0m\u001b[1mEpoch: [22/70], Progress: 3.23 %, Steps: 1957, Current Learning Rate: 0.0004837, \u001b[96mTrain Loss: 4.469\n",
      "\u001b[0m\u001b[1mEpoch: [22/70], Progress: 4.30 %, Steps: 1958, Current Learning Rate: 0.0004840, \u001b[96mTrain Loss: 4.405\n",
      "\u001b[0m\u001b[1mEpoch: [22/70], Progress: 5.38 %, Steps: 1959, Current Learning Rate: 0.0004842, \u001b[91mTrain Loss: 4.579\n",
      "\u001b[0m\u001b[1mEpoch: [22/70], Progress: 6.45 %, Steps: 1960, Current Learning Rate: 0.0004845, \u001b[96mTrain Loss: 4.324\n",
      "\u001b[0m\u001b[1mEpoch: [22/70], Progress: 7.53 %, Steps: 1961, Current Learning Rate: 0.0004847, \u001b[91mTrain Loss: 4.618\n",
      "\u001b[0m\u001b[1mEpoch: [22/70], Progress: 8.60 %, Steps: 1962, Current Learning Rate: 0.0004850, \u001b[96mTrain Loss: 4.412\n",
      "\u001b[0m\u001b[1mEpoch: [22/70], Progress: 9.68 %, Steps: 1963, Current Learning Rate: 0.0004852, \u001b[91mTrain Loss: 4.628\n",
      "\u001b[0m\u001b[1mEpoch: [22/70], Progress: 10.75 %, Steps: 1964, Current Learning Rate: 0.0004855, \u001b[96mTrain Loss: 4.555\n",
      "\u001b[0m\u001b[1mEpoch: [22/70], Progress: 11.83 %, Steps: 1965, Current Learning Rate: 0.0004857, \u001b[91mTrain Loss: 4.563\n",
      "\u001b[0m\u001b[1mEpoch: [22/70], Progress: 12.90 %, Steps: 1966, Current Learning Rate: 0.0004860, \u001b[96mTrain Loss: 4.431\n",
      "\u001b[0m\u001b[1mEpoch: [22/70], Progress: 13.98 %, Steps: 1967, Current Learning Rate: 0.0004862, \u001b[91mTrain Loss: 4.617\n",
      "\u001b[0m\u001b[1mEpoch: [22/70], Progress: 15.05 %, Steps: 1968, Current Learning Rate: 0.0004864, \u001b[96mTrain Loss: 4.553\n",
      "\u001b[0m\u001b[1mEpoch: [22/70], Progress: 16.13 %, Steps: 1969, Current Learning Rate: 0.0004867, \u001b[96mTrain Loss: 4.320\n",
      "\u001b[0m\u001b[1mEpoch: [22/70], Progress: 17.20 %, Steps: 1970, Current Learning Rate: 0.0004869, \u001b[91mTrain Loss: 4.407\n",
      "\u001b[0m\u001b[1mEpoch: [22/70], Progress: 18.28 %, Steps: 1971, Current Learning Rate: 0.0004872, \u001b[91mTrain Loss: 4.531\n",
      "\u001b[0m\u001b[1mEpoch: [22/70], Progress: 19.35 %, Steps: 1972, Current Learning Rate: 0.0004874, \u001b[96mTrain Loss: 4.381\n",
      "\u001b[0m\u001b[1mEpoch: [22/70], Progress: 20.43 %, Steps: 1973, Current Learning Rate: 0.0004877, \u001b[91mTrain Loss: 4.679\n",
      "\u001b[0m\u001b[1mEpoch: [22/70], Progress: 21.51 %, Steps: 1974, Current Learning Rate: 0.0004879, \u001b[96mTrain Loss: 4.574\n",
      "\u001b[0m\u001b[1mEpoch: [22/70], Progress: 22.58 %, Steps: 1975, Current Learning Rate: 0.0004882, \u001b[96mTrain Loss: 4.538\n",
      "\u001b[0m\u001b[1mEpoch: [22/70], Progress: 23.66 %, Steps: 1976, Current Learning Rate: 0.0004884, \u001b[91mTrain Loss: 4.542\n",
      "\u001b[0m\u001b[1mEpoch: [22/70], Progress: 24.73 %, Steps: 1977, Current Learning Rate: 0.0004887, \u001b[96mTrain Loss: 4.429\n",
      "\u001b[0m\u001b[1mEpoch: [22/70], Progress: 25.81 %, Steps: 1978, Current Learning Rate: 0.0004889, \u001b[91mTrain Loss: 4.473\n",
      "\u001b[0m\u001b[1mEpoch: [22/70], Progress: 26.88 %, Steps: 1979, Current Learning Rate: 0.0004892, \u001b[91mTrain Loss: 4.483\n",
      "\u001b[0m\u001b[1mEpoch: [22/70], Progress: 27.96 %, Steps: 1980, Current Learning Rate: 0.0004894, \u001b[91mTrain Loss: 4.497\n",
      "\u001b[0m\u001b[1mEpoch: [22/70], Progress: 29.03 %, Steps: 1981, Current Learning Rate: 0.0004897, \u001b[96mTrain Loss: 4.421\n",
      "\u001b[0m\u001b[1mEpoch: [22/70], Progress: 30.11 %, Steps: 1982, Current Learning Rate: 0.0004899, \u001b[91mTrain Loss: 4.611\n",
      "\u001b[0m\u001b[1mEpoch: [22/70], Progress: 31.18 %, Steps: 1983, Current Learning Rate: 0.0004902, \u001b[96mTrain Loss: 4.287\n",
      "\u001b[0m\u001b[1mEpoch: [22/70], Progress: 32.26 %, Steps: 1984, Current Learning Rate: 0.0004904, \u001b[91mTrain Loss: 4.577\n",
      "\u001b[0m\u001b[1mEpoch: [22/70], Progress: 33.33 %, Steps: 1985, Current Learning Rate: 0.0004906, \u001b[96mTrain Loss: 4.572\n",
      "\u001b[0m\u001b[1mEpoch: [22/70], Progress: 34.41 %, Steps: 1986, Current Learning Rate: 0.0004909, \u001b[96mTrain Loss: 4.405\n",
      "\u001b[0m\u001b[1mEpoch: [22/70], Progress: 35.48 %, Steps: 1987, Current Learning Rate: 0.0004911, \u001b[91mTrain Loss: 4.555\n",
      "\u001b[0m\u001b[1mEpoch: [22/70], Progress: 36.56 %, Steps: 1988, Current Learning Rate: 0.0004914, \u001b[96mTrain Loss: 4.491\n",
      "\u001b[0m\u001b[1mEpoch: [22/70], Progress: 37.63 %, Steps: 1989, Current Learning Rate: 0.0004916, \u001b[96mTrain Loss: 4.480\n",
      "\u001b[0m\u001b[1mEpoch: [22/70], Progress: 38.71 %, Steps: 1990, Current Learning Rate: 0.0004919, \u001b[91mTrain Loss: 4.900\n",
      "\u001b[0m\u001b[1mEpoch: [22/70], Progress: 39.78 %, Steps: 1991, Current Learning Rate: 0.0004921, \u001b[96mTrain Loss: 4.491\n",
      "\u001b[0m\u001b[1mEpoch: [22/70], Progress: 40.86 %, Steps: 1992, Current Learning Rate: 0.0004924, \u001b[96mTrain Loss: 4.426\n",
      "\u001b[0m\u001b[1mEpoch: [22/70], Progress: 41.94 %, Steps: 1993, Current Learning Rate: 0.0004926, \u001b[91mTrain Loss: 4.449\n",
      "\u001b[0m\u001b[1mEpoch: [22/70], Progress: 43.01 %, Steps: 1994, Current Learning Rate: 0.0004929, \u001b[91mTrain Loss: 4.575\n",
      "\u001b[0m\u001b[1mEpoch: [22/70], Progress: 44.09 %, Steps: 1995, Current Learning Rate: 0.0004931, \u001b[96mTrain Loss: 4.400\n",
      "\u001b[0m\u001b[1mEpoch: [22/70], Progress: 45.16 %, Steps: 1996, Current Learning Rate: 0.0004934, \u001b[91mTrain Loss: 4.546\n",
      "\u001b[0m\u001b[1mEpoch: [22/70], Progress: 46.24 %, Steps: 1997, Current Learning Rate: 0.0004936, \u001b[96mTrain Loss: 4.400\n",
      "\u001b[0m\u001b[1mEpoch: [22/70], Progress: 47.31 %, Steps: 1998, Current Learning Rate: 0.0004939, \u001b[91mTrain Loss: 4.506\n",
      "\u001b[0m\u001b[1mEpoch: [22/70], Progress: 48.39 %, Steps: 1999, Current Learning Rate: 0.0004941, \u001b[96mTrain Loss: 4.390\n",
      "\u001b[0m\u001b[1mEpoch: [22/70], Progress: 49.46 %, Steps: 2000, Current Learning Rate: 0.0004944, \u001b[91mTrain Loss: 4.452\n",
      "\u001b[0m\u001b[1mEpoch: [22/70], Progress: 50.54 %, Steps: 2001, Current Learning Rate: 0.0004946, \u001b[91mTrain Loss: 4.520\n",
      "\u001b[0m\u001b[1mEpoch: [22/70], Progress: 51.61 %, Steps: 2002, Current Learning Rate: 0.0004948, \u001b[91mTrain Loss: 4.628\n",
      "\u001b[0m\u001b[1mEpoch: [22/70], Progress: 52.69 %, Steps: 2003, Current Learning Rate: 0.0004951, \u001b[96mTrain Loss: 4.159\n",
      "\u001b[0m\u001b[1mEpoch: [22/70], Progress: 53.76 %, Steps: 2004, Current Learning Rate: 0.0004953, \u001b[91mTrain Loss: 4.715\n",
      "\u001b[0m\u001b[1mEpoch: [22/70], Progress: 54.84 %, Steps: 2005, Current Learning Rate: 0.0004956, \u001b[96mTrain Loss: 4.479\n",
      "\u001b[0m\u001b[1mEpoch: [22/70], Progress: 55.91 %, Steps: 2006, Current Learning Rate: 0.0004958, \u001b[91mTrain Loss: 4.753\n",
      "\u001b[0m\u001b[1mEpoch: [22/70], Progress: 56.99 %, Steps: 2007, Current Learning Rate: 0.0004961, \u001b[96mTrain Loss: 4.524\n",
      "\u001b[0m\u001b[1mEpoch: [22/70], Progress: 58.06 %, Steps: 2008, Current Learning Rate: 0.0004963, \u001b[96mTrain Loss: 4.468\n",
      "\u001b[0m\u001b[1mEpoch: [22/70], Progress: 59.14 %, Steps: 2009, Current Learning Rate: 0.0004966, \u001b[91mTrain Loss: 4.516\n",
      "\u001b[0m\u001b[1mEpoch: [22/70], Progress: 60.22 %, Steps: 2010, Current Learning Rate: 0.0004968, \u001b[91mTrain Loss: 4.595\n",
      "\u001b[0m\u001b[1mEpoch: [22/70], Progress: 61.29 %, Steps: 2011, Current Learning Rate: 0.0004971, \u001b[96mTrain Loss: 4.545\n",
      "\u001b[0m\u001b[1mEpoch: [22/70], Progress: 62.37 %, Steps: 2012, Current Learning Rate: 0.0004973, \u001b[96mTrain Loss: 4.507\n",
      "\u001b[0m\u001b[1mEpoch: [22/70], Progress: 63.44 %, Steps: 2013, Current Learning Rate: 0.0004976, \u001b[91mTrain Loss: 4.588\n",
      "\u001b[0m\u001b[1mEpoch: [22/70], Progress: 64.52 %, Steps: 2014, Current Learning Rate: 0.0004978, \u001b[96mTrain Loss: 4.497\n",
      "\u001b[0m\u001b[1mEpoch: [22/70], Progress: 65.59 %, Steps: 2015, Current Learning Rate: 0.0004981, \u001b[96mTrain Loss: 4.411\n",
      "\u001b[0m\u001b[1mEpoch: [22/70], Progress: 66.67 %, Steps: 2016, Current Learning Rate: 0.0004983, \u001b[96mTrain Loss: 4.340\n",
      "\u001b[0m\u001b[1mEpoch: [22/70], Progress: 67.74 %, Steps: 2017, Current Learning Rate: 0.0004986, \u001b[91mTrain Loss: 4.593\n",
      "\u001b[0m\u001b[1mEpoch: [22/70], Progress: 68.82 %, Steps: 2018, Current Learning Rate: 0.0004988, \u001b[96mTrain Loss: 4.385\n",
      "\u001b[0m\u001b[1mEpoch: [22/70], Progress: 69.89 %, Steps: 2019, Current Learning Rate: 0.0004990, \u001b[91mTrain Loss: 4.425\n",
      "\u001b[0m\u001b[1mEpoch: [22/70], Progress: 70.97 %, Steps: 2020, Current Learning Rate: 0.0004993, \u001b[91mTrain Loss: 4.585\n",
      "\u001b[0m\u001b[1mEpoch: [22/70], Progress: 72.04 %, Steps: 2021, Current Learning Rate: 0.0004995, \u001b[96mTrain Loss: 4.209\n",
      "\u001b[0m\u001b[1mEpoch: [22/70], Progress: 73.12 %, Steps: 2022, Current Learning Rate: 0.0004998, \u001b[91mTrain Loss: 4.745\n",
      "\u001b[0m\u001b[1mEpoch: [22/70], Progress: 74.19 %, Steps: 2023, Current Learning Rate: 0.0005000, \u001b[96mTrain Loss: 4.408\n",
      "\u001b[0m\u001b[1mEpoch: [22/70], Progress: 75.27 %, Steps: 2024, Current Learning Rate: 0.0005003, \u001b[91mTrain Loss: 4.591\n",
      "\u001b[0m\u001b[1mEpoch: [22/70], Progress: 76.34 %, Steps: 2025, Current Learning Rate: 0.0005005, \u001b[96mTrain Loss: 4.585\n",
      "\u001b[0m\u001b[1mEpoch: [22/70], Progress: 77.42 %, Steps: 2026, Current Learning Rate: 0.0005008, \u001b[96mTrain Loss: 4.480\n",
      "\u001b[0m\u001b[1mEpoch: [22/70], Progress: 78.49 %, Steps: 2027, Current Learning Rate: 0.0005010, \u001b[91mTrain Loss: 4.810\n",
      "\u001b[0m\u001b[1mEpoch: [22/70], Progress: 79.57 %, Steps: 2028, Current Learning Rate: 0.0005013, \u001b[96mTrain Loss: 4.421\n",
      "\u001b[0m\u001b[1mEpoch: [22/70], Progress: 80.65 %, Steps: 2029, Current Learning Rate: 0.0005015, \u001b[91mTrain Loss: 4.472\n",
      "\u001b[0m\u001b[1mEpoch: [22/70], Progress: 81.72 %, Steps: 2030, Current Learning Rate: 0.0005018, \u001b[96mTrain Loss: 4.271\n",
      "\u001b[0m\u001b[1mEpoch: [22/70], Progress: 82.80 %, Steps: 2031, Current Learning Rate: 0.0005020, \u001b[91mTrain Loss: 4.657\n",
      "\u001b[0m\u001b[1mEpoch: [22/70], Progress: 83.87 %, Steps: 2032, Current Learning Rate: 0.0005023, \u001b[96mTrain Loss: 4.404\n",
      "\u001b[0m\u001b[1mEpoch: [22/70], Progress: 84.95 %, Steps: 2033, Current Learning Rate: 0.0005025, \u001b[91mTrain Loss: 4.412\n",
      "\u001b[0m\u001b[1mEpoch: [22/70], Progress: 86.02 %, Steps: 2034, Current Learning Rate: 0.0005028, \u001b[96mTrain Loss: 4.383\n",
      "\u001b[0m\u001b[1mEpoch: [22/70], Progress: 87.10 %, Steps: 2035, Current Learning Rate: 0.0005030, \u001b[91mTrain Loss: 4.643\n",
      "\u001b[0m\u001b[1mEpoch: [22/70], Progress: 88.17 %, Steps: 2036, Current Learning Rate: 0.0005032, \u001b[96mTrain Loss: 4.389\n",
      "\u001b[0m\u001b[1mEpoch: [22/70], Progress: 89.25 %, Steps: 2037, Current Learning Rate: 0.0005035, \u001b[91mTrain Loss: 4.422\n",
      "\u001b[0m\u001b[1mEpoch: [22/70], Progress: 90.32 %, Steps: 2038, Current Learning Rate: 0.0005037, \u001b[91mTrain Loss: 4.609\n",
      "\u001b[0m\u001b[1mEpoch: [22/70], Progress: 91.40 %, Steps: 2039, Current Learning Rate: 0.0005040, \u001b[96mTrain Loss: 4.602\n",
      "\u001b[0m\u001b[1mEpoch: [22/70], Progress: 92.47 %, Steps: 2040, Current Learning Rate: 0.0005042, \u001b[96mTrain Loss: 4.364\n",
      "\u001b[0m\u001b[1mEpoch: [22/70], Progress: 93.55 %, Steps: 2041, Current Learning Rate: 0.0005045, \u001b[91mTrain Loss: 4.603\n",
      "\u001b[0m\u001b[1mEpoch: [22/70], Progress: 94.62 %, Steps: 2042, Current Learning Rate: 0.0005047, \u001b[91mTrain Loss: 4.646\n",
      "\u001b[0m\u001b[1mEpoch: [22/70], Progress: 95.70 %, Steps: 2043, Current Learning Rate: 0.0005050, \u001b[91mTrain Loss: 4.677\n",
      "\u001b[0m\u001b[1mEpoch: [22/70], Progress: 96.77 %, Steps: 2044, Current Learning Rate: 0.0005052, \u001b[96mTrain Loss: 4.606\n",
      "\u001b[0m\u001b[1mEpoch: [22/70], Progress: 97.85 %, Steps: 2045, Current Learning Rate: 0.0005055, \u001b[96mTrain Loss: 4.496\n",
      "\u001b[0m\u001b[1mEpoch: [22/70], Progress: 98.92 %, Steps: 2046, Current Learning Rate: 0.0005057, \u001b[96mTrain Loss: 4.186\n",
      "\u001b[1m\u001b[93mComputing Validation Loss...\n",
      "\u001b[1mEpoch 22 Completed! Average Train Loss: 4.504, Average Validation Loss: 3.808\n",
      "Best Model saved in best_model/best_model.pt.\n",
      "\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m= Epoch [23/70] ==========================================\n",
      "\u001b[0m\u001b[1mEpoch: [23/70], Progress: 0.00 %, Steps: 2047, Current Learning Rate: 0.0005060, \u001b[91mTrain Loss: 4.229\n",
      "\u001b[0m\u001b[1mEpoch: [23/70], Progress: 1.08 %, Steps: 2048, Current Learning Rate: 0.0005062, \u001b[96mTrain Loss: 4.089\n",
      "\u001b[0m\u001b[1mEpoch: [23/70], Progress: 2.15 %, Steps: 2049, Current Learning Rate: 0.0005065, \u001b[91mTrain Loss: 4.319\n",
      "\u001b[0m\u001b[1mEpoch: [23/70], Progress: 3.23 %, Steps: 2050, Current Learning Rate: 0.0005067, \u001b[91mTrain Loss: 4.368\n",
      "\u001b[0m\u001b[1mEpoch: [23/70], Progress: 4.30 %, Steps: 2051, Current Learning Rate: 0.0005070, \u001b[96mTrain Loss: 4.213\n",
      "\u001b[0m\u001b[1mEpoch: [23/70], Progress: 5.38 %, Steps: 2052, Current Learning Rate: 0.0005072, \u001b[91mTrain Loss: 4.224\n",
      "\u001b[0m\u001b[1mEpoch: [23/70], Progress: 6.45 %, Steps: 2053, Current Learning Rate: 0.0005074, \u001b[91mTrain Loss: 4.512\n",
      "\u001b[0m\u001b[1mEpoch: [23/70], Progress: 7.53 %, Steps: 2054, Current Learning Rate: 0.0005077, \u001b[96mTrain Loss: 4.260\n",
      "\u001b[0m\u001b[1mEpoch: [23/70], Progress: 8.60 %, Steps: 2055, Current Learning Rate: 0.0005079, \u001b[91mTrain Loss: 4.399\n",
      "\u001b[0m\u001b[1mEpoch: [23/70], Progress: 9.68 %, Steps: 2056, Current Learning Rate: 0.0005082, \u001b[91mTrain Loss: 4.509\n",
      "\u001b[0m\u001b[1mEpoch: [23/70], Progress: 10.75 %, Steps: 2057, Current Learning Rate: 0.0005084, \u001b[96mTrain Loss: 4.311\n",
      "\u001b[0m\u001b[1mEpoch: [23/70], Progress: 11.83 %, Steps: 2058, Current Learning Rate: 0.0005087, \u001b[91mTrain Loss: 4.324\n",
      "\u001b[0m\u001b[1mEpoch: [23/70], Progress: 12.90 %, Steps: 2059, Current Learning Rate: 0.0005089, \u001b[96mTrain Loss: 4.258\n",
      "\u001b[0m\u001b[1mEpoch: [23/70], Progress: 13.98 %, Steps: 2060, Current Learning Rate: 0.0005092, \u001b[91mTrain Loss: 4.487\n",
      "\u001b[0m\u001b[1mEpoch: [23/70], Progress: 15.05 %, Steps: 2061, Current Learning Rate: 0.0005094, \u001b[96mTrain Loss: 4.462\n",
      "\u001b[0m\u001b[1mEpoch: [23/70], Progress: 16.13 %, Steps: 2062, Current Learning Rate: 0.0005097, \u001b[96mTrain Loss: 4.430\n",
      "\u001b[0m\u001b[1mEpoch: [23/70], Progress: 17.20 %, Steps: 2063, Current Learning Rate: 0.0005099, \u001b[96mTrain Loss: 4.340\n",
      "\u001b[0m\u001b[1mEpoch: [23/70], Progress: 18.28 %, Steps: 2064, Current Learning Rate: 0.0005102, \u001b[96mTrain Loss: 4.312\n",
      "\u001b[0m\u001b[1mEpoch: [23/70], Progress: 19.35 %, Steps: 2065, Current Learning Rate: 0.0005104, \u001b[96mTrain Loss: 4.272\n",
      "\u001b[0m\u001b[1mEpoch: [23/70], Progress: 20.43 %, Steps: 2066, Current Learning Rate: 0.0005107, \u001b[91mTrain Loss: 4.324\n",
      "\u001b[0m\u001b[1mEpoch: [23/70], Progress: 21.51 %, Steps: 2067, Current Learning Rate: 0.0005109, \u001b[91mTrain Loss: 4.327\n",
      "\u001b[0m\u001b[1mEpoch: [23/70], Progress: 22.58 %, Steps: 2068, Current Learning Rate: 0.0005112, \u001b[96mTrain Loss: 4.241\n",
      "\u001b[0m\u001b[1mEpoch: [23/70], Progress: 23.66 %, Steps: 2069, Current Learning Rate: 0.0005114, \u001b[96mTrain Loss: 4.139\n",
      "\u001b[0m\u001b[1mEpoch: [23/70], Progress: 24.73 %, Steps: 2070, Current Learning Rate: 0.0005116, \u001b[96mTrain Loss: 4.121\n",
      "\u001b[0m\u001b[1mEpoch: [23/70], Progress: 25.81 %, Steps: 2071, Current Learning Rate: 0.0005119, \u001b[96mTrain Loss: 4.121\n",
      "\u001b[0m\u001b[1mEpoch: [23/70], Progress: 26.88 %, Steps: 2072, Current Learning Rate: 0.0005121, \u001b[91mTrain Loss: 4.137\n",
      "\u001b[0m\u001b[1mEpoch: [23/70], Progress: 27.96 %, Steps: 2073, Current Learning Rate: 0.0005124, \u001b[91mTrain Loss: 4.494\n",
      "\u001b[0m\u001b[1mEpoch: [23/70], Progress: 29.03 %, Steps: 2074, Current Learning Rate: 0.0005126, \u001b[96mTrain Loss: 4.276\n",
      "\u001b[0m\u001b[1mEpoch: [23/70], Progress: 30.11 %, Steps: 2075, Current Learning Rate: 0.0005129, \u001b[91mTrain Loss: 4.383\n",
      "\u001b[0m\u001b[1mEpoch: [23/70], Progress: 31.18 %, Steps: 2076, Current Learning Rate: 0.0005131, \u001b[96mTrain Loss: 4.359\n",
      "\u001b[0m\u001b[1mEpoch: [23/70], Progress: 32.26 %, Steps: 2077, Current Learning Rate: 0.0005134, \u001b[96mTrain Loss: 4.285\n",
      "\u001b[0m\u001b[1mEpoch: [23/70], Progress: 33.33 %, Steps: 2078, Current Learning Rate: 0.0005136, \u001b[91mTrain Loss: 4.302\n",
      "\u001b[0m\u001b[1mEpoch: [23/70], Progress: 34.41 %, Steps: 2079, Current Learning Rate: 0.0005139, \u001b[91mTrain Loss: 4.437\n",
      "\u001b[0m\u001b[1mEpoch: [23/70], Progress: 35.48 %, Steps: 2080, Current Learning Rate: 0.0005141, \u001b[96mTrain Loss: 4.426\n",
      "\u001b[0m\u001b[1mEpoch: [23/70], Progress: 36.56 %, Steps: 2081, Current Learning Rate: 0.0005144, \u001b[96mTrain Loss: 4.330\n",
      "\u001b[0m\u001b[1mEpoch: [23/70], Progress: 37.63 %, Steps: 2082, Current Learning Rate: 0.0005146, \u001b[96mTrain Loss: 4.215\n",
      "\u001b[0m\u001b[1mEpoch: [23/70], Progress: 38.71 %, Steps: 2083, Current Learning Rate: 0.0005149, \u001b[91mTrain Loss: 4.311\n",
      "\u001b[0m\u001b[1mEpoch: [23/70], Progress: 39.78 %, Steps: 2084, Current Learning Rate: 0.0005151, \u001b[96mTrain Loss: 4.286\n",
      "\u001b[0m\u001b[1mEpoch: [23/70], Progress: 40.86 %, Steps: 2085, Current Learning Rate: 0.0005154, \u001b[96mTrain Loss: 4.148\n",
      "\u001b[0m\u001b[1mEpoch: [23/70], Progress: 41.94 %, Steps: 2086, Current Learning Rate: 0.0005156, \u001b[91mTrain Loss: 4.358\n",
      "\u001b[0m\u001b[1mEpoch: [23/70], Progress: 43.01 %, Steps: 2087, Current Learning Rate: 0.0005158, \u001b[96mTrain Loss: 4.176\n",
      "\u001b[0m\u001b[1mEpoch: [23/70], Progress: 44.09 %, Steps: 2088, Current Learning Rate: 0.0005161, \u001b[96mTrain Loss: 3.979\n",
      "\u001b[0m\u001b[1mEpoch: [23/70], Progress: 45.16 %, Steps: 2089, Current Learning Rate: 0.0005163, \u001b[91mTrain Loss: 4.394\n",
      "\u001b[0m\u001b[1mEpoch: [23/70], Progress: 46.24 %, Steps: 2090, Current Learning Rate: 0.0005166, \u001b[96mTrain Loss: 4.237\n",
      "\u001b[0m\u001b[1mEpoch: [23/70], Progress: 47.31 %, Steps: 2091, Current Learning Rate: 0.0005168, \u001b[91mTrain Loss: 4.516\n",
      "\u001b[0m\u001b[1mEpoch: [23/70], Progress: 48.39 %, Steps: 2092, Current Learning Rate: 0.0005171, \u001b[96mTrain Loss: 4.434\n",
      "\u001b[0m\u001b[1mEpoch: [23/70], Progress: 49.46 %, Steps: 2093, Current Learning Rate: 0.0005173, \u001b[96mTrain Loss: 4.170\n",
      "\u001b[0m\u001b[1mEpoch: [23/70], Progress: 50.54 %, Steps: 2094, Current Learning Rate: 0.0005176, \u001b[91mTrain Loss: 4.302\n",
      "\u001b[0m\u001b[1mEpoch: [23/70], Progress: 51.61 %, Steps: 2095, Current Learning Rate: 0.0005178, \u001b[91mTrain Loss: 4.349\n",
      "\u001b[0m\u001b[1mEpoch: [23/70], Progress: 52.69 %, Steps: 2096, Current Learning Rate: 0.0005181, \u001b[91mTrain Loss: 4.418\n",
      "\u001b[0m\u001b[1mEpoch: [23/70], Progress: 53.76 %, Steps: 2097, Current Learning Rate: 0.0005183, \u001b[91mTrain Loss: 4.476\n",
      "\u001b[0m\u001b[1mEpoch: [23/70], Progress: 54.84 %, Steps: 2098, Current Learning Rate: 0.0005186, \u001b[96mTrain Loss: 4.180\n",
      "\u001b[0m\u001b[1mEpoch: [23/70], Progress: 55.91 %, Steps: 2099, Current Learning Rate: 0.0005188, \u001b[91mTrain Loss: 4.277\n",
      "\u001b[0m\u001b[1mEpoch: [23/70], Progress: 56.99 %, Steps: 2100, Current Learning Rate: 0.0005191, \u001b[91mTrain Loss: 4.404\n",
      "\u001b[0m\u001b[1mEpoch: [23/70], Progress: 58.06 %, Steps: 2101, Current Learning Rate: 0.0005193, \u001b[91mTrain Loss: 4.410\n",
      "\u001b[0m\u001b[1mEpoch: [23/70], Progress: 59.14 %, Steps: 2102, Current Learning Rate: 0.0005196, \u001b[96mTrain Loss: 4.356\n",
      "\u001b[0m\u001b[1mEpoch: [23/70], Progress: 60.22 %, Steps: 2103, Current Learning Rate: 0.0005198, \u001b[96mTrain Loss: 4.128\n",
      "\u001b[0m\u001b[1mEpoch: [23/70], Progress: 61.29 %, Steps: 2104, Current Learning Rate: 0.0005200, \u001b[91mTrain Loss: 4.591\n",
      "\u001b[0m\u001b[1mEpoch: [23/70], Progress: 62.37 %, Steps: 2105, Current Learning Rate: 0.0005203, \u001b[96mTrain Loss: 4.214\n",
      "\u001b[0m\u001b[1mEpoch: [23/70], Progress: 63.44 %, Steps: 2106, Current Learning Rate: 0.0005205, \u001b[91mTrain Loss: 4.290\n",
      "\u001b[0m\u001b[1mEpoch: [23/70], Progress: 64.52 %, Steps: 2107, Current Learning Rate: 0.0005208, \u001b[91mTrain Loss: 4.423\n",
      "\u001b[0m\u001b[1mEpoch: [23/70], Progress: 65.59 %, Steps: 2108, Current Learning Rate: 0.0005210, \u001b[96mTrain Loss: 4.286\n",
      "\u001b[0m\u001b[1mEpoch: [23/70], Progress: 66.67 %, Steps: 2109, Current Learning Rate: 0.0005213, \u001b[91mTrain Loss: 4.463\n",
      "\u001b[0m\u001b[1mEpoch: [23/70], Progress: 67.74 %, Steps: 2110, Current Learning Rate: 0.0005215, \u001b[96mTrain Loss: 4.285\n",
      "\u001b[0m\u001b[1mEpoch: [23/70], Progress: 68.82 %, Steps: 2111, Current Learning Rate: 0.0005218, \u001b[91mTrain Loss: 4.376\n",
      "\u001b[0m\u001b[1mEpoch: [23/70], Progress: 69.89 %, Steps: 2112, Current Learning Rate: 0.0005220, \u001b[91mTrain Loss: 4.385\n",
      "\u001b[0m\u001b[1mEpoch: [23/70], Progress: 70.97 %, Steps: 2113, Current Learning Rate: 0.0005223, \u001b[91mTrain Loss: 4.424\n",
      "\u001b[0m\u001b[1mEpoch: [23/70], Progress: 72.04 %, Steps: 2114, Current Learning Rate: 0.0005225, \u001b[96mTrain Loss: 4.295\n",
      "\u001b[0m\u001b[1mEpoch: [23/70], Progress: 73.12 %, Steps: 2115, Current Learning Rate: 0.0005228, \u001b[91mTrain Loss: 4.639\n",
      "\u001b[0m\u001b[1mEpoch: [23/70], Progress: 74.19 %, Steps: 2116, Current Learning Rate: 0.0005230, \u001b[96mTrain Loss: 3.988\n",
      "\u001b[0m\u001b[1mEpoch: [23/70], Progress: 75.27 %, Steps: 2117, Current Learning Rate: 0.0005233, \u001b[91mTrain Loss: 4.314\n",
      "\u001b[0m\u001b[1mEpoch: [23/70], Progress: 76.34 %, Steps: 2118, Current Learning Rate: 0.0005235, \u001b[91mTrain Loss: 4.324\n",
      "\u001b[0m\u001b[1mEpoch: [23/70], Progress: 77.42 %, Steps: 2119, Current Learning Rate: 0.0005238, \u001b[96mTrain Loss: 4.310\n",
      "\u001b[0m\u001b[1mEpoch: [23/70], Progress: 78.49 %, Steps: 2120, Current Learning Rate: 0.0005240, \u001b[96mTrain Loss: 4.227\n",
      "\u001b[0m\u001b[1mEpoch: [23/70], Progress: 79.57 %, Steps: 2121, Current Learning Rate: 0.0005242, \u001b[91mTrain Loss: 4.298\n",
      "\u001b[0m\u001b[1mEpoch: [23/70], Progress: 80.65 %, Steps: 2122, Current Learning Rate: 0.0005245, \u001b[96mTrain Loss: 4.119\n",
      "\u001b[0m\u001b[1mEpoch: [23/70], Progress: 81.72 %, Steps: 2123, Current Learning Rate: 0.0005247, \u001b[91mTrain Loss: 4.342\n",
      "\u001b[0m\u001b[1mEpoch: [23/70], Progress: 82.80 %, Steps: 2124, Current Learning Rate: 0.0005250, \u001b[96mTrain Loss: 4.292\n",
      "\u001b[0m\u001b[1mEpoch: [23/70], Progress: 83.87 %, Steps: 2125, Current Learning Rate: 0.0005252, \u001b[91mTrain Loss: 4.333\n",
      "\u001b[0m\u001b[1mEpoch: [23/70], Progress: 84.95 %, Steps: 2126, Current Learning Rate: 0.0005255, \u001b[91mTrain Loss: 4.459\n",
      "\u001b[0m\u001b[1mEpoch: [23/70], Progress: 86.02 %, Steps: 2127, Current Learning Rate: 0.0005257, \u001b[96mTrain Loss: 4.269\n",
      "\u001b[0m\u001b[1mEpoch: [23/70], Progress: 87.10 %, Steps: 2128, Current Learning Rate: 0.0005260, \u001b[91mTrain Loss: 4.484\n",
      "\u001b[0m\u001b[1mEpoch: [23/70], Progress: 88.17 %, Steps: 2129, Current Learning Rate: 0.0005262, \u001b[96mTrain Loss: 4.193\n",
      "\u001b[0m\u001b[1mEpoch: [23/70], Progress: 89.25 %, Steps: 2130, Current Learning Rate: 0.0005265, \u001b[91mTrain Loss: 4.368\n",
      "\u001b[0m\u001b[1mEpoch: [23/70], Progress: 90.32 %, Steps: 2131, Current Learning Rate: 0.0005267, \u001b[91mTrain Loss: 4.516\n",
      "\u001b[0m\u001b[1mEpoch: [23/70], Progress: 91.40 %, Steps: 2132, Current Learning Rate: 0.0005270, \u001b[96mTrain Loss: 4.250\n",
      "\u001b[0m\u001b[1mEpoch: [23/70], Progress: 92.47 %, Steps: 2133, Current Learning Rate: 0.0005272, \u001b[91mTrain Loss: 4.358\n",
      "\u001b[0m\u001b[1mEpoch: [23/70], Progress: 93.55 %, Steps: 2134, Current Learning Rate: 0.0005275, \u001b[91mTrain Loss: 4.652\n",
      "\u001b[0m\u001b[1mEpoch: [23/70], Progress: 94.62 %, Steps: 2135, Current Learning Rate: 0.0005277, \u001b[96mTrain Loss: 4.476\n",
      "\u001b[0m\u001b[1mEpoch: [23/70], Progress: 95.70 %, Steps: 2136, Current Learning Rate: 0.0005280, \u001b[96mTrain Loss: 4.328\n",
      "\u001b[0m\u001b[1mEpoch: [23/70], Progress: 96.77 %, Steps: 2137, Current Learning Rate: 0.0005282, \u001b[96mTrain Loss: 4.219\n",
      "\u001b[0m\u001b[1mEpoch: [23/70], Progress: 97.85 %, Steps: 2138, Current Learning Rate: 0.0005284, \u001b[91mTrain Loss: 4.233\n",
      "\u001b[0m\u001b[1mEpoch: [23/70], Progress: 98.92 %, Steps: 2139, Current Learning Rate: 0.0005287, \u001b[91mTrain Loss: 4.277\n",
      "\u001b[1m\u001b[93mComputing Validation Loss...\n",
      "\u001b[1mEpoch 23 Completed! Average Train Loss: 4.321, Average Validation Loss: 3.543\n",
      "Best Model saved in best_model/best_model.pt.\n",
      "\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m= Epoch [24/70] ==========================================\n",
      "\u001b[0m\u001b[1mEpoch: [24/70], Progress: 0.00 %, Steps: 2140, Current Learning Rate: 0.0005289, \u001b[91mTrain Loss: 4.073\n",
      "\u001b[0m\u001b[1mEpoch: [24/70], Progress: 1.08 %, Steps: 2141, Current Learning Rate: 0.0005292, \u001b[91mTrain Loss: 4.247\n",
      "\u001b[0m\u001b[1mEpoch: [24/70], Progress: 2.15 %, Steps: 2142, Current Learning Rate: 0.0005294, \u001b[96mTrain Loss: 4.140\n",
      "\u001b[0m\u001b[1mEpoch: [24/70], Progress: 3.23 %, Steps: 2143, Current Learning Rate: 0.0005297, \u001b[91mTrain Loss: 4.252\n",
      "\u001b[0m\u001b[1mEpoch: [24/70], Progress: 4.30 %, Steps: 2144, Current Learning Rate: 0.0005299, \u001b[96mTrain Loss: 4.003\n",
      "\u001b[0m\u001b[1mEpoch: [24/70], Progress: 5.38 %, Steps: 2145, Current Learning Rate: 0.0005302, \u001b[91mTrain Loss: 4.114\n",
      "\u001b[0m\u001b[1mEpoch: [24/70], Progress: 6.45 %, Steps: 2146, Current Learning Rate: 0.0005304, \u001b[91mTrain Loss: 4.304\n",
      "\u001b[0m\u001b[1mEpoch: [24/70], Progress: 7.53 %, Steps: 2147, Current Learning Rate: 0.0005307, \u001b[96mTrain Loss: 4.280\n",
      "\u001b[0m\u001b[1mEpoch: [24/70], Progress: 8.60 %, Steps: 2148, Current Learning Rate: 0.0005309, \u001b[96mTrain Loss: 3.933\n",
      "\u001b[0m\u001b[1mEpoch: [24/70], Progress: 9.68 %, Steps: 2149, Current Learning Rate: 0.0005312, \u001b[96mTrain Loss: 3.900\n",
      "\u001b[0m\u001b[1mEpoch: [24/70], Progress: 10.75 %, Steps: 2150, Current Learning Rate: 0.0005314, \u001b[91mTrain Loss: 4.247\n",
      "\u001b[0m\u001b[1mEpoch: [24/70], Progress: 11.83 %, Steps: 2151, Current Learning Rate: 0.0005317, \u001b[96mTrain Loss: 4.041\n",
      "\u001b[0m\u001b[1mEpoch: [24/70], Progress: 12.90 %, Steps: 2152, Current Learning Rate: 0.0005319, \u001b[91mTrain Loss: 4.154\n",
      "\u001b[0m\u001b[1mEpoch: [24/70], Progress: 13.98 %, Steps: 2153, Current Learning Rate: 0.0005322, \u001b[96mTrain Loss: 4.096\n",
      "\u001b[0m\u001b[1mEpoch: [24/70], Progress: 15.05 %, Steps: 2154, Current Learning Rate: 0.0005324, \u001b[96mTrain Loss: 4.084\n",
      "\u001b[0m\u001b[1mEpoch: [24/70], Progress: 16.13 %, Steps: 2155, Current Learning Rate: 0.0005326, \u001b[96mTrain Loss: 3.938\n",
      "\u001b[0m\u001b[1mEpoch: [24/70], Progress: 17.20 %, Steps: 2156, Current Learning Rate: 0.0005329, \u001b[91mTrain Loss: 4.032\n",
      "\u001b[0m\u001b[1mEpoch: [24/70], Progress: 18.28 %, Steps: 2157, Current Learning Rate: 0.0005331, \u001b[91mTrain Loss: 4.173\n",
      "\u001b[0m\u001b[1mEpoch: [24/70], Progress: 19.35 %, Steps: 2158, Current Learning Rate: 0.0005334, \u001b[91mTrain Loss: 4.219\n",
      "\u001b[0m\u001b[1mEpoch: [24/70], Progress: 20.43 %, Steps: 2159, Current Learning Rate: 0.0005336, \u001b[96mTrain Loss: 4.133\n",
      "\u001b[0m\u001b[1mEpoch: [24/70], Progress: 21.51 %, Steps: 2160, Current Learning Rate: 0.0005339, \u001b[91mTrain Loss: 4.262\n",
      "\u001b[0m\u001b[1mEpoch: [24/70], Progress: 22.58 %, Steps: 2161, Current Learning Rate: 0.0005341, \u001b[96mTrain Loss: 4.139\n",
      "\u001b[0m\u001b[1mEpoch: [24/70], Progress: 23.66 %, Steps: 2162, Current Learning Rate: 0.0005344, \u001b[96mTrain Loss: 4.086\n",
      "\u001b[0m\u001b[1mEpoch: [24/70], Progress: 24.73 %, Steps: 2163, Current Learning Rate: 0.0005346, \u001b[96mTrain Loss: 4.031\n",
      "\u001b[0m\u001b[1mEpoch: [24/70], Progress: 25.81 %, Steps: 2164, Current Learning Rate: 0.0005349, \u001b[96mTrain Loss: 3.877\n",
      "\u001b[0m\u001b[1mEpoch: [24/70], Progress: 26.88 %, Steps: 2165, Current Learning Rate: 0.0005351, \u001b[91mTrain Loss: 4.275\n",
      "\u001b[0m\u001b[1mEpoch: [24/70], Progress: 27.96 %, Steps: 2166, Current Learning Rate: 0.0005354, \u001b[96mTrain Loss: 4.215\n",
      "\u001b[0m\u001b[1mEpoch: [24/70], Progress: 29.03 %, Steps: 2167, Current Learning Rate: 0.0005356, \u001b[91mTrain Loss: 4.287\n",
      "\u001b[0m\u001b[1mEpoch: [24/70], Progress: 30.11 %, Steps: 2168, Current Learning Rate: 0.0005359, \u001b[96mTrain Loss: 4.136\n",
      "\u001b[0m\u001b[1mEpoch: [24/70], Progress: 31.18 %, Steps: 2169, Current Learning Rate: 0.0005361, \u001b[96mTrain Loss: 3.871\n",
      "\u001b[0m\u001b[1mEpoch: [24/70], Progress: 32.26 %, Steps: 2170, Current Learning Rate: 0.0005364, \u001b[96mTrain Loss: 3.841\n",
      "\u001b[0m\u001b[1mEpoch: [24/70], Progress: 33.33 %, Steps: 2171, Current Learning Rate: 0.0005366, \u001b[91mTrain Loss: 4.085\n",
      "\u001b[0m\u001b[1mEpoch: [24/70], Progress: 34.41 %, Steps: 2172, Current Learning Rate: 0.0005368, \u001b[91mTrain Loss: 4.265\n",
      "\u001b[0m\u001b[1mEpoch: [24/70], Progress: 35.48 %, Steps: 2173, Current Learning Rate: 0.0005371, \u001b[96mTrain Loss: 4.251\n",
      "\u001b[0m\u001b[1mEpoch: [24/70], Progress: 36.56 %, Steps: 2174, Current Learning Rate: 0.0005373, \u001b[96mTrain Loss: 4.151\n",
      "\u001b[0m\u001b[1mEpoch: [24/70], Progress: 37.63 %, Steps: 2175, Current Learning Rate: 0.0005376, \u001b[91mTrain Loss: 4.291\n",
      "\u001b[0m\u001b[1mEpoch: [24/70], Progress: 38.71 %, Steps: 2176, Current Learning Rate: 0.0005378, \u001b[96mTrain Loss: 4.063\n",
      "\u001b[0m\u001b[1mEpoch: [24/70], Progress: 39.78 %, Steps: 2177, Current Learning Rate: 0.0005381, \u001b[91mTrain Loss: 4.299\n",
      "\u001b[0m\u001b[1mEpoch: [24/70], Progress: 40.86 %, Steps: 2178, Current Learning Rate: 0.0005383, \u001b[96mTrain Loss: 4.118\n",
      "\u001b[0m\u001b[1mEpoch: [24/70], Progress: 41.94 %, Steps: 2179, Current Learning Rate: 0.0005386, \u001b[91mTrain Loss: 4.274\n",
      "\u001b[0m\u001b[1mEpoch: [24/70], Progress: 43.01 %, Steps: 2180, Current Learning Rate: 0.0005388, \u001b[96mTrain Loss: 4.103\n",
      "\u001b[0m\u001b[1mEpoch: [24/70], Progress: 44.09 %, Steps: 2181, Current Learning Rate: 0.0005391, \u001b[91mTrain Loss: 4.134\n",
      "\u001b[0m\u001b[1mEpoch: [24/70], Progress: 45.16 %, Steps: 2182, Current Learning Rate: 0.0005393, \u001b[96mTrain Loss: 4.058\n",
      "\u001b[0m\u001b[1mEpoch: [24/70], Progress: 46.24 %, Steps: 2183, Current Learning Rate: 0.0005396, \u001b[91mTrain Loss: 4.079\n",
      "\u001b[0m\u001b[1mEpoch: [24/70], Progress: 47.31 %, Steps: 2184, Current Learning Rate: 0.0005398, \u001b[91mTrain Loss: 4.175\n",
      "\u001b[0m\u001b[1mEpoch: [24/70], Progress: 48.39 %, Steps: 2185, Current Learning Rate: 0.0005401, \u001b[96mTrain Loss: 3.979\n",
      "\u001b[0m\u001b[1mEpoch: [24/70], Progress: 49.46 %, Steps: 2186, Current Learning Rate: 0.0005403, \u001b[96mTrain Loss: 3.957\n",
      "\u001b[0m\u001b[1mEpoch: [24/70], Progress: 50.54 %, Steps: 2187, Current Learning Rate: 0.0005406, \u001b[91mTrain Loss: 4.046\n",
      "\u001b[0m\u001b[1mEpoch: [24/70], Progress: 51.61 %, Steps: 2188, Current Learning Rate: 0.0005408, \u001b[96mTrain Loss: 3.971\n",
      "\u001b[0m\u001b[1mEpoch: [24/70], Progress: 52.69 %, Steps: 2189, Current Learning Rate: 0.0005410, \u001b[96mTrain Loss: 3.933\n",
      "\u001b[0m\u001b[1mEpoch: [24/70], Progress: 53.76 %, Steps: 2190, Current Learning Rate: 0.0005413, \u001b[91mTrain Loss: 4.165\n",
      "\u001b[0m\u001b[1mEpoch: [24/70], Progress: 54.84 %, Steps: 2191, Current Learning Rate: 0.0005415, \u001b[96mTrain Loss: 4.086\n",
      "\u001b[0m\u001b[1mEpoch: [24/70], Progress: 55.91 %, Steps: 2192, Current Learning Rate: 0.0005418, \u001b[96mTrain Loss: 4.043\n",
      "\u001b[0m\u001b[1mEpoch: [24/70], Progress: 56.99 %, Steps: 2193, Current Learning Rate: 0.0005420, \u001b[96mTrain Loss: 3.978\n",
      "\u001b[0m\u001b[1mEpoch: [24/70], Progress: 58.06 %, Steps: 2194, Current Learning Rate: 0.0005423, \u001b[91mTrain Loss: 4.250\n",
      "\u001b[0m\u001b[1mEpoch: [24/70], Progress: 59.14 %, Steps: 2195, Current Learning Rate: 0.0005425, \u001b[96mTrain Loss: 3.995\n",
      "\u001b[0m\u001b[1mEpoch: [24/70], Progress: 60.22 %, Steps: 2196, Current Learning Rate: 0.0005428, \u001b[91mTrain Loss: 4.369\n",
      "\u001b[0m\u001b[1mEpoch: [24/70], Progress: 61.29 %, Steps: 2197, Current Learning Rate: 0.0005430, \u001b[96mTrain Loss: 3.889\n",
      "\u001b[0m\u001b[1mEpoch: [24/70], Progress: 62.37 %, Steps: 2198, Current Learning Rate: 0.0005433, \u001b[91mTrain Loss: 4.188\n",
      "\u001b[0m\u001b[1mEpoch: [24/70], Progress: 63.44 %, Steps: 2199, Current Learning Rate: 0.0005435, \u001b[91mTrain Loss: 4.205\n",
      "\u001b[0m\u001b[1mEpoch: [24/70], Progress: 64.52 %, Steps: 2200, Current Learning Rate: 0.0005438, \u001b[96mTrain Loss: 4.108\n",
      "\u001b[0m\u001b[1mEpoch: [24/70], Progress: 65.59 %, Steps: 2201, Current Learning Rate: 0.0005440, \u001b[96mTrain Loss: 4.084\n",
      "\u001b[0m\u001b[1mEpoch: [24/70], Progress: 66.67 %, Steps: 2202, Current Learning Rate: 0.0005443, \u001b[91mTrain Loss: 4.244\n",
      "\u001b[0m\u001b[1mEpoch: [24/70], Progress: 67.74 %, Steps: 2203, Current Learning Rate: 0.0005445, \u001b[96mTrain Loss: 3.986\n",
      "\u001b[0m\u001b[1mEpoch: [24/70], Progress: 68.82 %, Steps: 2204, Current Learning Rate: 0.0005448, \u001b[91mTrain Loss: 4.116\n",
      "\u001b[0m\u001b[1mEpoch: [24/70], Progress: 69.89 %, Steps: 2205, Current Learning Rate: 0.0005450, \u001b[96mTrain Loss: 3.793\n",
      "\u001b[0m\u001b[1mEpoch: [24/70], Progress: 70.97 %, Steps: 2206, Current Learning Rate: 0.0005452, \u001b[91mTrain Loss: 4.169\n",
      "\u001b[0m\u001b[1mEpoch: [24/70], Progress: 72.04 %, Steps: 2207, Current Learning Rate: 0.0005455, \u001b[91mTrain Loss: 4.174\n",
      "\u001b[0m\u001b[1mEpoch: [24/70], Progress: 73.12 %, Steps: 2208, Current Learning Rate: 0.0005457, \u001b[96mTrain Loss: 3.801\n",
      "\u001b[0m\u001b[1mEpoch: [24/70], Progress: 74.19 %, Steps: 2209, Current Learning Rate: 0.0005460, \u001b[91mTrain Loss: 3.835\n",
      "\u001b[0m\u001b[1mEpoch: [24/70], Progress: 75.27 %, Steps: 2210, Current Learning Rate: 0.0005462, \u001b[91mTrain Loss: 4.239\n",
      "\u001b[0m\u001b[1mEpoch: [24/70], Progress: 76.34 %, Steps: 2211, Current Learning Rate: 0.0005465, \u001b[96mTrain Loss: 4.083\n",
      "\u001b[0m\u001b[1mEpoch: [24/70], Progress: 77.42 %, Steps: 2212, Current Learning Rate: 0.0005467, \u001b[96mTrain Loss: 4.077\n",
      "\u001b[0m\u001b[1mEpoch: [24/70], Progress: 78.49 %, Steps: 2213, Current Learning Rate: 0.0005470, \u001b[91mTrain Loss: 4.196\n",
      "\u001b[0m\u001b[1mEpoch: [24/70], Progress: 79.57 %, Steps: 2214, Current Learning Rate: 0.0005472, \u001b[96mTrain Loss: 4.170\n",
      "\u001b[0m\u001b[1mEpoch: [24/70], Progress: 80.65 %, Steps: 2215, Current Learning Rate: 0.0005475, \u001b[96mTrain Loss: 4.064\n",
      "\u001b[0m\u001b[1mEpoch: [24/70], Progress: 81.72 %, Steps: 2216, Current Learning Rate: 0.0005477, \u001b[91mTrain Loss: 4.159\n",
      "\u001b[0m\u001b[1mEpoch: [24/70], Progress: 82.80 %, Steps: 2217, Current Learning Rate: 0.0005480, \u001b[91mTrain Loss: 4.214\n",
      "\u001b[0m\u001b[1mEpoch: [24/70], Progress: 83.87 %, Steps: 2218, Current Learning Rate: 0.0005482, \u001b[96mTrain Loss: 4.092\n",
      "\u001b[0m\u001b[1mEpoch: [24/70], Progress: 84.95 %, Steps: 2219, Current Learning Rate: 0.0005485, \u001b[91mTrain Loss: 4.257\n",
      "\u001b[0m\u001b[1mEpoch: [24/70], Progress: 86.02 %, Steps: 2220, Current Learning Rate: 0.0005487, \u001b[96mTrain Loss: 4.035\n",
      "\u001b[0m\u001b[1mEpoch: [24/70], Progress: 87.10 %, Steps: 2221, Current Learning Rate: 0.0005490, \u001b[96mTrain Loss: 3.959\n",
      "\u001b[0m\u001b[1mEpoch: [24/70], Progress: 88.17 %, Steps: 2222, Current Learning Rate: 0.0005492, \u001b[91mTrain Loss: 4.211\n",
      "\u001b[0m\u001b[1mEpoch: [24/70], Progress: 89.25 %, Steps: 2223, Current Learning Rate: 0.0005494, \u001b[96mTrain Loss: 4.168\n",
      "\u001b[0m\u001b[1mEpoch: [24/70], Progress: 90.32 %, Steps: 2224, Current Learning Rate: 0.0005497, \u001b[91mTrain Loss: 4.289\n",
      "\u001b[0m\u001b[1mEpoch: [24/70], Progress: 91.40 %, Steps: 2225, Current Learning Rate: 0.0005499, \u001b[96mTrain Loss: 4.032\n",
      "\u001b[0m\u001b[1mEpoch: [24/70], Progress: 92.47 %, Steps: 2226, Current Learning Rate: 0.0005502, \u001b[91mTrain Loss: 4.388\n",
      "\u001b[0m\u001b[1mEpoch: [24/70], Progress: 93.55 %, Steps: 2227, Current Learning Rate: 0.0005504, \u001b[96mTrain Loss: 4.083\n",
      "\u001b[0m\u001b[1mEpoch: [24/70], Progress: 94.62 %, Steps: 2228, Current Learning Rate: 0.0005507, \u001b[96mTrain Loss: 3.901\n",
      "\u001b[0m\u001b[1mEpoch: [24/70], Progress: 95.70 %, Steps: 2229, Current Learning Rate: 0.0005509, \u001b[91mTrain Loss: 4.060\n",
      "\u001b[0m\u001b[1mEpoch: [24/70], Progress: 96.77 %, Steps: 2230, Current Learning Rate: 0.0005512, \u001b[91mTrain Loss: 4.286\n",
      "\u001b[0m\u001b[1mEpoch: [24/70], Progress: 97.85 %, Steps: 2231, Current Learning Rate: 0.0005514, \u001b[96mTrain Loss: 3.978\n",
      "\u001b[0m\u001b[1mEpoch: [24/70], Progress: 98.92 %, Steps: 2232, Current Learning Rate: 0.0005517, \u001b[91mTrain Loss: 4.113\n",
      "\u001b[1m\u001b[93mComputing Validation Loss...\n",
      "\u001b[1mEpoch 24 Completed! Average Train Loss: 4.109, Average Validation Loss: 3.339\n",
      "Best Model saved in best_model/best_model.pt.\n",
      "\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m= Epoch [25/70] ==========================================\n",
      "\u001b[0m\u001b[1mEpoch: [25/70], Progress: 0.00 %, Steps: 2233, Current Learning Rate: 0.0005519, \u001b[91mTrain Loss: 3.934\n",
      "\u001b[0m\u001b[1mEpoch: [25/70], Progress: 1.08 %, Steps: 2234, Current Learning Rate: 0.0005522, \u001b[96mTrain Loss: 3.927\n",
      "\u001b[0m\u001b[1mEpoch: [25/70], Progress: 2.15 %, Steps: 2235, Current Learning Rate: 0.0005524, \u001b[96mTrain Loss: 3.722\n",
      "\u001b[0m\u001b[1mEpoch: [25/70], Progress: 3.23 %, Steps: 2236, Current Learning Rate: 0.0005527, \u001b[91mTrain Loss: 4.038\n",
      "\u001b[0m\u001b[1mEpoch: [25/70], Progress: 4.30 %, Steps: 2237, Current Learning Rate: 0.0005529, \u001b[96mTrain Loss: 3.781\n",
      "\u001b[0m\u001b[1mEpoch: [25/70], Progress: 5.38 %, Steps: 2238, Current Learning Rate: 0.0005532, \u001b[96mTrain Loss: 3.655\n",
      "\u001b[0m\u001b[1mEpoch: [25/70], Progress: 6.45 %, Steps: 2239, Current Learning Rate: 0.0005534, \u001b[91mTrain Loss: 3.738\n",
      "\u001b[0m\u001b[1mEpoch: [25/70], Progress: 7.53 %, Steps: 2240, Current Learning Rate: 0.0005536, \u001b[91mTrain Loss: 3.796\n",
      "\u001b[0m\u001b[1mEpoch: [25/70], Progress: 8.60 %, Steps: 2241, Current Learning Rate: 0.0005539, \u001b[91mTrain Loss: 4.140\n",
      "\u001b[0m\u001b[1mEpoch: [25/70], Progress: 9.68 %, Steps: 2242, Current Learning Rate: 0.0005541, \u001b[96mTrain Loss: 3.985\n",
      "\u001b[0m\u001b[1mEpoch: [25/70], Progress: 10.75 %, Steps: 2243, Current Learning Rate: 0.0005544, \u001b[96mTrain Loss: 3.959\n",
      "\u001b[0m\u001b[1mEpoch: [25/70], Progress: 11.83 %, Steps: 2244, Current Learning Rate: 0.0005546, \u001b[96mTrain Loss: 3.820\n",
      "\u001b[0m\u001b[1mEpoch: [25/70], Progress: 12.90 %, Steps: 2245, Current Learning Rate: 0.0005549, \u001b[91mTrain Loss: 3.928\n",
      "\u001b[0m\u001b[1mEpoch: [25/70], Progress: 13.98 %, Steps: 2246, Current Learning Rate: 0.0005551, \u001b[96mTrain Loss: 3.732\n",
      "\u001b[0m\u001b[1mEpoch: [25/70], Progress: 15.05 %, Steps: 2247, Current Learning Rate: 0.0005554, \u001b[91mTrain Loss: 4.044\n",
      "\u001b[0m\u001b[1mEpoch: [25/70], Progress: 16.13 %, Steps: 2248, Current Learning Rate: 0.0005556, \u001b[96mTrain Loss: 3.887\n",
      "\u001b[0m\u001b[1mEpoch: [25/70], Progress: 17.20 %, Steps: 2249, Current Learning Rate: 0.0005559, \u001b[96mTrain Loss: 3.768\n",
      "\u001b[0m\u001b[1mEpoch: [25/70], Progress: 18.28 %, Steps: 2250, Current Learning Rate: 0.0005561, \u001b[91mTrain Loss: 3.881\n",
      "\u001b[0m\u001b[1mEpoch: [25/70], Progress: 19.35 %, Steps: 2251, Current Learning Rate: 0.0005564, \u001b[91mTrain Loss: 3.953\n",
      "\u001b[0m\u001b[1mEpoch: [25/70], Progress: 20.43 %, Steps: 2252, Current Learning Rate: 0.0005566, \u001b[96mTrain Loss: 3.896\n",
      "\u001b[0m\u001b[1mEpoch: [25/70], Progress: 21.51 %, Steps: 2253, Current Learning Rate: 0.0005569, \u001b[96mTrain Loss: 3.888\n",
      "\u001b[0m\u001b[1mEpoch: [25/70], Progress: 22.58 %, Steps: 2254, Current Learning Rate: 0.0005571, \u001b[91mTrain Loss: 3.992\n",
      "\u001b[0m\u001b[1mEpoch: [25/70], Progress: 23.66 %, Steps: 2255, Current Learning Rate: 0.0005574, \u001b[96mTrain Loss: 3.916\n",
      "\u001b[0m\u001b[1mEpoch: [25/70], Progress: 24.73 %, Steps: 2256, Current Learning Rate: 0.0005576, \u001b[96mTrain Loss: 3.863\n",
      "\u001b[0m\u001b[1mEpoch: [25/70], Progress: 25.81 %, Steps: 2257, Current Learning Rate: 0.0005578, \u001b[96mTrain Loss: 3.626\n",
      "\u001b[0m\u001b[1mEpoch: [25/70], Progress: 26.88 %, Steps: 2258, Current Learning Rate: 0.0005581, \u001b[91mTrain Loss: 4.003\n",
      "\u001b[0m\u001b[1mEpoch: [25/70], Progress: 27.96 %, Steps: 2259, Current Learning Rate: 0.0005583, \u001b[96mTrain Loss: 3.928\n",
      "\u001b[0m\u001b[1mEpoch: [25/70], Progress: 29.03 %, Steps: 2260, Current Learning Rate: 0.0005586, \u001b[96mTrain Loss: 3.888\n",
      "\u001b[0m\u001b[1mEpoch: [25/70], Progress: 30.11 %, Steps: 2261, Current Learning Rate: 0.0005588, \u001b[91mTrain Loss: 3.982\n",
      "\u001b[0m\u001b[1mEpoch: [25/70], Progress: 31.18 %, Steps: 2262, Current Learning Rate: 0.0005591, \u001b[96mTrain Loss: 3.827\n",
      "\u001b[0m\u001b[1mEpoch: [25/70], Progress: 32.26 %, Steps: 2263, Current Learning Rate: 0.0005593, \u001b[91mTrain Loss: 3.840\n",
      "\u001b[0m\u001b[1mEpoch: [25/70], Progress: 33.33 %, Steps: 2264, Current Learning Rate: 0.0005596, \u001b[91mTrain Loss: 4.015\n",
      "\u001b[0m\u001b[1mEpoch: [25/70], Progress: 34.41 %, Steps: 2265, Current Learning Rate: 0.0005598, \u001b[96mTrain Loss: 3.939\n",
      "\u001b[0m\u001b[1mEpoch: [25/70], Progress: 35.48 %, Steps: 2266, Current Learning Rate: 0.0005601, \u001b[91mTrain Loss: 3.973\n",
      "\u001b[0m\u001b[1mEpoch: [25/70], Progress: 36.56 %, Steps: 2267, Current Learning Rate: 0.0005603, \u001b[91mTrain Loss: 4.122\n",
      "\u001b[0m\u001b[1mEpoch: [25/70], Progress: 37.63 %, Steps: 2268, Current Learning Rate: 0.0005606, \u001b[96mTrain Loss: 3.826\n",
      "\u001b[0m\u001b[1mEpoch: [25/70], Progress: 38.71 %, Steps: 2269, Current Learning Rate: 0.0005608, \u001b[91mTrain Loss: 3.954\n",
      "\u001b[0m\u001b[1mEpoch: [25/70], Progress: 39.78 %, Steps: 2270, Current Learning Rate: 0.0005611, \u001b[96mTrain Loss: 3.848\n",
      "\u001b[0m\u001b[1mEpoch: [25/70], Progress: 40.86 %, Steps: 2271, Current Learning Rate: 0.0005613, \u001b[91mTrain Loss: 3.871\n",
      "\u001b[0m\u001b[1mEpoch: [25/70], Progress: 41.94 %, Steps: 2272, Current Learning Rate: 0.0005616, \u001b[96mTrain Loss: 3.720\n",
      "\u001b[0m\u001b[1mEpoch: [25/70], Progress: 43.01 %, Steps: 2273, Current Learning Rate: 0.0005618, \u001b[91mTrain Loss: 3.946\n",
      "\u001b[0m\u001b[1mEpoch: [25/70], Progress: 44.09 %, Steps: 2274, Current Learning Rate: 0.0005620, \u001b[96mTrain Loss: 3.785\n",
      "\u001b[0m\u001b[1mEpoch: [25/70], Progress: 45.16 %, Steps: 2275, Current Learning Rate: 0.0005623, \u001b[91mTrain Loss: 3.987\n",
      "\u001b[0m\u001b[1mEpoch: [25/70], Progress: 46.24 %, Steps: 2276, Current Learning Rate: 0.0005625, \u001b[91mTrain Loss: 4.022\n",
      "\u001b[0m\u001b[1mEpoch: [25/70], Progress: 47.31 %, Steps: 2277, Current Learning Rate: 0.0005628, \u001b[96mTrain Loss: 3.976\n",
      "\u001b[0m\u001b[1mEpoch: [25/70], Progress: 48.39 %, Steps: 2278, Current Learning Rate: 0.0005630, \u001b[91mTrain Loss: 4.021\n",
      "\u001b[0m\u001b[1mEpoch: [25/70], Progress: 49.46 %, Steps: 2279, Current Learning Rate: 0.0005633, \u001b[96mTrain Loss: 3.781\n",
      "\u001b[0m\u001b[1mEpoch: [25/70], Progress: 50.54 %, Steps: 2280, Current Learning Rate: 0.0005635, \u001b[96mTrain Loss: 3.741\n",
      "\u001b[0m\u001b[1mEpoch: [25/70], Progress: 51.61 %, Steps: 2281, Current Learning Rate: 0.0005638, \u001b[91mTrain Loss: 3.784\n",
      "\u001b[0m\u001b[1mEpoch: [25/70], Progress: 52.69 %, Steps: 2282, Current Learning Rate: 0.0005640, \u001b[91mTrain Loss: 3.941\n",
      "\u001b[0m\u001b[1mEpoch: [25/70], Progress: 53.76 %, Steps: 2283, Current Learning Rate: 0.0005643, \u001b[96mTrain Loss: 3.738\n",
      "\u001b[0m\u001b[1mEpoch: [25/70], Progress: 54.84 %, Steps: 2284, Current Learning Rate: 0.0005645, \u001b[96mTrain Loss: 3.639\n",
      "\u001b[0m\u001b[1mEpoch: [25/70], Progress: 55.91 %, Steps: 2285, Current Learning Rate: 0.0005648, \u001b[91mTrain Loss: 3.771\n",
      "\u001b[0m\u001b[1mEpoch: [25/70], Progress: 56.99 %, Steps: 2286, Current Learning Rate: 0.0005650, \u001b[91mTrain Loss: 3.890\n",
      "\u001b[0m\u001b[1mEpoch: [25/70], Progress: 58.06 %, Steps: 2287, Current Learning Rate: 0.0005653, \u001b[91mTrain Loss: 4.018\n",
      "\u001b[0m\u001b[1mEpoch: [25/70], Progress: 59.14 %, Steps: 2288, Current Learning Rate: 0.0005655, \u001b[96mTrain Loss: 3.684\n",
      "\u001b[0m\u001b[1mEpoch: [25/70], Progress: 60.22 %, Steps: 2289, Current Learning Rate: 0.0005658, \u001b[91mTrain Loss: 3.951\n",
      "\u001b[0m\u001b[1mEpoch: [25/70], Progress: 61.29 %, Steps: 2290, Current Learning Rate: 0.0005660, \u001b[91mTrain Loss: 4.075\n",
      "\u001b[0m\u001b[1mEpoch: [25/70], Progress: 62.37 %, Steps: 2291, Current Learning Rate: 0.0005662, \u001b[96mTrain Loss: 3.732\n",
      "\u001b[0m\u001b[1mEpoch: [25/70], Progress: 63.44 %, Steps: 2292, Current Learning Rate: 0.0005665, \u001b[91mTrain Loss: 4.047\n",
      "\u001b[0m\u001b[1mEpoch: [25/70], Progress: 64.52 %, Steps: 2293, Current Learning Rate: 0.0005667, \u001b[96mTrain Loss: 3.852\n",
      "\u001b[0m\u001b[1mEpoch: [25/70], Progress: 65.59 %, Steps: 2294, Current Learning Rate: 0.0005670, \u001b[96mTrain Loss: 3.847\n",
      "\u001b[0m\u001b[1mEpoch: [25/70], Progress: 66.67 %, Steps: 2295, Current Learning Rate: 0.0005672, \u001b[96mTrain Loss: 3.715\n",
      "\u001b[0m\u001b[1mEpoch: [25/70], Progress: 67.74 %, Steps: 2296, Current Learning Rate: 0.0005675, \u001b[91mTrain Loss: 3.896\n",
      "\u001b[0m\u001b[1mEpoch: [25/70], Progress: 68.82 %, Steps: 2297, Current Learning Rate: 0.0005677, \u001b[96mTrain Loss: 3.842\n",
      "\u001b[0m\u001b[1mEpoch: [25/70], Progress: 69.89 %, Steps: 2298, Current Learning Rate: 0.0005680, \u001b[96mTrain Loss: 3.801\n",
      "\u001b[0m\u001b[1mEpoch: [25/70], Progress: 70.97 %, Steps: 2299, Current Learning Rate: 0.0005682, \u001b[91mTrain Loss: 3.823\n",
      "\u001b[0m\u001b[1mEpoch: [25/70], Progress: 72.04 %, Steps: 2300, Current Learning Rate: 0.0005685, \u001b[91mTrain Loss: 3.843\n",
      "\u001b[0m\u001b[1mEpoch: [25/70], Progress: 73.12 %, Steps: 2301, Current Learning Rate: 0.0005687, \u001b[96mTrain Loss: 3.834\n",
      "\u001b[0m\u001b[1mEpoch: [25/70], Progress: 74.19 %, Steps: 2302, Current Learning Rate: 0.0005690, \u001b[91mTrain Loss: 3.947\n",
      "\u001b[0m\u001b[1mEpoch: [25/70], Progress: 75.27 %, Steps: 2303, Current Learning Rate: 0.0005692, \u001b[96mTrain Loss: 3.882\n",
      "\u001b[0m\u001b[1mEpoch: [25/70], Progress: 76.34 %, Steps: 2304, Current Learning Rate: 0.0005695, \u001b[91mTrain Loss: 4.199\n",
      "\u001b[0m\u001b[1mEpoch: [25/70], Progress: 77.42 %, Steps: 2305, Current Learning Rate: 0.0005697, \u001b[96mTrain Loss: 3.879\n",
      "\u001b[0m\u001b[1mEpoch: [25/70], Progress: 78.49 %, Steps: 2306, Current Learning Rate: 0.0005700, \u001b[91mTrain Loss: 3.915\n",
      "\u001b[0m\u001b[1mEpoch: [25/70], Progress: 79.57 %, Steps: 2307, Current Learning Rate: 0.0005702, \u001b[96mTrain Loss: 3.846\n",
      "\u001b[0m\u001b[1mEpoch: [25/70], Progress: 80.65 %, Steps: 2308, Current Learning Rate: 0.0005704, \u001b[91mTrain Loss: 3.939\n",
      "\u001b[0m\u001b[1mEpoch: [25/70], Progress: 81.72 %, Steps: 2309, Current Learning Rate: 0.0005707, \u001b[96mTrain Loss: 3.909\n",
      "\u001b[0m\u001b[1mEpoch: [25/70], Progress: 82.80 %, Steps: 2310, Current Learning Rate: 0.0005709, \u001b[91mTrain Loss: 3.973\n",
      "\u001b[0m\u001b[1mEpoch: [25/70], Progress: 83.87 %, Steps: 2311, Current Learning Rate: 0.0005712, \u001b[96mTrain Loss: 3.883\n",
      "\u001b[0m\u001b[1mEpoch: [25/70], Progress: 84.95 %, Steps: 2312, Current Learning Rate: 0.0005714, \u001b[91mTrain Loss: 4.005\n",
      "\u001b[0m\u001b[1mEpoch: [25/70], Progress: 86.02 %, Steps: 2313, Current Learning Rate: 0.0005717, \u001b[91mTrain Loss: 4.116\n",
      "\u001b[0m\u001b[1mEpoch: [25/70], Progress: 87.10 %, Steps: 2314, Current Learning Rate: 0.0005719, \u001b[96mTrain Loss: 3.709\n",
      "\u001b[0m\u001b[1mEpoch: [25/70], Progress: 88.17 %, Steps: 2315, Current Learning Rate: 0.0005722, \u001b[91mTrain Loss: 4.091\n",
      "\u001b[0m\u001b[1mEpoch: [25/70], Progress: 89.25 %, Steps: 2316, Current Learning Rate: 0.0005724, \u001b[96mTrain Loss: 3.991\n",
      "\u001b[0m\u001b[1mEpoch: [25/70], Progress: 90.32 %, Steps: 2317, Current Learning Rate: 0.0005727, \u001b[96mTrain Loss: 3.785\n",
      "\u001b[0m\u001b[1mEpoch: [25/70], Progress: 91.40 %, Steps: 2318, Current Learning Rate: 0.0005729, \u001b[91mTrain Loss: 3.894\n",
      "\u001b[0m\u001b[1mEpoch: [25/70], Progress: 92.47 %, Steps: 2319, Current Learning Rate: 0.0005732, \u001b[96mTrain Loss: 3.813\n",
      "\u001b[0m\u001b[1mEpoch: [25/70], Progress: 93.55 %, Steps: 2320, Current Learning Rate: 0.0005734, \u001b[91mTrain Loss: 3.978\n",
      "\u001b[0m\u001b[1mEpoch: [25/70], Progress: 94.62 %, Steps: 2321, Current Learning Rate: 0.0005737, \u001b[91mTrain Loss: 3.998\n",
      "\u001b[0m\u001b[1mEpoch: [25/70], Progress: 95.70 %, Steps: 2322, Current Learning Rate: 0.0005739, \u001b[96mTrain Loss: 3.967\n",
      "\u001b[0m\u001b[1mEpoch: [25/70], Progress: 96.77 %, Steps: 2323, Current Learning Rate: 0.0005742, \u001b[96mTrain Loss: 3.658\n",
      "\u001b[0m\u001b[1mEpoch: [25/70], Progress: 97.85 %, Steps: 2324, Current Learning Rate: 0.0005744, \u001b[91mTrain Loss: 4.092\n",
      "\u001b[0m\u001b[1mEpoch: [25/70], Progress: 98.92 %, Steps: 2325, Current Learning Rate: 0.0005746, \u001b[96mTrain Loss: 3.870\n",
      "\u001b[1m\u001b[93mComputing Validation Loss...\n",
      "\u001b[1mEpoch 25 Completed! Average Train Loss: 3.892, Average Validation Loss: 3.092\n",
      "Best Model saved in best_model/best_model.pt.\n",
      "\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m= Epoch [26/70] ==========================================\n",
      "\u001b[0m\u001b[1mEpoch: [26/70], Progress: 0.00 %, Steps: 2326, Current Learning Rate: 0.0005749, \u001b[91mTrain Loss: 3.729\n",
      "\u001b[0m\u001b[1mEpoch: [26/70], Progress: 1.08 %, Steps: 2327, Current Learning Rate: 0.0005751, \u001b[96mTrain Loss: 3.528\n",
      "\u001b[0m\u001b[1mEpoch: [26/70], Progress: 2.15 %, Steps: 2328, Current Learning Rate: 0.0005754, \u001b[91mTrain Loss: 3.753\n",
      "\u001b[0m\u001b[1mEpoch: [26/70], Progress: 3.23 %, Steps: 2329, Current Learning Rate: 0.0005756, \u001b[96mTrain Loss: 3.562\n",
      "\u001b[0m\u001b[1mEpoch: [26/70], Progress: 4.30 %, Steps: 2330, Current Learning Rate: 0.0005759, \u001b[96mTrain Loss: 3.377\n",
      "\u001b[0m\u001b[1mEpoch: [26/70], Progress: 5.38 %, Steps: 2331, Current Learning Rate: 0.0005761, \u001b[91mTrain Loss: 3.742\n",
      "\u001b[0m\u001b[1mEpoch: [26/70], Progress: 6.45 %, Steps: 2332, Current Learning Rate: 0.0005764, \u001b[96mTrain Loss: 3.532\n",
      "\u001b[0m\u001b[1mEpoch: [26/70], Progress: 7.53 %, Steps: 2333, Current Learning Rate: 0.0005766, \u001b[91mTrain Loss: 3.549\n",
      "\u001b[0m\u001b[1mEpoch: [26/70], Progress: 8.60 %, Steps: 2334, Current Learning Rate: 0.0005769, \u001b[96mTrain Loss: 3.505\n",
      "\u001b[0m\u001b[1mEpoch: [26/70], Progress: 9.68 %, Steps: 2335, Current Learning Rate: 0.0005771, \u001b[91mTrain Loss: 3.672\n",
      "\u001b[0m\u001b[1mEpoch: [26/70], Progress: 10.75 %, Steps: 2336, Current Learning Rate: 0.0005774, \u001b[96mTrain Loss: 3.345\n",
      "\u001b[0m\u001b[1mEpoch: [26/70], Progress: 11.83 %, Steps: 2337, Current Learning Rate: 0.0005776, \u001b[91mTrain Loss: 3.690\n",
      "\u001b[0m\u001b[1mEpoch: [26/70], Progress: 12.90 %, Steps: 2338, Current Learning Rate: 0.0005779, \u001b[91mTrain Loss: 3.699\n",
      "\u001b[0m\u001b[1mEpoch: [26/70], Progress: 13.98 %, Steps: 2339, Current Learning Rate: 0.0005781, \u001b[91mTrain Loss: 3.720\n",
      "\u001b[0m\u001b[1mEpoch: [26/70], Progress: 15.05 %, Steps: 2340, Current Learning Rate: 0.0005784, \u001b[96mTrain Loss: 3.702\n",
      "\u001b[0m\u001b[1mEpoch: [26/70], Progress: 16.13 %, Steps: 2341, Current Learning Rate: 0.0005786, \u001b[96mTrain Loss: 3.584\n",
      "\u001b[0m\u001b[1mEpoch: [26/70], Progress: 17.20 %, Steps: 2342, Current Learning Rate: 0.0005788, \u001b[96mTrain Loss: 3.506\n",
      "\u001b[0m\u001b[1mEpoch: [26/70], Progress: 18.28 %, Steps: 2343, Current Learning Rate: 0.0005791, \u001b[91mTrain Loss: 3.641\n",
      "\u001b[0m\u001b[1mEpoch: [26/70], Progress: 19.35 %, Steps: 2344, Current Learning Rate: 0.0005793, \u001b[96mTrain Loss: 3.517\n",
      "\u001b[0m\u001b[1mEpoch: [26/70], Progress: 20.43 %, Steps: 2345, Current Learning Rate: 0.0005796, \u001b[91mTrain Loss: 3.756\n",
      "\u001b[0m\u001b[1mEpoch: [26/70], Progress: 21.51 %, Steps: 2346, Current Learning Rate: 0.0005798, \u001b[96mTrain Loss: 3.558\n",
      "\u001b[0m\u001b[1mEpoch: [26/70], Progress: 22.58 %, Steps: 2347, Current Learning Rate: 0.0005801, \u001b[91mTrain Loss: 3.608\n",
      "\u001b[0m\u001b[1mEpoch: [26/70], Progress: 23.66 %, Steps: 2348, Current Learning Rate: 0.0005803, \u001b[96mTrain Loss: 3.225\n",
      "\u001b[0m\u001b[1mEpoch: [26/70], Progress: 24.73 %, Steps: 2349, Current Learning Rate: 0.0005806, \u001b[91mTrain Loss: 3.659\n",
      "\u001b[0m\u001b[1mEpoch: [26/70], Progress: 25.81 %, Steps: 2350, Current Learning Rate: 0.0005808, \u001b[91mTrain Loss: 3.757\n",
      "\u001b[0m\u001b[1mEpoch: [26/70], Progress: 26.88 %, Steps: 2351, Current Learning Rate: 0.0005811, \u001b[96mTrain Loss: 3.616\n",
      "\u001b[0m\u001b[1mEpoch: [26/70], Progress: 27.96 %, Steps: 2352, Current Learning Rate: 0.0005813, \u001b[91mTrain Loss: 3.783\n",
      "\u001b[0m\u001b[1mEpoch: [26/70], Progress: 29.03 %, Steps: 2353, Current Learning Rate: 0.0005816, \u001b[96mTrain Loss: 3.715\n",
      "\u001b[0m\u001b[1mEpoch: [26/70], Progress: 30.11 %, Steps: 2354, Current Learning Rate: 0.0005818, \u001b[96mTrain Loss: 3.661\n",
      "\u001b[0m\u001b[1mEpoch: [26/70], Progress: 31.18 %, Steps: 2355, Current Learning Rate: 0.0005821, \u001b[91mTrain Loss: 3.749\n",
      "\u001b[0m\u001b[1mEpoch: [26/70], Progress: 32.26 %, Steps: 2356, Current Learning Rate: 0.0005823, \u001b[96mTrain Loss: 3.462\n",
      "\u001b[0m\u001b[1mEpoch: [26/70], Progress: 33.33 %, Steps: 2357, Current Learning Rate: 0.0005826, \u001b[91mTrain Loss: 3.731\n",
      "\u001b[0m\u001b[1mEpoch: [26/70], Progress: 34.41 %, Steps: 2358, Current Learning Rate: 0.0005828, \u001b[96mTrain Loss: 3.503\n",
      "\u001b[0m\u001b[1mEpoch: [26/70], Progress: 35.48 %, Steps: 2359, Current Learning Rate: 0.0005830, \u001b[91mTrain Loss: 3.610\n",
      "\u001b[0m\u001b[1mEpoch: [26/70], Progress: 36.56 %, Steps: 2360, Current Learning Rate: 0.0005833, \u001b[96mTrain Loss: 3.487\n",
      "\u001b[0m\u001b[1mEpoch: [26/70], Progress: 37.63 %, Steps: 2361, Current Learning Rate: 0.0005835, \u001b[91mTrain Loss: 3.639\n",
      "\u001b[0m\u001b[1mEpoch: [26/70], Progress: 38.71 %, Steps: 2362, Current Learning Rate: 0.0005838, \u001b[91mTrain Loss: 4.017\n",
      "\u001b[0m\u001b[1mEpoch: [26/70], Progress: 39.78 %, Steps: 2363, Current Learning Rate: 0.0005840, \u001b[96mTrain Loss: 3.753\n",
      "\u001b[0m\u001b[1mEpoch: [26/70], Progress: 40.86 %, Steps: 2364, Current Learning Rate: 0.0005843, \u001b[96mTrain Loss: 3.706\n",
      "\u001b[0m\u001b[1mEpoch: [26/70], Progress: 41.94 %, Steps: 2365, Current Learning Rate: 0.0005845, \u001b[96mTrain Loss: 3.607\n",
      "\u001b[0m\u001b[1mEpoch: [26/70], Progress: 43.01 %, Steps: 2366, Current Learning Rate: 0.0005848, \u001b[96mTrain Loss: 3.605\n",
      "\u001b[0m\u001b[1mEpoch: [26/70], Progress: 44.09 %, Steps: 2367, Current Learning Rate: 0.0005850, \u001b[91mTrain Loss: 3.891\n",
      "\u001b[0m\u001b[1mEpoch: [26/70], Progress: 45.16 %, Steps: 2368, Current Learning Rate: 0.0005853, \u001b[96mTrain Loss: 3.868\n",
      "\u001b[0m\u001b[1mEpoch: [26/70], Progress: 46.24 %, Steps: 2369, Current Learning Rate: 0.0005855, \u001b[96mTrain Loss: 3.714\n",
      "\u001b[0m\u001b[1mEpoch: [26/70], Progress: 47.31 %, Steps: 2370, Current Learning Rate: 0.0005858, \u001b[91mTrain Loss: 3.764\n",
      "\u001b[0m\u001b[1mEpoch: [26/70], Progress: 48.39 %, Steps: 2371, Current Learning Rate: 0.0005860, \u001b[96mTrain Loss: 3.721\n",
      "\u001b[0m\u001b[1mEpoch: [26/70], Progress: 49.46 %, Steps: 2372, Current Learning Rate: 0.0005863, \u001b[91mTrain Loss: 3.873\n",
      "\u001b[0m\u001b[1mEpoch: [26/70], Progress: 50.54 %, Steps: 2373, Current Learning Rate: 0.0005865, \u001b[96mTrain Loss: 3.661\n",
      "\u001b[0m\u001b[1mEpoch: [26/70], Progress: 51.61 %, Steps: 2374, Current Learning Rate: 0.0005868, \u001b[96mTrain Loss: 3.472\n",
      "\u001b[0m\u001b[1mEpoch: [26/70], Progress: 52.69 %, Steps: 2375, Current Learning Rate: 0.0005870, \u001b[91mTrain Loss: 3.590\n",
      "\u001b[0m\u001b[1mEpoch: [26/70], Progress: 53.76 %, Steps: 2376, Current Learning Rate: 0.0005872, \u001b[91mTrain Loss: 3.595\n",
      "\u001b[0m\u001b[1mEpoch: [26/70], Progress: 54.84 %, Steps: 2377, Current Learning Rate: 0.0005875, \u001b[91mTrain Loss: 3.668\n",
      "\u001b[0m\u001b[1mEpoch: [26/70], Progress: 55.91 %, Steps: 2378, Current Learning Rate: 0.0005877, \u001b[91mTrain Loss: 3.773\n",
      "\u001b[0m\u001b[1mEpoch: [26/70], Progress: 56.99 %, Steps: 2379, Current Learning Rate: 0.0005880, \u001b[96mTrain Loss: 3.580\n",
      "\u001b[0m\u001b[1mEpoch: [26/70], Progress: 58.06 %, Steps: 2380, Current Learning Rate: 0.0005882, \u001b[91mTrain Loss: 3.611\n",
      "\u001b[0m\u001b[1mEpoch: [26/70], Progress: 59.14 %, Steps: 2381, Current Learning Rate: 0.0005885, \u001b[91mTrain Loss: 3.740\n",
      "\u001b[0m\u001b[1mEpoch: [26/70], Progress: 60.22 %, Steps: 2382, Current Learning Rate: 0.0005887, \u001b[91mTrain Loss: 3.804\n",
      "\u001b[0m\u001b[1mEpoch: [26/70], Progress: 61.29 %, Steps: 2383, Current Learning Rate: 0.0005890, \u001b[96mTrain Loss: 3.764\n",
      "\u001b[0m\u001b[1mEpoch: [26/70], Progress: 62.37 %, Steps: 2384, Current Learning Rate: 0.0005892, \u001b[96mTrain Loss: 3.537\n",
      "\u001b[0m\u001b[1mEpoch: [26/70], Progress: 63.44 %, Steps: 2385, Current Learning Rate: 0.0005895, \u001b[91mTrain Loss: 3.813\n",
      "\u001b[0m\u001b[1mEpoch: [26/70], Progress: 64.52 %, Steps: 2386, Current Learning Rate: 0.0005897, \u001b[91mTrain Loss: 3.825\n",
      "\u001b[0m\u001b[1mEpoch: [26/70], Progress: 65.59 %, Steps: 2387, Current Learning Rate: 0.0005900, \u001b[96mTrain Loss: 3.805\n",
      "\u001b[0m\u001b[1mEpoch: [26/70], Progress: 66.67 %, Steps: 2388, Current Learning Rate: 0.0005902, \u001b[96mTrain Loss: 3.719\n",
      "\u001b[0m\u001b[1mEpoch: [26/70], Progress: 67.74 %, Steps: 2389, Current Learning Rate: 0.0005905, \u001b[96mTrain Loss: 3.664\n",
      "\u001b[0m\u001b[1mEpoch: [26/70], Progress: 68.82 %, Steps: 2390, Current Learning Rate: 0.0005907, \u001b[91mTrain Loss: 3.765\n",
      "\u001b[0m\u001b[1mEpoch: [26/70], Progress: 69.89 %, Steps: 2391, Current Learning Rate: 0.0005910, \u001b[96mTrain Loss: 3.656\n",
      "\u001b[0m\u001b[1mEpoch: [26/70], Progress: 70.97 %, Steps: 2392, Current Learning Rate: 0.0005912, \u001b[91mTrain Loss: 3.763\n",
      "\u001b[0m\u001b[1mEpoch: [26/70], Progress: 72.04 %, Steps: 2393, Current Learning Rate: 0.0005914, \u001b[91mTrain Loss: 3.795\n",
      "\u001b[0m\u001b[1mEpoch: [26/70], Progress: 73.12 %, Steps: 2394, Current Learning Rate: 0.0005917, \u001b[96mTrain Loss: 3.620\n",
      "\u001b[0m\u001b[1mEpoch: [26/70], Progress: 74.19 %, Steps: 2395, Current Learning Rate: 0.0005919, \u001b[91mTrain Loss: 3.766\n",
      "\u001b[0m\u001b[1mEpoch: [26/70], Progress: 75.27 %, Steps: 2396, Current Learning Rate: 0.0005922, \u001b[96mTrain Loss: 3.652\n",
      "\u001b[0m\u001b[1mEpoch: [26/70], Progress: 76.34 %, Steps: 2397, Current Learning Rate: 0.0005924, \u001b[91mTrain Loss: 3.941\n",
      "\u001b[0m\u001b[1mEpoch: [26/70], Progress: 77.42 %, Steps: 2398, Current Learning Rate: 0.0005927, \u001b[96mTrain Loss: 3.925\n",
      "\u001b[0m\u001b[1mEpoch: [26/70], Progress: 78.49 %, Steps: 2399, Current Learning Rate: 0.0005929, \u001b[96mTrain Loss: 3.622\n",
      "\u001b[0m\u001b[1mEpoch: [26/70], Progress: 79.57 %, Steps: 2400, Current Learning Rate: 0.0005932, \u001b[91mTrain Loss: 3.807\n",
      "\u001b[0m\u001b[1mEpoch: [26/70], Progress: 80.65 %, Steps: 2401, Current Learning Rate: 0.0005934, \u001b[96mTrain Loss: 3.606\n",
      "\u001b[0m\u001b[1mEpoch: [26/70], Progress: 81.72 %, Steps: 2402, Current Learning Rate: 0.0005937, \u001b[96mTrain Loss: 3.570\n",
      "\u001b[0m\u001b[1mEpoch: [26/70], Progress: 82.80 %, Steps: 2403, Current Learning Rate: 0.0005939, \u001b[91mTrain Loss: 3.877\n",
      "\u001b[0m\u001b[1mEpoch: [26/70], Progress: 83.87 %, Steps: 2404, Current Learning Rate: 0.0005942, \u001b[96mTrain Loss: 3.729\n",
      "\u001b[0m\u001b[1mEpoch: [26/70], Progress: 84.95 %, Steps: 2405, Current Learning Rate: 0.0005944, \u001b[96mTrain Loss: 3.665\n",
      "\u001b[0m\u001b[1mEpoch: [26/70], Progress: 86.02 %, Steps: 2406, Current Learning Rate: 0.0005947, \u001b[91mTrain Loss: 3.686\n",
      "\u001b[0m\u001b[1mEpoch: [26/70], Progress: 87.10 %, Steps: 2407, Current Learning Rate: 0.0005949, \u001b[91mTrain Loss: 4.074\n",
      "\u001b[0m\u001b[1mEpoch: [26/70], Progress: 88.17 %, Steps: 2408, Current Learning Rate: 0.0005952, \u001b[96mTrain Loss: 3.728\n",
      "\u001b[0m\u001b[1mEpoch: [26/70], Progress: 89.25 %, Steps: 2409, Current Learning Rate: 0.0005954, \u001b[96mTrain Loss: 3.491\n",
      "\u001b[0m\u001b[1mEpoch: [26/70], Progress: 90.32 %, Steps: 2410, Current Learning Rate: 0.0005956, \u001b[91mTrain Loss: 3.827\n",
      "\u001b[0m\u001b[1mEpoch: [26/70], Progress: 91.40 %, Steps: 2411, Current Learning Rate: 0.0005959, \u001b[96mTrain Loss: 3.686\n",
      "\u001b[0m\u001b[1mEpoch: [26/70], Progress: 92.47 %, Steps: 2412, Current Learning Rate: 0.0005961, \u001b[96mTrain Loss: 3.493\n",
      "\u001b[0m\u001b[1mEpoch: [26/70], Progress: 93.55 %, Steps: 2413, Current Learning Rate: 0.0005964, \u001b[91mTrain Loss: 3.562\n",
      "\u001b[0m\u001b[1mEpoch: [26/70], Progress: 94.62 %, Steps: 2414, Current Learning Rate: 0.0005966, \u001b[91mTrain Loss: 3.583\n",
      "\u001b[0m\u001b[1mEpoch: [26/70], Progress: 95.70 %, Steps: 2415, Current Learning Rate: 0.0005969, \u001b[91mTrain Loss: 3.670\n",
      "\u001b[0m\u001b[1mEpoch: [26/70], Progress: 96.77 %, Steps: 2416, Current Learning Rate: 0.0005971, \u001b[91mTrain Loss: 3.723\n",
      "\u001b[0m\u001b[1mEpoch: [26/70], Progress: 97.85 %, Steps: 2417, Current Learning Rate: 0.0005974, \u001b[96mTrain Loss: 3.435\n",
      "\u001b[0m\u001b[1mEpoch: [26/70], Progress: 98.92 %, Steps: 2418, Current Learning Rate: 0.0005976, \u001b[96mTrain Loss: 3.373\n",
      "\u001b[1m\u001b[93mComputing Validation Loss...\n",
      "\u001b[1mEpoch 26 Completed! Average Train Loss: 3.668, Average Validation Loss: 2.878\n",
      "Best Model saved in best_model/best_model.pt.\n",
      "\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m= Epoch [27/70] ==========================================\n",
      "\u001b[0m\u001b[1mEpoch: [27/70], Progress: 0.00 %, Steps: 2419, Current Learning Rate: 0.0005979, \u001b[91mTrain Loss: 3.654\n",
      "\u001b[0m\u001b[1mEpoch: [27/70], Progress: 1.08 %, Steps: 2420, Current Learning Rate: 0.0005981, \u001b[96mTrain Loss: 3.487\n",
      "\u001b[0m\u001b[1mEpoch: [27/70], Progress: 2.15 %, Steps: 2421, Current Learning Rate: 0.0005984, \u001b[96mTrain Loss: 3.402\n",
      "\u001b[0m\u001b[1mEpoch: [27/70], Progress: 3.23 %, Steps: 2422, Current Learning Rate: 0.0005986, \u001b[91mTrain Loss: 3.490\n",
      "\u001b[0m\u001b[1mEpoch: [27/70], Progress: 4.30 %, Steps: 2423, Current Learning Rate: 0.0005989, \u001b[96mTrain Loss: 3.486\n",
      "\u001b[0m\u001b[1mEpoch: [27/70], Progress: 5.38 %, Steps: 2424, Current Learning Rate: 0.0005991, \u001b[96mTrain Loss: 3.359\n",
      "\u001b[0m\u001b[1mEpoch: [27/70], Progress: 6.45 %, Steps: 2425, Current Learning Rate: 0.0005994, \u001b[91mTrain Loss: 3.388\n",
      "\u001b[0m\u001b[1mEpoch: [27/70], Progress: 7.53 %, Steps: 2426, Current Learning Rate: 0.0005996, \u001b[91mTrain Loss: 3.454\n",
      "\u001b[0m\u001b[1mEpoch: [27/70], Progress: 8.60 %, Steps: 2427, Current Learning Rate: 0.0005998, \u001b[91mTrain Loss: 3.561\n",
      "\u001b[0m\u001b[1mEpoch: [27/70], Progress: 9.68 %, Steps: 2428, Current Learning Rate: 0.0006001, \u001b[96mTrain Loss: 3.303\n",
      "\u001b[0m\u001b[1mEpoch: [27/70], Progress: 10.75 %, Steps: 2429, Current Learning Rate: 0.0006003, \u001b[96mTrain Loss: 3.285\n",
      "\u001b[0m\u001b[1mEpoch: [27/70], Progress: 11.83 %, Steps: 2430, Current Learning Rate: 0.0006006, \u001b[91mTrain Loss: 3.345\n",
      "\u001b[0m\u001b[1mEpoch: [27/70], Progress: 12.90 %, Steps: 2431, Current Learning Rate: 0.0006008, \u001b[91mTrain Loss: 3.495\n",
      "\u001b[0m\u001b[1mEpoch: [27/70], Progress: 13.98 %, Steps: 2432, Current Learning Rate: 0.0006011, \u001b[96mTrain Loss: 3.492\n",
      "\u001b[0m\u001b[1mEpoch: [27/70], Progress: 15.05 %, Steps: 2433, Current Learning Rate: 0.0006013, \u001b[96mTrain Loss: 3.260\n",
      "\u001b[0m\u001b[1mEpoch: [27/70], Progress: 16.13 %, Steps: 2434, Current Learning Rate: 0.0006016, \u001b[91mTrain Loss: 3.416\n",
      "\u001b[0m\u001b[1mEpoch: [27/70], Progress: 17.20 %, Steps: 2435, Current Learning Rate: 0.0006018, \u001b[91mTrain Loss: 3.500\n",
      "\u001b[0m\u001b[1mEpoch: [27/70], Progress: 18.28 %, Steps: 2436, Current Learning Rate: 0.0006021, \u001b[96mTrain Loss: 3.379\n",
      "\u001b[0m\u001b[1mEpoch: [27/70], Progress: 19.35 %, Steps: 2437, Current Learning Rate: 0.0006023, \u001b[91mTrain Loss: 3.464\n",
      "\u001b[0m\u001b[1mEpoch: [27/70], Progress: 20.43 %, Steps: 2438, Current Learning Rate: 0.0006026, \u001b[96mTrain Loss: 3.441\n",
      "\u001b[0m\u001b[1mEpoch: [27/70], Progress: 21.51 %, Steps: 2439, Current Learning Rate: 0.0006028, \u001b[91mTrain Loss: 3.486\n",
      "\u001b[0m\u001b[1mEpoch: [27/70], Progress: 22.58 %, Steps: 2440, Current Learning Rate: 0.0006031, \u001b[96mTrain Loss: 3.279\n",
      "\u001b[0m\u001b[1mEpoch: [27/70], Progress: 23.66 %, Steps: 2441, Current Learning Rate: 0.0006033, \u001b[91mTrain Loss: 3.568\n",
      "\u001b[0m\u001b[1mEpoch: [27/70], Progress: 24.73 %, Steps: 2442, Current Learning Rate: 0.0006036, \u001b[96mTrain Loss: 3.408\n",
      "\u001b[0m\u001b[1mEpoch: [27/70], Progress: 25.81 %, Steps: 2443, Current Learning Rate: 0.0006038, \u001b[91mTrain Loss: 3.580\n",
      "\u001b[0m\u001b[1mEpoch: [27/70], Progress: 26.88 %, Steps: 2444, Current Learning Rate: 0.0006040, \u001b[96mTrain Loss: 3.244\n",
      "\u001b[0m\u001b[1mEpoch: [27/70], Progress: 27.96 %, Steps: 2445, Current Learning Rate: 0.0006043, \u001b[91mTrain Loss: 3.377\n",
      "\u001b[0m\u001b[1mEpoch: [27/70], Progress: 29.03 %, Steps: 2446, Current Learning Rate: 0.0006045, \u001b[96mTrain Loss: 3.302\n",
      "\u001b[0m\u001b[1mEpoch: [27/70], Progress: 30.11 %, Steps: 2447, Current Learning Rate: 0.0006048, \u001b[91mTrain Loss: 3.402\n",
      "\u001b[0m\u001b[1mEpoch: [27/70], Progress: 31.18 %, Steps: 2448, Current Learning Rate: 0.0006050, \u001b[91mTrain Loss: 3.507\n",
      "\u001b[0m\u001b[1mEpoch: [27/70], Progress: 32.26 %, Steps: 2449, Current Learning Rate: 0.0006053, \u001b[91mTrain Loss: 3.572\n",
      "\u001b[0m\u001b[1mEpoch: [27/70], Progress: 33.33 %, Steps: 2450, Current Learning Rate: 0.0006055, \u001b[91mTrain Loss: 3.607\n",
      "\u001b[0m\u001b[1mEpoch: [27/70], Progress: 34.41 %, Steps: 2451, Current Learning Rate: 0.0006058, \u001b[96mTrain Loss: 3.416\n",
      "\u001b[0m\u001b[1mEpoch: [27/70], Progress: 35.48 %, Steps: 2452, Current Learning Rate: 0.0006060, \u001b[91mTrain Loss: 3.754\n",
      "\u001b[0m\u001b[1mEpoch: [27/70], Progress: 36.56 %, Steps: 2453, Current Learning Rate: 0.0006063, \u001b[96mTrain Loss: 3.464\n",
      "\u001b[0m\u001b[1mEpoch: [27/70], Progress: 37.63 %, Steps: 2454, Current Learning Rate: 0.0006065, \u001b[96mTrain Loss: 3.404\n",
      "\u001b[0m\u001b[1mEpoch: [27/70], Progress: 38.71 %, Steps: 2455, Current Learning Rate: 0.0006068, \u001b[96mTrain Loss: 3.385\n",
      "\u001b[0m\u001b[1mEpoch: [27/70], Progress: 39.78 %, Steps: 2456, Current Learning Rate: 0.0006070, \u001b[91mTrain Loss: 3.474\n",
      "\u001b[0m\u001b[1mEpoch: [27/70], Progress: 40.86 %, Steps: 2457, Current Learning Rate: 0.0006073, \u001b[96mTrain Loss: 3.358\n",
      "\u001b[0m\u001b[1mEpoch: [27/70], Progress: 41.94 %, Steps: 2458, Current Learning Rate: 0.0006075, \u001b[91mTrain Loss: 3.448\n",
      "\u001b[0m\u001b[1mEpoch: [27/70], Progress: 43.01 %, Steps: 2459, Current Learning Rate: 0.0006078, \u001b[91mTrain Loss: 3.465\n",
      "\u001b[0m\u001b[1mEpoch: [27/70], Progress: 44.09 %, Steps: 2460, Current Learning Rate: 0.0006080, \u001b[91mTrain Loss: 3.476\n",
      "\u001b[0m\u001b[1mEpoch: [27/70], Progress: 45.16 %, Steps: 2461, Current Learning Rate: 0.0006082, \u001b[96mTrain Loss: 3.372\n",
      "\u001b[0m\u001b[1mEpoch: [27/70], Progress: 46.24 %, Steps: 2462, Current Learning Rate: 0.0006085, \u001b[91mTrain Loss: 3.439\n",
      "\u001b[0m\u001b[1mEpoch: [27/70], Progress: 47.31 %, Steps: 2463, Current Learning Rate: 0.0006087, \u001b[96mTrain Loss: 3.297\n",
      "\u001b[0m\u001b[1mEpoch: [27/70], Progress: 48.39 %, Steps: 2464, Current Learning Rate: 0.0006090, \u001b[91mTrain Loss: 3.478\n",
      "\u001b[0m\u001b[1mEpoch: [27/70], Progress: 49.46 %, Steps: 2465, Current Learning Rate: 0.0006092, \u001b[96mTrain Loss: 3.221\n",
      "\u001b[0m\u001b[1mEpoch: [27/70], Progress: 50.54 %, Steps: 2466, Current Learning Rate: 0.0006095, \u001b[91mTrain Loss: 3.604\n",
      "\u001b[0m\u001b[1mEpoch: [27/70], Progress: 51.61 %, Steps: 2467, Current Learning Rate: 0.0006097, \u001b[96mTrain Loss: 3.273\n",
      "\u001b[0m\u001b[1mEpoch: [27/70], Progress: 52.69 %, Steps: 2468, Current Learning Rate: 0.0006100, \u001b[96mTrain Loss: 3.170\n",
      "\u001b[0m\u001b[1mEpoch: [27/70], Progress: 53.76 %, Steps: 2469, Current Learning Rate: 0.0006102, \u001b[91mTrain Loss: 3.208\n",
      "\u001b[0m\u001b[1mEpoch: [27/70], Progress: 54.84 %, Steps: 2470, Current Learning Rate: 0.0006105, \u001b[91mTrain Loss: 3.377\n",
      "\u001b[0m\u001b[1mEpoch: [27/70], Progress: 55.91 %, Steps: 2471, Current Learning Rate: 0.0006107, \u001b[91mTrain Loss: 3.541\n",
      "\u001b[0m\u001b[1mEpoch: [27/70], Progress: 56.99 %, Steps: 2472, Current Learning Rate: 0.0006110, \u001b[96mTrain Loss: 3.475\n",
      "\u001b[0m\u001b[1mEpoch: [27/70], Progress: 58.06 %, Steps: 2473, Current Learning Rate: 0.0006112, \u001b[96mTrain Loss: 3.393\n",
      "\u001b[0m\u001b[1mEpoch: [27/70], Progress: 59.14 %, Steps: 2474, Current Learning Rate: 0.0006115, \u001b[91mTrain Loss: 3.431\n",
      "\u001b[0m\u001b[1mEpoch: [27/70], Progress: 60.22 %, Steps: 2475, Current Learning Rate: 0.0006117, \u001b[96mTrain Loss: 3.262\n",
      "\u001b[0m\u001b[1mEpoch: [27/70], Progress: 61.29 %, Steps: 2476, Current Learning Rate: 0.0006120, \u001b[91mTrain Loss: 3.481\n",
      "\u001b[0m\u001b[1mEpoch: [27/70], Progress: 62.37 %, Steps: 2477, Current Learning Rate: 0.0006122, \u001b[91mTrain Loss: 3.501\n",
      "\u001b[0m\u001b[1mEpoch: [27/70], Progress: 63.44 %, Steps: 2478, Current Learning Rate: 0.0006124, \u001b[96mTrain Loss: 3.440\n",
      "\u001b[0m\u001b[1mEpoch: [27/70], Progress: 64.52 %, Steps: 2479, Current Learning Rate: 0.0006127, \u001b[91mTrain Loss: 3.601\n",
      "\u001b[0m\u001b[1mEpoch: [27/70], Progress: 65.59 %, Steps: 2480, Current Learning Rate: 0.0006129, \u001b[96mTrain Loss: 3.516\n",
      "\u001b[0m\u001b[1mEpoch: [27/70], Progress: 66.67 %, Steps: 2481, Current Learning Rate: 0.0006132, \u001b[96mTrain Loss: 3.494\n",
      "\u001b[0m\u001b[1mEpoch: [27/70], Progress: 67.74 %, Steps: 2482, Current Learning Rate: 0.0006134, \u001b[91mTrain Loss: 3.495\n",
      "\u001b[0m\u001b[1mEpoch: [27/70], Progress: 68.82 %, Steps: 2483, Current Learning Rate: 0.0006137, \u001b[96mTrain Loss: 3.387\n",
      "\u001b[0m\u001b[1mEpoch: [27/70], Progress: 69.89 %, Steps: 2484, Current Learning Rate: 0.0006139, \u001b[91mTrain Loss: 3.510\n",
      "\u001b[0m\u001b[1mEpoch: [27/70], Progress: 70.97 %, Steps: 2485, Current Learning Rate: 0.0006142, \u001b[96mTrain Loss: 3.474\n",
      "\u001b[0m\u001b[1mEpoch: [27/70], Progress: 72.04 %, Steps: 2486, Current Learning Rate: 0.0006144, \u001b[91mTrain Loss: 3.511\n",
      "\u001b[0m\u001b[1mEpoch: [27/70], Progress: 73.12 %, Steps: 2487, Current Learning Rate: 0.0006147, \u001b[91mTrain Loss: 3.658\n",
      "\u001b[0m\u001b[1mEpoch: [27/70], Progress: 74.19 %, Steps: 2488, Current Learning Rate: 0.0006149, \u001b[96mTrain Loss: 3.606\n",
      "\u001b[0m\u001b[1mEpoch: [27/70], Progress: 75.27 %, Steps: 2489, Current Learning Rate: 0.0006152, \u001b[96mTrain Loss: 3.328\n",
      "\u001b[0m\u001b[1mEpoch: [27/70], Progress: 76.34 %, Steps: 2490, Current Learning Rate: 0.0006154, \u001b[91mTrain Loss: 3.576\n",
      "\u001b[0m\u001b[1mEpoch: [27/70], Progress: 77.42 %, Steps: 2491, Current Learning Rate: 0.0006157, \u001b[96mTrain Loss: 3.473\n",
      "\u001b[0m\u001b[1mEpoch: [27/70], Progress: 78.49 %, Steps: 2492, Current Learning Rate: 0.0006159, \u001b[91mTrain Loss: 3.534\n",
      "\u001b[0m\u001b[1mEpoch: [27/70], Progress: 79.57 %, Steps: 2493, Current Learning Rate: 0.0006162, \u001b[91mTrain Loss: 3.662\n",
      "\u001b[0m\u001b[1mEpoch: [27/70], Progress: 80.65 %, Steps: 2494, Current Learning Rate: 0.0006164, \u001b[96mTrain Loss: 3.476\n",
      "\u001b[0m\u001b[1mEpoch: [27/70], Progress: 81.72 %, Steps: 2495, Current Learning Rate: 0.0006166, \u001b[96mTrain Loss: 3.413\n",
      "\u001b[0m\u001b[1mEpoch: [27/70], Progress: 82.80 %, Steps: 2496, Current Learning Rate: 0.0006169, \u001b[91mTrain Loss: 3.665\n",
      "\u001b[0m\u001b[1mEpoch: [27/70], Progress: 83.87 %, Steps: 2497, Current Learning Rate: 0.0006171, \u001b[96mTrain Loss: 3.585\n",
      "\u001b[0m\u001b[1mEpoch: [27/70], Progress: 84.95 %, Steps: 2498, Current Learning Rate: 0.0006174, \u001b[96mTrain Loss: 3.509\n",
      "\u001b[0m\u001b[1mEpoch: [27/70], Progress: 86.02 %, Steps: 2499, Current Learning Rate: 0.0006176, \u001b[91mTrain Loss: 3.589\n",
      "\u001b[0m\u001b[1mEpoch: [27/70], Progress: 87.10 %, Steps: 2500, Current Learning Rate: 0.0006179, \u001b[96mTrain Loss: 3.347\n",
      "\u001b[0m\u001b[1mEpoch: [27/70], Progress: 88.17 %, Steps: 2501, Current Learning Rate: 0.0006181, \u001b[96mTrain Loss: 3.306\n",
      "\u001b[0m\u001b[1mEpoch: [27/70], Progress: 89.25 %, Steps: 2502, Current Learning Rate: 0.0006184, \u001b[91mTrain Loss: 3.768\n",
      "\u001b[0m\u001b[1mEpoch: [27/70], Progress: 90.32 %, Steps: 2503, Current Learning Rate: 0.0006186, \u001b[96mTrain Loss: 3.498\n",
      "\u001b[0m\u001b[1mEpoch: [27/70], Progress: 91.40 %, Steps: 2504, Current Learning Rate: 0.0006189, \u001b[91mTrain Loss: 3.624\n",
      "\u001b[0m\u001b[1mEpoch: [27/70], Progress: 92.47 %, Steps: 2505, Current Learning Rate: 0.0006191, \u001b[96mTrain Loss: 3.475\n",
      "\u001b[0m\u001b[1mEpoch: [27/70], Progress: 93.55 %, Steps: 2506, Current Learning Rate: 0.0006194, \u001b[96mTrain Loss: 3.401\n",
      "\u001b[0m\u001b[1mEpoch: [27/70], Progress: 94.62 %, Steps: 2507, Current Learning Rate: 0.0006196, \u001b[91mTrain Loss: 3.680\n",
      "\u001b[0m\u001b[1mEpoch: [27/70], Progress: 95.70 %, Steps: 2508, Current Learning Rate: 0.0006199, \u001b[96mTrain Loss: 3.612\n",
      "\u001b[0m\u001b[1mEpoch: [27/70], Progress: 96.77 %, Steps: 2509, Current Learning Rate: 0.0006201, \u001b[96mTrain Loss: 3.368\n",
      "\u001b[0m\u001b[1mEpoch: [27/70], Progress: 97.85 %, Steps: 2510, Current Learning Rate: 0.0006203, \u001b[91mTrain Loss: 3.441\n",
      "\u001b[0m\u001b[1mEpoch: [27/70], Progress: 98.92 %, Steps: 2511, Current Learning Rate: 0.0006206, \u001b[96mTrain Loss: 3.227\n",
      "\u001b[1m\u001b[93mComputing Validation Loss...\n",
      "\u001b[1mEpoch 27 Completed! Average Train Loss: 3.456, Average Validation Loss: 2.628\n",
      "Best Model saved in best_model/best_model.pt.\n",
      "\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m= Epoch [28/70] ==========================================\n",
      "\u001b[0m\u001b[1mEpoch: [28/70], Progress: 0.00 %, Steps: 2512, Current Learning Rate: 0.0006208, \u001b[91mTrain Loss: 3.019\n",
      "\u001b[0m\u001b[1mEpoch: [28/70], Progress: 1.08 %, Steps: 2513, Current Learning Rate: 0.0006211, \u001b[96mTrain Loss: 3.000\n",
      "\u001b[0m\u001b[1mEpoch: [28/70], Progress: 2.15 %, Steps: 2514, Current Learning Rate: 0.0006213, \u001b[91mTrain Loss: 3.295\n",
      "\u001b[0m\u001b[1mEpoch: [28/70], Progress: 3.23 %, Steps: 2515, Current Learning Rate: 0.0006216, \u001b[91mTrain Loss: 3.304\n",
      "\u001b[0m\u001b[1mEpoch: [28/70], Progress: 4.30 %, Steps: 2516, Current Learning Rate: 0.0006218, \u001b[96mTrain Loss: 3.203\n",
      "\u001b[0m\u001b[1mEpoch: [28/70], Progress: 5.38 %, Steps: 2517, Current Learning Rate: 0.0006221, \u001b[91mTrain Loss: 3.242\n",
      "\u001b[0m\u001b[1mEpoch: [28/70], Progress: 6.45 %, Steps: 2518, Current Learning Rate: 0.0006223, \u001b[91mTrain Loss: 3.254\n",
      "\u001b[0m\u001b[1mEpoch: [28/70], Progress: 7.53 %, Steps: 2519, Current Learning Rate: 0.0006226, \u001b[96mTrain Loss: 3.157\n",
      "\u001b[0m\u001b[1mEpoch: [28/70], Progress: 8.60 %, Steps: 2520, Current Learning Rate: 0.0006228, \u001b[91mTrain Loss: 3.255\n",
      "\u001b[0m\u001b[1mEpoch: [28/70], Progress: 9.68 %, Steps: 2521, Current Learning Rate: 0.0006231, \u001b[91mTrain Loss: 3.329\n",
      "\u001b[0m\u001b[1mEpoch: [28/70], Progress: 10.75 %, Steps: 2522, Current Learning Rate: 0.0006233, \u001b[96mTrain Loss: 3.154\n",
      "\u001b[0m\u001b[1mEpoch: [28/70], Progress: 11.83 %, Steps: 2523, Current Learning Rate: 0.0006236, \u001b[96mTrain Loss: 3.105\n",
      "\u001b[0m\u001b[1mEpoch: [28/70], Progress: 12.90 %, Steps: 2524, Current Learning Rate: 0.0006238, \u001b[91mTrain Loss: 3.320\n",
      "\u001b[0m\u001b[1mEpoch: [28/70], Progress: 13.98 %, Steps: 2525, Current Learning Rate: 0.0006241, \u001b[96mTrain Loss: 3.234\n",
      "\u001b[0m\u001b[1mEpoch: [28/70], Progress: 15.05 %, Steps: 2526, Current Learning Rate: 0.0006243, \u001b[96mTrain Loss: 3.211\n",
      "\u001b[0m\u001b[1mEpoch: [28/70], Progress: 16.13 %, Steps: 2527, Current Learning Rate: 0.0006245, \u001b[91mTrain Loss: 3.355\n",
      "\u001b[0m\u001b[1mEpoch: [28/70], Progress: 17.20 %, Steps: 2528, Current Learning Rate: 0.0006248, \u001b[91mTrain Loss: 3.364\n",
      "\u001b[0m\u001b[1mEpoch: [28/70], Progress: 18.28 %, Steps: 2529, Current Learning Rate: 0.0006250, \u001b[91mTrain Loss: 3.397\n",
      "\u001b[0m\u001b[1mEpoch: [28/70], Progress: 19.35 %, Steps: 2530, Current Learning Rate: 0.0006253, \u001b[96mTrain Loss: 3.176\n",
      "\u001b[0m\u001b[1mEpoch: [28/70], Progress: 20.43 %, Steps: 2531, Current Learning Rate: 0.0006255, \u001b[91mTrain Loss: 3.228\n",
      "\u001b[0m\u001b[1mEpoch: [28/70], Progress: 21.51 %, Steps: 2532, Current Learning Rate: 0.0006258, \u001b[96mTrain Loss: 3.181\n",
      "\u001b[0m\u001b[1mEpoch: [28/70], Progress: 22.58 %, Steps: 2533, Current Learning Rate: 0.0006260, \u001b[91mTrain Loss: 3.258\n",
      "\u001b[0m\u001b[1mEpoch: [28/70], Progress: 23.66 %, Steps: 2534, Current Learning Rate: 0.0006263, \u001b[96mTrain Loss: 3.088\n",
      "\u001b[0m\u001b[1mEpoch: [28/70], Progress: 24.73 %, Steps: 2535, Current Learning Rate: 0.0006265, \u001b[91mTrain Loss: 3.160\n",
      "\u001b[0m\u001b[1mEpoch: [28/70], Progress: 25.81 %, Steps: 2536, Current Learning Rate: 0.0006268, \u001b[96mTrain Loss: 2.959\n",
      "\u001b[0m\u001b[1mEpoch: [28/70], Progress: 26.88 %, Steps: 2537, Current Learning Rate: 0.0006270, \u001b[91mTrain Loss: 3.068\n",
      "\u001b[0m\u001b[1mEpoch: [28/70], Progress: 27.96 %, Steps: 2538, Current Learning Rate: 0.0006273, \u001b[91mTrain Loss: 3.120\n",
      "\u001b[0m\u001b[1mEpoch: [28/70], Progress: 29.03 %, Steps: 2539, Current Learning Rate: 0.0006275, \u001b[96mTrain Loss: 3.098\n",
      "\u001b[0m\u001b[1mEpoch: [28/70], Progress: 30.11 %, Steps: 2540, Current Learning Rate: 0.0006278, \u001b[96mTrain Loss: 3.036\n",
      "\u001b[0m\u001b[1mEpoch: [28/70], Progress: 31.18 %, Steps: 2541, Current Learning Rate: 0.0006280, \u001b[91mTrain Loss: 3.078\n",
      "\u001b[0m\u001b[1mEpoch: [28/70], Progress: 32.26 %, Steps: 2542, Current Learning Rate: 0.0006283, \u001b[91mTrain Loss: 3.320\n",
      "\u001b[0m\u001b[1mEpoch: [28/70], Progress: 33.33 %, Steps: 2543, Current Learning Rate: 0.0006285, \u001b[96mTrain Loss: 3.123\n",
      "\u001b[0m\u001b[1mEpoch: [28/70], Progress: 34.41 %, Steps: 2544, Current Learning Rate: 0.0006287, \u001b[91mTrain Loss: 3.220\n",
      "\u001b[0m\u001b[1mEpoch: [28/70], Progress: 35.48 %, Steps: 2545, Current Learning Rate: 0.0006290, \u001b[96mTrain Loss: 3.125\n",
      "\u001b[0m\u001b[1mEpoch: [28/70], Progress: 36.56 %, Steps: 2546, Current Learning Rate: 0.0006292, \u001b[91mTrain Loss: 3.402\n",
      "\u001b[0m\u001b[1mEpoch: [28/70], Progress: 37.63 %, Steps: 2547, Current Learning Rate: 0.0006295, \u001b[96mTrain Loss: 3.207\n",
      "\u001b[0m\u001b[1mEpoch: [28/70], Progress: 38.71 %, Steps: 2548, Current Learning Rate: 0.0006297, \u001b[91mTrain Loss: 3.328\n",
      "\u001b[0m\u001b[1mEpoch: [28/70], Progress: 39.78 %, Steps: 2549, Current Learning Rate: 0.0006300, \u001b[96mTrain Loss: 3.243\n",
      "\u001b[0m\u001b[1mEpoch: [28/70], Progress: 40.86 %, Steps: 2550, Current Learning Rate: 0.0006302, \u001b[91mTrain Loss: 3.381\n",
      "\u001b[0m\u001b[1mEpoch: [28/70], Progress: 41.94 %, Steps: 2551, Current Learning Rate: 0.0006305, \u001b[96mTrain Loss: 3.176\n",
      "\u001b[0m\u001b[1mEpoch: [28/70], Progress: 43.01 %, Steps: 2552, Current Learning Rate: 0.0006307, \u001b[91mTrain Loss: 3.296\n",
      "\u001b[0m\u001b[1mEpoch: [28/70], Progress: 44.09 %, Steps: 2553, Current Learning Rate: 0.0006310, \u001b[91mTrain Loss: 3.320\n",
      "\u001b[0m\u001b[1mEpoch: [28/70], Progress: 45.16 %, Steps: 2554, Current Learning Rate: 0.0006312, \u001b[91mTrain Loss: 3.478\n",
      "\u001b[0m\u001b[1mEpoch: [28/70], Progress: 46.24 %, Steps: 2555, Current Learning Rate: 0.0006315, \u001b[96mTrain Loss: 3.285\n",
      "\u001b[0m\u001b[1mEpoch: [28/70], Progress: 47.31 %, Steps: 2556, Current Learning Rate: 0.0006317, \u001b[91mTrain Loss: 3.397\n",
      "\u001b[0m\u001b[1mEpoch: [28/70], Progress: 48.39 %, Steps: 2557, Current Learning Rate: 0.0006320, \u001b[96mTrain Loss: 3.269\n",
      "\u001b[0m\u001b[1mEpoch: [28/70], Progress: 49.46 %, Steps: 2558, Current Learning Rate: 0.0006322, \u001b[96mTrain Loss: 3.237\n",
      "\u001b[0m\u001b[1mEpoch: [28/70], Progress: 50.54 %, Steps: 2559, Current Learning Rate: 0.0006325, \u001b[91mTrain Loss: 3.345\n",
      "\u001b[0m\u001b[1mEpoch: [28/70], Progress: 51.61 %, Steps: 2560, Current Learning Rate: 0.0006327, \u001b[96mTrain Loss: 3.324\n",
      "\u001b[0m\u001b[1mEpoch: [28/70], Progress: 52.69 %, Steps: 2561, Current Learning Rate: 0.0006329, \u001b[96mTrain Loss: 3.229\n",
      "\u001b[0m\u001b[1mEpoch: [28/70], Progress: 53.76 %, Steps: 2562, Current Learning Rate: 0.0006332, \u001b[96mTrain Loss: 3.174\n",
      "\u001b[0m\u001b[1mEpoch: [28/70], Progress: 54.84 %, Steps: 2563, Current Learning Rate: 0.0006334, \u001b[96mTrain Loss: 3.102\n",
      "\u001b[0m\u001b[1mEpoch: [28/70], Progress: 55.91 %, Steps: 2564, Current Learning Rate: 0.0006337, \u001b[91mTrain Loss: 3.228\n",
      "\u001b[0m\u001b[1mEpoch: [28/70], Progress: 56.99 %, Steps: 2565, Current Learning Rate: 0.0006339, \u001b[96mTrain Loss: 3.214\n",
      "\u001b[0m\u001b[1mEpoch: [28/70], Progress: 58.06 %, Steps: 2566, Current Learning Rate: 0.0006342, \u001b[91mTrain Loss: 3.342\n",
      "\u001b[0m\u001b[1mEpoch: [28/70], Progress: 59.14 %, Steps: 2567, Current Learning Rate: 0.0006344, \u001b[91mTrain Loss: 3.514\n",
      "\u001b[0m\u001b[1mEpoch: [28/70], Progress: 60.22 %, Steps: 2568, Current Learning Rate: 0.0006347, \u001b[96mTrain Loss: 3.106\n",
      "\u001b[0m\u001b[1mEpoch: [28/70], Progress: 61.29 %, Steps: 2569, Current Learning Rate: 0.0006349, \u001b[91mTrain Loss: 3.193\n",
      "\u001b[0m\u001b[1mEpoch: [28/70], Progress: 62.37 %, Steps: 2570, Current Learning Rate: 0.0006352, \u001b[91mTrain Loss: 3.221\n",
      "\u001b[0m\u001b[1mEpoch: [28/70], Progress: 63.44 %, Steps: 2571, Current Learning Rate: 0.0006354, \u001b[91mTrain Loss: 3.259\n",
      "\u001b[0m\u001b[1mEpoch: [28/70], Progress: 64.52 %, Steps: 2572, Current Learning Rate: 0.0006357, \u001b[96mTrain Loss: 3.197\n",
      "\u001b[0m\u001b[1mEpoch: [28/70], Progress: 65.59 %, Steps: 2573, Current Learning Rate: 0.0006359, \u001b[96mTrain Loss: 3.065\n",
      "\u001b[0m\u001b[1mEpoch: [28/70], Progress: 66.67 %, Steps: 2574, Current Learning Rate: 0.0006362, \u001b[91mTrain Loss: 3.311\n",
      "\u001b[0m\u001b[1mEpoch: [28/70], Progress: 67.74 %, Steps: 2575, Current Learning Rate: 0.0006364, \u001b[96mTrain Loss: 3.199\n",
      "\u001b[0m\u001b[1mEpoch: [28/70], Progress: 68.82 %, Steps: 2576, Current Learning Rate: 0.0006367, \u001b[91mTrain Loss: 3.228\n",
      "\u001b[0m\u001b[1mEpoch: [28/70], Progress: 69.89 %, Steps: 2577, Current Learning Rate: 0.0006369, \u001b[91mTrain Loss: 3.470\n",
      "\u001b[0m\u001b[1mEpoch: [28/70], Progress: 70.97 %, Steps: 2578, Current Learning Rate: 0.0006371, \u001b[96mTrain Loss: 3.034\n",
      "\u001b[0m\u001b[1mEpoch: [28/70], Progress: 72.04 %, Steps: 2579, Current Learning Rate: 0.0006374, \u001b[91mTrain Loss: 3.276\n",
      "\u001b[0m\u001b[1mEpoch: [28/70], Progress: 73.12 %, Steps: 2580, Current Learning Rate: 0.0006376, \u001b[96mTrain Loss: 3.189\n",
      "\u001b[0m\u001b[1mEpoch: [28/70], Progress: 74.19 %, Steps: 2581, Current Learning Rate: 0.0006379, \u001b[91mTrain Loss: 3.358\n",
      "\u001b[0m\u001b[1mEpoch: [28/70], Progress: 75.27 %, Steps: 2582, Current Learning Rate: 0.0006381, \u001b[91mTrain Loss: 3.380\n",
      "\u001b[0m\u001b[1mEpoch: [28/70], Progress: 76.34 %, Steps: 2583, Current Learning Rate: 0.0006384, \u001b[91mTrain Loss: 3.390\n",
      "\u001b[0m\u001b[1mEpoch: [28/70], Progress: 77.42 %, Steps: 2584, Current Learning Rate: 0.0006386, \u001b[96mTrain Loss: 3.162\n",
      "\u001b[0m\u001b[1mEpoch: [28/70], Progress: 78.49 %, Steps: 2585, Current Learning Rate: 0.0006389, \u001b[91mTrain Loss: 3.375\n",
      "\u001b[0m\u001b[1mEpoch: [28/70], Progress: 79.57 %, Steps: 2586, Current Learning Rate: 0.0006391, \u001b[96mTrain Loss: 3.169\n",
      "\u001b[0m\u001b[1mEpoch: [28/70], Progress: 80.65 %, Steps: 2587, Current Learning Rate: 0.0006394, \u001b[91mTrain Loss: 3.329\n",
      "\u001b[0m\u001b[1mEpoch: [28/70], Progress: 81.72 %, Steps: 2588, Current Learning Rate: 0.0006396, \u001b[91mTrain Loss: 3.389\n",
      "\u001b[0m\u001b[1mEpoch: [28/70], Progress: 82.80 %, Steps: 2589, Current Learning Rate: 0.0006399, \u001b[96mTrain Loss: 3.351\n",
      "\u001b[0m\u001b[1mEpoch: [28/70], Progress: 83.87 %, Steps: 2590, Current Learning Rate: 0.0006401, \u001b[91mTrain Loss: 3.439\n",
      "\u001b[0m\u001b[1mEpoch: [28/70], Progress: 84.95 %, Steps: 2591, Current Learning Rate: 0.0006404, \u001b[91mTrain Loss: 3.542\n",
      "\u001b[0m\u001b[1mEpoch: [28/70], Progress: 86.02 %, Steps: 2592, Current Learning Rate: 0.0006406, \u001b[96mTrain Loss: 3.324\n",
      "\u001b[0m\u001b[1mEpoch: [28/70], Progress: 87.10 %, Steps: 2593, Current Learning Rate: 0.0006409, \u001b[96mTrain Loss: 3.236\n",
      "\u001b[0m\u001b[1mEpoch: [28/70], Progress: 88.17 %, Steps: 2594, Current Learning Rate: 0.0006411, \u001b[91mTrain Loss: 3.274\n",
      "\u001b[0m\u001b[1mEpoch: [28/70], Progress: 89.25 %, Steps: 2595, Current Learning Rate: 0.0006413, \u001b[91mTrain Loss: 3.337\n",
      "\u001b[0m\u001b[1mEpoch: [28/70], Progress: 90.32 %, Steps: 2596, Current Learning Rate: 0.0006416, \u001b[91mTrain Loss: 3.344\n",
      "\u001b[0m\u001b[1mEpoch: [28/70], Progress: 91.40 %, Steps: 2597, Current Learning Rate: 0.0006418, \u001b[96mTrain Loss: 3.192\n",
      "\u001b[0m\u001b[1mEpoch: [28/70], Progress: 92.47 %, Steps: 2598, Current Learning Rate: 0.0006421, \u001b[91mTrain Loss: 3.324\n",
      "\u001b[0m\u001b[1mEpoch: [28/70], Progress: 93.55 %, Steps: 2599, Current Learning Rate: 0.0006423, \u001b[96mTrain Loss: 3.263\n",
      "\u001b[0m\u001b[1mEpoch: [28/70], Progress: 94.62 %, Steps: 2600, Current Learning Rate: 0.0006426, \u001b[91mTrain Loss: 3.317\n",
      "\u001b[0m\u001b[1mEpoch: [28/70], Progress: 95.70 %, Steps: 2601, Current Learning Rate: 0.0006428, \u001b[96mTrain Loss: 3.053\n",
      "\u001b[0m\u001b[1mEpoch: [28/70], Progress: 96.77 %, Steps: 2602, Current Learning Rate: 0.0006431, \u001b[91mTrain Loss: 3.295\n",
      "\u001b[0m\u001b[1mEpoch: [28/70], Progress: 97.85 %, Steps: 2603, Current Learning Rate: 0.0006433, \u001b[96mTrain Loss: 3.210\n",
      "\u001b[0m\u001b[1mEpoch: [28/70], Progress: 98.92 %, Steps: 2604, Current Learning Rate: 0.0006436, \u001b[96mTrain Loss: 3.198\n",
      "\u001b[1m\u001b[93mComputing Validation Loss...\n",
      "\u001b[1mEpoch 28 Completed! Average Train Loss: 3.246, Average Validation Loss: 2.459\n",
      "Best Model saved in best_model/best_model.pt.\n",
      "\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m= Epoch [29/70] ==========================================\n",
      "\u001b[0m\u001b[1mEpoch: [29/70], Progress: 0.00 %, Steps: 2605, Current Learning Rate: 0.0006438, \u001b[91mTrain Loss: 2.885\n",
      "\u001b[0m\u001b[1mEpoch: [29/70], Progress: 1.08 %, Steps: 2606, Current Learning Rate: 0.0006441, \u001b[91mTrain Loss: 3.149\n",
      "\u001b[0m\u001b[1mEpoch: [29/70], Progress: 2.15 %, Steps: 2607, Current Learning Rate: 0.0006443, \u001b[96mTrain Loss: 3.046\n",
      "\u001b[0m\u001b[1mEpoch: [29/70], Progress: 3.23 %, Steps: 2608, Current Learning Rate: 0.0006446, \u001b[96mTrain Loss: 3.017\n",
      "\u001b[0m\u001b[1mEpoch: [29/70], Progress: 4.30 %, Steps: 2609, Current Learning Rate: 0.0006448, \u001b[96mTrain Loss: 2.903\n",
      "\u001b[0m\u001b[1mEpoch: [29/70], Progress: 5.38 %, Steps: 2610, Current Learning Rate: 0.0006451, \u001b[91mTrain Loss: 2.913\n",
      "\u001b[0m\u001b[1mEpoch: [29/70], Progress: 6.45 %, Steps: 2611, Current Learning Rate: 0.0006453, \u001b[91mTrain Loss: 3.092\n",
      "\u001b[0m\u001b[1mEpoch: [29/70], Progress: 7.53 %, Steps: 2612, Current Learning Rate: 0.0006455, \u001b[96mTrain Loss: 2.971\n",
      "\u001b[0m\u001b[1mEpoch: [29/70], Progress: 8.60 %, Steps: 2613, Current Learning Rate: 0.0006458, \u001b[91mTrain Loss: 2.979\n",
      "\u001b[0m\u001b[1mEpoch: [29/70], Progress: 9.68 %, Steps: 2614, Current Learning Rate: 0.0006460, \u001b[96mTrain Loss: 2.873\n",
      "\u001b[0m\u001b[1mEpoch: [29/70], Progress: 10.75 %, Steps: 2615, Current Learning Rate: 0.0006463, \u001b[91mTrain Loss: 2.874\n",
      "\u001b[0m\u001b[1mEpoch: [29/70], Progress: 11.83 %, Steps: 2616, Current Learning Rate: 0.0006465, \u001b[91mTrain Loss: 3.090\n",
      "\u001b[0m\u001b[1mEpoch: [29/70], Progress: 12.90 %, Steps: 2617, Current Learning Rate: 0.0006468, \u001b[96mTrain Loss: 2.822\n",
      "\u001b[0m\u001b[1mEpoch: [29/70], Progress: 13.98 %, Steps: 2618, Current Learning Rate: 0.0006470, \u001b[91mTrain Loss: 2.860\n",
      "\u001b[0m\u001b[1mEpoch: [29/70], Progress: 15.05 %, Steps: 2619, Current Learning Rate: 0.0006473, \u001b[91mTrain Loss: 3.197\n",
      "\u001b[0m\u001b[1mEpoch: [29/70], Progress: 16.13 %, Steps: 2620, Current Learning Rate: 0.0006475, \u001b[96mTrain Loss: 2.982\n",
      "\u001b[0m\u001b[1mEpoch: [29/70], Progress: 17.20 %, Steps: 2621, Current Learning Rate: 0.0006478, \u001b[91mTrain Loss: 3.183\n",
      "\u001b[0m\u001b[1mEpoch: [29/70], Progress: 18.28 %, Steps: 2622, Current Learning Rate: 0.0006480, \u001b[96mTrain Loss: 2.979\n",
      "\u001b[0m\u001b[1mEpoch: [29/70], Progress: 19.35 %, Steps: 2623, Current Learning Rate: 0.0006483, \u001b[91mTrain Loss: 3.070\n",
      "\u001b[0m\u001b[1mEpoch: [29/70], Progress: 20.43 %, Steps: 2624, Current Learning Rate: 0.0006485, \u001b[91mTrain Loss: 3.145\n",
      "\u001b[0m\u001b[1mEpoch: [29/70], Progress: 21.51 %, Steps: 2625, Current Learning Rate: 0.0006488, \u001b[96mTrain Loss: 2.979\n",
      "\u001b[0m\u001b[1mEpoch: [29/70], Progress: 22.58 %, Steps: 2626, Current Learning Rate: 0.0006490, \u001b[91mTrain Loss: 3.147\n",
      "\u001b[0m\u001b[1mEpoch: [29/70], Progress: 23.66 %, Steps: 2627, Current Learning Rate: 0.0006493, \u001b[96mTrain Loss: 2.837\n",
      "\u001b[0m\u001b[1mEpoch: [29/70], Progress: 24.73 %, Steps: 2628, Current Learning Rate: 0.0006495, \u001b[91mTrain Loss: 3.013\n",
      "\u001b[0m\u001b[1mEpoch: [29/70], Progress: 25.81 %, Steps: 2629, Current Learning Rate: 0.0006497, \u001b[91mTrain Loss: 3.053\n",
      "\u001b[0m\u001b[1mEpoch: [29/70], Progress: 26.88 %, Steps: 2630, Current Learning Rate: 0.0006500, \u001b[96mTrain Loss: 2.929\n",
      "\u001b[0m\u001b[1mEpoch: [29/70], Progress: 27.96 %, Steps: 2631, Current Learning Rate: 0.0006502, \u001b[91mTrain Loss: 3.215\n",
      "\u001b[0m\u001b[1mEpoch: [29/70], Progress: 29.03 %, Steps: 2632, Current Learning Rate: 0.0006505, \u001b[96mTrain Loss: 2.919\n",
      "\u001b[0m\u001b[1mEpoch: [29/70], Progress: 30.11 %, Steps: 2633, Current Learning Rate: 0.0006507, \u001b[91mTrain Loss: 3.172\n",
      "\u001b[0m\u001b[1mEpoch: [29/70], Progress: 31.18 %, Steps: 2634, Current Learning Rate: 0.0006510, \u001b[96mTrain Loss: 2.693\n",
      "\u001b[0m\u001b[1mEpoch: [29/70], Progress: 32.26 %, Steps: 2635, Current Learning Rate: 0.0006512, \u001b[91mTrain Loss: 3.132\n",
      "\u001b[0m\u001b[1mEpoch: [29/70], Progress: 33.33 %, Steps: 2636, Current Learning Rate: 0.0006515, \u001b[96mTrain Loss: 3.014\n",
      "\u001b[0m\u001b[1mEpoch: [29/70], Progress: 34.41 %, Steps: 2637, Current Learning Rate: 0.0006517, \u001b[96mTrain Loss: 2.830\n",
      "\u001b[0m\u001b[1mEpoch: [29/70], Progress: 35.48 %, Steps: 2638, Current Learning Rate: 0.0006520, \u001b[91mTrain Loss: 3.141\n",
      "\u001b[0m\u001b[1mEpoch: [29/70], Progress: 36.56 %, Steps: 2639, Current Learning Rate: 0.0006522, \u001b[96mTrain Loss: 3.065\n",
      "\u001b[0m\u001b[1mEpoch: [29/70], Progress: 37.63 %, Steps: 2640, Current Learning Rate: 0.0006525, \u001b[96mTrain Loss: 2.966\n",
      "\u001b[0m\u001b[1mEpoch: [29/70], Progress: 38.71 %, Steps: 2641, Current Learning Rate: 0.0006527, \u001b[96mTrain Loss: 2.906\n",
      "\u001b[0m\u001b[1mEpoch: [29/70], Progress: 39.78 %, Steps: 2642, Current Learning Rate: 0.0006530, \u001b[91mTrain Loss: 3.043\n",
      "\u001b[0m\u001b[1mEpoch: [29/70], Progress: 40.86 %, Steps: 2643, Current Learning Rate: 0.0006532, \u001b[96mTrain Loss: 2.970\n",
      "\u001b[0m\u001b[1mEpoch: [29/70], Progress: 41.94 %, Steps: 2644, Current Learning Rate: 0.0006535, \u001b[91mTrain Loss: 3.136\n",
      "\u001b[0m\u001b[1mEpoch: [29/70], Progress: 43.01 %, Steps: 2645, Current Learning Rate: 0.0006537, \u001b[96mTrain Loss: 3.130\n",
      "\u001b[0m\u001b[1mEpoch: [29/70], Progress: 44.09 %, Steps: 2646, Current Learning Rate: 0.0006539, \u001b[96mTrain Loss: 3.054\n",
      "\u001b[0m\u001b[1mEpoch: [29/70], Progress: 45.16 %, Steps: 2647, Current Learning Rate: 0.0006542, \u001b[96mTrain Loss: 2.856\n",
      "\u001b[0m\u001b[1mEpoch: [29/70], Progress: 46.24 %, Steps: 2648, Current Learning Rate: 0.0006544, \u001b[91mTrain Loss: 3.197\n",
      "\u001b[0m\u001b[1mEpoch: [29/70], Progress: 47.31 %, Steps: 2649, Current Learning Rate: 0.0006547, \u001b[96mTrain Loss: 3.051\n",
      "\u001b[0m\u001b[1mEpoch: [29/70], Progress: 48.39 %, Steps: 2650, Current Learning Rate: 0.0006549, \u001b[91mTrain Loss: 3.079\n",
      "\u001b[0m\u001b[1mEpoch: [29/70], Progress: 49.46 %, Steps: 2651, Current Learning Rate: 0.0006552, \u001b[96mTrain Loss: 2.923\n",
      "\u001b[0m\u001b[1mEpoch: [29/70], Progress: 50.54 %, Steps: 2652, Current Learning Rate: 0.0006554, \u001b[96mTrain Loss: 2.797\n",
      "\u001b[0m\u001b[1mEpoch: [29/70], Progress: 51.61 %, Steps: 2653, Current Learning Rate: 0.0006557, \u001b[91mTrain Loss: 3.299\n",
      "\u001b[0m\u001b[1mEpoch: [29/70], Progress: 52.69 %, Steps: 2654, Current Learning Rate: 0.0006559, \u001b[96mTrain Loss: 2.918\n",
      "\u001b[0m\u001b[1mEpoch: [29/70], Progress: 53.76 %, Steps: 2655, Current Learning Rate: 0.0006562, \u001b[91mTrain Loss: 2.937\n",
      "\u001b[0m\u001b[1mEpoch: [29/70], Progress: 54.84 %, Steps: 2656, Current Learning Rate: 0.0006564, \u001b[91mTrain Loss: 3.116\n",
      "\u001b[0m\u001b[1mEpoch: [29/70], Progress: 55.91 %, Steps: 2657, Current Learning Rate: 0.0006567, \u001b[96mTrain Loss: 3.044\n",
      "\u001b[0m\u001b[1mEpoch: [29/70], Progress: 56.99 %, Steps: 2658, Current Learning Rate: 0.0006569, \u001b[96mTrain Loss: 2.974\n",
      "\u001b[0m\u001b[1mEpoch: [29/70], Progress: 58.06 %, Steps: 2659, Current Learning Rate: 0.0006572, \u001b[91mTrain Loss: 3.114\n",
      "\u001b[0m\u001b[1mEpoch: [29/70], Progress: 59.14 %, Steps: 2660, Current Learning Rate: 0.0006574, \u001b[96mTrain Loss: 3.106\n",
      "\u001b[0m\u001b[1mEpoch: [29/70], Progress: 60.22 %, Steps: 2661, Current Learning Rate: 0.0006577, \u001b[96mTrain Loss: 2.951\n",
      "\u001b[0m\u001b[1mEpoch: [29/70], Progress: 61.29 %, Steps: 2662, Current Learning Rate: 0.0006579, \u001b[96mTrain Loss: 2.863\n",
      "\u001b[0m\u001b[1mEpoch: [29/70], Progress: 62.37 %, Steps: 2663, Current Learning Rate: 0.0006581, \u001b[91mTrain Loss: 3.094\n",
      "\u001b[0m\u001b[1mEpoch: [29/70], Progress: 63.44 %, Steps: 2664, Current Learning Rate: 0.0006584, \u001b[96mTrain Loss: 3.038\n",
      "\u001b[0m\u001b[1mEpoch: [29/70], Progress: 64.52 %, Steps: 2665, Current Learning Rate: 0.0006586, \u001b[91mTrain Loss: 3.055\n",
      "\u001b[0m\u001b[1mEpoch: [29/70], Progress: 65.59 %, Steps: 2666, Current Learning Rate: 0.0006589, \u001b[91mTrain Loss: 3.252\n",
      "\u001b[0m\u001b[1mEpoch: [29/70], Progress: 66.67 %, Steps: 2667, Current Learning Rate: 0.0006591, \u001b[96mTrain Loss: 3.082\n",
      "\u001b[0m\u001b[1mEpoch: [29/70], Progress: 67.74 %, Steps: 2668, Current Learning Rate: 0.0006594, \u001b[91mTrain Loss: 3.095\n",
      "\u001b[0m\u001b[1mEpoch: [29/70], Progress: 68.82 %, Steps: 2669, Current Learning Rate: 0.0006596, \u001b[91mTrain Loss: 3.173\n",
      "\u001b[0m\u001b[1mEpoch: [29/70], Progress: 69.89 %, Steps: 2670, Current Learning Rate: 0.0006599, \u001b[96mTrain Loss: 3.101\n",
      "\u001b[0m\u001b[1mEpoch: [29/70], Progress: 70.97 %, Steps: 2671, Current Learning Rate: 0.0006601, \u001b[91mTrain Loss: 3.225\n",
      "\u001b[0m\u001b[1mEpoch: [29/70], Progress: 72.04 %, Steps: 2672, Current Learning Rate: 0.0006604, \u001b[96mTrain Loss: 3.062\n",
      "\u001b[0m\u001b[1mEpoch: [29/70], Progress: 73.12 %, Steps: 2673, Current Learning Rate: 0.0006606, \u001b[91mTrain Loss: 3.180\n",
      "\u001b[0m\u001b[1mEpoch: [29/70], Progress: 74.19 %, Steps: 2674, Current Learning Rate: 0.0006609, \u001b[96mTrain Loss: 3.157\n",
      "\u001b[0m\u001b[1mEpoch: [29/70], Progress: 75.27 %, Steps: 2675, Current Learning Rate: 0.0006611, \u001b[96mTrain Loss: 3.099\n",
      "\u001b[0m\u001b[1mEpoch: [29/70], Progress: 76.34 %, Steps: 2676, Current Learning Rate: 0.0006614, \u001b[96mTrain Loss: 2.993\n",
      "\u001b[0m\u001b[1mEpoch: [29/70], Progress: 77.42 %, Steps: 2677, Current Learning Rate: 0.0006616, \u001b[91mTrain Loss: 3.069\n",
      "\u001b[0m\u001b[1mEpoch: [29/70], Progress: 78.49 %, Steps: 2678, Current Learning Rate: 0.0006619, \u001b[91mTrain Loss: 3.106\n",
      "\u001b[0m\u001b[1mEpoch: [29/70], Progress: 79.57 %, Steps: 2679, Current Learning Rate: 0.0006621, \u001b[96mTrain Loss: 2.919\n",
      "\u001b[0m\u001b[1mEpoch: [29/70], Progress: 80.65 %, Steps: 2680, Current Learning Rate: 0.0006623, \u001b[91mTrain Loss: 3.047\n",
      "\u001b[0m\u001b[1mEpoch: [29/70], Progress: 81.72 %, Steps: 2681, Current Learning Rate: 0.0006626, \u001b[96mTrain Loss: 2.964\n",
      "\u001b[0m\u001b[1mEpoch: [29/70], Progress: 82.80 %, Steps: 2682, Current Learning Rate: 0.0006628, \u001b[96mTrain Loss: 2.936\n",
      "\u001b[0m\u001b[1mEpoch: [29/70], Progress: 83.87 %, Steps: 2683, Current Learning Rate: 0.0006631, \u001b[91mTrain Loss: 3.058\n",
      "\u001b[0m\u001b[1mEpoch: [29/70], Progress: 84.95 %, Steps: 2684, Current Learning Rate: 0.0006633, \u001b[91mTrain Loss: 3.086\n",
      "\u001b[0m\u001b[1mEpoch: [29/70], Progress: 86.02 %, Steps: 2685, Current Learning Rate: 0.0006636, \u001b[91mTrain Loss: 3.314\n",
      "\u001b[0m\u001b[1mEpoch: [29/70], Progress: 87.10 %, Steps: 2686, Current Learning Rate: 0.0006638, \u001b[96mTrain Loss: 3.128\n",
      "\u001b[0m\u001b[1mEpoch: [29/70], Progress: 88.17 %, Steps: 2687, Current Learning Rate: 0.0006641, \u001b[96mTrain Loss: 2.945\n",
      "\u001b[0m\u001b[1mEpoch: [29/70], Progress: 89.25 %, Steps: 2688, Current Learning Rate: 0.0006643, \u001b[96mTrain Loss: 2.911\n",
      "\u001b[0m\u001b[1mEpoch: [29/70], Progress: 90.32 %, Steps: 2689, Current Learning Rate: 0.0006646, \u001b[91mTrain Loss: 3.125\n",
      "\u001b[0m\u001b[1mEpoch: [29/70], Progress: 91.40 %, Steps: 2690, Current Learning Rate: 0.0006648, \u001b[96mTrain Loss: 3.113\n",
      "\u001b[0m\u001b[1mEpoch: [29/70], Progress: 92.47 %, Steps: 2691, Current Learning Rate: 0.0006651, \u001b[91mTrain Loss: 3.156\n",
      "\u001b[0m\u001b[1mEpoch: [29/70], Progress: 93.55 %, Steps: 2692, Current Learning Rate: 0.0006653, \u001b[91mTrain Loss: 3.162\n",
      "\u001b[0m\u001b[1mEpoch: [29/70], Progress: 94.62 %, Steps: 2693, Current Learning Rate: 0.0006656, \u001b[96mTrain Loss: 3.141\n",
      "\u001b[0m\u001b[1mEpoch: [29/70], Progress: 95.70 %, Steps: 2694, Current Learning Rate: 0.0006658, \u001b[91mTrain Loss: 3.380\n",
      "\u001b[0m\u001b[1mEpoch: [29/70], Progress: 96.77 %, Steps: 2695, Current Learning Rate: 0.0006661, \u001b[96mTrain Loss: 2.921\n",
      "\u001b[0m\u001b[1mEpoch: [29/70], Progress: 97.85 %, Steps: 2696, Current Learning Rate: 0.0006663, \u001b[91mTrain Loss: 3.016\n",
      "\u001b[0m\u001b[1mEpoch: [29/70], Progress: 98.92 %, Steps: 2697, Current Learning Rate: 0.0006665, \u001b[91mTrain Loss: 3.197\n",
      "\u001b[1m\u001b[93mComputing Validation Loss...\n",
      "\u001b[1mEpoch 29 Completed! Average Train Loss: 3.042, Average Validation Loss: 2.275\n",
      "Best Model saved in best_model/best_model.pt.\n",
      "\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m= Epoch [30/70] ==========================================\n",
      "\u001b[0m\u001b[1mEpoch: [30/70], Progress: 0.00 %, Steps: 2698, Current Learning Rate: 0.0006668, \u001b[91mTrain Loss: 2.719\n",
      "\u001b[0m\u001b[1mEpoch: [30/70], Progress: 1.08 %, Steps: 2699, Current Learning Rate: 0.0006670, \u001b[91mTrain Loss: 3.119\n",
      "\u001b[0m\u001b[1mEpoch: [30/70], Progress: 2.15 %, Steps: 2700, Current Learning Rate: 0.0006673, \u001b[96mTrain Loss: 2.662\n",
      "\u001b[0m\u001b[1mEpoch: [30/70], Progress: 3.23 %, Steps: 2701, Current Learning Rate: 0.0006675, \u001b[91mTrain Loss: 2.852\n",
      "\u001b[0m\u001b[1mEpoch: [30/70], Progress: 4.30 %, Steps: 2702, Current Learning Rate: 0.0006678, \u001b[96mTrain Loss: 2.688\n",
      "\u001b[0m\u001b[1mEpoch: [30/70], Progress: 5.38 %, Steps: 2703, Current Learning Rate: 0.0006680, \u001b[91mTrain Loss: 2.724\n",
      "\u001b[0m\u001b[1mEpoch: [30/70], Progress: 6.45 %, Steps: 2704, Current Learning Rate: 0.0006683, \u001b[91mTrain Loss: 2.900\n",
      "\u001b[0m\u001b[1mEpoch: [30/70], Progress: 7.53 %, Steps: 2705, Current Learning Rate: 0.0006685, \u001b[96mTrain Loss: 2.596\n",
      "\u001b[0m\u001b[1mEpoch: [30/70], Progress: 8.60 %, Steps: 2706, Current Learning Rate: 0.0006688, \u001b[91mTrain Loss: 2.821\n",
      "\u001b[0m\u001b[1mEpoch: [30/70], Progress: 9.68 %, Steps: 2707, Current Learning Rate: 0.0006690, \u001b[91mTrain Loss: 2.839\n",
      "\u001b[0m\u001b[1mEpoch: [30/70], Progress: 10.75 %, Steps: 2708, Current Learning Rate: 0.0006693, \u001b[96mTrain Loss: 2.651\n",
      "\u001b[0m\u001b[1mEpoch: [30/70], Progress: 11.83 %, Steps: 2709, Current Learning Rate: 0.0006695, \u001b[91mTrain Loss: 2.815\n",
      "\u001b[0m\u001b[1mEpoch: [30/70], Progress: 12.90 %, Steps: 2710, Current Learning Rate: 0.0006698, \u001b[96mTrain Loss: 2.813\n",
      "\u001b[0m\u001b[1mEpoch: [30/70], Progress: 13.98 %, Steps: 2711, Current Learning Rate: 0.0006700, \u001b[96mTrain Loss: 2.559\n",
      "\u001b[0m\u001b[1mEpoch: [30/70], Progress: 15.05 %, Steps: 2712, Current Learning Rate: 0.0006703, \u001b[91mTrain Loss: 2.783\n",
      "\u001b[0m\u001b[1mEpoch: [30/70], Progress: 16.13 %, Steps: 2713, Current Learning Rate: 0.0006705, \u001b[96mTrain Loss: 2.660\n",
      "\u001b[0m\u001b[1mEpoch: [30/70], Progress: 17.20 %, Steps: 2714, Current Learning Rate: 0.0006707, \u001b[91mTrain Loss: 2.876\n",
      "\u001b[0m\u001b[1mEpoch: [30/70], Progress: 18.28 %, Steps: 2715, Current Learning Rate: 0.0006710, \u001b[96mTrain Loss: 2.788\n",
      "\u001b[0m\u001b[1mEpoch: [30/70], Progress: 19.35 %, Steps: 2716, Current Learning Rate: 0.0006712, \u001b[96mTrain Loss: 2.671\n",
      "\u001b[0m\u001b[1mEpoch: [30/70], Progress: 20.43 %, Steps: 2717, Current Learning Rate: 0.0006715, \u001b[91mTrain Loss: 2.844\n",
      "\u001b[0m\u001b[1mEpoch: [30/70], Progress: 21.51 %, Steps: 2718, Current Learning Rate: 0.0006717, \u001b[96mTrain Loss: 2.840\n",
      "\u001b[0m\u001b[1mEpoch: [30/70], Progress: 22.58 %, Steps: 2719, Current Learning Rate: 0.0006720, \u001b[96mTrain Loss: 2.703\n",
      "\u001b[0m\u001b[1mEpoch: [30/70], Progress: 23.66 %, Steps: 2720, Current Learning Rate: 0.0006722, \u001b[91mTrain Loss: 2.967\n",
      "\u001b[0m\u001b[1mEpoch: [30/70], Progress: 24.73 %, Steps: 2721, Current Learning Rate: 0.0006725, \u001b[96mTrain Loss: 2.782\n",
      "\u001b[0m\u001b[1mEpoch: [30/70], Progress: 25.81 %, Steps: 2722, Current Learning Rate: 0.0006727, \u001b[91mTrain Loss: 2.906\n",
      "\u001b[0m\u001b[1mEpoch: [30/70], Progress: 26.88 %, Steps: 2723, Current Learning Rate: 0.0006730, \u001b[96mTrain Loss: 2.827\n",
      "\u001b[0m\u001b[1mEpoch: [30/70], Progress: 27.96 %, Steps: 2724, Current Learning Rate: 0.0006732, \u001b[96mTrain Loss: 2.720\n",
      "\u001b[0m\u001b[1mEpoch: [30/70], Progress: 29.03 %, Steps: 2725, Current Learning Rate: 0.0006735, \u001b[91mTrain Loss: 2.823\n",
      "\u001b[0m\u001b[1mEpoch: [30/70], Progress: 30.11 %, Steps: 2726, Current Learning Rate: 0.0006737, \u001b[96mTrain Loss: 2.693\n",
      "\u001b[0m\u001b[1mEpoch: [30/70], Progress: 31.18 %, Steps: 2727, Current Learning Rate: 0.0006740, \u001b[96mTrain Loss: 2.647\n",
      "\u001b[0m\u001b[1mEpoch: [30/70], Progress: 32.26 %, Steps: 2728, Current Learning Rate: 0.0006742, \u001b[91mTrain Loss: 2.777\n",
      "\u001b[0m\u001b[1mEpoch: [30/70], Progress: 33.33 %, Steps: 2729, Current Learning Rate: 0.0006745, \u001b[91mTrain Loss: 2.794\n",
      "\u001b[0m\u001b[1mEpoch: [30/70], Progress: 34.41 %, Steps: 2730, Current Learning Rate: 0.0006747, \u001b[91mTrain Loss: 2.991\n",
      "\u001b[0m\u001b[1mEpoch: [30/70], Progress: 35.48 %, Steps: 2731, Current Learning Rate: 0.0006749, \u001b[96mTrain Loss: 2.835\n",
      "\u001b[0m\u001b[1mEpoch: [30/70], Progress: 36.56 %, Steps: 2732, Current Learning Rate: 0.0006752, \u001b[96mTrain Loss: 2.821\n",
      "\u001b[0m\u001b[1mEpoch: [30/70], Progress: 37.63 %, Steps: 2733, Current Learning Rate: 0.0006754, \u001b[96mTrain Loss: 2.802\n",
      "\u001b[0m\u001b[1mEpoch: [30/70], Progress: 38.71 %, Steps: 2734, Current Learning Rate: 0.0006757, \u001b[91mTrain Loss: 2.830\n",
      "\u001b[0m\u001b[1mEpoch: [30/70], Progress: 39.78 %, Steps: 2735, Current Learning Rate: 0.0006759, \u001b[96mTrain Loss: 2.696\n",
      "\u001b[0m\u001b[1mEpoch: [30/70], Progress: 40.86 %, Steps: 2736, Current Learning Rate: 0.0006762, \u001b[91mTrain Loss: 2.753\n",
      "\u001b[0m\u001b[1mEpoch: [30/70], Progress: 41.94 %, Steps: 2737, Current Learning Rate: 0.0006764, \u001b[91mTrain Loss: 2.895\n",
      "\u001b[0m\u001b[1mEpoch: [30/70], Progress: 43.01 %, Steps: 2738, Current Learning Rate: 0.0006767, \u001b[96mTrain Loss: 2.882\n",
      "\u001b[0m\u001b[1mEpoch: [30/70], Progress: 44.09 %, Steps: 2739, Current Learning Rate: 0.0006769, \u001b[96mTrain Loss: 2.626\n",
      "\u001b[0m\u001b[1mEpoch: [30/70], Progress: 45.16 %, Steps: 2740, Current Learning Rate: 0.0006772, \u001b[91mTrain Loss: 2.689\n",
      "\u001b[0m\u001b[1mEpoch: [30/70], Progress: 46.24 %, Steps: 2741, Current Learning Rate: 0.0006774, \u001b[91mTrain Loss: 3.014\n",
      "\u001b[0m\u001b[1mEpoch: [30/70], Progress: 47.31 %, Steps: 2742, Current Learning Rate: 0.0006777, \u001b[96mTrain Loss: 2.959\n",
      "\u001b[0m\u001b[1mEpoch: [30/70], Progress: 48.39 %, Steps: 2743, Current Learning Rate: 0.0006779, \u001b[91mTrain Loss: 3.113\n",
      "\u001b[0m\u001b[1mEpoch: [30/70], Progress: 49.46 %, Steps: 2744, Current Learning Rate: 0.0006782, \u001b[96mTrain Loss: 2.904\n",
      "\u001b[0m\u001b[1mEpoch: [30/70], Progress: 50.54 %, Steps: 2745, Current Learning Rate: 0.0006784, \u001b[91mTrain Loss: 2.960\n",
      "\u001b[0m\u001b[1mEpoch: [30/70], Progress: 51.61 %, Steps: 2746, Current Learning Rate: 0.0006787, \u001b[96mTrain Loss: 2.753\n",
      "\u001b[0m\u001b[1mEpoch: [30/70], Progress: 52.69 %, Steps: 2747, Current Learning Rate: 0.0006789, \u001b[96mTrain Loss: 2.747\n",
      "\u001b[0m\u001b[1mEpoch: [30/70], Progress: 53.76 %, Steps: 2748, Current Learning Rate: 0.0006791, \u001b[91mTrain Loss: 2.925\n",
      "\u001b[0m\u001b[1mEpoch: [30/70], Progress: 54.84 %, Steps: 2749, Current Learning Rate: 0.0006794, \u001b[91mTrain Loss: 3.080\n",
      "\u001b[0m\u001b[1mEpoch: [30/70], Progress: 55.91 %, Steps: 2750, Current Learning Rate: 0.0006796, \u001b[96mTrain Loss: 2.934\n",
      "\u001b[0m\u001b[1mEpoch: [30/70], Progress: 56.99 %, Steps: 2751, Current Learning Rate: 0.0006799, \u001b[91mTrain Loss: 2.965\n",
      "\u001b[0m\u001b[1mEpoch: [30/70], Progress: 58.06 %, Steps: 2752, Current Learning Rate: 0.0006801, \u001b[96mTrain Loss: 2.906\n",
      "\u001b[0m\u001b[1mEpoch: [30/70], Progress: 59.14 %, Steps: 2753, Current Learning Rate: 0.0006804, \u001b[96mTrain Loss: 2.704\n",
      "\u001b[0m\u001b[1mEpoch: [30/70], Progress: 60.22 %, Steps: 2754, Current Learning Rate: 0.0006806, \u001b[96mTrain Loss: 2.608\n",
      "\u001b[0m\u001b[1mEpoch: [30/70], Progress: 61.29 %, Steps: 2755, Current Learning Rate: 0.0006809, \u001b[91mTrain Loss: 2.822\n",
      "\u001b[0m\u001b[1mEpoch: [30/70], Progress: 62.37 %, Steps: 2756, Current Learning Rate: 0.0006811, \u001b[91mTrain Loss: 3.000\n",
      "\u001b[0m\u001b[1mEpoch: [30/70], Progress: 63.44 %, Steps: 2757, Current Learning Rate: 0.0006814, \u001b[96mTrain Loss: 2.897\n",
      "\u001b[0m\u001b[1mEpoch: [30/70], Progress: 64.52 %, Steps: 2758, Current Learning Rate: 0.0006816, \u001b[96mTrain Loss: 2.831\n",
      "\u001b[0m\u001b[1mEpoch: [30/70], Progress: 65.59 %, Steps: 2759, Current Learning Rate: 0.0006819, \u001b[91mTrain Loss: 2.849\n",
      "\u001b[0m\u001b[1mEpoch: [30/70], Progress: 66.67 %, Steps: 2760, Current Learning Rate: 0.0006821, \u001b[96mTrain Loss: 2.780\n",
      "\u001b[0m\u001b[1mEpoch: [30/70], Progress: 67.74 %, Steps: 2761, Current Learning Rate: 0.0006824, \u001b[91mTrain Loss: 2.905\n",
      "\u001b[0m\u001b[1mEpoch: [30/70], Progress: 68.82 %, Steps: 2762, Current Learning Rate: 0.0006826, \u001b[96mTrain Loss: 2.731\n",
      "\u001b[0m\u001b[1mEpoch: [30/70], Progress: 69.89 %, Steps: 2763, Current Learning Rate: 0.0006829, \u001b[91mTrain Loss: 2.869\n",
      "\u001b[0m\u001b[1mEpoch: [30/70], Progress: 70.97 %, Steps: 2764, Current Learning Rate: 0.0006831, \u001b[96mTrain Loss: 2.861\n",
      "\u001b[0m\u001b[1mEpoch: [30/70], Progress: 72.04 %, Steps: 2765, Current Learning Rate: 0.0006833, \u001b[91mTrain Loss: 2.895\n",
      "\u001b[0m\u001b[1mEpoch: [30/70], Progress: 73.12 %, Steps: 2766, Current Learning Rate: 0.0006836, \u001b[91mTrain Loss: 3.053\n",
      "\u001b[0m\u001b[1mEpoch: [30/70], Progress: 74.19 %, Steps: 2767, Current Learning Rate: 0.0006838, \u001b[91mTrain Loss: 3.122\n",
      "\u001b[0m\u001b[1mEpoch: [30/70], Progress: 75.27 %, Steps: 2768, Current Learning Rate: 0.0006841, \u001b[96mTrain Loss: 2.911\n",
      "\u001b[0m\u001b[1mEpoch: [30/70], Progress: 76.34 %, Steps: 2769, Current Learning Rate: 0.0006843, \u001b[96mTrain Loss: 2.903\n",
      "\u001b[0m\u001b[1mEpoch: [30/70], Progress: 77.42 %, Steps: 2770, Current Learning Rate: 0.0006846, \u001b[91mTrain Loss: 3.017\n",
      "\u001b[0m\u001b[1mEpoch: [30/70], Progress: 78.49 %, Steps: 2771, Current Learning Rate: 0.0006848, \u001b[91mTrain Loss: 3.020\n",
      "\u001b[0m\u001b[1mEpoch: [30/70], Progress: 79.57 %, Steps: 2772, Current Learning Rate: 0.0006851, \u001b[96mTrain Loss: 2.926\n",
      "\u001b[0m\u001b[1mEpoch: [30/70], Progress: 80.65 %, Steps: 2773, Current Learning Rate: 0.0006853, \u001b[91mTrain Loss: 2.932\n",
      "\u001b[0m\u001b[1mEpoch: [30/70], Progress: 81.72 %, Steps: 2774, Current Learning Rate: 0.0006856, \u001b[96mTrain Loss: 2.768\n",
      "\u001b[0m\u001b[1mEpoch: [30/70], Progress: 82.80 %, Steps: 2775, Current Learning Rate: 0.0006858, \u001b[91mTrain Loss: 3.089\n",
      "\u001b[0m\u001b[1mEpoch: [30/70], Progress: 83.87 %, Steps: 2776, Current Learning Rate: 0.0006861, \u001b[96mTrain Loss: 2.848\n",
      "\u001b[0m\u001b[1mEpoch: [30/70], Progress: 84.95 %, Steps: 2777, Current Learning Rate: 0.0006863, \u001b[91mTrain Loss: 3.014\n",
      "\u001b[0m\u001b[1mEpoch: [30/70], Progress: 86.02 %, Steps: 2778, Current Learning Rate: 0.0006866, \u001b[91mTrain Loss: 3.022\n",
      "\u001b[0m\u001b[1mEpoch: [30/70], Progress: 87.10 %, Steps: 2779, Current Learning Rate: 0.0006868, \u001b[91mTrain Loss: 3.169\n",
      "\u001b[0m\u001b[1mEpoch: [30/70], Progress: 88.17 %, Steps: 2780, Current Learning Rate: 0.0006871, \u001b[96mTrain Loss: 2.844\n",
      "\u001b[0m\u001b[1mEpoch: [30/70], Progress: 89.25 %, Steps: 2781, Current Learning Rate: 0.0006873, \u001b[91mTrain Loss: 3.092\n",
      "\u001b[0m\u001b[1mEpoch: [30/70], Progress: 90.32 %, Steps: 2782, Current Learning Rate: 0.0006875, \u001b[96mTrain Loss: 2.837\n",
      "\u001b[0m\u001b[1mEpoch: [30/70], Progress: 91.40 %, Steps: 2783, Current Learning Rate: 0.0006878, \u001b[91mTrain Loss: 2.839\n",
      "\u001b[0m\u001b[1mEpoch: [30/70], Progress: 92.47 %, Steps: 2784, Current Learning Rate: 0.0006880, \u001b[91mTrain Loss: 3.070\n",
      "\u001b[0m\u001b[1mEpoch: [30/70], Progress: 93.55 %, Steps: 2785, Current Learning Rate: 0.0006883, \u001b[96mTrain Loss: 3.006\n",
      "\u001b[0m\u001b[1mEpoch: [30/70], Progress: 94.62 %, Steps: 2786, Current Learning Rate: 0.0006885, \u001b[96mTrain Loss: 2.760\n",
      "\u001b[0m\u001b[1mEpoch: [30/70], Progress: 95.70 %, Steps: 2787, Current Learning Rate: 0.0006888, \u001b[96mTrain Loss: 2.688\n",
      "\u001b[0m\u001b[1mEpoch: [30/70], Progress: 96.77 %, Steps: 2788, Current Learning Rate: 0.0006890, \u001b[91mTrain Loss: 3.113\n",
      "\u001b[0m\u001b[1mEpoch: [30/70], Progress: 97.85 %, Steps: 2789, Current Learning Rate: 0.0006893, \u001b[96mTrain Loss: 3.049\n",
      "\u001b[0m\u001b[1mEpoch: [30/70], Progress: 98.92 %, Steps: 2790, Current Learning Rate: 0.0006895, \u001b[91mTrain Loss: 3.219\n",
      "\u001b[1m\u001b[93mComputing Validation Loss...\n",
      "\u001b[1mEpoch 30 Completed! Average Train Loss: 2.860, Average Validation Loss: 2.031\n",
      "Best Model saved in best_model/best_model.pt.\n",
      "\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m= Epoch [31/70] ==========================================\n",
      "\u001b[0m\u001b[1mEpoch: [31/70], Progress: 0.00 %, Steps: 2791, Current Learning Rate: 0.0006898, \u001b[91mTrain Loss: 2.613\n",
      "\u001b[0m\u001b[1mEpoch: [31/70], Progress: 1.08 %, Steps: 2792, Current Learning Rate: 0.0006900, \u001b[96mTrain Loss: 2.321\n",
      "\u001b[0m\u001b[1mEpoch: [31/70], Progress: 2.15 %, Steps: 2793, Current Learning Rate: 0.0006903, \u001b[91mTrain Loss: 2.495\n",
      "\u001b[0m\u001b[1mEpoch: [31/70], Progress: 3.23 %, Steps: 2794, Current Learning Rate: 0.0006905, \u001b[91mTrain Loss: 2.675\n",
      "\u001b[0m\u001b[1mEpoch: [31/70], Progress: 4.30 %, Steps: 2795, Current Learning Rate: 0.0006908, \u001b[96mTrain Loss: 2.630\n",
      "\u001b[0m\u001b[1mEpoch: [31/70], Progress: 5.38 %, Steps: 2796, Current Learning Rate: 0.0006910, \u001b[91mTrain Loss: 2.695\n",
      "\u001b[0m\u001b[1mEpoch: [31/70], Progress: 6.45 %, Steps: 2797, Current Learning Rate: 0.0006913, \u001b[96mTrain Loss: 2.543\n",
      "\u001b[0m\u001b[1mEpoch: [31/70], Progress: 7.53 %, Steps: 2798, Current Learning Rate: 0.0006915, \u001b[91mTrain Loss: 2.613\n",
      "\u001b[0m\u001b[1mEpoch: [31/70], Progress: 8.60 %, Steps: 2799, Current Learning Rate: 0.0006917, \u001b[91mTrain Loss: 2.659\n",
      "\u001b[0m\u001b[1mEpoch: [31/70], Progress: 9.68 %, Steps: 2800, Current Learning Rate: 0.0006920, \u001b[96mTrain Loss: 2.467\n",
      "\u001b[0m\u001b[1mEpoch: [31/70], Progress: 10.75 %, Steps: 2801, Current Learning Rate: 0.0006922, \u001b[91mTrain Loss: 2.590\n",
      "\u001b[0m\u001b[1mEpoch: [31/70], Progress: 11.83 %, Steps: 2802, Current Learning Rate: 0.0006925, \u001b[91mTrain Loss: 2.664\n",
      "\u001b[0m\u001b[1mEpoch: [31/70], Progress: 12.90 %, Steps: 2803, Current Learning Rate: 0.0006927, \u001b[96mTrain Loss: 2.582\n",
      "\u001b[0m\u001b[1mEpoch: [31/70], Progress: 13.98 %, Steps: 2804, Current Learning Rate: 0.0006930, \u001b[96mTrain Loss: 2.521\n",
      "\u001b[0m\u001b[1mEpoch: [31/70], Progress: 15.05 %, Steps: 2805, Current Learning Rate: 0.0006932, \u001b[91mTrain Loss: 2.754\n",
      "\u001b[0m\u001b[1mEpoch: [31/70], Progress: 16.13 %, Steps: 2806, Current Learning Rate: 0.0006935, \u001b[91mTrain Loss: 2.762\n",
      "\u001b[0m\u001b[1mEpoch: [31/70], Progress: 17.20 %, Steps: 2807, Current Learning Rate: 0.0006937, \u001b[96mTrain Loss: 2.707\n",
      "\u001b[0m\u001b[1mEpoch: [31/70], Progress: 18.28 %, Steps: 2808, Current Learning Rate: 0.0006940, \u001b[96mTrain Loss: 2.457\n",
      "\u001b[0m\u001b[1mEpoch: [31/70], Progress: 19.35 %, Steps: 2809, Current Learning Rate: 0.0006942, \u001b[91mTrain Loss: 2.539\n",
      "\u001b[0m\u001b[1mEpoch: [31/70], Progress: 20.43 %, Steps: 2810, Current Learning Rate: 0.0006945, \u001b[91mTrain Loss: 2.676\n",
      "\u001b[0m\u001b[1mEpoch: [31/70], Progress: 21.51 %, Steps: 2811, Current Learning Rate: 0.0006947, \u001b[96mTrain Loss: 2.605\n",
      "\u001b[0m\u001b[1mEpoch: [31/70], Progress: 22.58 %, Steps: 2812, Current Learning Rate: 0.0006950, \u001b[96mTrain Loss: 2.560\n",
      "\u001b[0m\u001b[1mEpoch: [31/70], Progress: 23.66 %, Steps: 2813, Current Learning Rate: 0.0006952, \u001b[91mTrain Loss: 2.638\n",
      "\u001b[0m\u001b[1mEpoch: [31/70], Progress: 24.73 %, Steps: 2814, Current Learning Rate: 0.0006955, \u001b[96mTrain Loss: 2.511\n",
      "\u001b[0m\u001b[1mEpoch: [31/70], Progress: 25.81 %, Steps: 2815, Current Learning Rate: 0.0006957, \u001b[91mTrain Loss: 2.941\n",
      "\u001b[0m\u001b[1mEpoch: [31/70], Progress: 26.88 %, Steps: 2816, Current Learning Rate: 0.0006959, \u001b[96mTrain Loss: 2.708\n",
      "\u001b[0m\u001b[1mEpoch: [31/70], Progress: 27.96 %, Steps: 2817, Current Learning Rate: 0.0006962, \u001b[96mTrain Loss: 2.341\n",
      "\u001b[0m\u001b[1mEpoch: [31/70], Progress: 29.03 %, Steps: 2818, Current Learning Rate: 0.0006964, \u001b[91mTrain Loss: 2.568\n",
      "\u001b[0m\u001b[1mEpoch: [31/70], Progress: 30.11 %, Steps: 2819, Current Learning Rate: 0.0006967, \u001b[96mTrain Loss: 2.568\n",
      "\u001b[0m\u001b[1mEpoch: [31/70], Progress: 31.18 %, Steps: 2820, Current Learning Rate: 0.0006969, \u001b[91mTrain Loss: 2.640\n",
      "\u001b[0m\u001b[1mEpoch: [31/70], Progress: 32.26 %, Steps: 2821, Current Learning Rate: 0.0006972, \u001b[91mTrain Loss: 2.695\n",
      "\u001b[0m\u001b[1mEpoch: [31/70], Progress: 33.33 %, Steps: 2822, Current Learning Rate: 0.0006974, \u001b[96mTrain Loss: 2.583\n",
      "\u001b[0m\u001b[1mEpoch: [31/70], Progress: 34.41 %, Steps: 2823, Current Learning Rate: 0.0006977, \u001b[96mTrain Loss: 2.503\n",
      "\u001b[0m\u001b[1mEpoch: [31/70], Progress: 35.48 %, Steps: 2824, Current Learning Rate: 0.0006979, \u001b[91mTrain Loss: 2.785\n",
      "\u001b[0m\u001b[1mEpoch: [31/70], Progress: 36.56 %, Steps: 2825, Current Learning Rate: 0.0006982, \u001b[91mTrain Loss: 2.851\n",
      "\u001b[0m\u001b[1mEpoch: [31/70], Progress: 37.63 %, Steps: 2826, Current Learning Rate: 0.0006984, \u001b[96mTrain Loss: 2.619\n",
      "\u001b[0m\u001b[1mEpoch: [31/70], Progress: 38.71 %, Steps: 2827, Current Learning Rate: 0.0006987, \u001b[91mTrain Loss: 2.887\n",
      "\u001b[0m\u001b[1mEpoch: [31/70], Progress: 39.78 %, Steps: 2828, Current Learning Rate: 0.0006989, \u001b[96mTrain Loss: 2.822\n",
      "\u001b[0m\u001b[1mEpoch: [31/70], Progress: 40.86 %, Steps: 2829, Current Learning Rate: 0.0006992, \u001b[96mTrain Loss: 2.814\n",
      "\u001b[0m\u001b[1mEpoch: [31/70], Progress: 41.94 %, Steps: 2830, Current Learning Rate: 0.0006994, \u001b[91mTrain Loss: 2.815\n",
      "\u001b[0m\u001b[1mEpoch: [31/70], Progress: 43.01 %, Steps: 2831, Current Learning Rate: 0.0006997, \u001b[91mTrain Loss: 2.880\n",
      "\u001b[0m\u001b[1mEpoch: [31/70], Progress: 44.09 %, Steps: 2832, Current Learning Rate: 0.0006999, \u001b[96mTrain Loss: 2.837\n",
      "\u001b[0m\u001b[1mEpoch: [31/70], Progress: 45.16 %, Steps: 2833, Current Learning Rate: 0.0007001, \u001b[96mTrain Loss: 2.679\n",
      "\u001b[0m\u001b[1mEpoch: [31/70], Progress: 46.24 %, Steps: 2834, Current Learning Rate: 0.0007004, \u001b[91mTrain Loss: 2.770\n",
      "\u001b[0m\u001b[1mEpoch: [31/70], Progress: 47.31 %, Steps: 2835, Current Learning Rate: 0.0007006, \u001b[96mTrain Loss: 2.706\n",
      "\u001b[0m\u001b[1mEpoch: [31/70], Progress: 48.39 %, Steps: 2836, Current Learning Rate: 0.0007009, \u001b[91mTrain Loss: 2.707\n",
      "\u001b[0m\u001b[1mEpoch: [31/70], Progress: 49.46 %, Steps: 2837, Current Learning Rate: 0.0007011, \u001b[96mTrain Loss: 2.692\n",
      "\u001b[0m\u001b[1mEpoch: [31/70], Progress: 50.54 %, Steps: 2838, Current Learning Rate: 0.0007014, \u001b[96mTrain Loss: 2.610\n",
      "\u001b[0m\u001b[1mEpoch: [31/70], Progress: 51.61 %, Steps: 2839, Current Learning Rate: 0.0007016, \u001b[91mTrain Loss: 2.793\n",
      "\u001b[0m\u001b[1mEpoch: [31/70], Progress: 52.69 %, Steps: 2840, Current Learning Rate: 0.0007019, \u001b[96mTrain Loss: 2.745\n",
      "\u001b[0m\u001b[1mEpoch: [31/70], Progress: 53.76 %, Steps: 2841, Current Learning Rate: 0.0007021, \u001b[96mTrain Loss: 2.605\n",
      "\u001b[0m\u001b[1mEpoch: [31/70], Progress: 54.84 %, Steps: 2842, Current Learning Rate: 0.0007024, \u001b[96mTrain Loss: 2.546\n",
      "\u001b[0m\u001b[1mEpoch: [31/70], Progress: 55.91 %, Steps: 2843, Current Learning Rate: 0.0007026, \u001b[91mTrain Loss: 2.744\n",
      "\u001b[0m\u001b[1mEpoch: [31/70], Progress: 56.99 %, Steps: 2844, Current Learning Rate: 0.0007029, \u001b[91mTrain Loss: 2.749\n",
      "\u001b[0m\u001b[1mEpoch: [31/70], Progress: 58.06 %, Steps: 2845, Current Learning Rate: 0.0007031, \u001b[96mTrain Loss: 2.667\n",
      "\u001b[0m\u001b[1mEpoch: [31/70], Progress: 59.14 %, Steps: 2846, Current Learning Rate: 0.0007034, \u001b[96mTrain Loss: 2.598\n",
      "\u001b[0m\u001b[1mEpoch: [31/70], Progress: 60.22 %, Steps: 2847, Current Learning Rate: 0.0007036, \u001b[91mTrain Loss: 2.723\n",
      "\u001b[0m\u001b[1mEpoch: [31/70], Progress: 61.29 %, Steps: 2848, Current Learning Rate: 0.0007039, \u001b[91mTrain Loss: 2.827\n",
      "\u001b[0m\u001b[1mEpoch: [31/70], Progress: 62.37 %, Steps: 2849, Current Learning Rate: 0.0007041, \u001b[91mTrain Loss: 2.867\n",
      "\u001b[0m\u001b[1mEpoch: [31/70], Progress: 63.44 %, Steps: 2850, Current Learning Rate: 0.0007043, \u001b[96mTrain Loss: 2.728\n",
      "\u001b[0m\u001b[1mEpoch: [31/70], Progress: 64.52 %, Steps: 2851, Current Learning Rate: 0.0007046, \u001b[96mTrain Loss: 2.615\n",
      "\u001b[0m\u001b[1mEpoch: [31/70], Progress: 65.59 %, Steps: 2852, Current Learning Rate: 0.0007048, \u001b[91mTrain Loss: 2.678\n",
      "\u001b[0m\u001b[1mEpoch: [31/70], Progress: 66.67 %, Steps: 2853, Current Learning Rate: 0.0007051, \u001b[91mTrain Loss: 2.762\n",
      "\u001b[0m\u001b[1mEpoch: [31/70], Progress: 67.74 %, Steps: 2854, Current Learning Rate: 0.0007053, \u001b[91mTrain Loss: 2.818\n",
      "\u001b[0m\u001b[1mEpoch: [31/70], Progress: 68.82 %, Steps: 2855, Current Learning Rate: 0.0007056, \u001b[96mTrain Loss: 2.684\n",
      "\u001b[0m\u001b[1mEpoch: [31/70], Progress: 69.89 %, Steps: 2856, Current Learning Rate: 0.0007058, \u001b[96mTrain Loss: 2.579\n",
      "\u001b[0m\u001b[1mEpoch: [31/70], Progress: 70.97 %, Steps: 2857, Current Learning Rate: 0.0007061, \u001b[91mTrain Loss: 2.738\n",
      "\u001b[0m\u001b[1mEpoch: [31/70], Progress: 72.04 %, Steps: 2858, Current Learning Rate: 0.0007063, \u001b[96mTrain Loss: 2.566\n",
      "\u001b[0m\u001b[1mEpoch: [31/70], Progress: 73.12 %, Steps: 2859, Current Learning Rate: 0.0007066, \u001b[91mTrain Loss: 2.738\n",
      "\u001b[0m\u001b[1mEpoch: [31/70], Progress: 74.19 %, Steps: 2860, Current Learning Rate: 0.0007068, \u001b[96mTrain Loss: 2.664\n",
      "\u001b[0m\u001b[1mEpoch: [31/70], Progress: 75.27 %, Steps: 2861, Current Learning Rate: 0.0007071, \u001b[96mTrain Loss: 2.664\n",
      "\u001b[0m\u001b[1mEpoch: [31/70], Progress: 76.34 %, Steps: 2862, Current Learning Rate: 0.0007073, \u001b[96mTrain Loss: 2.645\n",
      "\u001b[0m\u001b[1mEpoch: [31/70], Progress: 77.42 %, Steps: 2863, Current Learning Rate: 0.0007076, \u001b[91mTrain Loss: 2.872\n",
      "\u001b[0m\u001b[1mEpoch: [31/70], Progress: 78.49 %, Steps: 2864, Current Learning Rate: 0.0007078, \u001b[96mTrain Loss: 2.694\n",
      "\u001b[0m\u001b[1mEpoch: [31/70], Progress: 79.57 %, Steps: 2865, Current Learning Rate: 0.0007081, \u001b[96mTrain Loss: 2.639\n",
      "\u001b[0m\u001b[1mEpoch: [31/70], Progress: 80.65 %, Steps: 2866, Current Learning Rate: 0.0007083, \u001b[96mTrain Loss: 2.561\n",
      "\u001b[0m\u001b[1mEpoch: [31/70], Progress: 81.72 %, Steps: 2867, Current Learning Rate: 0.0007085, \u001b[91mTrain Loss: 2.589\n",
      "\u001b[0m\u001b[1mEpoch: [31/70], Progress: 82.80 %, Steps: 2868, Current Learning Rate: 0.0007088, \u001b[91mTrain Loss: 2.729\n",
      "\u001b[0m\u001b[1mEpoch: [31/70], Progress: 83.87 %, Steps: 2869, Current Learning Rate: 0.0007090, \u001b[96mTrain Loss: 2.582\n",
      "\u001b[0m\u001b[1mEpoch: [31/70], Progress: 84.95 %, Steps: 2870, Current Learning Rate: 0.0007093, \u001b[91mTrain Loss: 2.912\n",
      "\u001b[0m\u001b[1mEpoch: [31/70], Progress: 86.02 %, Steps: 2871, Current Learning Rate: 0.0007095, \u001b[96mTrain Loss: 2.840\n",
      "\u001b[0m\u001b[1mEpoch: [31/70], Progress: 87.10 %, Steps: 2872, Current Learning Rate: 0.0007098, \u001b[96mTrain Loss: 2.706\n",
      "\u001b[0m\u001b[1mEpoch: [31/70], Progress: 88.17 %, Steps: 2873, Current Learning Rate: 0.0007100, \u001b[91mTrain Loss: 2.866\n",
      "\u001b[0m\u001b[1mEpoch: [31/70], Progress: 89.25 %, Steps: 2874, Current Learning Rate: 0.0007103, \u001b[96mTrain Loss: 2.720\n",
      "\u001b[0m\u001b[1mEpoch: [31/70], Progress: 90.32 %, Steps: 2875, Current Learning Rate: 0.0007105, \u001b[91mTrain Loss: 2.814\n",
      "\u001b[0m\u001b[1mEpoch: [31/70], Progress: 91.40 %, Steps: 2876, Current Learning Rate: 0.0007108, \u001b[96mTrain Loss: 2.725\n",
      "\u001b[0m\u001b[1mEpoch: [31/70], Progress: 92.47 %, Steps: 2877, Current Learning Rate: 0.0007110, \u001b[91mTrain Loss: 3.035\n",
      "\u001b[0m\u001b[1mEpoch: [31/70], Progress: 93.55 %, Steps: 2878, Current Learning Rate: 0.0007113, \u001b[96mTrain Loss: 2.763\n",
      "\u001b[0m\u001b[1mEpoch: [31/70], Progress: 94.62 %, Steps: 2879, Current Learning Rate: 0.0007115, \u001b[91mTrain Loss: 2.843\n",
      "\u001b[0m\u001b[1mEpoch: [31/70], Progress: 95.70 %, Steps: 2880, Current Learning Rate: 0.0007118, \u001b[96mTrain Loss: 2.824\n",
      "\u001b[0m\u001b[1mEpoch: [31/70], Progress: 96.77 %, Steps: 2881, Current Learning Rate: 0.0007120, \u001b[96mTrain Loss: 2.551\n",
      "\u001b[0m\u001b[1mEpoch: [31/70], Progress: 97.85 %, Steps: 2882, Current Learning Rate: 0.0007123, \u001b[91mTrain Loss: 2.853\n",
      "\u001b[0m\u001b[1mEpoch: [31/70], Progress: 98.92 %, Steps: 2883, Current Learning Rate: 0.0007125, \u001b[96mTrain Loss: 2.637\n",
      "\u001b[1m\u001b[93mComputing Validation Loss...\n",
      "\u001b[1mEpoch 31 Completed! Average Train Loss: 2.686, Average Validation Loss: 1.924\n",
      "Best Model saved in best_model/best_model.pt.\n",
      "\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m= Epoch [32/70] ==========================================\n",
      "\u001b[0m\u001b[1mEpoch: [32/70], Progress: 0.00 %, Steps: 2884, Current Learning Rate: 0.0007127, \u001b[91mTrain Loss: 2.569\n",
      "\u001b[0m\u001b[1mEpoch: [32/70], Progress: 1.08 %, Steps: 2885, Current Learning Rate: 0.0007130, \u001b[96mTrain Loss: 2.376\n",
      "\u001b[0m\u001b[1mEpoch: [32/70], Progress: 2.15 %, Steps: 2886, Current Learning Rate: 0.0007132, \u001b[91mTrain Loss: 2.540\n",
      "\u001b[0m\u001b[1mEpoch: [32/70], Progress: 3.23 %, Steps: 2887, Current Learning Rate: 0.0007135, \u001b[96mTrain Loss: 2.426\n",
      "\u001b[0m\u001b[1mEpoch: [32/70], Progress: 4.30 %, Steps: 2888, Current Learning Rate: 0.0007137, \u001b[91mTrain Loss: 2.516\n",
      "\u001b[0m\u001b[1mEpoch: [32/70], Progress: 5.38 %, Steps: 2889, Current Learning Rate: 0.0007140, \u001b[96mTrain Loss: 2.452\n",
      "\u001b[0m\u001b[1mEpoch: [32/70], Progress: 6.45 %, Steps: 2890, Current Learning Rate: 0.0007142, \u001b[96mTrain Loss: 2.302\n",
      "\u001b[0m\u001b[1mEpoch: [32/70], Progress: 7.53 %, Steps: 2891, Current Learning Rate: 0.0007145, \u001b[91mTrain Loss: 2.398\n",
      "\u001b[0m\u001b[1mEpoch: [32/70], Progress: 8.60 %, Steps: 2892, Current Learning Rate: 0.0007147, \u001b[91mTrain Loss: 2.665\n",
      "\u001b[0m\u001b[1mEpoch: [32/70], Progress: 9.68 %, Steps: 2893, Current Learning Rate: 0.0007150, \u001b[96mTrain Loss: 2.594\n",
      "\u001b[0m\u001b[1mEpoch: [32/70], Progress: 10.75 %, Steps: 2894, Current Learning Rate: 0.0007152, \u001b[96mTrain Loss: 2.566\n",
      "\u001b[0m\u001b[1mEpoch: [32/70], Progress: 11.83 %, Steps: 2895, Current Learning Rate: 0.0007155, \u001b[96mTrain Loss: 2.464\n",
      "\u001b[0m\u001b[1mEpoch: [32/70], Progress: 12.90 %, Steps: 2896, Current Learning Rate: 0.0007157, \u001b[96mTrain Loss: 2.402\n",
      "\u001b[0m\u001b[1mEpoch: [32/70], Progress: 13.98 %, Steps: 2897, Current Learning Rate: 0.0007160, \u001b[96mTrain Loss: 2.262\n",
      "\u001b[0m\u001b[1mEpoch: [32/70], Progress: 15.05 %, Steps: 2898, Current Learning Rate: 0.0007162, \u001b[91mTrain Loss: 2.367\n",
      "\u001b[0m\u001b[1mEpoch: [32/70], Progress: 16.13 %, Steps: 2899, Current Learning Rate: 0.0007165, \u001b[91mTrain Loss: 2.370\n",
      "\u001b[0m\u001b[1mEpoch: [32/70], Progress: 17.20 %, Steps: 2900, Current Learning Rate: 0.0007167, \u001b[91mTrain Loss: 2.422\n",
      "\u001b[0m\u001b[1mEpoch: [32/70], Progress: 18.28 %, Steps: 2901, Current Learning Rate: 0.0007169, \u001b[91mTrain Loss: 2.489\n",
      "\u001b[0m\u001b[1mEpoch: [32/70], Progress: 19.35 %, Steps: 2902, Current Learning Rate: 0.0007172, \u001b[91mTrain Loss: 2.493\n",
      "\u001b[0m\u001b[1mEpoch: [32/70], Progress: 20.43 %, Steps: 2903, Current Learning Rate: 0.0007174, \u001b[91mTrain Loss: 2.627\n",
      "\u001b[0m\u001b[1mEpoch: [32/70], Progress: 21.51 %, Steps: 2904, Current Learning Rate: 0.0007177, \u001b[96mTrain Loss: 2.418\n",
      "\u001b[0m\u001b[1mEpoch: [32/70], Progress: 22.58 %, Steps: 2905, Current Learning Rate: 0.0007179, \u001b[91mTrain Loss: 2.422\n",
      "\u001b[0m\u001b[1mEpoch: [32/70], Progress: 23.66 %, Steps: 2906, Current Learning Rate: 0.0007182, \u001b[96mTrain Loss: 2.406\n",
      "\u001b[0m\u001b[1mEpoch: [32/70], Progress: 24.73 %, Steps: 2907, Current Learning Rate: 0.0007184, \u001b[91mTrain Loss: 2.429\n",
      "\u001b[0m\u001b[1mEpoch: [32/70], Progress: 25.81 %, Steps: 2908, Current Learning Rate: 0.0007187, \u001b[91mTrain Loss: 2.462\n",
      "\u001b[0m\u001b[1mEpoch: [32/70], Progress: 26.88 %, Steps: 2909, Current Learning Rate: 0.0007189, \u001b[96mTrain Loss: 2.458\n",
      "\u001b[0m\u001b[1mEpoch: [32/70], Progress: 27.96 %, Steps: 2910, Current Learning Rate: 0.0007192, \u001b[96mTrain Loss: 2.299\n",
      "\u001b[0m\u001b[1mEpoch: [32/70], Progress: 29.03 %, Steps: 2911, Current Learning Rate: 0.0007194, \u001b[91mTrain Loss: 2.558\n",
      "\u001b[0m\u001b[1mEpoch: [32/70], Progress: 30.11 %, Steps: 2912, Current Learning Rate: 0.0007197, \u001b[91mTrain Loss: 2.607\n",
      "\u001b[0m\u001b[1mEpoch: [32/70], Progress: 31.18 %, Steps: 2913, Current Learning Rate: 0.0007199, \u001b[96mTrain Loss: 2.503\n",
      "\u001b[0m\u001b[1mEpoch: [32/70], Progress: 32.26 %, Steps: 2914, Current Learning Rate: 0.0007202, \u001b[96mTrain Loss: 2.462\n",
      "\u001b[0m\u001b[1mEpoch: [32/70], Progress: 33.33 %, Steps: 2915, Current Learning Rate: 0.0007204, \u001b[91mTrain Loss: 2.466\n",
      "\u001b[0m\u001b[1mEpoch: [32/70], Progress: 34.41 %, Steps: 2916, Current Learning Rate: 0.0007207, \u001b[91mTrain Loss: 2.691\n",
      "\u001b[0m\u001b[1mEpoch: [32/70], Progress: 35.48 %, Steps: 2917, Current Learning Rate: 0.0007209, \u001b[96mTrain Loss: 2.534\n",
      "\u001b[0m\u001b[1mEpoch: [32/70], Progress: 36.56 %, Steps: 2918, Current Learning Rate: 0.0007211, \u001b[96mTrain Loss: 2.373\n",
      "\u001b[0m\u001b[1mEpoch: [32/70], Progress: 37.63 %, Steps: 2919, Current Learning Rate: 0.0007214, \u001b[91mTrain Loss: 2.860\n",
      "\u001b[0m\u001b[1mEpoch: [32/70], Progress: 38.71 %, Steps: 2920, Current Learning Rate: 0.0007216, \u001b[96mTrain Loss: 2.708\n",
      "\u001b[0m\u001b[1mEpoch: [32/70], Progress: 39.78 %, Steps: 2921, Current Learning Rate: 0.0007219, \u001b[96mTrain Loss: 2.579\n",
      "\u001b[0m\u001b[1mEpoch: [32/70], Progress: 40.86 %, Steps: 2922, Current Learning Rate: 0.0007221, \u001b[91mTrain Loss: 2.667\n",
      "\u001b[0m\u001b[1mEpoch: [32/70], Progress: 41.94 %, Steps: 2923, Current Learning Rate: 0.0007224, \u001b[96mTrain Loss: 2.507\n",
      "\u001b[0m\u001b[1mEpoch: [32/70], Progress: 43.01 %, Steps: 2924, Current Learning Rate: 0.0007226, \u001b[96mTrain Loss: 2.504\n",
      "\u001b[0m\u001b[1mEpoch: [32/70], Progress: 44.09 %, Steps: 2925, Current Learning Rate: 0.0007229, \u001b[96mTrain Loss: 2.494\n",
      "\u001b[0m\u001b[1mEpoch: [32/70], Progress: 45.16 %, Steps: 2926, Current Learning Rate: 0.0007231, \u001b[91mTrain Loss: 2.659\n",
      "\u001b[0m\u001b[1mEpoch: [32/70], Progress: 46.24 %, Steps: 2927, Current Learning Rate: 0.0007234, \u001b[96mTrain Loss: 2.655\n",
      "\u001b[0m\u001b[1mEpoch: [32/70], Progress: 47.31 %, Steps: 2928, Current Learning Rate: 0.0007236, \u001b[96mTrain Loss: 2.483\n",
      "\u001b[0m\u001b[1mEpoch: [32/70], Progress: 48.39 %, Steps: 2929, Current Learning Rate: 0.0007239, \u001b[96mTrain Loss: 2.419\n",
      "\u001b[0m\u001b[1mEpoch: [32/70], Progress: 49.46 %, Steps: 2930, Current Learning Rate: 0.0007241, \u001b[91mTrain Loss: 2.533\n",
      "\u001b[0m\u001b[1mEpoch: [32/70], Progress: 50.54 %, Steps: 2931, Current Learning Rate: 0.0007244, \u001b[96mTrain Loss: 2.393\n",
      "\u001b[0m\u001b[1mEpoch: [32/70], Progress: 51.61 %, Steps: 2932, Current Learning Rate: 0.0007246, \u001b[96mTrain Loss: 2.326\n",
      "\u001b[0m\u001b[1mEpoch: [32/70], Progress: 52.69 %, Steps: 2933, Current Learning Rate: 0.0007249, \u001b[91mTrain Loss: 2.659\n",
      "\u001b[0m\u001b[1mEpoch: [32/70], Progress: 53.76 %, Steps: 2934, Current Learning Rate: 0.0007251, \u001b[96mTrain Loss: 2.421\n",
      "\u001b[0m\u001b[1mEpoch: [32/70], Progress: 54.84 %, Steps: 2935, Current Learning Rate: 0.0007253, \u001b[91mTrain Loss: 2.601\n",
      "\u001b[0m\u001b[1mEpoch: [32/70], Progress: 55.91 %, Steps: 2936, Current Learning Rate: 0.0007256, \u001b[96mTrain Loss: 2.481\n",
      "\u001b[0m\u001b[1mEpoch: [32/70], Progress: 56.99 %, Steps: 2937, Current Learning Rate: 0.0007258, \u001b[91mTrain Loss: 2.535\n",
      "\u001b[0m\u001b[1mEpoch: [32/70], Progress: 58.06 %, Steps: 2938, Current Learning Rate: 0.0007261, \u001b[96mTrain Loss: 2.423\n",
      "\u001b[0m\u001b[1mEpoch: [32/70], Progress: 59.14 %, Steps: 2939, Current Learning Rate: 0.0007263, \u001b[91mTrain Loss: 2.481\n",
      "\u001b[0m\u001b[1mEpoch: [32/70], Progress: 60.22 %, Steps: 2940, Current Learning Rate: 0.0007266, \u001b[91mTrain Loss: 2.553\n",
      "\u001b[0m\u001b[1mEpoch: [32/70], Progress: 61.29 %, Steps: 2941, Current Learning Rate: 0.0007268, \u001b[96mTrain Loss: 2.474\n",
      "\u001b[0m\u001b[1mEpoch: [32/70], Progress: 62.37 %, Steps: 2942, Current Learning Rate: 0.0007271, \u001b[91mTrain Loss: 2.841\n",
      "\u001b[0m\u001b[1mEpoch: [32/70], Progress: 63.44 %, Steps: 2943, Current Learning Rate: 0.0007273, \u001b[96mTrain Loss: 2.674\n",
      "\u001b[0m\u001b[1mEpoch: [32/70], Progress: 64.52 %, Steps: 2944, Current Learning Rate: 0.0007276, \u001b[96mTrain Loss: 2.475\n",
      "\u001b[0m\u001b[1mEpoch: [32/70], Progress: 65.59 %, Steps: 2945, Current Learning Rate: 0.0007278, \u001b[96mTrain Loss: 2.406\n",
      "\u001b[0m\u001b[1mEpoch: [32/70], Progress: 66.67 %, Steps: 2946, Current Learning Rate: 0.0007281, \u001b[91mTrain Loss: 2.634\n",
      "\u001b[0m\u001b[1mEpoch: [32/70], Progress: 67.74 %, Steps: 2947, Current Learning Rate: 0.0007283, \u001b[91mTrain Loss: 2.799\n",
      "\u001b[0m\u001b[1mEpoch: [32/70], Progress: 68.82 %, Steps: 2948, Current Learning Rate: 0.0007286, \u001b[91mTrain Loss: 2.820\n",
      "\u001b[0m\u001b[1mEpoch: [32/70], Progress: 69.89 %, Steps: 2949, Current Learning Rate: 0.0007288, \u001b[96mTrain Loss: 2.521\n",
      "\u001b[0m\u001b[1mEpoch: [32/70], Progress: 70.97 %, Steps: 2950, Current Learning Rate: 0.0007291, \u001b[91mTrain Loss: 2.729\n",
      "\u001b[0m\u001b[1mEpoch: [32/70], Progress: 72.04 %, Steps: 2951, Current Learning Rate: 0.0007293, \u001b[96mTrain Loss: 2.610\n",
      "\u001b[0m\u001b[1mEpoch: [32/70], Progress: 73.12 %, Steps: 2952, Current Learning Rate: 0.0007295, \u001b[96mTrain Loss: 2.440\n",
      "\u001b[0m\u001b[1mEpoch: [32/70], Progress: 74.19 %, Steps: 2953, Current Learning Rate: 0.0007298, \u001b[96mTrain Loss: 2.417\n",
      "\u001b[0m\u001b[1mEpoch: [32/70], Progress: 75.27 %, Steps: 2954, Current Learning Rate: 0.0007300, \u001b[91mTrain Loss: 2.463\n",
      "\u001b[0m\u001b[1mEpoch: [32/70], Progress: 76.34 %, Steps: 2955, Current Learning Rate: 0.0007303, \u001b[91mTrain Loss: 2.669\n",
      "\u001b[0m\u001b[1mEpoch: [32/70], Progress: 77.42 %, Steps: 2956, Current Learning Rate: 0.0007305, \u001b[96mTrain Loss: 2.530\n",
      "\u001b[0m\u001b[1mEpoch: [32/70], Progress: 78.49 %, Steps: 2957, Current Learning Rate: 0.0007308, \u001b[91mTrain Loss: 2.633\n",
      "\u001b[0m\u001b[1mEpoch: [32/70], Progress: 79.57 %, Steps: 2958, Current Learning Rate: 0.0007310, \u001b[96mTrain Loss: 2.552\n",
      "\u001b[0m\u001b[1mEpoch: [32/70], Progress: 80.65 %, Steps: 2959, Current Learning Rate: 0.0007313, \u001b[96mTrain Loss: 2.551\n",
      "\u001b[0m\u001b[1mEpoch: [32/70], Progress: 81.72 %, Steps: 2960, Current Learning Rate: 0.0007315, \u001b[96mTrain Loss: 2.486\n",
      "\u001b[0m\u001b[1mEpoch: [32/70], Progress: 82.80 %, Steps: 2961, Current Learning Rate: 0.0007318, \u001b[91mTrain Loss: 2.701\n",
      "\u001b[0m\u001b[1mEpoch: [32/70], Progress: 83.87 %, Steps: 2962, Current Learning Rate: 0.0007320, \u001b[96mTrain Loss: 2.656\n",
      "\u001b[0m\u001b[1mEpoch: [32/70], Progress: 84.95 %, Steps: 2963, Current Learning Rate: 0.0007323, \u001b[96mTrain Loss: 2.427\n",
      "\u001b[0m\u001b[1mEpoch: [32/70], Progress: 86.02 %, Steps: 2964, Current Learning Rate: 0.0007325, \u001b[91mTrain Loss: 2.499\n",
      "\u001b[0m\u001b[1mEpoch: [32/70], Progress: 87.10 %, Steps: 2965, Current Learning Rate: 0.0007328, \u001b[96mTrain Loss: 2.433\n",
      "\u001b[0m\u001b[1mEpoch: [32/70], Progress: 88.17 %, Steps: 2966, Current Learning Rate: 0.0007330, \u001b[91mTrain Loss: 2.602\n",
      "\u001b[0m\u001b[1mEpoch: [32/70], Progress: 89.25 %, Steps: 2967, Current Learning Rate: 0.0007333, \u001b[96mTrain Loss: 2.569\n",
      "\u001b[0m\u001b[1mEpoch: [32/70], Progress: 90.32 %, Steps: 2968, Current Learning Rate: 0.0007335, \u001b[91mTrain Loss: 2.622\n",
      "\u001b[0m\u001b[1mEpoch: [32/70], Progress: 91.40 %, Steps: 2969, Current Learning Rate: 0.0007337, \u001b[96mTrain Loss: 2.519\n",
      "\u001b[0m\u001b[1mEpoch: [32/70], Progress: 92.47 %, Steps: 2970, Current Learning Rate: 0.0007340, \u001b[96mTrain Loss: 2.485\n",
      "\u001b[0m\u001b[1mEpoch: [32/70], Progress: 93.55 %, Steps: 2971, Current Learning Rate: 0.0007342, \u001b[96mTrain Loss: 2.444\n",
      "\u001b[0m\u001b[1mEpoch: [32/70], Progress: 94.62 %, Steps: 2972, Current Learning Rate: 0.0007345, \u001b[91mTrain Loss: 2.614\n",
      "\u001b[0m\u001b[1mEpoch: [32/70], Progress: 95.70 %, Steps: 2973, Current Learning Rate: 0.0007347, \u001b[96mTrain Loss: 2.435\n",
      "\u001b[0m\u001b[1mEpoch: [32/70], Progress: 96.77 %, Steps: 2974, Current Learning Rate: 0.0007350, \u001b[91mTrain Loss: 2.588\n",
      "\u001b[0m\u001b[1mEpoch: [32/70], Progress: 97.85 %, Steps: 2975, Current Learning Rate: 0.0007352, \u001b[91mTrain Loss: 2.603\n",
      "\u001b[0m\u001b[1mEpoch: [32/70], Progress: 98.92 %, Steps: 2976, Current Learning Rate: 0.0007355, \u001b[91mTrain Loss: 2.675\n",
      "\u001b[1m\u001b[93mComputing Validation Loss...\n",
      "\u001b[1mEpoch 32 Completed! Average Train Loss: 2.526, Average Validation Loss: 1.723\n",
      "Best Model saved in best_model/best_model.pt.\n",
      "\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m= Epoch [33/70] ==========================================\n",
      "\u001b[0m\u001b[1mEpoch: [33/70], Progress: 0.00 %, Steps: 2977, Current Learning Rate: 0.0007357, \u001b[91mTrain Loss: 2.187\n",
      "\u001b[0m\u001b[1mEpoch: [33/70], Progress: 1.08 %, Steps: 2978, Current Learning Rate: 0.0007360, \u001b[96mTrain Loss: 2.142\n",
      "\u001b[0m\u001b[1mEpoch: [33/70], Progress: 2.15 %, Steps: 2979, Current Learning Rate: 0.0007362, \u001b[91mTrain Loss: 2.367\n",
      "\u001b[0m\u001b[1mEpoch: [33/70], Progress: 3.23 %, Steps: 2980, Current Learning Rate: 0.0007365, \u001b[96mTrain Loss: 2.171\n",
      "\u001b[0m\u001b[1mEpoch: [33/70], Progress: 4.30 %, Steps: 2981, Current Learning Rate: 0.0007367, \u001b[91mTrain Loss: 2.221\n",
      "\u001b[0m\u001b[1mEpoch: [33/70], Progress: 5.38 %, Steps: 2982, Current Learning Rate: 0.0007370, \u001b[91mTrain Loss: 2.237\n",
      "\u001b[0m\u001b[1mEpoch: [33/70], Progress: 6.45 %, Steps: 2983, Current Learning Rate: 0.0007372, \u001b[96mTrain Loss: 2.132\n",
      "\u001b[0m\u001b[1mEpoch: [33/70], Progress: 7.53 %, Steps: 2984, Current Learning Rate: 0.0007375, \u001b[91mTrain Loss: 2.254\n",
      "\u001b[0m\u001b[1mEpoch: [33/70], Progress: 8.60 %, Steps: 2985, Current Learning Rate: 0.0007377, \u001b[96mTrain Loss: 2.199\n",
      "\u001b[0m\u001b[1mEpoch: [33/70], Progress: 9.68 %, Steps: 2986, Current Learning Rate: 0.0007379, \u001b[96mTrain Loss: 2.164\n",
      "\u001b[0m\u001b[1mEpoch: [33/70], Progress: 10.75 %, Steps: 2987, Current Learning Rate: 0.0007382, \u001b[91mTrain Loss: 2.335\n",
      "\u001b[0m\u001b[1mEpoch: [33/70], Progress: 11.83 %, Steps: 2988, Current Learning Rate: 0.0007384, \u001b[96mTrain Loss: 2.150\n",
      "\u001b[0m\u001b[1mEpoch: [33/70], Progress: 12.90 %, Steps: 2989, Current Learning Rate: 0.0007387, \u001b[96mTrain Loss: 2.097\n",
      "\u001b[0m\u001b[1mEpoch: [33/70], Progress: 13.98 %, Steps: 2990, Current Learning Rate: 0.0007389, \u001b[91mTrain Loss: 2.184\n",
      "\u001b[0m\u001b[1mEpoch: [33/70], Progress: 15.05 %, Steps: 2991, Current Learning Rate: 0.0007392, \u001b[91mTrain Loss: 2.261\n",
      "\u001b[0m\u001b[1mEpoch: [33/70], Progress: 16.13 %, Steps: 2992, Current Learning Rate: 0.0007394, \u001b[91mTrain Loss: 2.341\n",
      "\u001b[0m\u001b[1mEpoch: [33/70], Progress: 17.20 %, Steps: 2993, Current Learning Rate: 0.0007397, \u001b[96mTrain Loss: 2.279\n",
      "\u001b[0m\u001b[1mEpoch: [33/70], Progress: 18.28 %, Steps: 2994, Current Learning Rate: 0.0007399, \u001b[91mTrain Loss: 2.355\n",
      "\u001b[0m\u001b[1mEpoch: [33/70], Progress: 19.35 %, Steps: 2995, Current Learning Rate: 0.0007402, \u001b[96mTrain Loss: 2.237\n",
      "\u001b[0m\u001b[1mEpoch: [33/70], Progress: 20.43 %, Steps: 2996, Current Learning Rate: 0.0007404, \u001b[91mTrain Loss: 2.259\n",
      "\u001b[0m\u001b[1mEpoch: [33/70], Progress: 21.51 %, Steps: 2997, Current Learning Rate: 0.0007407, \u001b[91mTrain Loss: 2.341\n",
      "\u001b[0m\u001b[1mEpoch: [33/70], Progress: 22.58 %, Steps: 2998, Current Learning Rate: 0.0007409, \u001b[96mTrain Loss: 2.323\n",
      "\u001b[0m\u001b[1mEpoch: [33/70], Progress: 23.66 %, Steps: 2999, Current Learning Rate: 0.0007412, \u001b[96mTrain Loss: 2.163\n",
      "\u001b[0m\u001b[1mEpoch: [33/70], Progress: 24.73 %, Steps: 3000, Current Learning Rate: 0.0007414, \u001b[91mTrain Loss: 2.376\n",
      "\u001b[0m\u001b[1mEpoch: [33/70], Progress: 25.81 %, Steps: 3001, Current Learning Rate: 0.0007417, \u001b[91mTrain Loss: 2.502\n",
      "\u001b[0m\u001b[1mEpoch: [33/70], Progress: 26.88 %, Steps: 3002, Current Learning Rate: 0.0007419, \u001b[96mTrain Loss: 2.322\n",
      "\u001b[0m\u001b[1mEpoch: [33/70], Progress: 27.96 %, Steps: 3003, Current Learning Rate: 0.0007421, \u001b[96mTrain Loss: 2.266\n",
      "\u001b[0m\u001b[1mEpoch: [33/70], Progress: 29.03 %, Steps: 3004, Current Learning Rate: 0.0007424, \u001b[91mTrain Loss: 2.366\n",
      "\u001b[0m\u001b[1mEpoch: [33/70], Progress: 30.11 %, Steps: 3005, Current Learning Rate: 0.0007426, \u001b[96mTrain Loss: 2.319\n",
      "\u001b[0m\u001b[1mEpoch: [33/70], Progress: 31.18 %, Steps: 3006, Current Learning Rate: 0.0007429, \u001b[91mTrain Loss: 2.324\n",
      "\u001b[0m\u001b[1mEpoch: [33/70], Progress: 32.26 %, Steps: 3007, Current Learning Rate: 0.0007431, \u001b[96mTrain Loss: 2.304\n",
      "\u001b[0m\u001b[1mEpoch: [33/70], Progress: 33.33 %, Steps: 3008, Current Learning Rate: 0.0007434, \u001b[91mTrain Loss: 2.489\n",
      "\u001b[0m\u001b[1mEpoch: [33/70], Progress: 34.41 %, Steps: 3009, Current Learning Rate: 0.0007436, \u001b[91mTrain Loss: 2.498\n",
      "\u001b[0m\u001b[1mEpoch: [33/70], Progress: 35.48 %, Steps: 3010, Current Learning Rate: 0.0007439, \u001b[96mTrain Loss: 2.464\n",
      "\u001b[0m\u001b[1mEpoch: [33/70], Progress: 36.56 %, Steps: 3011, Current Learning Rate: 0.0007441, \u001b[91mTrain Loss: 2.500\n",
      "\u001b[0m\u001b[1mEpoch: [33/70], Progress: 37.63 %, Steps: 3012, Current Learning Rate: 0.0007444, \u001b[96mTrain Loss: 2.045\n",
      "\u001b[0m\u001b[1mEpoch: [33/70], Progress: 38.71 %, Steps: 3013, Current Learning Rate: 0.0007446, \u001b[91mTrain Loss: 2.279\n",
      "\u001b[0m\u001b[1mEpoch: [33/70], Progress: 39.78 %, Steps: 3014, Current Learning Rate: 0.0007449, \u001b[91mTrain Loss: 2.303\n",
      "\u001b[0m\u001b[1mEpoch: [33/70], Progress: 40.86 %, Steps: 3015, Current Learning Rate: 0.0007451, \u001b[91mTrain Loss: 2.336\n",
      "\u001b[0m\u001b[1mEpoch: [33/70], Progress: 41.94 %, Steps: 3016, Current Learning Rate: 0.0007454, \u001b[91mTrain Loss: 2.453\n",
      "\u001b[0m\u001b[1mEpoch: [33/70], Progress: 43.01 %, Steps: 3017, Current Learning Rate: 0.0007456, \u001b[96mTrain Loss: 2.283\n",
      "\u001b[0m\u001b[1mEpoch: [33/70], Progress: 44.09 %, Steps: 3018, Current Learning Rate: 0.0007459, \u001b[91mTrain Loss: 2.343\n",
      "\u001b[0m\u001b[1mEpoch: [33/70], Progress: 45.16 %, Steps: 3019, Current Learning Rate: 0.0007461, \u001b[91mTrain Loss: 2.509\n",
      "\u001b[0m\u001b[1mEpoch: [33/70], Progress: 46.24 %, Steps: 3020, Current Learning Rate: 0.0007463, \u001b[96mTrain Loss: 2.220\n",
      "\u001b[0m\u001b[1mEpoch: [33/70], Progress: 47.31 %, Steps: 3021, Current Learning Rate: 0.0007466, \u001b[91mTrain Loss: 2.238\n",
      "\u001b[0m\u001b[1mEpoch: [33/70], Progress: 48.39 %, Steps: 3022, Current Learning Rate: 0.0007468, \u001b[91mTrain Loss: 2.367\n",
      "\u001b[0m\u001b[1mEpoch: [33/70], Progress: 49.46 %, Steps: 3023, Current Learning Rate: 0.0007471, \u001b[91mTrain Loss: 2.427\n",
      "\u001b[0m\u001b[1mEpoch: [33/70], Progress: 50.54 %, Steps: 3024, Current Learning Rate: 0.0007473, \u001b[96mTrain Loss: 2.389\n",
      "\u001b[0m\u001b[1mEpoch: [33/70], Progress: 51.61 %, Steps: 3025, Current Learning Rate: 0.0007476, \u001b[96mTrain Loss: 2.254\n",
      "\u001b[0m\u001b[1mEpoch: [33/70], Progress: 52.69 %, Steps: 3026, Current Learning Rate: 0.0007478, \u001b[91mTrain Loss: 2.415\n",
      "\u001b[0m\u001b[1mEpoch: [33/70], Progress: 53.76 %, Steps: 3027, Current Learning Rate: 0.0007481, \u001b[91mTrain Loss: 2.614\n",
      "\u001b[0m\u001b[1mEpoch: [33/70], Progress: 54.84 %, Steps: 3028, Current Learning Rate: 0.0007483, \u001b[96mTrain Loss: 2.294\n",
      "\u001b[0m\u001b[1mEpoch: [33/70], Progress: 55.91 %, Steps: 3029, Current Learning Rate: 0.0007486, \u001b[91mTrain Loss: 2.577\n",
      "\u001b[0m\u001b[1mEpoch: [33/70], Progress: 56.99 %, Steps: 3030, Current Learning Rate: 0.0007488, \u001b[96mTrain Loss: 2.368\n",
      "\u001b[0m\u001b[1mEpoch: [33/70], Progress: 58.06 %, Steps: 3031, Current Learning Rate: 0.0007491, \u001b[96mTrain Loss: 2.327\n",
      "\u001b[0m\u001b[1mEpoch: [33/70], Progress: 59.14 %, Steps: 3032, Current Learning Rate: 0.0007493, \u001b[91mTrain Loss: 2.517\n",
      "\u001b[0m\u001b[1mEpoch: [33/70], Progress: 60.22 %, Steps: 3033, Current Learning Rate: 0.0007496, \u001b[96mTrain Loss: 2.428\n",
      "\u001b[0m\u001b[1mEpoch: [33/70], Progress: 61.29 %, Steps: 3034, Current Learning Rate: 0.0007498, \u001b[96mTrain Loss: 2.195\n",
      "\u001b[0m\u001b[1mEpoch: [33/70], Progress: 62.37 %, Steps: 3035, Current Learning Rate: 0.0007501, \u001b[91mTrain Loss: 2.537\n",
      "\u001b[0m\u001b[1mEpoch: [33/70], Progress: 63.44 %, Steps: 3036, Current Learning Rate: 0.0007503, \u001b[96mTrain Loss: 2.076\n",
      "\u001b[0m\u001b[1mEpoch: [33/70], Progress: 64.52 %, Steps: 3037, Current Learning Rate: 0.0007505, \u001b[91mTrain Loss: 2.440\n",
      "\u001b[0m\u001b[1mEpoch: [33/70], Progress: 65.59 %, Steps: 3038, Current Learning Rate: 0.0007508, \u001b[96mTrain Loss: 2.433\n",
      "\u001b[0m\u001b[1mEpoch: [33/70], Progress: 66.67 %, Steps: 3039, Current Learning Rate: 0.0007510, \u001b[96mTrain Loss: 2.417\n",
      "\u001b[0m\u001b[1mEpoch: [33/70], Progress: 67.74 %, Steps: 3040, Current Learning Rate: 0.0007513, \u001b[91mTrain Loss: 2.417\n",
      "\u001b[0m\u001b[1mEpoch: [33/70], Progress: 68.82 %, Steps: 3041, Current Learning Rate: 0.0007515, \u001b[91mTrain Loss: 2.447\n",
      "\u001b[0m\u001b[1mEpoch: [33/70], Progress: 69.89 %, Steps: 3042, Current Learning Rate: 0.0007518, \u001b[96mTrain Loss: 2.200\n",
      "\u001b[0m\u001b[1mEpoch: [33/70], Progress: 70.97 %, Steps: 3043, Current Learning Rate: 0.0007520, \u001b[91mTrain Loss: 2.491\n",
      "\u001b[0m\u001b[1mEpoch: [33/70], Progress: 72.04 %, Steps: 3044, Current Learning Rate: 0.0007523, \u001b[96mTrain Loss: 2.299\n",
      "\u001b[0m\u001b[1mEpoch: [33/70], Progress: 73.12 %, Steps: 3045, Current Learning Rate: 0.0007525, \u001b[91mTrain Loss: 2.314\n",
      "\u001b[0m\u001b[1mEpoch: [33/70], Progress: 74.19 %, Steps: 3046, Current Learning Rate: 0.0007528, \u001b[91mTrain Loss: 2.509\n",
      "\u001b[0m\u001b[1mEpoch: [33/70], Progress: 75.27 %, Steps: 3047, Current Learning Rate: 0.0007530, \u001b[96mTrain Loss: 2.485\n",
      "\u001b[0m\u001b[1mEpoch: [33/70], Progress: 76.34 %, Steps: 3048, Current Learning Rate: 0.0007533, \u001b[96mTrain Loss: 2.304\n",
      "\u001b[0m\u001b[1mEpoch: [33/70], Progress: 77.42 %, Steps: 3049, Current Learning Rate: 0.0007535, \u001b[91mTrain Loss: 2.404\n",
      "\u001b[0m\u001b[1mEpoch: [33/70], Progress: 78.49 %, Steps: 3050, Current Learning Rate: 0.0007538, \u001b[91mTrain Loss: 2.420\n",
      "\u001b[0m\u001b[1mEpoch: [33/70], Progress: 79.57 %, Steps: 3051, Current Learning Rate: 0.0007540, \u001b[96mTrain Loss: 2.406\n",
      "\u001b[0m\u001b[1mEpoch: [33/70], Progress: 80.65 %, Steps: 3052, Current Learning Rate: 0.0007543, \u001b[96mTrain Loss: 2.334\n",
      "\u001b[0m\u001b[1mEpoch: [33/70], Progress: 81.72 %, Steps: 3053, Current Learning Rate: 0.0007545, \u001b[91mTrain Loss: 2.491\n",
      "\u001b[0m\u001b[1mEpoch: [33/70], Progress: 82.80 %, Steps: 3054, Current Learning Rate: 0.0007547, \u001b[91mTrain Loss: 2.545\n",
      "\u001b[0m\u001b[1mEpoch: [33/70], Progress: 83.87 %, Steps: 3055, Current Learning Rate: 0.0007550, \u001b[96mTrain Loss: 2.451\n",
      "\u001b[0m\u001b[1mEpoch: [33/70], Progress: 84.95 %, Steps: 3056, Current Learning Rate: 0.0007552, \u001b[91mTrain Loss: 2.468\n",
      "\u001b[0m\u001b[1mEpoch: [33/70], Progress: 86.02 %, Steps: 3057, Current Learning Rate: 0.0007555, \u001b[91mTrain Loss: 2.482\n",
      "\u001b[0m\u001b[1mEpoch: [33/70], Progress: 87.10 %, Steps: 3058, Current Learning Rate: 0.0007557, \u001b[96mTrain Loss: 2.453\n",
      "\u001b[0m\u001b[1mEpoch: [33/70], Progress: 88.17 %, Steps: 3059, Current Learning Rate: 0.0007560, \u001b[91mTrain Loss: 2.525\n",
      "\u001b[0m\u001b[1mEpoch: [33/70], Progress: 89.25 %, Steps: 3060, Current Learning Rate: 0.0007562, \u001b[91mTrain Loss: 2.654\n",
      "\u001b[0m\u001b[1mEpoch: [33/70], Progress: 90.32 %, Steps: 3061, Current Learning Rate: 0.0007565, \u001b[96mTrain Loss: 2.520\n",
      "\u001b[0m\u001b[1mEpoch: [33/70], Progress: 91.40 %, Steps: 3062, Current Learning Rate: 0.0007567, \u001b[96mTrain Loss: 2.335\n",
      "\u001b[0m\u001b[1mEpoch: [33/70], Progress: 92.47 %, Steps: 3063, Current Learning Rate: 0.0007570, \u001b[91mTrain Loss: 2.422\n",
      "\u001b[0m\u001b[1mEpoch: [33/70], Progress: 93.55 %, Steps: 3064, Current Learning Rate: 0.0007572, \u001b[96mTrain Loss: 2.344\n",
      "\u001b[0m\u001b[1mEpoch: [33/70], Progress: 94.62 %, Steps: 3065, Current Learning Rate: 0.0007575, \u001b[91mTrain Loss: 2.562\n",
      "\u001b[0m\u001b[1mEpoch: [33/70], Progress: 95.70 %, Steps: 3066, Current Learning Rate: 0.0007577, \u001b[96mTrain Loss: 2.268\n",
      "\u001b[0m\u001b[1mEpoch: [33/70], Progress: 96.77 %, Steps: 3067, Current Learning Rate: 0.0007580, \u001b[91mTrain Loss: 2.327\n",
      "\u001b[0m\u001b[1mEpoch: [33/70], Progress: 97.85 %, Steps: 3068, Current Learning Rate: 0.0007582, \u001b[91mTrain Loss: 2.480\n",
      "\u001b[0m\u001b[1mEpoch: [33/70], Progress: 98.92 %, Steps: 3069, Current Learning Rate: 0.0007585, \u001b[91mTrain Loss: 2.494\n",
      "\u001b[1m\u001b[93mComputing Validation Loss...\n",
      "\u001b[1mEpoch 33 Completed! Average Train Loss: 2.356, Average Validation Loss: 1.578\n",
      "Best Model saved in best_model/best_model.pt.\n",
      "\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m= Epoch [34/70] ==========================================\n",
      "\u001b[0m\u001b[1mEpoch: [34/70], Progress: 0.00 %, Steps: 3070, Current Learning Rate: 0.0007587, \u001b[91mTrain Loss: 2.121\n",
      "\u001b[0m\u001b[1mEpoch: [34/70], Progress: 1.08 %, Steps: 3071, Current Learning Rate: 0.0007589, \u001b[91mTrain Loss: 2.269\n",
      "\u001b[0m\u001b[1mEpoch: [34/70], Progress: 2.15 %, Steps: 3072, Current Learning Rate: 0.0007592, \u001b[96mTrain Loss: 2.014\n",
      "\u001b[0m\u001b[1mEpoch: [34/70], Progress: 3.23 %, Steps: 3073, Current Learning Rate: 0.0007594, \u001b[91mTrain Loss: 2.176\n",
      "\u001b[0m\u001b[1mEpoch: [34/70], Progress: 4.30 %, Steps: 3074, Current Learning Rate: 0.0007597, \u001b[91mTrain Loss: 2.247\n",
      "\u001b[0m\u001b[1mEpoch: [34/70], Progress: 5.38 %, Steps: 3075, Current Learning Rate: 0.0007599, \u001b[96mTrain Loss: 1.912\n",
      "\u001b[0m\u001b[1mEpoch: [34/70], Progress: 6.45 %, Steps: 3076, Current Learning Rate: 0.0007602, \u001b[91mTrain Loss: 2.026\n",
      "\u001b[0m\u001b[1mEpoch: [34/70], Progress: 7.53 %, Steps: 3077, Current Learning Rate: 0.0007604, \u001b[91mTrain Loss: 2.150\n",
      "\u001b[0m\u001b[1mEpoch: [34/70], Progress: 8.60 %, Steps: 3078, Current Learning Rate: 0.0007607, \u001b[96mTrain Loss: 2.004\n",
      "\u001b[0m\u001b[1mEpoch: [34/70], Progress: 9.68 %, Steps: 3079, Current Learning Rate: 0.0007609, \u001b[91mTrain Loss: 2.140\n",
      "\u001b[0m\u001b[1mEpoch: [34/70], Progress: 10.75 %, Steps: 3080, Current Learning Rate: 0.0007612, \u001b[91mTrain Loss: 2.182\n",
      "\u001b[0m\u001b[1mEpoch: [34/70], Progress: 11.83 %, Steps: 3081, Current Learning Rate: 0.0007614, \u001b[96mTrain Loss: 2.026\n",
      "\u001b[0m\u001b[1mEpoch: [34/70], Progress: 12.90 %, Steps: 3082, Current Learning Rate: 0.0007617, \u001b[91mTrain Loss: 2.148\n",
      "\u001b[0m\u001b[1mEpoch: [34/70], Progress: 13.98 %, Steps: 3083, Current Learning Rate: 0.0007619, \u001b[96mTrain Loss: 2.123\n",
      "\u001b[0m\u001b[1mEpoch: [34/70], Progress: 15.05 %, Steps: 3084, Current Learning Rate: 0.0007622, \u001b[96mTrain Loss: 2.085\n",
      "\u001b[0m\u001b[1mEpoch: [34/70], Progress: 16.13 %, Steps: 3085, Current Learning Rate: 0.0007624, \u001b[96mTrain Loss: 1.979\n",
      "\u001b[0m\u001b[1mEpoch: [34/70], Progress: 17.20 %, Steps: 3086, Current Learning Rate: 0.0007627, \u001b[91mTrain Loss: 2.089\n",
      "\u001b[0m\u001b[1mEpoch: [34/70], Progress: 18.28 %, Steps: 3087, Current Learning Rate: 0.0007629, \u001b[96mTrain Loss: 2.024\n",
      "\u001b[0m\u001b[1mEpoch: [34/70], Progress: 19.35 %, Steps: 3088, Current Learning Rate: 0.0007631, \u001b[91mTrain Loss: 2.150\n",
      "\u001b[0m\u001b[1mEpoch: [34/70], Progress: 20.43 %, Steps: 3089, Current Learning Rate: 0.0007634, \u001b[96mTrain Loss: 1.894\n",
      "\u001b[0m\u001b[1mEpoch: [34/70], Progress: 21.51 %, Steps: 3090, Current Learning Rate: 0.0007636, \u001b[91mTrain Loss: 1.985\n",
      "\u001b[0m\u001b[1mEpoch: [34/70], Progress: 22.58 %, Steps: 3091, Current Learning Rate: 0.0007639, \u001b[91mTrain Loss: 2.174\n",
      "\u001b[0m\u001b[1mEpoch: [34/70], Progress: 23.66 %, Steps: 3092, Current Learning Rate: 0.0007641, \u001b[96mTrain Loss: 1.987\n",
      "\u001b[0m\u001b[1mEpoch: [34/70], Progress: 24.73 %, Steps: 3093, Current Learning Rate: 0.0007644, \u001b[91mTrain Loss: 2.095\n",
      "\u001b[0m\u001b[1mEpoch: [34/70], Progress: 25.81 %, Steps: 3094, Current Learning Rate: 0.0007646, \u001b[91mTrain Loss: 2.147\n",
      "\u001b[0m\u001b[1mEpoch: [34/70], Progress: 26.88 %, Steps: 3095, Current Learning Rate: 0.0007649, \u001b[91mTrain Loss: 2.184\n",
      "\u001b[0m\u001b[1mEpoch: [34/70], Progress: 27.96 %, Steps: 3096, Current Learning Rate: 0.0007651, \u001b[91mTrain Loss: 2.278\n",
      "\u001b[0m\u001b[1mEpoch: [34/70], Progress: 29.03 %, Steps: 3097, Current Learning Rate: 0.0007654, \u001b[96mTrain Loss: 2.193\n",
      "\u001b[0m\u001b[1mEpoch: [34/70], Progress: 30.11 %, Steps: 3098, Current Learning Rate: 0.0007656, \u001b[96mTrain Loss: 2.114\n",
      "\u001b[0m\u001b[1mEpoch: [34/70], Progress: 31.18 %, Steps: 3099, Current Learning Rate: 0.0007659, \u001b[91mTrain Loss: 2.231\n",
      "\u001b[0m\u001b[1mEpoch: [34/70], Progress: 32.26 %, Steps: 3100, Current Learning Rate: 0.0007661, \u001b[96mTrain Loss: 2.225\n",
      "\u001b[0m\u001b[1mEpoch: [34/70], Progress: 33.33 %, Steps: 3101, Current Learning Rate: 0.0007664, \u001b[96mTrain Loss: 2.097\n",
      "\u001b[0m\u001b[1mEpoch: [34/70], Progress: 34.41 %, Steps: 3102, Current Learning Rate: 0.0007666, \u001b[91mTrain Loss: 2.140\n",
      "\u001b[0m\u001b[1mEpoch: [34/70], Progress: 35.48 %, Steps: 3103, Current Learning Rate: 0.0007669, \u001b[91mTrain Loss: 2.163\n",
      "\u001b[0m\u001b[1mEpoch: [34/70], Progress: 36.56 %, Steps: 3104, Current Learning Rate: 0.0007671, \u001b[96mTrain Loss: 2.071\n",
      "\u001b[0m\u001b[1mEpoch: [34/70], Progress: 37.63 %, Steps: 3105, Current Learning Rate: 0.0007673, \u001b[91mTrain Loss: 2.288\n",
      "\u001b[0m\u001b[1mEpoch: [34/70], Progress: 38.71 %, Steps: 3106, Current Learning Rate: 0.0007676, \u001b[96mTrain Loss: 2.138\n",
      "\u001b[0m\u001b[1mEpoch: [34/70], Progress: 39.78 %, Steps: 3107, Current Learning Rate: 0.0007678, \u001b[91mTrain Loss: 2.264\n",
      "\u001b[0m\u001b[1mEpoch: [34/70], Progress: 40.86 %, Steps: 3108, Current Learning Rate: 0.0007681, \u001b[96mTrain Loss: 2.204\n",
      "\u001b[0m\u001b[1mEpoch: [34/70], Progress: 41.94 %, Steps: 3109, Current Learning Rate: 0.0007683, \u001b[96mTrain Loss: 1.981\n",
      "\u001b[0m\u001b[1mEpoch: [34/70], Progress: 43.01 %, Steps: 3110, Current Learning Rate: 0.0007686, \u001b[91mTrain Loss: 2.184\n",
      "\u001b[0m\u001b[1mEpoch: [34/70], Progress: 44.09 %, Steps: 3111, Current Learning Rate: 0.0007688, \u001b[91mTrain Loss: 2.198\n",
      "\u001b[0m\u001b[1mEpoch: [34/70], Progress: 45.16 %, Steps: 3112, Current Learning Rate: 0.0007691, \u001b[91mTrain Loss: 2.383\n",
      "\u001b[0m\u001b[1mEpoch: [34/70], Progress: 46.24 %, Steps: 3113, Current Learning Rate: 0.0007693, \u001b[96mTrain Loss: 2.258\n",
      "\u001b[0m\u001b[1mEpoch: [34/70], Progress: 47.31 %, Steps: 3114, Current Learning Rate: 0.0007696, \u001b[91mTrain Loss: 2.343\n",
      "\u001b[0m\u001b[1mEpoch: [34/70], Progress: 48.39 %, Steps: 3115, Current Learning Rate: 0.0007698, \u001b[96mTrain Loss: 2.227\n",
      "\u001b[0m\u001b[1mEpoch: [34/70], Progress: 49.46 %, Steps: 3116, Current Learning Rate: 0.0007701, \u001b[91mTrain Loss: 2.236\n",
      "\u001b[0m\u001b[1mEpoch: [34/70], Progress: 50.54 %, Steps: 3117, Current Learning Rate: 0.0007703, \u001b[91mTrain Loss: 2.238\n",
      "\u001b[0m\u001b[1mEpoch: [34/70], Progress: 51.61 %, Steps: 3118, Current Learning Rate: 0.0007706, \u001b[91mTrain Loss: 2.258\n",
      "\u001b[0m\u001b[1mEpoch: [34/70], Progress: 52.69 %, Steps: 3119, Current Learning Rate: 0.0007708, \u001b[96mTrain Loss: 2.248\n",
      "\u001b[0m\u001b[1mEpoch: [34/70], Progress: 53.76 %, Steps: 3120, Current Learning Rate: 0.0007711, \u001b[96mTrain Loss: 2.168\n",
      "\u001b[0m\u001b[1mEpoch: [34/70], Progress: 54.84 %, Steps: 3121, Current Learning Rate: 0.0007713, \u001b[91mTrain Loss: 2.258\n",
      "\u001b[0m\u001b[1mEpoch: [34/70], Progress: 55.91 %, Steps: 3122, Current Learning Rate: 0.0007715, \u001b[91mTrain Loss: 2.364\n",
      "\u001b[0m\u001b[1mEpoch: [34/70], Progress: 56.99 %, Steps: 3123, Current Learning Rate: 0.0007718, \u001b[96mTrain Loss: 2.103\n",
      "\u001b[0m\u001b[1mEpoch: [34/70], Progress: 58.06 %, Steps: 3124, Current Learning Rate: 0.0007720, \u001b[91mTrain Loss: 2.380\n",
      "\u001b[0m\u001b[1mEpoch: [34/70], Progress: 59.14 %, Steps: 3125, Current Learning Rate: 0.0007723, \u001b[96mTrain Loss: 2.229\n",
      "\u001b[0m\u001b[1mEpoch: [34/70], Progress: 60.22 %, Steps: 3126, Current Learning Rate: 0.0007725, \u001b[96mTrain Loss: 2.146\n",
      "\u001b[0m\u001b[1mEpoch: [34/70], Progress: 61.29 %, Steps: 3127, Current Learning Rate: 0.0007728, \u001b[96mTrain Loss: 2.138\n",
      "\u001b[0m\u001b[1mEpoch: [34/70], Progress: 62.37 %, Steps: 3128, Current Learning Rate: 0.0007730, \u001b[91mTrain Loss: 2.362\n",
      "\u001b[0m\u001b[1mEpoch: [34/70], Progress: 63.44 %, Steps: 3129, Current Learning Rate: 0.0007733, \u001b[96mTrain Loss: 2.216\n",
      "\u001b[0m\u001b[1mEpoch: [34/70], Progress: 64.52 %, Steps: 3130, Current Learning Rate: 0.0007735, \u001b[91mTrain Loss: 2.242\n",
      "\u001b[0m\u001b[1mEpoch: [34/70], Progress: 65.59 %, Steps: 3131, Current Learning Rate: 0.0007738, \u001b[91mTrain Loss: 2.351\n",
      "\u001b[0m\u001b[1mEpoch: [34/70], Progress: 66.67 %, Steps: 3132, Current Learning Rate: 0.0007740, \u001b[91mTrain Loss: 2.560\n",
      "\u001b[0m\u001b[1mEpoch: [34/70], Progress: 67.74 %, Steps: 3133, Current Learning Rate: 0.0007743, \u001b[96mTrain Loss: 2.272\n",
      "\u001b[0m\u001b[1mEpoch: [34/70], Progress: 68.82 %, Steps: 3134, Current Learning Rate: 0.0007745, \u001b[96mTrain Loss: 2.108\n",
      "\u001b[0m\u001b[1mEpoch: [34/70], Progress: 69.89 %, Steps: 3135, Current Learning Rate: 0.0007748, \u001b[91mTrain Loss: 2.368\n",
      "\u001b[0m\u001b[1mEpoch: [34/70], Progress: 70.97 %, Steps: 3136, Current Learning Rate: 0.0007750, \u001b[96mTrain Loss: 2.251\n",
      "\u001b[0m\u001b[1mEpoch: [34/70], Progress: 72.04 %, Steps: 3137, Current Learning Rate: 0.0007753, \u001b[91mTrain Loss: 2.304\n",
      "\u001b[0m\u001b[1mEpoch: [34/70], Progress: 73.12 %, Steps: 3138, Current Learning Rate: 0.0007755, \u001b[96mTrain Loss: 2.229\n",
      "\u001b[0m\u001b[1mEpoch: [34/70], Progress: 74.19 %, Steps: 3139, Current Learning Rate: 0.0007757, \u001b[91mTrain Loss: 2.347\n",
      "\u001b[0m\u001b[1mEpoch: [34/70], Progress: 75.27 %, Steps: 3140, Current Learning Rate: 0.0007760, \u001b[91mTrain Loss: 2.420\n",
      "\u001b[0m\u001b[1mEpoch: [34/70], Progress: 76.34 %, Steps: 3141, Current Learning Rate: 0.0007762, \u001b[96mTrain Loss: 2.079\n",
      "\u001b[0m\u001b[1mEpoch: [34/70], Progress: 77.42 %, Steps: 3142, Current Learning Rate: 0.0007765, \u001b[91mTrain Loss: 2.281\n",
      "\u001b[0m\u001b[1mEpoch: [34/70], Progress: 78.49 %, Steps: 3143, Current Learning Rate: 0.0007767, \u001b[96mTrain Loss: 2.243\n",
      "\u001b[0m\u001b[1mEpoch: [34/70], Progress: 79.57 %, Steps: 3144, Current Learning Rate: 0.0007770, \u001b[91mTrain Loss: 2.263\n",
      "\u001b[0m\u001b[1mEpoch: [34/70], Progress: 80.65 %, Steps: 3145, Current Learning Rate: 0.0007772, \u001b[96mTrain Loss: 2.172\n",
      "\u001b[0m\u001b[1mEpoch: [34/70], Progress: 81.72 %, Steps: 3146, Current Learning Rate: 0.0007775, \u001b[96mTrain Loss: 2.038\n",
      "\u001b[0m\u001b[1mEpoch: [34/70], Progress: 82.80 %, Steps: 3147, Current Learning Rate: 0.0007777, \u001b[91mTrain Loss: 2.348\n",
      "\u001b[0m\u001b[1mEpoch: [34/70], Progress: 83.87 %, Steps: 3148, Current Learning Rate: 0.0007780, \u001b[91mTrain Loss: 2.384\n",
      "\u001b[0m\u001b[1mEpoch: [34/70], Progress: 84.95 %, Steps: 3149, Current Learning Rate: 0.0007782, \u001b[96mTrain Loss: 2.144\n",
      "\u001b[0m\u001b[1mEpoch: [34/70], Progress: 86.02 %, Steps: 3150, Current Learning Rate: 0.0007785, \u001b[96mTrain Loss: 2.011\n",
      "\u001b[0m\u001b[1mEpoch: [34/70], Progress: 87.10 %, Steps: 3151, Current Learning Rate: 0.0007787, \u001b[91mTrain Loss: 2.075\n",
      "\u001b[0m\u001b[1mEpoch: [34/70], Progress: 88.17 %, Steps: 3152, Current Learning Rate: 0.0007790, \u001b[91mTrain Loss: 2.316\n",
      "\u001b[0m\u001b[1mEpoch: [34/70], Progress: 89.25 %, Steps: 3153, Current Learning Rate: 0.0007792, \u001b[91mTrain Loss: 2.387\n",
      "\u001b[0m\u001b[1mEpoch: [34/70], Progress: 90.32 %, Steps: 3154, Current Learning Rate: 0.0007795, \u001b[91mTrain Loss: 2.387\n",
      "\u001b[0m\u001b[1mEpoch: [34/70], Progress: 91.40 %, Steps: 3155, Current Learning Rate: 0.0007797, \u001b[96mTrain Loss: 2.378\n",
      "\u001b[0m\u001b[1mEpoch: [34/70], Progress: 92.47 %, Steps: 3156, Current Learning Rate: 0.0007799, \u001b[96mTrain Loss: 2.225\n",
      "\u001b[0m\u001b[1mEpoch: [34/70], Progress: 93.55 %, Steps: 3157, Current Learning Rate: 0.0007802, \u001b[96mTrain Loss: 2.125\n",
      "\u001b[0m\u001b[1mEpoch: [34/70], Progress: 94.62 %, Steps: 3158, Current Learning Rate: 0.0007804, \u001b[91mTrain Loss: 2.205\n",
      "\u001b[0m\u001b[1mEpoch: [34/70], Progress: 95.70 %, Steps: 3159, Current Learning Rate: 0.0007807, \u001b[91mTrain Loss: 2.275\n",
      "\u001b[0m\u001b[1mEpoch: [34/70], Progress: 96.77 %, Steps: 3160, Current Learning Rate: 0.0007809, \u001b[96mTrain Loss: 2.109\n",
      "\u001b[0m\u001b[1mEpoch: [34/70], Progress: 97.85 %, Steps: 3161, Current Learning Rate: 0.0007812, \u001b[91mTrain Loss: 2.267\n",
      "\u001b[0m\u001b[1mEpoch: [34/70], Progress: 98.92 %, Steps: 3162, Current Learning Rate: 0.0007814, \u001b[96mTrain Loss: 1.943\n",
      "\u001b[1m\u001b[93mComputing Validation Loss...\n",
      "\u001b[1mEpoch 34 Completed! Average Train Loss: 2.191, Average Validation Loss: 1.386\n",
      "Best Model saved in best_model/best_model.pt.\n",
      "\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m= Epoch [35/70] ==========================================\n",
      "\u001b[0m\u001b[1mEpoch: [35/70], Progress: 0.00 %, Steps: 3163, Current Learning Rate: 0.0007817, \u001b[91mTrain Loss: 1.986\n",
      "\u001b[0m\u001b[1mEpoch: [35/70], Progress: 1.08 %, Steps: 3164, Current Learning Rate: 0.0007819, \u001b[96mTrain Loss: 1.984\n",
      "\u001b[0m\u001b[1mEpoch: [35/70], Progress: 2.15 %, Steps: 3165, Current Learning Rate: 0.0007822, \u001b[96mTrain Loss: 1.951\n",
      "\u001b[0m\u001b[1mEpoch: [35/70], Progress: 3.23 %, Steps: 3166, Current Learning Rate: 0.0007824, \u001b[96mTrain Loss: 1.924\n",
      "\u001b[0m\u001b[1mEpoch: [35/70], Progress: 4.30 %, Steps: 3167, Current Learning Rate: 0.0007827, \u001b[91mTrain Loss: 2.074\n",
      "\u001b[0m\u001b[1mEpoch: [35/70], Progress: 5.38 %, Steps: 3168, Current Learning Rate: 0.0007829, \u001b[96mTrain Loss: 1.910\n",
      "\u001b[0m\u001b[1mEpoch: [35/70], Progress: 6.45 %, Steps: 3169, Current Learning Rate: 0.0007832, \u001b[96mTrain Loss: 1.784\n",
      "\u001b[0m\u001b[1mEpoch: [35/70], Progress: 7.53 %, Steps: 3170, Current Learning Rate: 0.0007834, \u001b[91mTrain Loss: 1.874\n",
      "\u001b[0m\u001b[1mEpoch: [35/70], Progress: 8.60 %, Steps: 3171, Current Learning Rate: 0.0007837, \u001b[96mTrain Loss: 1.814\n",
      "\u001b[0m\u001b[1mEpoch: [35/70], Progress: 9.68 %, Steps: 3172, Current Learning Rate: 0.0007839, \u001b[91mTrain Loss: 2.051\n",
      "\u001b[0m\u001b[1mEpoch: [35/70], Progress: 10.75 %, Steps: 3173, Current Learning Rate: 0.0007841, \u001b[96mTrain Loss: 1.824\n",
      "\u001b[0m\u001b[1mEpoch: [35/70], Progress: 11.83 %, Steps: 3174, Current Learning Rate: 0.0007844, \u001b[91mTrain Loss: 1.933\n",
      "\u001b[0m\u001b[1mEpoch: [35/70], Progress: 12.90 %, Steps: 3175, Current Learning Rate: 0.0007846, \u001b[91mTrain Loss: 2.016\n",
      "\u001b[0m\u001b[1mEpoch: [35/70], Progress: 13.98 %, Steps: 3176, Current Learning Rate: 0.0007849, \u001b[96mTrain Loss: 1.970\n",
      "\u001b[0m\u001b[1mEpoch: [35/70], Progress: 15.05 %, Steps: 3177, Current Learning Rate: 0.0007851, \u001b[96mTrain Loss: 1.840\n",
      "\u001b[0m\u001b[1mEpoch: [35/70], Progress: 16.13 %, Steps: 3178, Current Learning Rate: 0.0007854, \u001b[91mTrain Loss: 1.868\n",
      "\u001b[0m\u001b[1mEpoch: [35/70], Progress: 17.20 %, Steps: 3179, Current Learning Rate: 0.0007856, \u001b[91mTrain Loss: 1.966\n",
      "\u001b[0m\u001b[1mEpoch: [35/70], Progress: 18.28 %, Steps: 3180, Current Learning Rate: 0.0007859, \u001b[96mTrain Loss: 1.910\n",
      "\u001b[0m\u001b[1mEpoch: [35/70], Progress: 19.35 %, Steps: 3181, Current Learning Rate: 0.0007861, \u001b[91mTrain Loss: 2.011\n",
      "\u001b[0m\u001b[1mEpoch: [35/70], Progress: 20.43 %, Steps: 3182, Current Learning Rate: 0.0007864, \u001b[96mTrain Loss: 1.903\n",
      "\u001b[0m\u001b[1mEpoch: [35/70], Progress: 21.51 %, Steps: 3183, Current Learning Rate: 0.0007866, \u001b[96mTrain Loss: 1.877\n",
      "\u001b[0m\u001b[1mEpoch: [35/70], Progress: 22.58 %, Steps: 3184, Current Learning Rate: 0.0007869, \u001b[91mTrain Loss: 2.028\n",
      "\u001b[0m\u001b[1mEpoch: [35/70], Progress: 23.66 %, Steps: 3185, Current Learning Rate: 0.0007871, \u001b[96mTrain Loss: 1.760\n",
      "\u001b[0m\u001b[1mEpoch: [35/70], Progress: 24.73 %, Steps: 3186, Current Learning Rate: 0.0007874, \u001b[91mTrain Loss: 2.039\n",
      "\u001b[0m\u001b[1mEpoch: [35/70], Progress: 25.81 %, Steps: 3187, Current Learning Rate: 0.0007876, \u001b[96mTrain Loss: 1.911\n",
      "\u001b[0m\u001b[1mEpoch: [35/70], Progress: 26.88 %, Steps: 3188, Current Learning Rate: 0.0007879, \u001b[91mTrain Loss: 1.963\n",
      "\u001b[0m\u001b[1mEpoch: [35/70], Progress: 27.96 %, Steps: 3189, Current Learning Rate: 0.0007881, \u001b[91mTrain Loss: 2.315\n",
      "\u001b[0m\u001b[1mEpoch: [35/70], Progress: 29.03 %, Steps: 3190, Current Learning Rate: 0.0007883, \u001b[96mTrain Loss: 1.916\n",
      "\u001b[0m\u001b[1mEpoch: [35/70], Progress: 30.11 %, Steps: 3191, Current Learning Rate: 0.0007886, \u001b[96mTrain Loss: 1.739\n",
      "\u001b[0m\u001b[1mEpoch: [35/70], Progress: 31.18 %, Steps: 3192, Current Learning Rate: 0.0007888, \u001b[91mTrain Loss: 1.863\n",
      "\u001b[0m\u001b[1mEpoch: [35/70], Progress: 32.26 %, Steps: 3193, Current Learning Rate: 0.0007891, \u001b[91mTrain Loss: 2.050\n",
      "\u001b[0m\u001b[1mEpoch: [35/70], Progress: 33.33 %, Steps: 3194, Current Learning Rate: 0.0007893, \u001b[96mTrain Loss: 1.918\n",
      "\u001b[0m\u001b[1mEpoch: [35/70], Progress: 34.41 %, Steps: 3195, Current Learning Rate: 0.0007896, \u001b[91mTrain Loss: 1.992\n",
      "\u001b[0m\u001b[1mEpoch: [35/70], Progress: 35.48 %, Steps: 3196, Current Learning Rate: 0.0007898, \u001b[91mTrain Loss: 2.014\n",
      "\u001b[0m\u001b[1mEpoch: [35/70], Progress: 36.56 %, Steps: 3197, Current Learning Rate: 0.0007901, \u001b[91mTrain Loss: 2.138\n",
      "\u001b[0m\u001b[1mEpoch: [35/70], Progress: 37.63 %, Steps: 3198, Current Learning Rate: 0.0007903, \u001b[96mTrain Loss: 2.127\n",
      "\u001b[0m\u001b[1mEpoch: [35/70], Progress: 38.71 %, Steps: 3199, Current Learning Rate: 0.0007906, \u001b[91mTrain Loss: 2.190\n",
      "\u001b[0m\u001b[1mEpoch: [35/70], Progress: 39.78 %, Steps: 3200, Current Learning Rate: 0.0007908, \u001b[96mTrain Loss: 2.076\n",
      "\u001b[0m\u001b[1mEpoch: [35/70], Progress: 40.86 %, Steps: 3201, Current Learning Rate: 0.0007911, \u001b[91mTrain Loss: 2.105\n",
      "\u001b[0m\u001b[1mEpoch: [35/70], Progress: 41.94 %, Steps: 3202, Current Learning Rate: 0.0007913, \u001b[96mTrain Loss: 2.014\n",
      "\u001b[0m\u001b[1mEpoch: [35/70], Progress: 43.01 %, Steps: 3203, Current Learning Rate: 0.0007916, \u001b[91mTrain Loss: 2.068\n",
      "\u001b[0m\u001b[1mEpoch: [35/70], Progress: 44.09 %, Steps: 3204, Current Learning Rate: 0.0007918, \u001b[96mTrain Loss: 1.931\n",
      "\u001b[0m\u001b[1mEpoch: [35/70], Progress: 45.16 %, Steps: 3205, Current Learning Rate: 0.0007921, \u001b[91mTrain Loss: 2.083\n",
      "\u001b[0m\u001b[1mEpoch: [35/70], Progress: 46.24 %, Steps: 3206, Current Learning Rate: 0.0007923, \u001b[96mTrain Loss: 1.994\n",
      "\u001b[0m\u001b[1mEpoch: [35/70], Progress: 47.31 %, Steps: 3207, Current Learning Rate: 0.0007925, \u001b[91mTrain Loss: 2.068\n",
      "\u001b[0m\u001b[1mEpoch: [35/70], Progress: 48.39 %, Steps: 3208, Current Learning Rate: 0.0007928, \u001b[91mTrain Loss: 2.078\n",
      "\u001b[0m\u001b[1mEpoch: [35/70], Progress: 49.46 %, Steps: 3209, Current Learning Rate: 0.0007930, \u001b[91mTrain Loss: 2.140\n",
      "\u001b[0m\u001b[1mEpoch: [35/70], Progress: 50.54 %, Steps: 3210, Current Learning Rate: 0.0007933, \u001b[96mTrain Loss: 1.954\n",
      "\u001b[0m\u001b[1mEpoch: [35/70], Progress: 51.61 %, Steps: 3211, Current Learning Rate: 0.0007935, \u001b[96mTrain Loss: 1.875\n",
      "\u001b[0m\u001b[1mEpoch: [35/70], Progress: 52.69 %, Steps: 3212, Current Learning Rate: 0.0007938, \u001b[91mTrain Loss: 2.110\n",
      "\u001b[0m\u001b[1mEpoch: [35/70], Progress: 53.76 %, Steps: 3213, Current Learning Rate: 0.0007940, \u001b[96mTrain Loss: 2.023\n",
      "\u001b[0m\u001b[1mEpoch: [35/70], Progress: 54.84 %, Steps: 3214, Current Learning Rate: 0.0007943, \u001b[91mTrain Loss: 2.049\n",
      "\u001b[0m\u001b[1mEpoch: [35/70], Progress: 55.91 %, Steps: 3215, Current Learning Rate: 0.0007945, \u001b[91mTrain Loss: 2.062\n",
      "\u001b[0m\u001b[1mEpoch: [35/70], Progress: 56.99 %, Steps: 3216, Current Learning Rate: 0.0007948, \u001b[96mTrain Loss: 2.055\n",
      "\u001b[0m\u001b[1mEpoch: [35/70], Progress: 58.06 %, Steps: 3217, Current Learning Rate: 0.0007950, \u001b[91mTrain Loss: 2.173\n",
      "\u001b[0m\u001b[1mEpoch: [35/70], Progress: 59.14 %, Steps: 3218, Current Learning Rate: 0.0007953, \u001b[91mTrain Loss: 2.179\n",
      "\u001b[0m\u001b[1mEpoch: [35/70], Progress: 60.22 %, Steps: 3219, Current Learning Rate: 0.0007955, \u001b[96mTrain Loss: 2.020\n",
      "\u001b[0m\u001b[1mEpoch: [35/70], Progress: 61.29 %, Steps: 3220, Current Learning Rate: 0.0007958, \u001b[91mTrain Loss: 2.188\n",
      "\u001b[0m\u001b[1mEpoch: [35/70], Progress: 62.37 %, Steps: 3221, Current Learning Rate: 0.0007960, \u001b[91mTrain Loss: 2.207\n",
      "\u001b[0m\u001b[1mEpoch: [35/70], Progress: 63.44 %, Steps: 3222, Current Learning Rate: 0.0007963, \u001b[96mTrain Loss: 2.118\n",
      "\u001b[0m\u001b[1mEpoch: [35/70], Progress: 64.52 %, Steps: 3223, Current Learning Rate: 0.0007965, \u001b[91mTrain Loss: 2.124\n",
      "\u001b[0m\u001b[1mEpoch: [35/70], Progress: 65.59 %, Steps: 3224, Current Learning Rate: 0.0007967, \u001b[96mTrain Loss: 1.958\n",
      "\u001b[0m\u001b[1mEpoch: [35/70], Progress: 66.67 %, Steps: 3225, Current Learning Rate: 0.0007970, \u001b[91mTrain Loss: 2.220\n",
      "\u001b[0m\u001b[1mEpoch: [35/70], Progress: 67.74 %, Steps: 3226, Current Learning Rate: 0.0007972, \u001b[96mTrain Loss: 2.006\n",
      "\u001b[0m\u001b[1mEpoch: [35/70], Progress: 68.82 %, Steps: 3227, Current Learning Rate: 0.0007975, \u001b[91mTrain Loss: 2.027\n",
      "\u001b[0m\u001b[1mEpoch: [35/70], Progress: 69.89 %, Steps: 3228, Current Learning Rate: 0.0007977, \u001b[96mTrain Loss: 1.949\n",
      "\u001b[0m\u001b[1mEpoch: [35/70], Progress: 70.97 %, Steps: 3229, Current Learning Rate: 0.0007980, \u001b[91mTrain Loss: 2.252\n",
      "\u001b[0m\u001b[1mEpoch: [35/70], Progress: 72.04 %, Steps: 3230, Current Learning Rate: 0.0007982, \u001b[96mTrain Loss: 2.032\n",
      "\u001b[0m\u001b[1mEpoch: [35/70], Progress: 73.12 %, Steps: 3231, Current Learning Rate: 0.0007985, \u001b[91mTrain Loss: 2.154\n",
      "\u001b[0m\u001b[1mEpoch: [35/70], Progress: 74.19 %, Steps: 3232, Current Learning Rate: 0.0007987, \u001b[96mTrain Loss: 2.035\n",
      "\u001b[0m\u001b[1mEpoch: [35/70], Progress: 75.27 %, Steps: 3233, Current Learning Rate: 0.0007990, \u001b[91mTrain Loss: 2.271\n",
      "\u001b[0m\u001b[1mEpoch: [35/70], Progress: 76.34 %, Steps: 3234, Current Learning Rate: 0.0007992, \u001b[96mTrain Loss: 2.053\n",
      "\u001b[0m\u001b[1mEpoch: [35/70], Progress: 77.42 %, Steps: 3235, Current Learning Rate: 0.0007995, \u001b[91mTrain Loss: 2.202\n",
      "\u001b[0m\u001b[1mEpoch: [35/70], Progress: 78.49 %, Steps: 3236, Current Learning Rate: 0.0007997, \u001b[96mTrain Loss: 2.107\n",
      "\u001b[0m\u001b[1mEpoch: [35/70], Progress: 79.57 %, Steps: 3237, Current Learning Rate: 0.0008000, \u001b[96mTrain Loss: 1.999\n",
      "\u001b[0m\u001b[1mEpoch: [35/70], Progress: 80.65 %, Steps: 3238, Current Learning Rate: 0.0008002, \u001b[91mTrain Loss: 2.204\n",
      "\u001b[0m\u001b[1mEpoch: [35/70], Progress: 81.72 %, Steps: 3239, Current Learning Rate: 0.0008005, \u001b[96mTrain Loss: 2.194\n",
      "\u001b[0m\u001b[1mEpoch: [35/70], Progress: 82.80 %, Steps: 3240, Current Learning Rate: 0.0008007, \u001b[96mTrain Loss: 2.108\n",
      "\u001b[0m\u001b[1mEpoch: [35/70], Progress: 83.87 %, Steps: 3241, Current Learning Rate: 0.0008009, \u001b[96mTrain Loss: 1.935\n",
      "\u001b[0m\u001b[1mEpoch: [35/70], Progress: 84.95 %, Steps: 3242, Current Learning Rate: 0.0008012, \u001b[91mTrain Loss: 2.246\n",
      "\u001b[0m\u001b[1mEpoch: [35/70], Progress: 86.02 %, Steps: 3243, Current Learning Rate: 0.0008014, \u001b[96mTrain Loss: 1.990\n",
      "\u001b[0m\u001b[1mEpoch: [35/70], Progress: 87.10 %, Steps: 3244, Current Learning Rate: 0.0008017, \u001b[96mTrain Loss: 1.920\n",
      "\u001b[0m\u001b[1mEpoch: [35/70], Progress: 88.17 %, Steps: 3245, Current Learning Rate: 0.0008019, \u001b[91mTrain Loss: 1.926\n",
      "\u001b[0m\u001b[1mEpoch: [35/70], Progress: 89.25 %, Steps: 3246, Current Learning Rate: 0.0008022, \u001b[91mTrain Loss: 2.078\n",
      "\u001b[0m\u001b[1mEpoch: [35/70], Progress: 90.32 %, Steps: 3247, Current Learning Rate: 0.0008024, \u001b[91mTrain Loss: 2.151\n",
      "\u001b[0m\u001b[1mEpoch: [35/70], Progress: 91.40 %, Steps: 3248, Current Learning Rate: 0.0008027, \u001b[96mTrain Loss: 1.990\n",
      "\u001b[0m\u001b[1mEpoch: [35/70], Progress: 92.47 %, Steps: 3249, Current Learning Rate: 0.0008029, \u001b[91mTrain Loss: 2.086\n",
      "\u001b[0m\u001b[1mEpoch: [35/70], Progress: 93.55 %, Steps: 3250, Current Learning Rate: 0.0008032, \u001b[96mTrain Loss: 1.991\n",
      "\u001b[0m\u001b[1mEpoch: [35/70], Progress: 94.62 %, Steps: 3251, Current Learning Rate: 0.0008034, \u001b[91mTrain Loss: 2.062\n",
      "\u001b[0m\u001b[1mEpoch: [35/70], Progress: 95.70 %, Steps: 3252, Current Learning Rate: 0.0008037, \u001b[91mTrain Loss: 2.115\n",
      "\u001b[0m\u001b[1mEpoch: [35/70], Progress: 96.77 %, Steps: 3253, Current Learning Rate: 0.0008039, \u001b[96mTrain Loss: 2.028\n",
      "\u001b[0m\u001b[1mEpoch: [35/70], Progress: 97.85 %, Steps: 3254, Current Learning Rate: 0.0008042, \u001b[91mTrain Loss: 2.157\n",
      "\u001b[0m\u001b[1mEpoch: [35/70], Progress: 98.92 %, Steps: 3255, Current Learning Rate: 0.0008044, \u001b[96mTrain Loss: 2.065\n",
      "\u001b[1m\u001b[93mComputing Validation Loss...\n",
      "\u001b[1mEpoch 35 Completed! Average Train Loss: 2.028, Average Validation Loss: 1.238\n",
      "Best Model saved in best_model/best_model.pt.\n",
      "\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m= Epoch [36/70] ==========================================\n",
      "\u001b[0m\u001b[1mEpoch: [36/70], Progress: 0.00 %, Steps: 3256, Current Learning Rate: 0.0008047, \u001b[91mTrain Loss: 1.752\n",
      "\u001b[0m\u001b[1mEpoch: [36/70], Progress: 1.08 %, Steps: 3257, Current Learning Rate: 0.0008049, \u001b[96mTrain Loss: 1.673\n",
      "\u001b[0m\u001b[1mEpoch: [36/70], Progress: 2.15 %, Steps: 3258, Current Learning Rate: 0.0008051, \u001b[91mTrain Loss: 1.763\n",
      "\u001b[0m\u001b[1mEpoch: [36/70], Progress: 3.23 %, Steps: 3259, Current Learning Rate: 0.0008054, \u001b[91mTrain Loss: 1.815\n",
      "\u001b[0m\u001b[1mEpoch: [36/70], Progress: 4.30 %, Steps: 3260, Current Learning Rate: 0.0008056, \u001b[96mTrain Loss: 1.612\n",
      "\u001b[0m\u001b[1mEpoch: [36/70], Progress: 5.38 %, Steps: 3261, Current Learning Rate: 0.0008059, \u001b[91mTrain Loss: 1.761\n",
      "\u001b[0m\u001b[1mEpoch: [36/70], Progress: 6.45 %, Steps: 3262, Current Learning Rate: 0.0008061, \u001b[96mTrain Loss: 1.736\n",
      "\u001b[0m\u001b[1mEpoch: [36/70], Progress: 7.53 %, Steps: 3263, Current Learning Rate: 0.0008064, \u001b[91mTrain Loss: 1.736\n",
      "\u001b[0m\u001b[1mEpoch: [36/70], Progress: 8.60 %, Steps: 3264, Current Learning Rate: 0.0008066, \u001b[91mTrain Loss: 1.745\n",
      "\u001b[0m\u001b[1mEpoch: [36/70], Progress: 9.68 %, Steps: 3265, Current Learning Rate: 0.0008069, \u001b[96mTrain Loss: 1.731\n",
      "\u001b[0m\u001b[1mEpoch: [36/70], Progress: 10.75 %, Steps: 3266, Current Learning Rate: 0.0008071, \u001b[91mTrain Loss: 1.940\n",
      "\u001b[0m\u001b[1mEpoch: [36/70], Progress: 11.83 %, Steps: 3267, Current Learning Rate: 0.0008074, \u001b[96mTrain Loss: 1.826\n",
      "\u001b[0m\u001b[1mEpoch: [36/70], Progress: 12.90 %, Steps: 3268, Current Learning Rate: 0.0008076, \u001b[91mTrain Loss: 1.860\n",
      "\u001b[0m\u001b[1mEpoch: [36/70], Progress: 13.98 %, Steps: 3269, Current Learning Rate: 0.0008079, \u001b[96mTrain Loss: 1.788\n",
      "\u001b[0m\u001b[1mEpoch: [36/70], Progress: 15.05 %, Steps: 3270, Current Learning Rate: 0.0008081, \u001b[96mTrain Loss: 1.721\n",
      "\u001b[0m\u001b[1mEpoch: [36/70], Progress: 16.13 %, Steps: 3271, Current Learning Rate: 0.0008084, \u001b[91mTrain Loss: 1.780\n",
      "\u001b[0m\u001b[1mEpoch: [36/70], Progress: 17.20 %, Steps: 3272, Current Learning Rate: 0.0008086, \u001b[91mTrain Loss: 1.783\n",
      "\u001b[0m\u001b[1mEpoch: [36/70], Progress: 18.28 %, Steps: 3273, Current Learning Rate: 0.0008089, \u001b[91mTrain Loss: 1.868\n",
      "\u001b[0m\u001b[1mEpoch: [36/70], Progress: 19.35 %, Steps: 3274, Current Learning Rate: 0.0008091, \u001b[96mTrain Loss: 1.776\n",
      "\u001b[0m\u001b[1mEpoch: [36/70], Progress: 20.43 %, Steps: 3275, Current Learning Rate: 0.0008093, \u001b[91mTrain Loss: 1.925\n",
      "\u001b[0m\u001b[1mEpoch: [36/70], Progress: 21.51 %, Steps: 3276, Current Learning Rate: 0.0008096, \u001b[96mTrain Loss: 1.788\n",
      "\u001b[0m\u001b[1mEpoch: [36/70], Progress: 22.58 %, Steps: 3277, Current Learning Rate: 0.0008098, \u001b[91mTrain Loss: 2.062\n",
      "\u001b[0m\u001b[1mEpoch: [36/70], Progress: 23.66 %, Steps: 3278, Current Learning Rate: 0.0008101, \u001b[96mTrain Loss: 1.723\n",
      "\u001b[0m\u001b[1mEpoch: [36/70], Progress: 24.73 %, Steps: 3279, Current Learning Rate: 0.0008103, \u001b[91mTrain Loss: 1.771\n",
      "\u001b[0m\u001b[1mEpoch: [36/70], Progress: 25.81 %, Steps: 3280, Current Learning Rate: 0.0008106, \u001b[91mTrain Loss: 1.916\n",
      "\u001b[0m\u001b[1mEpoch: [36/70], Progress: 26.88 %, Steps: 3281, Current Learning Rate: 0.0008108, \u001b[96mTrain Loss: 1.766\n",
      "\u001b[0m\u001b[1mEpoch: [36/70], Progress: 27.96 %, Steps: 3282, Current Learning Rate: 0.0008111, \u001b[91mTrain Loss: 1.845\n",
      "\u001b[0m\u001b[1mEpoch: [36/70], Progress: 29.03 %, Steps: 3283, Current Learning Rate: 0.0008113, \u001b[91mTrain Loss: 1.908\n",
      "\u001b[0m\u001b[1mEpoch: [36/70], Progress: 30.11 %, Steps: 3284, Current Learning Rate: 0.0008116, \u001b[96mTrain Loss: 1.835\n",
      "\u001b[0m\u001b[1mEpoch: [36/70], Progress: 31.18 %, Steps: 3285, Current Learning Rate: 0.0008118, \u001b[96mTrain Loss: 1.728\n",
      "\u001b[0m\u001b[1mEpoch: [36/70], Progress: 32.26 %, Steps: 3286, Current Learning Rate: 0.0008121, \u001b[91mTrain Loss: 1.859\n",
      "\u001b[0m\u001b[1mEpoch: [36/70], Progress: 33.33 %, Steps: 3287, Current Learning Rate: 0.0008123, \u001b[91mTrain Loss: 1.898\n",
      "\u001b[0m\u001b[1mEpoch: [36/70], Progress: 34.41 %, Steps: 3288, Current Learning Rate: 0.0008126, \u001b[96mTrain Loss: 1.818\n",
      "\u001b[0m\u001b[1mEpoch: [36/70], Progress: 35.48 %, Steps: 3289, Current Learning Rate: 0.0008128, \u001b[91mTrain Loss: 1.872\n",
      "\u001b[0m\u001b[1mEpoch: [36/70], Progress: 36.56 %, Steps: 3290, Current Learning Rate: 0.0008131, \u001b[91mTrain Loss: 1.917\n",
      "\u001b[0m\u001b[1mEpoch: [36/70], Progress: 37.63 %, Steps: 3291, Current Learning Rate: 0.0008133, \u001b[96mTrain Loss: 1.752\n",
      "\u001b[0m\u001b[1mEpoch: [36/70], Progress: 38.71 %, Steps: 3292, Current Learning Rate: 0.0008135, \u001b[91mTrain Loss: 1.916\n",
      "\u001b[0m\u001b[1mEpoch: [36/70], Progress: 39.78 %, Steps: 3293, Current Learning Rate: 0.0008138, \u001b[96mTrain Loss: 1.896\n",
      "\u001b[0m\u001b[1mEpoch: [36/70], Progress: 40.86 %, Steps: 3294, Current Learning Rate: 0.0008140, \u001b[96mTrain Loss: 1.845\n",
      "\u001b[0m\u001b[1mEpoch: [36/70], Progress: 41.94 %, Steps: 3295, Current Learning Rate: 0.0008143, \u001b[91mTrain Loss: 2.065\n",
      "\u001b[0m\u001b[1mEpoch: [36/70], Progress: 43.01 %, Steps: 3296, Current Learning Rate: 0.0008145, \u001b[96mTrain Loss: 1.858\n",
      "\u001b[0m\u001b[1mEpoch: [36/70], Progress: 44.09 %, Steps: 3297, Current Learning Rate: 0.0008148, \u001b[91mTrain Loss: 1.870\n",
      "\u001b[0m\u001b[1mEpoch: [36/70], Progress: 45.16 %, Steps: 3298, Current Learning Rate: 0.0008150, \u001b[91mTrain Loss: 2.079\n",
      "\u001b[0m\u001b[1mEpoch: [36/70], Progress: 46.24 %, Steps: 3299, Current Learning Rate: 0.0008153, \u001b[96mTrain Loss: 1.875\n",
      "\u001b[0m\u001b[1mEpoch: [36/70], Progress: 47.31 %, Steps: 3300, Current Learning Rate: 0.0008155, \u001b[91mTrain Loss: 1.999\n",
      "\u001b[0m\u001b[1mEpoch: [36/70], Progress: 48.39 %, Steps: 3301, Current Learning Rate: 0.0008158, \u001b[96mTrain Loss: 1.830\n",
      "\u001b[0m\u001b[1mEpoch: [36/70], Progress: 49.46 %, Steps: 3302, Current Learning Rate: 0.0008160, \u001b[91mTrain Loss: 1.902\n",
      "\u001b[0m\u001b[1mEpoch: [36/70], Progress: 50.54 %, Steps: 3303, Current Learning Rate: 0.0008163, \u001b[96mTrain Loss: 1.807\n",
      "\u001b[0m\u001b[1mEpoch: [36/70], Progress: 51.61 %, Steps: 3304, Current Learning Rate: 0.0008165, \u001b[91mTrain Loss: 1.901\n",
      "\u001b[0m\u001b[1mEpoch: [36/70], Progress: 52.69 %, Steps: 3305, Current Learning Rate: 0.0008168, \u001b[96mTrain Loss: 1.892\n",
      "\u001b[0m\u001b[1mEpoch: [36/70], Progress: 53.76 %, Steps: 3306, Current Learning Rate: 0.0008170, \u001b[91mTrain Loss: 1.963\n",
      "\u001b[0m\u001b[1mEpoch: [36/70], Progress: 54.84 %, Steps: 3307, Current Learning Rate: 0.0008173, \u001b[96mTrain Loss: 1.928\n",
      "\u001b[0m\u001b[1mEpoch: [36/70], Progress: 55.91 %, Steps: 3308, Current Learning Rate: 0.0008175, \u001b[96mTrain Loss: 1.829\n",
      "\u001b[0m\u001b[1mEpoch: [36/70], Progress: 56.99 %, Steps: 3309, Current Learning Rate: 0.0008177, \u001b[91mTrain Loss: 1.963\n",
      "\u001b[0m\u001b[1mEpoch: [36/70], Progress: 58.06 %, Steps: 3310, Current Learning Rate: 0.0008180, \u001b[96mTrain Loss: 1.822\n",
      "\u001b[0m\u001b[1mEpoch: [36/70], Progress: 59.14 %, Steps: 3311, Current Learning Rate: 0.0008182, \u001b[91mTrain Loss: 1.978\n",
      "\u001b[0m\u001b[1mEpoch: [36/70], Progress: 60.22 %, Steps: 3312, Current Learning Rate: 0.0008185, \u001b[91mTrain Loss: 1.993\n",
      "\u001b[0m\u001b[1mEpoch: [36/70], Progress: 61.29 %, Steps: 3313, Current Learning Rate: 0.0008187, \u001b[96mTrain Loss: 1.863\n",
      "\u001b[0m\u001b[1mEpoch: [36/70], Progress: 62.37 %, Steps: 3314, Current Learning Rate: 0.0008190, \u001b[91mTrain Loss: 2.042\n",
      "\u001b[0m\u001b[1mEpoch: [36/70], Progress: 63.44 %, Steps: 3315, Current Learning Rate: 0.0008192, \u001b[96mTrain Loss: 2.022\n",
      "\u001b[0m\u001b[1mEpoch: [36/70], Progress: 64.52 %, Steps: 3316, Current Learning Rate: 0.0008195, \u001b[91mTrain Loss: 2.059\n",
      "\u001b[0m\u001b[1mEpoch: [36/70], Progress: 65.59 %, Steps: 3317, Current Learning Rate: 0.0008197, \u001b[96mTrain Loss: 1.906\n",
      "\u001b[0m\u001b[1mEpoch: [36/70], Progress: 66.67 %, Steps: 3318, Current Learning Rate: 0.0008200, \u001b[91mTrain Loss: 2.010\n",
      "\u001b[0m\u001b[1mEpoch: [36/70], Progress: 67.74 %, Steps: 3319, Current Learning Rate: 0.0008202, \u001b[96mTrain Loss: 1.882\n",
      "\u001b[0m\u001b[1mEpoch: [36/70], Progress: 68.82 %, Steps: 3320, Current Learning Rate: 0.0008205, \u001b[91mTrain Loss: 1.937\n",
      "\u001b[0m\u001b[1mEpoch: [36/70], Progress: 69.89 %, Steps: 3321, Current Learning Rate: 0.0008207, \u001b[96mTrain Loss: 1.838\n",
      "\u001b[0m\u001b[1mEpoch: [36/70], Progress: 70.97 %, Steps: 3322, Current Learning Rate: 0.0008210, \u001b[91mTrain Loss: 1.921\n",
      "\u001b[0m\u001b[1mEpoch: [36/70], Progress: 72.04 %, Steps: 3323, Current Learning Rate: 0.0008212, \u001b[91mTrain Loss: 1.933\n",
      "\u001b[0m\u001b[1mEpoch: [36/70], Progress: 73.12 %, Steps: 3324, Current Learning Rate: 0.0008215, \u001b[91mTrain Loss: 2.005\n",
      "\u001b[0m\u001b[1mEpoch: [36/70], Progress: 74.19 %, Steps: 3325, Current Learning Rate: 0.0008217, \u001b[96mTrain Loss: 1.992\n",
      "\u001b[0m\u001b[1mEpoch: [36/70], Progress: 75.27 %, Steps: 3326, Current Learning Rate: 0.0008219, \u001b[91mTrain Loss: 2.041\n",
      "\u001b[0m\u001b[1mEpoch: [36/70], Progress: 76.34 %, Steps: 3327, Current Learning Rate: 0.0008222, \u001b[96mTrain Loss: 1.875\n",
      "\u001b[0m\u001b[1mEpoch: [36/70], Progress: 77.42 %, Steps: 3328, Current Learning Rate: 0.0008224, \u001b[96mTrain Loss: 1.814\n",
      "\u001b[0m\u001b[1mEpoch: [36/70], Progress: 78.49 %, Steps: 3329, Current Learning Rate: 0.0008227, \u001b[91mTrain Loss: 1.946\n",
      "\u001b[0m\u001b[1mEpoch: [36/70], Progress: 79.57 %, Steps: 3330, Current Learning Rate: 0.0008229, \u001b[91mTrain Loss: 1.977\n",
      "\u001b[0m\u001b[1mEpoch: [36/70], Progress: 80.65 %, Steps: 3331, Current Learning Rate: 0.0008232, \u001b[96mTrain Loss: 1.862\n",
      "\u001b[0m\u001b[1mEpoch: [36/70], Progress: 81.72 %, Steps: 3332, Current Learning Rate: 0.0008234, \u001b[91mTrain Loss: 2.086\n",
      "\u001b[0m\u001b[1mEpoch: [36/70], Progress: 82.80 %, Steps: 3333, Current Learning Rate: 0.0008237, \u001b[96mTrain Loss: 2.028\n",
      "\u001b[0m\u001b[1mEpoch: [36/70], Progress: 83.87 %, Steps: 3334, Current Learning Rate: 0.0008239, \u001b[91mTrain Loss: 2.053\n",
      "\u001b[0m\u001b[1mEpoch: [36/70], Progress: 84.95 %, Steps: 3335, Current Learning Rate: 0.0008242, \u001b[96mTrain Loss: 1.841\n",
      "\u001b[0m\u001b[1mEpoch: [36/70], Progress: 86.02 %, Steps: 3336, Current Learning Rate: 0.0008244, \u001b[91mTrain Loss: 1.968\n",
      "\u001b[0m\u001b[1mEpoch: [36/70], Progress: 87.10 %, Steps: 3337, Current Learning Rate: 0.0008247, \u001b[91mTrain Loss: 2.062\n",
      "\u001b[0m\u001b[1mEpoch: [36/70], Progress: 88.17 %, Steps: 3338, Current Learning Rate: 0.0008249, \u001b[91mTrain Loss: 2.099\n",
      "\u001b[0m\u001b[1mEpoch: [36/70], Progress: 89.25 %, Steps: 3339, Current Learning Rate: 0.0008252, \u001b[96mTrain Loss: 1.916\n",
      "\u001b[0m\u001b[1mEpoch: [36/70], Progress: 90.32 %, Steps: 3340, Current Learning Rate: 0.0008254, \u001b[91mTrain Loss: 2.115\n",
      "\u001b[0m\u001b[1mEpoch: [36/70], Progress: 91.40 %, Steps: 3341, Current Learning Rate: 0.0008257, \u001b[96mTrain Loss: 1.896\n",
      "\u001b[0m\u001b[1mEpoch: [36/70], Progress: 92.47 %, Steps: 3342, Current Learning Rate: 0.0008259, \u001b[91mTrain Loss: 1.925\n",
      "\u001b[0m\u001b[1mEpoch: [36/70], Progress: 93.55 %, Steps: 3343, Current Learning Rate: 0.0008261, \u001b[91mTrain Loss: 1.950\n",
      "\u001b[0m\u001b[1mEpoch: [36/70], Progress: 94.62 %, Steps: 3344, Current Learning Rate: 0.0008264, \u001b[96mTrain Loss: 1.859\n",
      "\u001b[0m\u001b[1mEpoch: [36/70], Progress: 95.70 %, Steps: 3345, Current Learning Rate: 0.0008266, \u001b[91mTrain Loss: 2.001\n",
      "\u001b[0m\u001b[1mEpoch: [36/70], Progress: 96.77 %, Steps: 3346, Current Learning Rate: 0.0008269, \u001b[96mTrain Loss: 1.875\n",
      "\u001b[0m\u001b[1mEpoch: [36/70], Progress: 97.85 %, Steps: 3347, Current Learning Rate: 0.0008271, \u001b[91mTrain Loss: 2.035\n",
      "\u001b[0m\u001b[1mEpoch: [36/70], Progress: 98.92 %, Steps: 3348, Current Learning Rate: 0.0008274, \u001b[96mTrain Loss: 1.984\n",
      "\u001b[1m\u001b[93mComputing Validation Loss...\n",
      "\u001b[1mEpoch 36 Completed! Average Train Loss: 1.891, Average Validation Loss: 1.119\n",
      "Best Model saved in best_model/best_model.pt.\n",
      "\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m= Epoch [37/70] ==========================================\n",
      "\u001b[0m\u001b[1mEpoch: [37/70], Progress: 0.00 %, Steps: 3349, Current Learning Rate: 0.0008276, \u001b[91mTrain Loss: 1.732\n",
      "\u001b[0m\u001b[1mEpoch: [37/70], Progress: 1.08 %, Steps: 3350, Current Learning Rate: 0.0008279, \u001b[96mTrain Loss: 1.580\n",
      "\u001b[0m\u001b[1mEpoch: [37/70], Progress: 2.15 %, Steps: 3351, Current Learning Rate: 0.0008281, \u001b[96mTrain Loss: 1.470\n",
      "\u001b[0m\u001b[1mEpoch: [37/70], Progress: 3.23 %, Steps: 3352, Current Learning Rate: 0.0008284, \u001b[91mTrain Loss: 1.699\n",
      "\u001b[0m\u001b[1mEpoch: [37/70], Progress: 4.30 %, Steps: 3353, Current Learning Rate: 0.0008286, \u001b[96mTrain Loss: 1.374\n",
      "\u001b[0m\u001b[1mEpoch: [37/70], Progress: 5.38 %, Steps: 3354, Current Learning Rate: 0.0008289, \u001b[91mTrain Loss: 1.717\n",
      "\u001b[0m\u001b[1mEpoch: [37/70], Progress: 6.45 %, Steps: 3355, Current Learning Rate: 0.0008291, \u001b[91mTrain Loss: 1.721\n",
      "\u001b[0m\u001b[1mEpoch: [37/70], Progress: 7.53 %, Steps: 3356, Current Learning Rate: 0.0008294, \u001b[96mTrain Loss: 1.507\n",
      "\u001b[0m\u001b[1mEpoch: [37/70], Progress: 8.60 %, Steps: 3357, Current Learning Rate: 0.0008296, \u001b[96mTrain Loss: 1.468\n",
      "\u001b[0m\u001b[1mEpoch: [37/70], Progress: 9.68 %, Steps: 3358, Current Learning Rate: 0.0008299, \u001b[91mTrain Loss: 1.565\n",
      "\u001b[0m\u001b[1mEpoch: [37/70], Progress: 10.75 %, Steps: 3359, Current Learning Rate: 0.0008301, \u001b[91mTrain Loss: 1.603\n",
      "\u001b[0m\u001b[1mEpoch: [37/70], Progress: 11.83 %, Steps: 3360, Current Learning Rate: 0.0008303, \u001b[91mTrain Loss: 1.692\n",
      "\u001b[0m\u001b[1mEpoch: [37/70], Progress: 12.90 %, Steps: 3361, Current Learning Rate: 0.0008306, \u001b[96mTrain Loss: 1.644\n",
      "\u001b[0m\u001b[1mEpoch: [37/70], Progress: 13.98 %, Steps: 3362, Current Learning Rate: 0.0008308, \u001b[96mTrain Loss: 1.616\n",
      "\u001b[0m\u001b[1mEpoch: [37/70], Progress: 15.05 %, Steps: 3363, Current Learning Rate: 0.0008311, \u001b[91mTrain Loss: 1.743\n",
      "\u001b[0m\u001b[1mEpoch: [37/70], Progress: 16.13 %, Steps: 3364, Current Learning Rate: 0.0008313, \u001b[96mTrain Loss: 1.447\n",
      "\u001b[0m\u001b[1mEpoch: [37/70], Progress: 17.20 %, Steps: 3365, Current Learning Rate: 0.0008316, \u001b[91mTrain Loss: 1.788\n",
      "\u001b[0m\u001b[1mEpoch: [37/70], Progress: 18.28 %, Steps: 3366, Current Learning Rate: 0.0008318, \u001b[96mTrain Loss: 1.607\n",
      "\u001b[0m\u001b[1mEpoch: [37/70], Progress: 19.35 %, Steps: 3367, Current Learning Rate: 0.0008321, \u001b[91mTrain Loss: 1.672\n",
      "\u001b[0m\u001b[1mEpoch: [37/70], Progress: 20.43 %, Steps: 3368, Current Learning Rate: 0.0008323, \u001b[91mTrain Loss: 1.804\n",
      "\u001b[0m\u001b[1mEpoch: [37/70], Progress: 21.51 %, Steps: 3369, Current Learning Rate: 0.0008326, \u001b[96mTrain Loss: 1.525\n",
      "\u001b[0m\u001b[1mEpoch: [37/70], Progress: 22.58 %, Steps: 3370, Current Learning Rate: 0.0008328, \u001b[91mTrain Loss: 1.680\n",
      "\u001b[0m\u001b[1mEpoch: [37/70], Progress: 23.66 %, Steps: 3371, Current Learning Rate: 0.0008331, \u001b[91mTrain Loss: 1.726\n",
      "\u001b[0m\u001b[1mEpoch: [37/70], Progress: 24.73 %, Steps: 3372, Current Learning Rate: 0.0008333, \u001b[96mTrain Loss: 1.626\n",
      "\u001b[0m\u001b[1mEpoch: [37/70], Progress: 25.81 %, Steps: 3373, Current Learning Rate: 0.0008336, \u001b[91mTrain Loss: 1.747\n",
      "\u001b[0m\u001b[1mEpoch: [37/70], Progress: 26.88 %, Steps: 3374, Current Learning Rate: 0.0008338, \u001b[91mTrain Loss: 1.847\n",
      "\u001b[0m\u001b[1mEpoch: [37/70], Progress: 27.96 %, Steps: 3375, Current Learning Rate: 0.0008341, \u001b[96mTrain Loss: 1.822\n",
      "\u001b[0m\u001b[1mEpoch: [37/70], Progress: 29.03 %, Steps: 3376, Current Learning Rate: 0.0008343, \u001b[96mTrain Loss: 1.701\n",
      "\u001b[0m\u001b[1mEpoch: [37/70], Progress: 30.11 %, Steps: 3377, Current Learning Rate: 0.0008345, \u001b[96mTrain Loss: 1.497\n",
      "\u001b[0m\u001b[1mEpoch: [37/70], Progress: 31.18 %, Steps: 3378, Current Learning Rate: 0.0008348, \u001b[91mTrain Loss: 1.800\n",
      "\u001b[0m\u001b[1mEpoch: [37/70], Progress: 32.26 %, Steps: 3379, Current Learning Rate: 0.0008350, \u001b[91mTrain Loss: 1.871\n",
      "\u001b[0m\u001b[1mEpoch: [37/70], Progress: 33.33 %, Steps: 3380, Current Learning Rate: 0.0008353, \u001b[96mTrain Loss: 1.807\n",
      "\u001b[0m\u001b[1mEpoch: [37/70], Progress: 34.41 %, Steps: 3381, Current Learning Rate: 0.0008355, \u001b[91mTrain Loss: 1.819\n",
      "\u001b[0m\u001b[1mEpoch: [37/70], Progress: 35.48 %, Steps: 3382, Current Learning Rate: 0.0008358, \u001b[96mTrain Loss: 1.730\n",
      "\u001b[0m\u001b[1mEpoch: [37/70], Progress: 36.56 %, Steps: 3383, Current Learning Rate: 0.0008360, \u001b[96mTrain Loss: 1.706\n",
      "\u001b[0m\u001b[1mEpoch: [37/70], Progress: 37.63 %, Steps: 3384, Current Learning Rate: 0.0008363, \u001b[96mTrain Loss: 1.662\n",
      "\u001b[0m\u001b[1mEpoch: [37/70], Progress: 38.71 %, Steps: 3385, Current Learning Rate: 0.0008365, \u001b[91mTrain Loss: 1.680\n",
      "\u001b[0m\u001b[1mEpoch: [37/70], Progress: 39.78 %, Steps: 3386, Current Learning Rate: 0.0008368, \u001b[91mTrain Loss: 1.768\n",
      "\u001b[0m\u001b[1mEpoch: [37/70], Progress: 40.86 %, Steps: 3387, Current Learning Rate: 0.0008370, \u001b[96mTrain Loss: 1.669\n",
      "\u001b[0m\u001b[1mEpoch: [37/70], Progress: 41.94 %, Steps: 3388, Current Learning Rate: 0.0008373, \u001b[96mTrain Loss: 1.657\n",
      "\u001b[0m\u001b[1mEpoch: [37/70], Progress: 43.01 %, Steps: 3389, Current Learning Rate: 0.0008375, \u001b[96mTrain Loss: 1.656\n",
      "\u001b[0m\u001b[1mEpoch: [37/70], Progress: 44.09 %, Steps: 3390, Current Learning Rate: 0.0008378, \u001b[96mTrain Loss: 1.600\n",
      "\u001b[0m\u001b[1mEpoch: [37/70], Progress: 45.16 %, Steps: 3391, Current Learning Rate: 0.0008380, \u001b[91mTrain Loss: 1.610\n",
      "\u001b[0m\u001b[1mEpoch: [37/70], Progress: 46.24 %, Steps: 3392, Current Learning Rate: 0.0008383, \u001b[91mTrain Loss: 1.666\n",
      "\u001b[0m\u001b[1mEpoch: [37/70], Progress: 47.31 %, Steps: 3393, Current Learning Rate: 0.0008385, \u001b[91mTrain Loss: 1.851\n",
      "\u001b[0m\u001b[1mEpoch: [37/70], Progress: 48.39 %, Steps: 3394, Current Learning Rate: 0.0008387, \u001b[96mTrain Loss: 1.733\n",
      "\u001b[0m\u001b[1mEpoch: [37/70], Progress: 49.46 %, Steps: 3395, Current Learning Rate: 0.0008390, \u001b[91mTrain Loss: 1.931\n",
      "\u001b[0m\u001b[1mEpoch: [37/70], Progress: 50.54 %, Steps: 3396, Current Learning Rate: 0.0008392, \u001b[96mTrain Loss: 1.736\n",
      "\u001b[0m\u001b[1mEpoch: [37/70], Progress: 51.61 %, Steps: 3397, Current Learning Rate: 0.0008395, \u001b[96mTrain Loss: 1.653\n",
      "\u001b[0m\u001b[1mEpoch: [37/70], Progress: 52.69 %, Steps: 3398, Current Learning Rate: 0.0008397, \u001b[91mTrain Loss: 1.662\n",
      "\u001b[0m\u001b[1mEpoch: [37/70], Progress: 53.76 %, Steps: 3399, Current Learning Rate: 0.0008400, \u001b[91mTrain Loss: 1.868\n",
      "\u001b[0m\u001b[1mEpoch: [37/70], Progress: 54.84 %, Steps: 3400, Current Learning Rate: 0.0008402, \u001b[91mTrain Loss: 1.912\n",
      "\u001b[0m\u001b[1mEpoch: [37/70], Progress: 55.91 %, Steps: 3401, Current Learning Rate: 0.0008405, \u001b[96mTrain Loss: 1.886\n",
      "\u001b[0m\u001b[1mEpoch: [37/70], Progress: 56.99 %, Steps: 3402, Current Learning Rate: 0.0008407, \u001b[96mTrain Loss: 1.773\n",
      "\u001b[0m\u001b[1mEpoch: [37/70], Progress: 58.06 %, Steps: 3403, Current Learning Rate: 0.0008410, \u001b[91mTrain Loss: 1.776\n",
      "\u001b[0m\u001b[1mEpoch: [37/70], Progress: 59.14 %, Steps: 3404, Current Learning Rate: 0.0008412, \u001b[96mTrain Loss: 1.676\n",
      "\u001b[0m\u001b[1mEpoch: [37/70], Progress: 60.22 %, Steps: 3405, Current Learning Rate: 0.0008415, \u001b[96mTrain Loss: 1.651\n",
      "\u001b[0m\u001b[1mEpoch: [37/70], Progress: 61.29 %, Steps: 3406, Current Learning Rate: 0.0008417, \u001b[91mTrain Loss: 1.789\n",
      "\u001b[0m\u001b[1mEpoch: [37/70], Progress: 62.37 %, Steps: 3407, Current Learning Rate: 0.0008420, \u001b[91mTrain Loss: 1.806\n",
      "\u001b[0m\u001b[1mEpoch: [37/70], Progress: 63.44 %, Steps: 3408, Current Learning Rate: 0.0008422, \u001b[96mTrain Loss: 1.674\n",
      "\u001b[0m\u001b[1mEpoch: [37/70], Progress: 64.52 %, Steps: 3409, Current Learning Rate: 0.0008425, \u001b[91mTrain Loss: 1.783\n",
      "\u001b[0m\u001b[1mEpoch: [37/70], Progress: 65.59 %, Steps: 3410, Current Learning Rate: 0.0008427, \u001b[91mTrain Loss: 1.870\n",
      "\u001b[0m\u001b[1mEpoch: [37/70], Progress: 66.67 %, Steps: 3411, Current Learning Rate: 0.0008429, \u001b[91mTrain Loss: 1.871\n",
      "\u001b[0m\u001b[1mEpoch: [37/70], Progress: 67.74 %, Steps: 3412, Current Learning Rate: 0.0008432, \u001b[91mTrain Loss: 2.037\n",
      "\u001b[0m\u001b[1mEpoch: [37/70], Progress: 68.82 %, Steps: 3413, Current Learning Rate: 0.0008434, \u001b[96mTrain Loss: 1.871\n",
      "\u001b[0m\u001b[1mEpoch: [37/70], Progress: 69.89 %, Steps: 3414, Current Learning Rate: 0.0008437, \u001b[96mTrain Loss: 1.727\n",
      "\u001b[0m\u001b[1mEpoch: [37/70], Progress: 70.97 %, Steps: 3415, Current Learning Rate: 0.0008439, \u001b[91mTrain Loss: 1.855\n",
      "\u001b[0m\u001b[1mEpoch: [37/70], Progress: 72.04 %, Steps: 3416, Current Learning Rate: 0.0008442, \u001b[96mTrain Loss: 1.811\n",
      "\u001b[0m\u001b[1mEpoch: [37/70], Progress: 73.12 %, Steps: 3417, Current Learning Rate: 0.0008444, \u001b[96mTrain Loss: 1.789\n",
      "\u001b[0m\u001b[1mEpoch: [37/70], Progress: 74.19 %, Steps: 3418, Current Learning Rate: 0.0008447, \u001b[96mTrain Loss: 1.696\n",
      "\u001b[0m\u001b[1mEpoch: [37/70], Progress: 75.27 %, Steps: 3419, Current Learning Rate: 0.0008449, \u001b[96mTrain Loss: 1.658\n",
      "\u001b[0m\u001b[1mEpoch: [37/70], Progress: 76.34 %, Steps: 3420, Current Learning Rate: 0.0008452, \u001b[91mTrain Loss: 1.769\n",
      "\u001b[0m\u001b[1mEpoch: [37/70], Progress: 77.42 %, Steps: 3421, Current Learning Rate: 0.0008454, \u001b[96mTrain Loss: 1.749\n",
      "\u001b[0m\u001b[1mEpoch: [37/70], Progress: 78.49 %, Steps: 3422, Current Learning Rate: 0.0008457, \u001b[91mTrain Loss: 1.823\n",
      "\u001b[0m\u001b[1mEpoch: [37/70], Progress: 79.57 %, Steps: 3423, Current Learning Rate: 0.0008459, \u001b[96mTrain Loss: 1.818\n",
      "\u001b[0m\u001b[1mEpoch: [37/70], Progress: 80.65 %, Steps: 3424, Current Learning Rate: 0.0008462, \u001b[96mTrain Loss: 1.686\n",
      "\u001b[0m\u001b[1mEpoch: [37/70], Progress: 81.72 %, Steps: 3425, Current Learning Rate: 0.0008464, \u001b[91mTrain Loss: 1.714\n",
      "\u001b[0m\u001b[1mEpoch: [37/70], Progress: 82.80 %, Steps: 3426, Current Learning Rate: 0.0008467, \u001b[96mTrain Loss: 1.700\n",
      "\u001b[0m\u001b[1mEpoch: [37/70], Progress: 83.87 %, Steps: 3427, Current Learning Rate: 0.0008469, \u001b[91mTrain Loss: 1.705\n",
      "\u001b[0m\u001b[1mEpoch: [37/70], Progress: 84.95 %, Steps: 3428, Current Learning Rate: 0.0008471, \u001b[91mTrain Loss: 1.712\n",
      "\u001b[0m\u001b[1mEpoch: [37/70], Progress: 86.02 %, Steps: 3429, Current Learning Rate: 0.0008474, \u001b[91mTrain Loss: 1.914\n",
      "\u001b[0m\u001b[1mEpoch: [37/70], Progress: 87.10 %, Steps: 3430, Current Learning Rate: 0.0008476, \u001b[96mTrain Loss: 1.838\n",
      "\u001b[0m\u001b[1mEpoch: [37/70], Progress: 88.17 %, Steps: 3431, Current Learning Rate: 0.0008479, \u001b[96mTrain Loss: 1.825\n",
      "\u001b[0m\u001b[1mEpoch: [37/70], Progress: 89.25 %, Steps: 3432, Current Learning Rate: 0.0008481, \u001b[96mTrain Loss: 1.813\n",
      "\u001b[0m\u001b[1mEpoch: [37/70], Progress: 90.32 %, Steps: 3433, Current Learning Rate: 0.0008484, \u001b[91mTrain Loss: 2.047\n",
      "\u001b[0m\u001b[1mEpoch: [37/70], Progress: 91.40 %, Steps: 3434, Current Learning Rate: 0.0008486, \u001b[96mTrain Loss: 2.002\n",
      "\u001b[0m\u001b[1mEpoch: [37/70], Progress: 92.47 %, Steps: 3435, Current Learning Rate: 0.0008489, \u001b[96mTrain Loss: 1.600\n",
      "\u001b[0m\u001b[1mEpoch: [37/70], Progress: 93.55 %, Steps: 3436, Current Learning Rate: 0.0008491, \u001b[91mTrain Loss: 1.715\n",
      "\u001b[0m\u001b[1mEpoch: [37/70], Progress: 94.62 %, Steps: 3437, Current Learning Rate: 0.0008494, \u001b[91mTrain Loss: 1.886\n",
      "\u001b[0m\u001b[1mEpoch: [37/70], Progress: 95.70 %, Steps: 3438, Current Learning Rate: 0.0008496, \u001b[96mTrain Loss: 1.823\n",
      "\u001b[0m\u001b[1mEpoch: [37/70], Progress: 96.77 %, Steps: 3439, Current Learning Rate: 0.0008499, \u001b[96mTrain Loss: 1.812\n",
      "\u001b[0m\u001b[1mEpoch: [37/70], Progress: 97.85 %, Steps: 3440, Current Learning Rate: 0.0008501, \u001b[91mTrain Loss: 1.984\n",
      "\u001b[0m\u001b[1mEpoch: [37/70], Progress: 98.92 %, Steps: 3441, Current Learning Rate: 0.0008504, \u001b[96mTrain Loss: 1.786\n",
      "\u001b[1m\u001b[93mComputing Validation Loss...\n",
      "\u001b[1mEpoch 37 Completed! Average Train Loss: 1.735, Average Validation Loss: 0.982\n",
      "Best Model saved in best_model/best_model.pt.\n",
      "\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m= Epoch [38/70] ==========================================\n",
      "\u001b[0m\u001b[1mEpoch: [38/70], Progress: 0.00 %, Steps: 3442, Current Learning Rate: 0.0008506, \u001b[91mTrain Loss: 1.441\n",
      "\u001b[0m\u001b[1mEpoch: [38/70], Progress: 1.08 %, Steps: 3443, Current Learning Rate: 0.0008509, \u001b[91mTrain Loss: 1.449\n",
      "\u001b[0m\u001b[1mEpoch: [38/70], Progress: 2.15 %, Steps: 3444, Current Learning Rate: 0.0008511, \u001b[96mTrain Loss: 1.447\n",
      "\u001b[0m\u001b[1mEpoch: [38/70], Progress: 3.23 %, Steps: 3445, Current Learning Rate: 0.0008513, \u001b[96mTrain Loss: 1.426\n",
      "\u001b[0m\u001b[1mEpoch: [38/70], Progress: 4.30 %, Steps: 3446, Current Learning Rate: 0.0008516, \u001b[91mTrain Loss: 1.707\n",
      "\u001b[0m\u001b[1mEpoch: [38/70], Progress: 5.38 %, Steps: 3447, Current Learning Rate: 0.0008518, \u001b[96mTrain Loss: 1.444\n",
      "\u001b[0m\u001b[1mEpoch: [38/70], Progress: 6.45 %, Steps: 3448, Current Learning Rate: 0.0008521, \u001b[96mTrain Loss: 1.413\n",
      "\u001b[0m\u001b[1mEpoch: [38/70], Progress: 7.53 %, Steps: 3449, Current Learning Rate: 0.0008523, \u001b[91mTrain Loss: 1.425\n",
      "\u001b[0m\u001b[1mEpoch: [38/70], Progress: 8.60 %, Steps: 3450, Current Learning Rate: 0.0008526, \u001b[91mTrain Loss: 1.666\n",
      "\u001b[0m\u001b[1mEpoch: [38/70], Progress: 9.68 %, Steps: 3451, Current Learning Rate: 0.0008528, \u001b[96mTrain Loss: 1.448\n",
      "\u001b[0m\u001b[1mEpoch: [38/70], Progress: 10.75 %, Steps: 3452, Current Learning Rate: 0.0008531, \u001b[96mTrain Loss: 1.404\n",
      "\u001b[0m\u001b[1mEpoch: [38/70], Progress: 11.83 %, Steps: 3453, Current Learning Rate: 0.0008533, \u001b[91mTrain Loss: 1.498\n",
      "\u001b[0m\u001b[1mEpoch: [38/70], Progress: 12.90 %, Steps: 3454, Current Learning Rate: 0.0008536, \u001b[91mTrain Loss: 1.530\n",
      "\u001b[0m\u001b[1mEpoch: [38/70], Progress: 13.98 %, Steps: 3455, Current Learning Rate: 0.0008538, \u001b[96mTrain Loss: 1.453\n",
      "\u001b[0m\u001b[1mEpoch: [38/70], Progress: 15.05 %, Steps: 3456, Current Learning Rate: 0.0008541, \u001b[91mTrain Loss: 1.539\n",
      "\u001b[0m\u001b[1mEpoch: [38/70], Progress: 16.13 %, Steps: 3457, Current Learning Rate: 0.0008543, \u001b[91mTrain Loss: 1.574\n",
      "\u001b[0m\u001b[1mEpoch: [38/70], Progress: 17.20 %, Steps: 3458, Current Learning Rate: 0.0008546, \u001b[91mTrain Loss: 1.648\n",
      "\u001b[0m\u001b[1mEpoch: [38/70], Progress: 18.28 %, Steps: 3459, Current Learning Rate: 0.0008548, \u001b[96mTrain Loss: 1.505\n",
      "\u001b[0m\u001b[1mEpoch: [38/70], Progress: 19.35 %, Steps: 3460, Current Learning Rate: 0.0008551, \u001b[96mTrain Loss: 1.504\n",
      "\u001b[0m\u001b[1mEpoch: [38/70], Progress: 20.43 %, Steps: 3461, Current Learning Rate: 0.0008553, \u001b[91mTrain Loss: 1.546\n",
      "\u001b[0m\u001b[1mEpoch: [38/70], Progress: 21.51 %, Steps: 3462, Current Learning Rate: 0.0008555, \u001b[91mTrain Loss: 1.657\n",
      "\u001b[0m\u001b[1mEpoch: [38/70], Progress: 22.58 %, Steps: 3463, Current Learning Rate: 0.0008558, \u001b[96mTrain Loss: 1.598\n",
      "\u001b[0m\u001b[1mEpoch: [38/70], Progress: 23.66 %, Steps: 3464, Current Learning Rate: 0.0008560, \u001b[96mTrain Loss: 1.476\n",
      "\u001b[0m\u001b[1mEpoch: [38/70], Progress: 24.73 %, Steps: 3465, Current Learning Rate: 0.0008563, \u001b[91mTrain Loss: 1.582\n",
      "\u001b[0m\u001b[1mEpoch: [38/70], Progress: 25.81 %, Steps: 3466, Current Learning Rate: 0.0008565, \u001b[96mTrain Loss: 1.367\n",
      "\u001b[0m\u001b[1mEpoch: [38/70], Progress: 26.88 %, Steps: 3467, Current Learning Rate: 0.0008568, \u001b[91mTrain Loss: 1.466\n",
      "\u001b[0m\u001b[1mEpoch: [38/70], Progress: 27.96 %, Steps: 3468, Current Learning Rate: 0.0008570, \u001b[96mTrain Loss: 1.457\n",
      "\u001b[0m\u001b[1mEpoch: [38/70], Progress: 29.03 %, Steps: 3469, Current Learning Rate: 0.0008573, \u001b[91mTrain Loss: 1.594\n",
      "\u001b[0m\u001b[1mEpoch: [38/70], Progress: 30.11 %, Steps: 3470, Current Learning Rate: 0.0008575, \u001b[96mTrain Loss: 1.580\n",
      "\u001b[0m\u001b[1mEpoch: [38/70], Progress: 31.18 %, Steps: 3471, Current Learning Rate: 0.0008578, \u001b[96mTrain Loss: 1.472\n",
      "\u001b[0m\u001b[1mEpoch: [38/70], Progress: 32.26 %, Steps: 3472, Current Learning Rate: 0.0008580, \u001b[91mTrain Loss: 1.501\n",
      "\u001b[0m\u001b[1mEpoch: [38/70], Progress: 33.33 %, Steps: 3473, Current Learning Rate: 0.0008583, \u001b[91mTrain Loss: 1.700\n",
      "\u001b[0m\u001b[1mEpoch: [38/70], Progress: 34.41 %, Steps: 3474, Current Learning Rate: 0.0008585, \u001b[96mTrain Loss: 1.376\n",
      "\u001b[0m\u001b[1mEpoch: [38/70], Progress: 35.48 %, Steps: 3475, Current Learning Rate: 0.0008588, \u001b[91mTrain Loss: 1.631\n",
      "\u001b[0m\u001b[1mEpoch: [38/70], Progress: 36.56 %, Steps: 3476, Current Learning Rate: 0.0008590, \u001b[96mTrain Loss: 1.508\n",
      "\u001b[0m\u001b[1mEpoch: [38/70], Progress: 37.63 %, Steps: 3477, Current Learning Rate: 0.0008593, \u001b[96mTrain Loss: 1.495\n",
      "\u001b[0m\u001b[1mEpoch: [38/70], Progress: 38.71 %, Steps: 3478, Current Learning Rate: 0.0008595, \u001b[96mTrain Loss: 1.414\n",
      "\u001b[0m\u001b[1mEpoch: [38/70], Progress: 39.78 %, Steps: 3479, Current Learning Rate: 0.0008597, \u001b[91mTrain Loss: 1.590\n",
      "\u001b[0m\u001b[1mEpoch: [38/70], Progress: 40.86 %, Steps: 3480, Current Learning Rate: 0.0008600, \u001b[91mTrain Loss: 1.631\n",
      "\u001b[0m\u001b[1mEpoch: [38/70], Progress: 41.94 %, Steps: 3481, Current Learning Rate: 0.0008602, \u001b[91mTrain Loss: 1.681\n",
      "\u001b[0m\u001b[1mEpoch: [38/70], Progress: 43.01 %, Steps: 3482, Current Learning Rate: 0.0008605, \u001b[96mTrain Loss: 1.450\n",
      "\u001b[0m\u001b[1mEpoch: [38/70], Progress: 44.09 %, Steps: 3483, Current Learning Rate: 0.0008607, \u001b[91mTrain Loss: 1.516\n",
      "\u001b[0m\u001b[1mEpoch: [38/70], Progress: 45.16 %, Steps: 3484, Current Learning Rate: 0.0008610, \u001b[91mTrain Loss: 1.568\n",
      "\u001b[0m\u001b[1mEpoch: [38/70], Progress: 46.24 %, Steps: 3485, Current Learning Rate: 0.0008612, \u001b[91mTrain Loss: 1.693\n",
      "\u001b[0m\u001b[1mEpoch: [38/70], Progress: 47.31 %, Steps: 3486, Current Learning Rate: 0.0008615, \u001b[96mTrain Loss: 1.602\n",
      "\u001b[0m\u001b[1mEpoch: [38/70], Progress: 48.39 %, Steps: 3487, Current Learning Rate: 0.0008617, \u001b[96mTrain Loss: 1.593\n",
      "\u001b[0m\u001b[1mEpoch: [38/70], Progress: 49.46 %, Steps: 3488, Current Learning Rate: 0.0008620, \u001b[96mTrain Loss: 1.499\n",
      "\u001b[0m\u001b[1mEpoch: [38/70], Progress: 50.54 %, Steps: 3489, Current Learning Rate: 0.0008622, \u001b[91mTrain Loss: 1.619\n",
      "\u001b[0m\u001b[1mEpoch: [38/70], Progress: 51.61 %, Steps: 3490, Current Learning Rate: 0.0008625, \u001b[91mTrain Loss: 1.681\n",
      "\u001b[0m\u001b[1mEpoch: [38/70], Progress: 52.69 %, Steps: 3491, Current Learning Rate: 0.0008627, \u001b[96mTrain Loss: 1.607\n",
      "\u001b[0m\u001b[1mEpoch: [38/70], Progress: 53.76 %, Steps: 3492, Current Learning Rate: 0.0008630, \u001b[96mTrain Loss: 1.530\n",
      "\u001b[0m\u001b[1mEpoch: [38/70], Progress: 54.84 %, Steps: 3493, Current Learning Rate: 0.0008632, \u001b[91mTrain Loss: 1.566\n",
      "\u001b[0m\u001b[1mEpoch: [38/70], Progress: 55.91 %, Steps: 3494, Current Learning Rate: 0.0008635, \u001b[91mTrain Loss: 1.657\n",
      "\u001b[0m\u001b[1mEpoch: [38/70], Progress: 56.99 %, Steps: 3495, Current Learning Rate: 0.0008637, \u001b[96mTrain Loss: 1.612\n",
      "\u001b[0m\u001b[1mEpoch: [38/70], Progress: 58.06 %, Steps: 3496, Current Learning Rate: 0.0008639, \u001b[91mTrain Loss: 1.742\n",
      "\u001b[0m\u001b[1mEpoch: [38/70], Progress: 59.14 %, Steps: 3497, Current Learning Rate: 0.0008642, \u001b[91mTrain Loss: 1.854\n",
      "\u001b[0m\u001b[1mEpoch: [38/70], Progress: 60.22 %, Steps: 3498, Current Learning Rate: 0.0008644, \u001b[96mTrain Loss: 1.599\n",
      "\u001b[0m\u001b[1mEpoch: [38/70], Progress: 61.29 %, Steps: 3499, Current Learning Rate: 0.0008647, \u001b[96mTrain Loss: 1.537\n",
      "\u001b[0m\u001b[1mEpoch: [38/70], Progress: 62.37 %, Steps: 3500, Current Learning Rate: 0.0008649, \u001b[91mTrain Loss: 1.631\n",
      "\u001b[0m\u001b[1mEpoch: [38/70], Progress: 63.44 %, Steps: 3501, Current Learning Rate: 0.0008652, \u001b[96mTrain Loss: 1.579\n",
      "\u001b[0m\u001b[1mEpoch: [38/70], Progress: 64.52 %, Steps: 3502, Current Learning Rate: 0.0008654, \u001b[96mTrain Loss: 1.520\n",
      "\u001b[0m\u001b[1mEpoch: [38/70], Progress: 65.59 %, Steps: 3503, Current Learning Rate: 0.0008657, \u001b[91mTrain Loss: 1.727\n",
      "\u001b[0m\u001b[1mEpoch: [38/70], Progress: 66.67 %, Steps: 3504, Current Learning Rate: 0.0008659, \u001b[96mTrain Loss: 1.633\n",
      "\u001b[0m\u001b[1mEpoch: [38/70], Progress: 67.74 %, Steps: 3505, Current Learning Rate: 0.0008662, \u001b[91mTrain Loss: 1.717\n",
      "\u001b[0m\u001b[1mEpoch: [38/70], Progress: 68.82 %, Steps: 3506, Current Learning Rate: 0.0008664, \u001b[96mTrain Loss: 1.697\n",
      "\u001b[0m\u001b[1mEpoch: [38/70], Progress: 69.89 %, Steps: 3507, Current Learning Rate: 0.0008667, \u001b[96mTrain Loss: 1.665\n",
      "\u001b[0m\u001b[1mEpoch: [38/70], Progress: 70.97 %, Steps: 3508, Current Learning Rate: 0.0008669, \u001b[96mTrain Loss: 1.616\n",
      "\u001b[0m\u001b[1mEpoch: [38/70], Progress: 72.04 %, Steps: 3509, Current Learning Rate: 0.0008672, \u001b[96mTrain Loss: 1.587\n",
      "\u001b[0m\u001b[1mEpoch: [38/70], Progress: 73.12 %, Steps: 3510, Current Learning Rate: 0.0008674, \u001b[96mTrain Loss: 1.513\n",
      "\u001b[0m\u001b[1mEpoch: [38/70], Progress: 74.19 %, Steps: 3511, Current Learning Rate: 0.0008676, \u001b[91mTrain Loss: 1.621\n",
      "\u001b[0m\u001b[1mEpoch: [38/70], Progress: 75.27 %, Steps: 3512, Current Learning Rate: 0.0008679, \u001b[91mTrain Loss: 1.680\n",
      "\u001b[0m\u001b[1mEpoch: [38/70], Progress: 76.34 %, Steps: 3513, Current Learning Rate: 0.0008681, \u001b[96mTrain Loss: 1.580\n",
      "\u001b[0m\u001b[1mEpoch: [38/70], Progress: 77.42 %, Steps: 3514, Current Learning Rate: 0.0008684, \u001b[91mTrain Loss: 1.675\n",
      "\u001b[0m\u001b[1mEpoch: [38/70], Progress: 78.49 %, Steps: 3515, Current Learning Rate: 0.0008686, \u001b[96mTrain Loss: 1.633\n",
      "\u001b[0m\u001b[1mEpoch: [38/70], Progress: 79.57 %, Steps: 3516, Current Learning Rate: 0.0008689, \u001b[91mTrain Loss: 1.697\n",
      "\u001b[0m\u001b[1mEpoch: [38/70], Progress: 80.65 %, Steps: 3517, Current Learning Rate: 0.0008691, \u001b[91mTrain Loss: 1.730\n",
      "\u001b[0m\u001b[1mEpoch: [38/70], Progress: 81.72 %, Steps: 3518, Current Learning Rate: 0.0008694, \u001b[91mTrain Loss: 1.897\n",
      "\u001b[0m\u001b[1mEpoch: [38/70], Progress: 82.80 %, Steps: 3519, Current Learning Rate: 0.0008696, \u001b[96mTrain Loss: 1.585\n",
      "\u001b[0m\u001b[1mEpoch: [38/70], Progress: 83.87 %, Steps: 3520, Current Learning Rate: 0.0008699, \u001b[91mTrain Loss: 1.650\n",
      "\u001b[0m\u001b[1mEpoch: [38/70], Progress: 84.95 %, Steps: 3521, Current Learning Rate: 0.0008701, \u001b[96mTrain Loss: 1.622\n",
      "\u001b[0m\u001b[1mEpoch: [38/70], Progress: 86.02 %, Steps: 3522, Current Learning Rate: 0.0008704, \u001b[91mTrain Loss: 1.779\n",
      "\u001b[0m\u001b[1mEpoch: [38/70], Progress: 87.10 %, Steps: 3523, Current Learning Rate: 0.0008706, \u001b[96mTrain Loss: 1.735\n",
      "\u001b[0m\u001b[1mEpoch: [38/70], Progress: 88.17 %, Steps: 3524, Current Learning Rate: 0.0008709, \u001b[96mTrain Loss: 1.715\n",
      "\u001b[0m\u001b[1mEpoch: [38/70], Progress: 89.25 %, Steps: 3525, Current Learning Rate: 0.0008711, \u001b[91mTrain Loss: 1.954\n",
      "\u001b[0m\u001b[1mEpoch: [38/70], Progress: 90.32 %, Steps: 3526, Current Learning Rate: 0.0008714, \u001b[96mTrain Loss: 1.569\n",
      "\u001b[0m\u001b[1mEpoch: [38/70], Progress: 91.40 %, Steps: 3527, Current Learning Rate: 0.0008716, \u001b[96mTrain Loss: 1.549\n",
      "\u001b[0m\u001b[1mEpoch: [38/70], Progress: 92.47 %, Steps: 3528, Current Learning Rate: 0.0008718, \u001b[91mTrain Loss: 1.689\n",
      "\u001b[0m\u001b[1mEpoch: [38/70], Progress: 93.55 %, Steps: 3529, Current Learning Rate: 0.0008721, \u001b[96mTrain Loss: 1.673\n",
      "\u001b[0m\u001b[1mEpoch: [38/70], Progress: 94.62 %, Steps: 3530, Current Learning Rate: 0.0008723, \u001b[96mTrain Loss: 1.578\n",
      "\u001b[0m\u001b[1mEpoch: [38/70], Progress: 95.70 %, Steps: 3531, Current Learning Rate: 0.0008726, \u001b[96mTrain Loss: 1.534\n",
      "\u001b[0m\u001b[1mEpoch: [38/70], Progress: 96.77 %, Steps: 3532, Current Learning Rate: 0.0008728, \u001b[91mTrain Loss: 1.689\n",
      "\u001b[0m\u001b[1mEpoch: [38/70], Progress: 97.85 %, Steps: 3533, Current Learning Rate: 0.0008731, \u001b[91mTrain Loss: 1.772\n",
      "\u001b[0m\u001b[1mEpoch: [38/70], Progress: 98.92 %, Steps: 3534, Current Learning Rate: 0.0008733, \u001b[91mTrain Loss: 1.865\n",
      "\u001b[1m\u001b[93mComputing Validation Loss...\n",
      "\u001b[1mEpoch 38 Completed! Average Train Loss: 1.593, Average Validation Loss: 0.893\n",
      "Best Model saved in best_model/best_model.pt.\n",
      "\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m= Epoch [39/70] ==========================================\n",
      "\u001b[0m\u001b[1mEpoch: [39/70], Progress: 0.00 %, Steps: 3535, Current Learning Rate: 0.0008736, \u001b[91mTrain Loss: 1.318\n",
      "\u001b[0m\u001b[1mEpoch: [39/70], Progress: 1.08 %, Steps: 3536, Current Learning Rate: 0.0008738, \u001b[91mTrain Loss: 1.436\n",
      "\u001b[0m\u001b[1mEpoch: [39/70], Progress: 2.15 %, Steps: 3537, Current Learning Rate: 0.0008741, \u001b[91mTrain Loss: 1.440\n",
      "\u001b[0m\u001b[1mEpoch: [39/70], Progress: 3.23 %, Steps: 3538, Current Learning Rate: 0.0008743, \u001b[96mTrain Loss: 1.382\n",
      "\u001b[0m\u001b[1mEpoch: [39/70], Progress: 4.30 %, Steps: 3539, Current Learning Rate: 0.0008746, \u001b[91mTrain Loss: 1.424\n",
      "\u001b[0m\u001b[1mEpoch: [39/70], Progress: 5.38 %, Steps: 3540, Current Learning Rate: 0.0008748, \u001b[91mTrain Loss: 1.445\n",
      "\u001b[0m\u001b[1mEpoch: [39/70], Progress: 6.45 %, Steps: 3541, Current Learning Rate: 0.0008751, \u001b[96mTrain Loss: 1.351\n",
      "\u001b[0m\u001b[1mEpoch: [39/70], Progress: 7.53 %, Steps: 3542, Current Learning Rate: 0.0008753, \u001b[96mTrain Loss: 1.345\n",
      "\u001b[0m\u001b[1mEpoch: [39/70], Progress: 8.60 %, Steps: 3543, Current Learning Rate: 0.0008756, \u001b[91mTrain Loss: 1.395\n",
      "\u001b[0m\u001b[1mEpoch: [39/70], Progress: 9.68 %, Steps: 3544, Current Learning Rate: 0.0008758, \u001b[96mTrain Loss: 1.365\n",
      "\u001b[0m\u001b[1mEpoch: [39/70], Progress: 10.75 %, Steps: 3545, Current Learning Rate: 0.0008760, \u001b[91mTrain Loss: 1.389\n",
      "\u001b[0m\u001b[1mEpoch: [39/70], Progress: 11.83 %, Steps: 3546, Current Learning Rate: 0.0008763, \u001b[91mTrain Loss: 1.492\n",
      "\u001b[0m\u001b[1mEpoch: [39/70], Progress: 12.90 %, Steps: 3547, Current Learning Rate: 0.0008765, \u001b[91mTrain Loss: 1.495\n",
      "\u001b[0m\u001b[1mEpoch: [39/70], Progress: 13.98 %, Steps: 3548, Current Learning Rate: 0.0008768, \u001b[96mTrain Loss: 1.332\n",
      "\u001b[0m\u001b[1mEpoch: [39/70], Progress: 15.05 %, Steps: 3549, Current Learning Rate: 0.0008770, \u001b[91mTrain Loss: 1.416\n",
      "\u001b[0m\u001b[1mEpoch: [39/70], Progress: 16.13 %, Steps: 3550, Current Learning Rate: 0.0008773, \u001b[96mTrain Loss: 1.380\n",
      "\u001b[0m\u001b[1mEpoch: [39/70], Progress: 17.20 %, Steps: 3551, Current Learning Rate: 0.0008775, \u001b[91mTrain Loss: 1.417\n",
      "\u001b[0m\u001b[1mEpoch: [39/70], Progress: 18.28 %, Steps: 3552, Current Learning Rate: 0.0008778, \u001b[96mTrain Loss: 1.407\n",
      "\u001b[0m\u001b[1mEpoch: [39/70], Progress: 19.35 %, Steps: 3553, Current Learning Rate: 0.0008780, \u001b[96mTrain Loss: 1.343\n",
      "\u001b[0m\u001b[1mEpoch: [39/70], Progress: 20.43 %, Steps: 3554, Current Learning Rate: 0.0008783, \u001b[91mTrain Loss: 1.480\n",
      "\u001b[0m\u001b[1mEpoch: [39/70], Progress: 21.51 %, Steps: 3555, Current Learning Rate: 0.0008785, \u001b[96mTrain Loss: 1.318\n",
      "\u001b[0m\u001b[1mEpoch: [39/70], Progress: 22.58 %, Steps: 3556, Current Learning Rate: 0.0008788, \u001b[91mTrain Loss: 1.401\n",
      "\u001b[0m\u001b[1mEpoch: [39/70], Progress: 23.66 %, Steps: 3557, Current Learning Rate: 0.0008790, \u001b[96mTrain Loss: 1.300\n",
      "\u001b[0m\u001b[1mEpoch: [39/70], Progress: 24.73 %, Steps: 3558, Current Learning Rate: 0.0008793, \u001b[91mTrain Loss: 1.407\n",
      "\u001b[0m\u001b[1mEpoch: [39/70], Progress: 25.81 %, Steps: 3559, Current Learning Rate: 0.0008795, \u001b[91mTrain Loss: 1.444\n",
      "\u001b[0m\u001b[1mEpoch: [39/70], Progress: 26.88 %, Steps: 3560, Current Learning Rate: 0.0008798, \u001b[96mTrain Loss: 1.441\n",
      "\u001b[0m\u001b[1mEpoch: [39/70], Progress: 27.96 %, Steps: 3561, Current Learning Rate: 0.0008800, \u001b[96mTrain Loss: 1.362\n",
      "\u001b[0m\u001b[1mEpoch: [39/70], Progress: 29.03 %, Steps: 3562, Current Learning Rate: 0.0008802, \u001b[91mTrain Loss: 1.437\n",
      "\u001b[0m\u001b[1mEpoch: [39/70], Progress: 30.11 %, Steps: 3563, Current Learning Rate: 0.0008805, \u001b[91mTrain Loss: 1.530\n",
      "\u001b[0m\u001b[1mEpoch: [39/70], Progress: 31.18 %, Steps: 3564, Current Learning Rate: 0.0008807, \u001b[96mTrain Loss: 1.415\n",
      "\u001b[0m\u001b[1mEpoch: [39/70], Progress: 32.26 %, Steps: 3565, Current Learning Rate: 0.0008810, \u001b[91mTrain Loss: 1.443\n",
      "\u001b[0m\u001b[1mEpoch: [39/70], Progress: 33.33 %, Steps: 3566, Current Learning Rate: 0.0008812, \u001b[91mTrain Loss: 1.471\n",
      "\u001b[0m\u001b[1mEpoch: [39/70], Progress: 34.41 %, Steps: 3567, Current Learning Rate: 0.0008815, \u001b[91mTrain Loss: 1.514\n",
      "\u001b[0m\u001b[1mEpoch: [39/70], Progress: 35.48 %, Steps: 3568, Current Learning Rate: 0.0008817, \u001b[96mTrain Loss: 1.420\n",
      "\u001b[0m\u001b[1mEpoch: [39/70], Progress: 36.56 %, Steps: 3569, Current Learning Rate: 0.0008820, \u001b[96mTrain Loss: 1.396\n",
      "\u001b[0m\u001b[1mEpoch: [39/70], Progress: 37.63 %, Steps: 3570, Current Learning Rate: 0.0008822, \u001b[91mTrain Loss: 1.532\n",
      "\u001b[0m\u001b[1mEpoch: [39/70], Progress: 38.71 %, Steps: 3571, Current Learning Rate: 0.0008825, \u001b[96mTrain Loss: 1.430\n",
      "\u001b[0m\u001b[1mEpoch: [39/70], Progress: 39.78 %, Steps: 3572, Current Learning Rate: 0.0008827, \u001b[91mTrain Loss: 1.430\n",
      "\u001b[0m\u001b[1mEpoch: [39/70], Progress: 40.86 %, Steps: 3573, Current Learning Rate: 0.0008830, \u001b[91mTrain Loss: 1.541\n",
      "\u001b[0m\u001b[1mEpoch: [39/70], Progress: 41.94 %, Steps: 3574, Current Learning Rate: 0.0008832, \u001b[96mTrain Loss: 1.330\n",
      "\u001b[0m\u001b[1mEpoch: [39/70], Progress: 43.01 %, Steps: 3575, Current Learning Rate: 0.0008835, \u001b[91mTrain Loss: 1.399\n",
      "\u001b[0m\u001b[1mEpoch: [39/70], Progress: 44.09 %, Steps: 3576, Current Learning Rate: 0.0008837, \u001b[91mTrain Loss: 1.460\n",
      "\u001b[0m\u001b[1mEpoch: [39/70], Progress: 45.16 %, Steps: 3577, Current Learning Rate: 0.0008840, \u001b[96mTrain Loss: 1.416\n",
      "\u001b[0m\u001b[1mEpoch: [39/70], Progress: 46.24 %, Steps: 3578, Current Learning Rate: 0.0008842, \u001b[91mTrain Loss: 1.608\n",
      "\u001b[0m\u001b[1mEpoch: [39/70], Progress: 47.31 %, Steps: 3579, Current Learning Rate: 0.0008844, \u001b[96mTrain Loss: 1.361\n",
      "\u001b[0m\u001b[1mEpoch: [39/70], Progress: 48.39 %, Steps: 3580, Current Learning Rate: 0.0008847, \u001b[91mTrain Loss: 1.455\n",
      "\u001b[0m\u001b[1mEpoch: [39/70], Progress: 49.46 %, Steps: 3581, Current Learning Rate: 0.0008849, \u001b[96mTrain Loss: 1.408\n",
      "\u001b[0m\u001b[1mEpoch: [39/70], Progress: 50.54 %, Steps: 3582, Current Learning Rate: 0.0008852, \u001b[91mTrain Loss: 1.621\n",
      "\u001b[0m\u001b[1mEpoch: [39/70], Progress: 51.61 %, Steps: 3583, Current Learning Rate: 0.0008854, \u001b[96mTrain Loss: 1.444\n",
      "\u001b[0m\u001b[1mEpoch: [39/70], Progress: 52.69 %, Steps: 3584, Current Learning Rate: 0.0008857, \u001b[91mTrain Loss: 1.556\n",
      "\u001b[0m\u001b[1mEpoch: [39/70], Progress: 53.76 %, Steps: 3585, Current Learning Rate: 0.0008859, \u001b[96mTrain Loss: 1.423\n",
      "\u001b[0m\u001b[1mEpoch: [39/70], Progress: 54.84 %, Steps: 3586, Current Learning Rate: 0.0008862, \u001b[96mTrain Loss: 1.412\n",
      "\u001b[0m\u001b[1mEpoch: [39/70], Progress: 55.91 %, Steps: 3587, Current Learning Rate: 0.0008864, \u001b[91mTrain Loss: 1.440\n",
      "\u001b[0m\u001b[1mEpoch: [39/70], Progress: 56.99 %, Steps: 3588, Current Learning Rate: 0.0008867, \u001b[91mTrain Loss: 1.654\n",
      "\u001b[0m\u001b[1mEpoch: [39/70], Progress: 58.06 %, Steps: 3589, Current Learning Rate: 0.0008869, \u001b[96mTrain Loss: 1.580\n",
      "\u001b[0m\u001b[1mEpoch: [39/70], Progress: 59.14 %, Steps: 3590, Current Learning Rate: 0.0008872, \u001b[96mTrain Loss: 1.580\n",
      "\u001b[0m\u001b[1mEpoch: [39/70], Progress: 60.22 %, Steps: 3591, Current Learning Rate: 0.0008874, \u001b[96mTrain Loss: 1.549\n",
      "\u001b[0m\u001b[1mEpoch: [39/70], Progress: 61.29 %, Steps: 3592, Current Learning Rate: 0.0008877, \u001b[96mTrain Loss: 1.474\n",
      "\u001b[0m\u001b[1mEpoch: [39/70], Progress: 62.37 %, Steps: 3593, Current Learning Rate: 0.0008879, \u001b[96mTrain Loss: 1.428\n",
      "\u001b[0m\u001b[1mEpoch: [39/70], Progress: 63.44 %, Steps: 3594, Current Learning Rate: 0.0008882, \u001b[96mTrain Loss: 1.385\n",
      "\u001b[0m\u001b[1mEpoch: [39/70], Progress: 64.52 %, Steps: 3595, Current Learning Rate: 0.0008884, \u001b[91mTrain Loss: 1.403\n",
      "\u001b[0m\u001b[1mEpoch: [39/70], Progress: 65.59 %, Steps: 3596, Current Learning Rate: 0.0008886, \u001b[91mTrain Loss: 1.648\n",
      "\u001b[0m\u001b[1mEpoch: [39/70], Progress: 66.67 %, Steps: 3597, Current Learning Rate: 0.0008889, \u001b[96mTrain Loss: 1.497\n",
      "\u001b[0m\u001b[1mEpoch: [39/70], Progress: 67.74 %, Steps: 3598, Current Learning Rate: 0.0008891, \u001b[91mTrain Loss: 1.526\n",
      "\u001b[0m\u001b[1mEpoch: [39/70], Progress: 68.82 %, Steps: 3599, Current Learning Rate: 0.0008894, \u001b[96mTrain Loss: 1.501\n",
      "\u001b[0m\u001b[1mEpoch: [39/70], Progress: 69.89 %, Steps: 3600, Current Learning Rate: 0.0008896, \u001b[91mTrain Loss: 1.612\n",
      "\u001b[0m\u001b[1mEpoch: [39/70], Progress: 70.97 %, Steps: 3601, Current Learning Rate: 0.0008899, \u001b[96mTrain Loss: 1.599\n",
      "\u001b[0m\u001b[1mEpoch: [39/70], Progress: 72.04 %, Steps: 3602, Current Learning Rate: 0.0008901, \u001b[91mTrain Loss: 1.711\n",
      "\u001b[0m\u001b[1mEpoch: [39/70], Progress: 73.12 %, Steps: 3603, Current Learning Rate: 0.0008904, \u001b[96mTrain Loss: 1.444\n",
      "\u001b[0m\u001b[1mEpoch: [39/70], Progress: 74.19 %, Steps: 3604, Current Learning Rate: 0.0008906, \u001b[91mTrain Loss: 1.495\n",
      "\u001b[0m\u001b[1mEpoch: [39/70], Progress: 75.27 %, Steps: 3605, Current Learning Rate: 0.0008909, \u001b[96mTrain Loss: 1.469\n",
      "\u001b[0m\u001b[1mEpoch: [39/70], Progress: 76.34 %, Steps: 3606, Current Learning Rate: 0.0008911, \u001b[96mTrain Loss: 1.380\n",
      "\u001b[0m\u001b[1mEpoch: [39/70], Progress: 77.42 %, Steps: 3607, Current Learning Rate: 0.0008914, \u001b[91mTrain Loss: 1.586\n",
      "\u001b[0m\u001b[1mEpoch: [39/70], Progress: 78.49 %, Steps: 3608, Current Learning Rate: 0.0008916, \u001b[96mTrain Loss: 1.572\n",
      "\u001b[0m\u001b[1mEpoch: [39/70], Progress: 79.57 %, Steps: 3609, Current Learning Rate: 0.0008919, \u001b[91mTrain Loss: 1.589\n",
      "\u001b[0m\u001b[1mEpoch: [39/70], Progress: 80.65 %, Steps: 3610, Current Learning Rate: 0.0008921, \u001b[96mTrain Loss: 1.487\n",
      "\u001b[0m\u001b[1mEpoch: [39/70], Progress: 81.72 %, Steps: 3611, Current Learning Rate: 0.0008924, \u001b[96mTrain Loss: 1.463\n",
      "\u001b[0m\u001b[1mEpoch: [39/70], Progress: 82.80 %, Steps: 3612, Current Learning Rate: 0.0008926, \u001b[91mTrain Loss: 1.588\n",
      "\u001b[0m\u001b[1mEpoch: [39/70], Progress: 83.87 %, Steps: 3613, Current Learning Rate: 0.0008928, \u001b[96mTrain Loss: 1.554\n",
      "\u001b[0m\u001b[1mEpoch: [39/70], Progress: 84.95 %, Steps: 3614, Current Learning Rate: 0.0008931, \u001b[96mTrain Loss: 1.536\n",
      "\u001b[0m\u001b[1mEpoch: [39/70], Progress: 86.02 %, Steps: 3615, Current Learning Rate: 0.0008933, \u001b[96mTrain Loss: 1.492\n",
      "\u001b[0m\u001b[1mEpoch: [39/70], Progress: 87.10 %, Steps: 3616, Current Learning Rate: 0.0008936, \u001b[91mTrain Loss: 1.732\n",
      "\u001b[0m\u001b[1mEpoch: [39/70], Progress: 88.17 %, Steps: 3617, Current Learning Rate: 0.0008938, \u001b[96mTrain Loss: 1.521\n",
      "\u001b[0m\u001b[1mEpoch: [39/70], Progress: 89.25 %, Steps: 3618, Current Learning Rate: 0.0008941, \u001b[91mTrain Loss: 1.571\n",
      "\u001b[0m\u001b[1mEpoch: [39/70], Progress: 90.32 %, Steps: 3619, Current Learning Rate: 0.0008943, \u001b[91mTrain Loss: 1.685\n",
      "\u001b[0m\u001b[1mEpoch: [39/70], Progress: 91.40 %, Steps: 3620, Current Learning Rate: 0.0008946, \u001b[96mTrain Loss: 1.603\n",
      "\u001b[0m\u001b[1mEpoch: [39/70], Progress: 92.47 %, Steps: 3621, Current Learning Rate: 0.0008948, \u001b[96mTrain Loss: 1.500\n",
      "\u001b[0m\u001b[1mEpoch: [39/70], Progress: 93.55 %, Steps: 3622, Current Learning Rate: 0.0008951, \u001b[91mTrain Loss: 1.517\n",
      "\u001b[0m\u001b[1mEpoch: [39/70], Progress: 94.62 %, Steps: 3623, Current Learning Rate: 0.0008953, \u001b[96mTrain Loss: 1.444\n",
      "\u001b[0m\u001b[1mEpoch: [39/70], Progress: 95.70 %, Steps: 3624, Current Learning Rate: 0.0008956, \u001b[91mTrain Loss: 1.516\n",
      "\u001b[0m\u001b[1mEpoch: [39/70], Progress: 96.77 %, Steps: 3625, Current Learning Rate: 0.0008958, \u001b[96mTrain Loss: 1.509\n",
      "\u001b[0m\u001b[1mEpoch: [39/70], Progress: 97.85 %, Steps: 3626, Current Learning Rate: 0.0008961, \u001b[91mTrain Loss: 1.521\n",
      "\u001b[0m\u001b[1mEpoch: [39/70], Progress: 98.92 %, Steps: 3627, Current Learning Rate: 0.0008963, \u001b[91mTrain Loss: 1.693\n",
      "\u001b[1m\u001b[93mComputing Validation Loss...\n",
      "\u001b[1mEpoch 39 Completed! Average Train Loss: 1.476, Average Validation Loss: 0.784\n",
      "Best Model saved in best_model/best_model.pt.\n",
      "\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m= Epoch [40/70] ==========================================\n",
      "\u001b[0m\u001b[1mEpoch: [40/70], Progress: 0.00 %, Steps: 3628, Current Learning Rate: 0.0008966, \u001b[91mTrain Loss: 1.176\n",
      "\u001b[0m\u001b[1mEpoch: [40/70], Progress: 1.08 %, Steps: 3629, Current Learning Rate: 0.0008968, \u001b[91mTrain Loss: 1.271\n",
      "\u001b[0m\u001b[1mEpoch: [40/70], Progress: 2.15 %, Steps: 3630, Current Learning Rate: 0.0008970, \u001b[96mTrain Loss: 1.210\n",
      "\u001b[0m\u001b[1mEpoch: [40/70], Progress: 3.23 %, Steps: 3631, Current Learning Rate: 0.0008973, \u001b[96mTrain Loss: 1.150\n",
      "\u001b[0m\u001b[1mEpoch: [40/70], Progress: 4.30 %, Steps: 3632, Current Learning Rate: 0.0008975, \u001b[96mTrain Loss: 1.094\n",
      "\u001b[0m\u001b[1mEpoch: [40/70], Progress: 5.38 %, Steps: 3633, Current Learning Rate: 0.0008978, \u001b[91mTrain Loss: 1.153\n",
      "\u001b[0m\u001b[1mEpoch: [40/70], Progress: 6.45 %, Steps: 3634, Current Learning Rate: 0.0008980, \u001b[91mTrain Loss: 1.234\n",
      "\u001b[0m\u001b[1mEpoch: [40/70], Progress: 7.53 %, Steps: 3635, Current Learning Rate: 0.0008983, \u001b[91mTrain Loss: 1.257\n",
      "\u001b[0m\u001b[1mEpoch: [40/70], Progress: 8.60 %, Steps: 3636, Current Learning Rate: 0.0008985, \u001b[91mTrain Loss: 1.299\n",
      "\u001b[0m\u001b[1mEpoch: [40/70], Progress: 9.68 %, Steps: 3637, Current Learning Rate: 0.0008988, \u001b[96mTrain Loss: 1.277\n",
      "\u001b[0m\u001b[1mEpoch: [40/70], Progress: 10.75 %, Steps: 3638, Current Learning Rate: 0.0008990, \u001b[96mTrain Loss: 1.205\n",
      "\u001b[0m\u001b[1mEpoch: [40/70], Progress: 11.83 %, Steps: 3639, Current Learning Rate: 0.0008993, \u001b[91mTrain Loss: 1.265\n",
      "\u001b[0m\u001b[1mEpoch: [40/70], Progress: 12.90 %, Steps: 3640, Current Learning Rate: 0.0008995, \u001b[96mTrain Loss: 1.217\n",
      "\u001b[0m\u001b[1mEpoch: [40/70], Progress: 13.98 %, Steps: 3641, Current Learning Rate: 0.0008998, \u001b[91mTrain Loss: 1.229\n",
      "\u001b[0m\u001b[1mEpoch: [40/70], Progress: 15.05 %, Steps: 3642, Current Learning Rate: 0.0009000, \u001b[96mTrain Loss: 1.165\n",
      "\u001b[0m\u001b[1mEpoch: [40/70], Progress: 16.13 %, Steps: 3643, Current Learning Rate: 0.0009003, \u001b[96mTrain Loss: 1.161\n",
      "\u001b[0m\u001b[1mEpoch: [40/70], Progress: 17.20 %, Steps: 3644, Current Learning Rate: 0.0009005, \u001b[91mTrain Loss: 1.316\n",
      "\u001b[0m\u001b[1mEpoch: [40/70], Progress: 18.28 %, Steps: 3645, Current Learning Rate: 0.0009008, \u001b[91mTrain Loss: 1.402\n",
      "\u001b[0m\u001b[1mEpoch: [40/70], Progress: 19.35 %, Steps: 3646, Current Learning Rate: 0.0009010, \u001b[96mTrain Loss: 1.347\n",
      "\u001b[0m\u001b[1mEpoch: [40/70], Progress: 20.43 %, Steps: 3647, Current Learning Rate: 0.0009012, \u001b[96mTrain Loss: 1.217\n",
      "\u001b[0m\u001b[1mEpoch: [40/70], Progress: 21.51 %, Steps: 3648, Current Learning Rate: 0.0009015, \u001b[91mTrain Loss: 1.323\n",
      "\u001b[0m\u001b[1mEpoch: [40/70], Progress: 22.58 %, Steps: 3649, Current Learning Rate: 0.0009017, \u001b[91mTrain Loss: 1.393\n",
      "\u001b[0m\u001b[1mEpoch: [40/70], Progress: 23.66 %, Steps: 3650, Current Learning Rate: 0.0009020, \u001b[96mTrain Loss: 1.299\n",
      "\u001b[0m\u001b[1mEpoch: [40/70], Progress: 24.73 %, Steps: 3651, Current Learning Rate: 0.0009022, \u001b[91mTrain Loss: 1.300\n",
      "\u001b[0m\u001b[1mEpoch: [40/70], Progress: 25.81 %, Steps: 3652, Current Learning Rate: 0.0009025, \u001b[96mTrain Loss: 1.278\n",
      "\u001b[0m\u001b[1mEpoch: [40/70], Progress: 26.88 %, Steps: 3653, Current Learning Rate: 0.0009027, \u001b[91mTrain Loss: 1.279\n",
      "\u001b[0m\u001b[1mEpoch: [40/70], Progress: 27.96 %, Steps: 3654, Current Learning Rate: 0.0009030, \u001b[91mTrain Loss: 1.327\n",
      "\u001b[0m\u001b[1mEpoch: [40/70], Progress: 29.03 %, Steps: 3655, Current Learning Rate: 0.0009032, \u001b[91mTrain Loss: 1.362\n",
      "\u001b[0m\u001b[1mEpoch: [40/70], Progress: 30.11 %, Steps: 3656, Current Learning Rate: 0.0009035, \u001b[96mTrain Loss: 1.316\n",
      "\u001b[0m\u001b[1mEpoch: [40/70], Progress: 31.18 %, Steps: 3657, Current Learning Rate: 0.0009037, \u001b[91mTrain Loss: 1.355\n",
      "\u001b[0m\u001b[1mEpoch: [40/70], Progress: 32.26 %, Steps: 3658, Current Learning Rate: 0.0009040, \u001b[91mTrain Loss: 1.434\n",
      "\u001b[0m\u001b[1mEpoch: [40/70], Progress: 33.33 %, Steps: 3659, Current Learning Rate: 0.0009042, \u001b[96mTrain Loss: 1.349\n",
      "\u001b[0m\u001b[1mEpoch: [40/70], Progress: 34.41 %, Steps: 3660, Current Learning Rate: 0.0009045, \u001b[91mTrain Loss: 1.350\n",
      "\u001b[0m\u001b[1mEpoch: [40/70], Progress: 35.48 %, Steps: 3661, Current Learning Rate: 0.0009047, \u001b[91mTrain Loss: 1.397\n",
      "\u001b[0m\u001b[1mEpoch: [40/70], Progress: 36.56 %, Steps: 3662, Current Learning Rate: 0.0009050, \u001b[96mTrain Loss: 1.293\n",
      "\u001b[0m\u001b[1mEpoch: [40/70], Progress: 37.63 %, Steps: 3663, Current Learning Rate: 0.0009052, \u001b[91mTrain Loss: 1.510\n",
      "\u001b[0m\u001b[1mEpoch: [40/70], Progress: 38.71 %, Steps: 3664, Current Learning Rate: 0.0009054, \u001b[96mTrain Loss: 1.306\n",
      "\u001b[0m\u001b[1mEpoch: [40/70], Progress: 39.78 %, Steps: 3665, Current Learning Rate: 0.0009057, \u001b[96mTrain Loss: 1.285\n",
      "\u001b[0m\u001b[1mEpoch: [40/70], Progress: 40.86 %, Steps: 3666, Current Learning Rate: 0.0009059, \u001b[91mTrain Loss: 1.330\n",
      "\u001b[0m\u001b[1mEpoch: [40/70], Progress: 41.94 %, Steps: 3667, Current Learning Rate: 0.0009062, \u001b[91mTrain Loss: 1.347\n",
      "\u001b[0m\u001b[1mEpoch: [40/70], Progress: 43.01 %, Steps: 3668, Current Learning Rate: 0.0009064, \u001b[96mTrain Loss: 1.282\n",
      "\u001b[0m\u001b[1mEpoch: [40/70], Progress: 44.09 %, Steps: 3669, Current Learning Rate: 0.0009067, \u001b[91mTrain Loss: 1.500\n",
      "\u001b[0m\u001b[1mEpoch: [40/70], Progress: 45.16 %, Steps: 3670, Current Learning Rate: 0.0009069, \u001b[96mTrain Loss: 1.419\n",
      "\u001b[0m\u001b[1mEpoch: [40/70], Progress: 46.24 %, Steps: 3671, Current Learning Rate: 0.0009072, \u001b[91mTrain Loss: 1.429\n",
      "\u001b[0m\u001b[1mEpoch: [40/70], Progress: 47.31 %, Steps: 3672, Current Learning Rate: 0.0009074, \u001b[96mTrain Loss: 1.368\n",
      "\u001b[0m\u001b[1mEpoch: [40/70], Progress: 48.39 %, Steps: 3673, Current Learning Rate: 0.0009077, \u001b[91mTrain Loss: 1.371\n",
      "\u001b[0m\u001b[1mEpoch: [40/70], Progress: 49.46 %, Steps: 3674, Current Learning Rate: 0.0009079, \u001b[96mTrain Loss: 1.364\n",
      "\u001b[0m\u001b[1mEpoch: [40/70], Progress: 50.54 %, Steps: 3675, Current Learning Rate: 0.0009082, \u001b[96mTrain Loss: 1.283\n",
      "\u001b[0m\u001b[1mEpoch: [40/70], Progress: 51.61 %, Steps: 3676, Current Learning Rate: 0.0009084, \u001b[91mTrain Loss: 1.392\n",
      "\u001b[0m\u001b[1mEpoch: [40/70], Progress: 52.69 %, Steps: 3677, Current Learning Rate: 0.0009087, \u001b[96mTrain Loss: 1.252\n",
      "\u001b[0m\u001b[1mEpoch: [40/70], Progress: 53.76 %, Steps: 3678, Current Learning Rate: 0.0009089, \u001b[91mTrain Loss: 1.388\n",
      "\u001b[0m\u001b[1mEpoch: [40/70], Progress: 54.84 %, Steps: 3679, Current Learning Rate: 0.0009092, \u001b[91mTrain Loss: 1.413\n",
      "\u001b[0m\u001b[1mEpoch: [40/70], Progress: 55.91 %, Steps: 3680, Current Learning Rate: 0.0009094, \u001b[91mTrain Loss: 1.597\n",
      "\u001b[0m\u001b[1mEpoch: [40/70], Progress: 56.99 %, Steps: 3681, Current Learning Rate: 0.0009096, \u001b[96mTrain Loss: 1.329\n",
      "\u001b[0m\u001b[1mEpoch: [40/70], Progress: 58.06 %, Steps: 3682, Current Learning Rate: 0.0009099, \u001b[91mTrain Loss: 1.482\n",
      "\u001b[0m\u001b[1mEpoch: [40/70], Progress: 59.14 %, Steps: 3683, Current Learning Rate: 0.0009101, \u001b[96mTrain Loss: 1.316\n",
      "\u001b[0m\u001b[1mEpoch: [40/70], Progress: 60.22 %, Steps: 3684, Current Learning Rate: 0.0009104, \u001b[91mTrain Loss: 1.386\n",
      "\u001b[0m\u001b[1mEpoch: [40/70], Progress: 61.29 %, Steps: 3685, Current Learning Rate: 0.0009106, \u001b[91mTrain Loss: 1.499\n",
      "\u001b[0m\u001b[1mEpoch: [40/70], Progress: 62.37 %, Steps: 3686, Current Learning Rate: 0.0009109, \u001b[96mTrain Loss: 1.436\n",
      "\u001b[0m\u001b[1mEpoch: [40/70], Progress: 63.44 %, Steps: 3687, Current Learning Rate: 0.0009111, \u001b[96mTrain Loss: 1.254\n",
      "\u001b[0m\u001b[1mEpoch: [40/70], Progress: 64.52 %, Steps: 3688, Current Learning Rate: 0.0009114, \u001b[91mTrain Loss: 1.359\n",
      "\u001b[0m\u001b[1mEpoch: [40/70], Progress: 65.59 %, Steps: 3689, Current Learning Rate: 0.0009116, \u001b[96mTrain Loss: 1.355\n",
      "\u001b[0m\u001b[1mEpoch: [40/70], Progress: 66.67 %, Steps: 3690, Current Learning Rate: 0.0009119, \u001b[91mTrain Loss: 1.411\n",
      "\u001b[0m\u001b[1mEpoch: [40/70], Progress: 67.74 %, Steps: 3691, Current Learning Rate: 0.0009121, \u001b[91mTrain Loss: 1.440\n",
      "\u001b[0m\u001b[1mEpoch: [40/70], Progress: 68.82 %, Steps: 3692, Current Learning Rate: 0.0009124, \u001b[96mTrain Loss: 1.422\n",
      "\u001b[0m\u001b[1mEpoch: [40/70], Progress: 69.89 %, Steps: 3693, Current Learning Rate: 0.0009126, \u001b[96mTrain Loss: 1.371\n",
      "\u001b[0m\u001b[1mEpoch: [40/70], Progress: 70.97 %, Steps: 3694, Current Learning Rate: 0.0009129, \u001b[96mTrain Loss: 1.265\n",
      "\u001b[0m\u001b[1mEpoch: [40/70], Progress: 72.04 %, Steps: 3695, Current Learning Rate: 0.0009131, \u001b[91mTrain Loss: 1.619\n",
      "\u001b[0m\u001b[1mEpoch: [40/70], Progress: 73.12 %, Steps: 3696, Current Learning Rate: 0.0009134, \u001b[96mTrain Loss: 1.325\n",
      "\u001b[0m\u001b[1mEpoch: [40/70], Progress: 74.19 %, Steps: 3697, Current Learning Rate: 0.0009136, \u001b[91mTrain Loss: 1.342\n",
      "\u001b[0m\u001b[1mEpoch: [40/70], Progress: 75.27 %, Steps: 3698, Current Learning Rate: 0.0009138, \u001b[91mTrain Loss: 1.390\n",
      "\u001b[0m\u001b[1mEpoch: [40/70], Progress: 76.34 %, Steps: 3699, Current Learning Rate: 0.0009141, \u001b[96mTrain Loss: 1.390\n",
      "\u001b[0m\u001b[1mEpoch: [40/70], Progress: 77.42 %, Steps: 3700, Current Learning Rate: 0.0009143, \u001b[96mTrain Loss: 1.383\n",
      "\u001b[0m\u001b[1mEpoch: [40/70], Progress: 78.49 %, Steps: 3701, Current Learning Rate: 0.0009146, \u001b[91mTrain Loss: 1.502\n",
      "\u001b[0m\u001b[1mEpoch: [40/70], Progress: 79.57 %, Steps: 3702, Current Learning Rate: 0.0009148, \u001b[96mTrain Loss: 1.397\n",
      "\u001b[0m\u001b[1mEpoch: [40/70], Progress: 80.65 %, Steps: 3703, Current Learning Rate: 0.0009151, \u001b[91mTrain Loss: 1.461\n",
      "\u001b[0m\u001b[1mEpoch: [40/70], Progress: 81.72 %, Steps: 3704, Current Learning Rate: 0.0009153, \u001b[96mTrain Loss: 1.413\n",
      "\u001b[0m\u001b[1mEpoch: [40/70], Progress: 82.80 %, Steps: 3705, Current Learning Rate: 0.0009156, \u001b[91mTrain Loss: 1.446\n",
      "\u001b[0m\u001b[1mEpoch: [40/70], Progress: 83.87 %, Steps: 3706, Current Learning Rate: 0.0009158, \u001b[91mTrain Loss: 1.519\n",
      "\u001b[0m\u001b[1mEpoch: [40/70], Progress: 84.95 %, Steps: 3707, Current Learning Rate: 0.0009161, \u001b[96mTrain Loss: 1.481\n",
      "\u001b[0m\u001b[1mEpoch: [40/70], Progress: 86.02 %, Steps: 3708, Current Learning Rate: 0.0009163, \u001b[96mTrain Loss: 1.325\n",
      "\u001b[0m\u001b[1mEpoch: [40/70], Progress: 87.10 %, Steps: 3709, Current Learning Rate: 0.0009166, \u001b[91mTrain Loss: 1.342\n",
      "\u001b[0m\u001b[1mEpoch: [40/70], Progress: 88.17 %, Steps: 3710, Current Learning Rate: 0.0009168, \u001b[91mTrain Loss: 1.519\n",
      "\u001b[0m\u001b[1mEpoch: [40/70], Progress: 89.25 %, Steps: 3711, Current Learning Rate: 0.0009171, \u001b[96mTrain Loss: 1.347\n",
      "\u001b[0m\u001b[1mEpoch: [40/70], Progress: 90.32 %, Steps: 3712, Current Learning Rate: 0.0009173, \u001b[91mTrain Loss: 1.444\n",
      "\u001b[0m\u001b[1mEpoch: [40/70], Progress: 91.40 %, Steps: 3713, Current Learning Rate: 0.0009176, \u001b[91mTrain Loss: 1.498\n",
      "\u001b[0m\u001b[1mEpoch: [40/70], Progress: 92.47 %, Steps: 3714, Current Learning Rate: 0.0009178, \u001b[96mTrain Loss: 1.264\n",
      "\u001b[0m\u001b[1mEpoch: [40/70], Progress: 93.55 %, Steps: 3715, Current Learning Rate: 0.0009180, \u001b[91mTrain Loss: 1.534\n",
      "\u001b[0m\u001b[1mEpoch: [40/70], Progress: 94.62 %, Steps: 3716, Current Learning Rate: 0.0009183, \u001b[96mTrain Loss: 1.490\n",
      "\u001b[0m\u001b[1mEpoch: [40/70], Progress: 95.70 %, Steps: 3717, Current Learning Rate: 0.0009185, \u001b[96mTrain Loss: 1.416\n",
      "\u001b[0m\u001b[1mEpoch: [40/70], Progress: 96.77 %, Steps: 3718, Current Learning Rate: 0.0009188, \u001b[91mTrain Loss: 1.502\n",
      "\u001b[0m\u001b[1mEpoch: [40/70], Progress: 97.85 %, Steps: 3719, Current Learning Rate: 0.0009190, \u001b[96mTrain Loss: 1.328\n",
      "\u001b[0m\u001b[1mEpoch: [40/70], Progress: 98.92 %, Steps: 3720, Current Learning Rate: 0.0009193, \u001b[91mTrain Loss: 1.698\n",
      "\u001b[1m\u001b[93mComputing Validation Loss...\n",
      "\u001b[1mEpoch 40 Completed! Average Train Loss: 1.357, Average Validation Loss: 0.664\n",
      "Best Model saved in best_model/best_model.pt.\n",
      "\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m= Epoch [41/70] ==========================================\n",
      "\u001b[0m\u001b[1mEpoch: [41/70], Progress: 0.00 %, Steps: 3721, Current Learning Rate: 0.0009195, \u001b[91mTrain Loss: 1.112\n",
      "\u001b[0m\u001b[1mEpoch: [41/70], Progress: 1.08 %, Steps: 3722, Current Learning Rate: 0.0009198, \u001b[91mTrain Loss: 1.167\n",
      "\u001b[0m\u001b[1mEpoch: [41/70], Progress: 2.15 %, Steps: 3723, Current Learning Rate: 0.0009200, \u001b[96mTrain Loss: 1.059\n",
      "\u001b[0m\u001b[1mEpoch: [41/70], Progress: 3.23 %, Steps: 3724, Current Learning Rate: 0.0009203, \u001b[96mTrain Loss: 1.023\n",
      "\u001b[0m\u001b[1mEpoch: [41/70], Progress: 4.30 %, Steps: 3725, Current Learning Rate: 0.0009205, \u001b[91mTrain Loss: 1.055\n",
      "\u001b[0m\u001b[1mEpoch: [41/70], Progress: 5.38 %, Steps: 3726, Current Learning Rate: 0.0009208, \u001b[96mTrain Loss: 1.054\n",
      "\u001b[0m\u001b[1mEpoch: [41/70], Progress: 6.45 %, Steps: 3727, Current Learning Rate: 0.0009210, \u001b[96mTrain Loss: 1.046\n",
      "\u001b[0m\u001b[1mEpoch: [41/70], Progress: 7.53 %, Steps: 3728, Current Learning Rate: 0.0009213, \u001b[96mTrain Loss: 1.036\n",
      "\u001b[0m\u001b[1mEpoch: [41/70], Progress: 8.60 %, Steps: 3729, Current Learning Rate: 0.0009215, \u001b[91mTrain Loss: 1.037\n",
      "\u001b[0m\u001b[1mEpoch: [41/70], Progress: 9.68 %, Steps: 3730, Current Learning Rate: 0.0009218, \u001b[96mTrain Loss: 0.944\n",
      "\u001b[0m\u001b[1mEpoch: [41/70], Progress: 10.75 %, Steps: 3731, Current Learning Rate: 0.0009220, \u001b[91mTrain Loss: 1.171\n",
      "\u001b[0m\u001b[1mEpoch: [41/70], Progress: 11.83 %, Steps: 3732, Current Learning Rate: 0.0009222, \u001b[91mTrain Loss: 1.184\n",
      "\u001b[0m\u001b[1mEpoch: [41/70], Progress: 12.90 %, Steps: 3733, Current Learning Rate: 0.0009225, \u001b[96mTrain Loss: 1.113\n",
      "\u001b[0m\u001b[1mEpoch: [41/70], Progress: 13.98 %, Steps: 3734, Current Learning Rate: 0.0009227, \u001b[91mTrain Loss: 1.187\n",
      "\u001b[0m\u001b[1mEpoch: [41/70], Progress: 15.05 %, Steps: 3735, Current Learning Rate: 0.0009230, \u001b[91mTrain Loss: 1.193\n",
      "\u001b[0m\u001b[1mEpoch: [41/70], Progress: 16.13 %, Steps: 3736, Current Learning Rate: 0.0009232, \u001b[96mTrain Loss: 1.106\n",
      "\u001b[0m\u001b[1mEpoch: [41/70], Progress: 17.20 %, Steps: 3737, Current Learning Rate: 0.0009235, \u001b[91mTrain Loss: 1.191\n",
      "\u001b[0m\u001b[1mEpoch: [41/70], Progress: 18.28 %, Steps: 3738, Current Learning Rate: 0.0009237, \u001b[96mTrain Loss: 1.185\n",
      "\u001b[0m\u001b[1mEpoch: [41/70], Progress: 19.35 %, Steps: 3739, Current Learning Rate: 0.0009240, \u001b[91mTrain Loss: 1.187\n",
      "\u001b[0m\u001b[1mEpoch: [41/70], Progress: 20.43 %, Steps: 3740, Current Learning Rate: 0.0009242, \u001b[91mTrain Loss: 1.267\n",
      "\u001b[0m\u001b[1mEpoch: [41/70], Progress: 21.51 %, Steps: 3741, Current Learning Rate: 0.0009245, \u001b[96mTrain Loss: 1.235\n",
      "\u001b[0m\u001b[1mEpoch: [41/70], Progress: 22.58 %, Steps: 3742, Current Learning Rate: 0.0009247, \u001b[96mTrain Loss: 1.163\n",
      "\u001b[0m\u001b[1mEpoch: [41/70], Progress: 23.66 %, Steps: 3743, Current Learning Rate: 0.0009250, \u001b[91mTrain Loss: 1.205\n",
      "\u001b[0m\u001b[1mEpoch: [41/70], Progress: 24.73 %, Steps: 3744, Current Learning Rate: 0.0009252, \u001b[96mTrain Loss: 1.039\n",
      "\u001b[0m\u001b[1mEpoch: [41/70], Progress: 25.81 %, Steps: 3745, Current Learning Rate: 0.0009255, \u001b[91mTrain Loss: 1.178\n",
      "\u001b[0m\u001b[1mEpoch: [41/70], Progress: 26.88 %, Steps: 3746, Current Learning Rate: 0.0009257, \u001b[91mTrain Loss: 1.310\n",
      "\u001b[0m\u001b[1mEpoch: [41/70], Progress: 27.96 %, Steps: 3747, Current Learning Rate: 0.0009260, \u001b[96mTrain Loss: 1.274\n",
      "\u001b[0m\u001b[1mEpoch: [41/70], Progress: 29.03 %, Steps: 3748, Current Learning Rate: 0.0009262, \u001b[96mTrain Loss: 1.183\n",
      "\u001b[0m\u001b[1mEpoch: [41/70], Progress: 30.11 %, Steps: 3749, Current Learning Rate: 0.0009264, \u001b[91mTrain Loss: 1.219\n",
      "\u001b[0m\u001b[1mEpoch: [41/70], Progress: 31.18 %, Steps: 3750, Current Learning Rate: 0.0009267, \u001b[91mTrain Loss: 1.252\n",
      "\u001b[0m\u001b[1mEpoch: [41/70], Progress: 32.26 %, Steps: 3751, Current Learning Rate: 0.0009269, \u001b[96mTrain Loss: 1.234\n",
      "\u001b[0m\u001b[1mEpoch: [41/70], Progress: 33.33 %, Steps: 3752, Current Learning Rate: 0.0009272, \u001b[91mTrain Loss: 1.242\n",
      "\u001b[0m\u001b[1mEpoch: [41/70], Progress: 34.41 %, Steps: 3753, Current Learning Rate: 0.0009274, \u001b[96mTrain Loss: 1.215\n",
      "\u001b[0m\u001b[1mEpoch: [41/70], Progress: 35.48 %, Steps: 3754, Current Learning Rate: 0.0009277, \u001b[91mTrain Loss: 1.262\n",
      "\u001b[0m\u001b[1mEpoch: [41/70], Progress: 36.56 %, Steps: 3755, Current Learning Rate: 0.0009279, \u001b[96mTrain Loss: 1.178\n",
      "\u001b[0m\u001b[1mEpoch: [41/70], Progress: 37.63 %, Steps: 3756, Current Learning Rate: 0.0009282, \u001b[91mTrain Loss: 1.210\n",
      "\u001b[0m\u001b[1mEpoch: [41/70], Progress: 38.71 %, Steps: 3757, Current Learning Rate: 0.0009284, \u001b[96mTrain Loss: 1.156\n",
      "\u001b[0m\u001b[1mEpoch: [41/70], Progress: 39.78 %, Steps: 3758, Current Learning Rate: 0.0009287, \u001b[91mTrain Loss: 1.205\n",
      "\u001b[0m\u001b[1mEpoch: [41/70], Progress: 40.86 %, Steps: 3759, Current Learning Rate: 0.0009289, \u001b[96mTrain Loss: 1.184\n",
      "\u001b[0m\u001b[1mEpoch: [41/70], Progress: 41.94 %, Steps: 3760, Current Learning Rate: 0.0009292, \u001b[96mTrain Loss: 1.148\n",
      "\u001b[0m\u001b[1mEpoch: [41/70], Progress: 43.01 %, Steps: 3761, Current Learning Rate: 0.0009294, \u001b[96mTrain Loss: 1.105\n",
      "\u001b[0m\u001b[1mEpoch: [41/70], Progress: 44.09 %, Steps: 3762, Current Learning Rate: 0.0009297, \u001b[91mTrain Loss: 1.218\n",
      "\u001b[0m\u001b[1mEpoch: [41/70], Progress: 45.16 %, Steps: 3763, Current Learning Rate: 0.0009299, \u001b[91mTrain Loss: 1.253\n",
      "\u001b[0m\u001b[1mEpoch: [41/70], Progress: 46.24 %, Steps: 3764, Current Learning Rate: 0.0009302, \u001b[91mTrain Loss: 1.359\n",
      "\u001b[0m\u001b[1mEpoch: [41/70], Progress: 47.31 %, Steps: 3765, Current Learning Rate: 0.0009304, \u001b[96mTrain Loss: 1.353\n",
      "\u001b[0m\u001b[1mEpoch: [41/70], Progress: 48.39 %, Steps: 3766, Current Learning Rate: 0.0009306, \u001b[91mTrain Loss: 1.408\n",
      "\u001b[0m\u001b[1mEpoch: [41/70], Progress: 49.46 %, Steps: 3767, Current Learning Rate: 0.0009309, \u001b[96mTrain Loss: 1.219\n",
      "\u001b[0m\u001b[1mEpoch: [41/70], Progress: 50.54 %, Steps: 3768, Current Learning Rate: 0.0009311, \u001b[91mTrain Loss: 1.283\n",
      "\u001b[0m\u001b[1mEpoch: [41/70], Progress: 51.61 %, Steps: 3769, Current Learning Rate: 0.0009314, \u001b[96mTrain Loss: 1.176\n",
      "\u001b[0m\u001b[1mEpoch: [41/70], Progress: 52.69 %, Steps: 3770, Current Learning Rate: 0.0009316, \u001b[91mTrain Loss: 1.229\n",
      "\u001b[0m\u001b[1mEpoch: [41/70], Progress: 53.76 %, Steps: 3771, Current Learning Rate: 0.0009319, \u001b[96mTrain Loss: 1.196\n",
      "\u001b[0m\u001b[1mEpoch: [41/70], Progress: 54.84 %, Steps: 3772, Current Learning Rate: 0.0009321, \u001b[91mTrain Loss: 1.216\n",
      "\u001b[0m\u001b[1mEpoch: [41/70], Progress: 55.91 %, Steps: 3773, Current Learning Rate: 0.0009324, \u001b[96mTrain Loss: 1.175\n",
      "\u001b[0m\u001b[1mEpoch: [41/70], Progress: 56.99 %, Steps: 3774, Current Learning Rate: 0.0009326, \u001b[91mTrain Loss: 1.292\n",
      "\u001b[0m\u001b[1mEpoch: [41/70], Progress: 58.06 %, Steps: 3775, Current Learning Rate: 0.0009329, \u001b[96mTrain Loss: 1.282\n",
      "\u001b[0m\u001b[1mEpoch: [41/70], Progress: 59.14 %, Steps: 3776, Current Learning Rate: 0.0009331, \u001b[96mTrain Loss: 1.258\n",
      "\u001b[0m\u001b[1mEpoch: [41/70], Progress: 60.22 %, Steps: 3777, Current Learning Rate: 0.0009334, \u001b[96mTrain Loss: 1.195\n",
      "\u001b[0m\u001b[1mEpoch: [41/70], Progress: 61.29 %, Steps: 3778, Current Learning Rate: 0.0009336, \u001b[91mTrain Loss: 1.254\n",
      "\u001b[0m\u001b[1mEpoch: [41/70], Progress: 62.37 %, Steps: 3779, Current Learning Rate: 0.0009339, \u001b[96mTrain Loss: 1.163\n",
      "\u001b[0m\u001b[1mEpoch: [41/70], Progress: 63.44 %, Steps: 3780, Current Learning Rate: 0.0009341, \u001b[91mTrain Loss: 1.460\n",
      "\u001b[0m\u001b[1mEpoch: [41/70], Progress: 64.52 %, Steps: 3781, Current Learning Rate: 0.0009344, \u001b[96mTrain Loss: 1.239\n",
      "\u001b[0m\u001b[1mEpoch: [41/70], Progress: 65.59 %, Steps: 3782, Current Learning Rate: 0.0009346, \u001b[91mTrain Loss: 1.280\n",
      "\u001b[0m\u001b[1mEpoch: [41/70], Progress: 66.67 %, Steps: 3783, Current Learning Rate: 0.0009348, \u001b[96mTrain Loss: 1.276\n",
      "\u001b[0m\u001b[1mEpoch: [41/70], Progress: 67.74 %, Steps: 3784, Current Learning Rate: 0.0009351, \u001b[91mTrain Loss: 1.340\n",
      "\u001b[0m\u001b[1mEpoch: [41/70], Progress: 68.82 %, Steps: 3785, Current Learning Rate: 0.0009353, \u001b[96mTrain Loss: 1.216\n",
      "\u001b[0m\u001b[1mEpoch: [41/70], Progress: 69.89 %, Steps: 3786, Current Learning Rate: 0.0009356, \u001b[91mTrain Loss: 1.260\n",
      "\u001b[0m\u001b[1mEpoch: [41/70], Progress: 70.97 %, Steps: 3787, Current Learning Rate: 0.0009358, \u001b[96mTrain Loss: 1.252\n",
      "\u001b[0m\u001b[1mEpoch: [41/70], Progress: 72.04 %, Steps: 3788, Current Learning Rate: 0.0009361, \u001b[91mTrain Loss: 1.317\n",
      "\u001b[0m\u001b[1mEpoch: [41/70], Progress: 73.12 %, Steps: 3789, Current Learning Rate: 0.0009363, \u001b[96mTrain Loss: 1.215\n",
      "\u001b[0m\u001b[1mEpoch: [41/70], Progress: 74.19 %, Steps: 3790, Current Learning Rate: 0.0009366, \u001b[91mTrain Loss: 1.438\n",
      "\u001b[0m\u001b[1mEpoch: [41/70], Progress: 75.27 %, Steps: 3791, Current Learning Rate: 0.0009368, \u001b[96mTrain Loss: 1.326\n",
      "\u001b[0m\u001b[1mEpoch: [41/70], Progress: 76.34 %, Steps: 3792, Current Learning Rate: 0.0009371, \u001b[96mTrain Loss: 1.314\n",
      "\u001b[0m\u001b[1mEpoch: [41/70], Progress: 77.42 %, Steps: 3793, Current Learning Rate: 0.0009373, \u001b[91mTrain Loss: 1.406\n",
      "\u001b[0m\u001b[1mEpoch: [41/70], Progress: 78.49 %, Steps: 3794, Current Learning Rate: 0.0009376, \u001b[96mTrain Loss: 1.200\n",
      "\u001b[0m\u001b[1mEpoch: [41/70], Progress: 79.57 %, Steps: 3795, Current Learning Rate: 0.0009378, \u001b[91mTrain Loss: 1.368\n",
      "\u001b[0m\u001b[1mEpoch: [41/70], Progress: 80.65 %, Steps: 3796, Current Learning Rate: 0.0009381, \u001b[96mTrain Loss: 1.238\n",
      "\u001b[0m\u001b[1mEpoch: [41/70], Progress: 81.72 %, Steps: 3797, Current Learning Rate: 0.0009383, \u001b[91mTrain Loss: 1.246\n",
      "\u001b[0m\u001b[1mEpoch: [41/70], Progress: 82.80 %, Steps: 3798, Current Learning Rate: 0.0009386, \u001b[91mTrain Loss: 1.327\n",
      "\u001b[0m\u001b[1mEpoch: [41/70], Progress: 83.87 %, Steps: 3799, Current Learning Rate: 0.0009388, \u001b[96mTrain Loss: 1.306\n",
      "\u001b[0m\u001b[1mEpoch: [41/70], Progress: 84.95 %, Steps: 3800, Current Learning Rate: 0.0009390, \u001b[91mTrain Loss: 1.366\n",
      "\u001b[0m\u001b[1mEpoch: [41/70], Progress: 86.02 %, Steps: 3801, Current Learning Rate: 0.0009393, \u001b[96mTrain Loss: 1.190\n",
      "\u001b[0m\u001b[1mEpoch: [41/70], Progress: 87.10 %, Steps: 3802, Current Learning Rate: 0.0009395, \u001b[91mTrain Loss: 1.491\n",
      "\u001b[0m\u001b[1mEpoch: [41/70], Progress: 88.17 %, Steps: 3803, Current Learning Rate: 0.0009398, \u001b[96mTrain Loss: 1.412\n",
      "\u001b[0m\u001b[1mEpoch: [41/70], Progress: 89.25 %, Steps: 3804, Current Learning Rate: 0.0009400, \u001b[96mTrain Loss: 1.383\n",
      "\u001b[0m\u001b[1mEpoch: [41/70], Progress: 90.32 %, Steps: 3805, Current Learning Rate: 0.0009403, \u001b[96mTrain Loss: 1.282\n",
      "\u001b[0m\u001b[1mEpoch: [41/70], Progress: 91.40 %, Steps: 3806, Current Learning Rate: 0.0009405, \u001b[96mTrain Loss: 1.235\n",
      "\u001b[0m\u001b[1mEpoch: [41/70], Progress: 92.47 %, Steps: 3807, Current Learning Rate: 0.0009408, \u001b[91mTrain Loss: 1.366\n",
      "\u001b[0m\u001b[1mEpoch: [41/70], Progress: 93.55 %, Steps: 3808, Current Learning Rate: 0.0009410, \u001b[96mTrain Loss: 1.288\n",
      "\u001b[0m\u001b[1mEpoch: [41/70], Progress: 94.62 %, Steps: 3809, Current Learning Rate: 0.0009413, \u001b[91mTrain Loss: 1.412\n",
      "\u001b[0m\u001b[1mEpoch: [41/70], Progress: 95.70 %, Steps: 3810, Current Learning Rate: 0.0009415, \u001b[91mTrain Loss: 1.430\n",
      "\u001b[0m\u001b[1mEpoch: [41/70], Progress: 96.77 %, Steps: 3811, Current Learning Rate: 0.0009418, \u001b[96mTrain Loss: 1.364\n",
      "\u001b[0m\u001b[1mEpoch: [41/70], Progress: 97.85 %, Steps: 3812, Current Learning Rate: 0.0009420, \u001b[96mTrain Loss: 1.293\n",
      "\u001b[0m\u001b[1mEpoch: [41/70], Progress: 98.92 %, Steps: 3813, Current Learning Rate: 0.0009423, \u001b[96mTrain Loss: 1.208\n",
      "\u001b[1m\u001b[93mComputing Validation Loss...\n",
      "\u001b[1mEpoch 41 Completed! Average Train Loss: 1.234, Average Validation Loss: 0.601\n",
      "Best Model saved in best_model/best_model.pt.\n",
      "\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m= Epoch [42/70] ==========================================\n",
      "\u001b[0m\u001b[1mEpoch: [42/70], Progress: 0.00 %, Steps: 3814, Current Learning Rate: 0.0009425, \u001b[91mTrain Loss: 0.996\n",
      "\u001b[0m\u001b[1mEpoch: [42/70], Progress: 1.08 %, Steps: 3815, Current Learning Rate: 0.0009428, \u001b[91mTrain Loss: 1.066\n",
      "\u001b[0m\u001b[1mEpoch: [42/70], Progress: 2.15 %, Steps: 3816, Current Learning Rate: 0.0009430, \u001b[96mTrain Loss: 1.014\n",
      "\u001b[0m\u001b[1mEpoch: [42/70], Progress: 3.23 %, Steps: 3817, Current Learning Rate: 0.0009432, \u001b[96mTrain Loss: 0.944\n",
      "\u001b[0m\u001b[1mEpoch: [42/70], Progress: 4.30 %, Steps: 3818, Current Learning Rate: 0.0009435, \u001b[91mTrain Loss: 0.964\n",
      "\u001b[0m\u001b[1mEpoch: [42/70], Progress: 5.38 %, Steps: 3819, Current Learning Rate: 0.0009437, \u001b[91mTrain Loss: 1.015\n",
      "\u001b[0m\u001b[1mEpoch: [42/70], Progress: 6.45 %, Steps: 3820, Current Learning Rate: 0.0009440, \u001b[91mTrain Loss: 1.118\n",
      "\u001b[0m\u001b[1mEpoch: [42/70], Progress: 7.53 %, Steps: 3821, Current Learning Rate: 0.0009442, \u001b[96mTrain Loss: 1.011\n",
      "\u001b[0m\u001b[1mEpoch: [42/70], Progress: 8.60 %, Steps: 3822, Current Learning Rate: 0.0009445, \u001b[91mTrain Loss: 1.078\n",
      "\u001b[0m\u001b[1mEpoch: [42/70], Progress: 9.68 %, Steps: 3823, Current Learning Rate: 0.0009447, \u001b[96mTrain Loss: 1.030\n",
      "\u001b[0m\u001b[1mEpoch: [42/70], Progress: 10.75 %, Steps: 3824, Current Learning Rate: 0.0009450, \u001b[96mTrain Loss: 0.930\n",
      "\u001b[0m\u001b[1mEpoch: [42/70], Progress: 11.83 %, Steps: 3825, Current Learning Rate: 0.0009452, \u001b[91mTrain Loss: 1.122\n",
      "\u001b[0m\u001b[1mEpoch: [42/70], Progress: 12.90 %, Steps: 3826, Current Learning Rate: 0.0009455, \u001b[96mTrain Loss: 0.991\n",
      "\u001b[0m\u001b[1mEpoch: [42/70], Progress: 13.98 %, Steps: 3827, Current Learning Rate: 0.0009457, \u001b[96mTrain Loss: 0.985\n",
      "\u001b[0m\u001b[1mEpoch: [42/70], Progress: 15.05 %, Steps: 3828, Current Learning Rate: 0.0009460, \u001b[91mTrain Loss: 1.184\n",
      "\u001b[0m\u001b[1mEpoch: [42/70], Progress: 16.13 %, Steps: 3829, Current Learning Rate: 0.0009462, \u001b[96mTrain Loss: 1.005\n",
      "\u001b[0m\u001b[1mEpoch: [42/70], Progress: 17.20 %, Steps: 3830, Current Learning Rate: 0.0009465, \u001b[91mTrain Loss: 1.017\n",
      "\u001b[0m\u001b[1mEpoch: [42/70], Progress: 18.28 %, Steps: 3831, Current Learning Rate: 0.0009467, \u001b[96mTrain Loss: 0.971\n",
      "\u001b[0m\u001b[1mEpoch: [42/70], Progress: 19.35 %, Steps: 3832, Current Learning Rate: 0.0009470, \u001b[91mTrain Loss: 1.040\n",
      "\u001b[0m\u001b[1mEpoch: [42/70], Progress: 20.43 %, Steps: 3833, Current Learning Rate: 0.0009472, \u001b[91mTrain Loss: 1.287\n",
      "\u001b[0m\u001b[1mEpoch: [42/70], Progress: 21.51 %, Steps: 3834, Current Learning Rate: 0.0009474, \u001b[96mTrain Loss: 1.144\n",
      "\u001b[0m\u001b[1mEpoch: [42/70], Progress: 22.58 %, Steps: 3835, Current Learning Rate: 0.0009477, \u001b[96mTrain Loss: 1.052\n",
      "\u001b[0m\u001b[1mEpoch: [42/70], Progress: 23.66 %, Steps: 3836, Current Learning Rate: 0.0009479, \u001b[91mTrain Loss: 1.119\n",
      "\u001b[0m\u001b[1mEpoch: [42/70], Progress: 24.73 %, Steps: 3837, Current Learning Rate: 0.0009482, \u001b[96mTrain Loss: 1.001\n",
      "\u001b[0m\u001b[1mEpoch: [42/70], Progress: 25.81 %, Steps: 3838, Current Learning Rate: 0.0009484, \u001b[91mTrain Loss: 1.041\n",
      "\u001b[0m\u001b[1mEpoch: [42/70], Progress: 26.88 %, Steps: 3839, Current Learning Rate: 0.0009487, \u001b[91mTrain Loss: 1.180\n",
      "\u001b[0m\u001b[1mEpoch: [42/70], Progress: 27.96 %, Steps: 3840, Current Learning Rate: 0.0009489, \u001b[96mTrain Loss: 1.045\n",
      "\u001b[0m\u001b[1mEpoch: [42/70], Progress: 29.03 %, Steps: 3841, Current Learning Rate: 0.0009492, \u001b[96mTrain Loss: 1.038\n",
      "\u001b[0m\u001b[1mEpoch: [42/70], Progress: 30.11 %, Steps: 3842, Current Learning Rate: 0.0009494, \u001b[96mTrain Loss: 0.916\n",
      "\u001b[0m\u001b[1mEpoch: [42/70], Progress: 31.18 %, Steps: 3843, Current Learning Rate: 0.0009497, \u001b[91mTrain Loss: 1.114\n",
      "\u001b[0m\u001b[1mEpoch: [42/70], Progress: 32.26 %, Steps: 3844, Current Learning Rate: 0.0009499, \u001b[96mTrain Loss: 1.106\n",
      "\u001b[0m\u001b[1mEpoch: [42/70], Progress: 33.33 %, Steps: 3845, Current Learning Rate: 0.0009502, \u001b[91mTrain Loss: 1.128\n",
      "\u001b[0m\u001b[1mEpoch: [42/70], Progress: 34.41 %, Steps: 3846, Current Learning Rate: 0.0009504, \u001b[96mTrain Loss: 1.038\n",
      "\u001b[0m\u001b[1mEpoch: [42/70], Progress: 35.48 %, Steps: 3847, Current Learning Rate: 0.0009507, \u001b[91mTrain Loss: 1.166\n",
      "\u001b[0m\u001b[1mEpoch: [42/70], Progress: 36.56 %, Steps: 3848, Current Learning Rate: 0.0009509, \u001b[96mTrain Loss: 1.071\n",
      "\u001b[0m\u001b[1mEpoch: [42/70], Progress: 37.63 %, Steps: 3849, Current Learning Rate: 0.0009512, \u001b[91mTrain Loss: 1.108\n",
      "\u001b[0m\u001b[1mEpoch: [42/70], Progress: 38.71 %, Steps: 3850, Current Learning Rate: 0.0009514, \u001b[91mTrain Loss: 1.120\n",
      "\u001b[0m\u001b[1mEpoch: [42/70], Progress: 39.78 %, Steps: 3851, Current Learning Rate: 0.0009516, \u001b[96mTrain Loss: 0.923\n",
      "\u001b[0m\u001b[1mEpoch: [42/70], Progress: 40.86 %, Steps: 3852, Current Learning Rate: 0.0009519, \u001b[91mTrain Loss: 1.084\n",
      "\u001b[0m\u001b[1mEpoch: [42/70], Progress: 41.94 %, Steps: 3853, Current Learning Rate: 0.0009521, \u001b[91mTrain Loss: 1.193\n",
      "\u001b[0m\u001b[1mEpoch: [42/70], Progress: 43.01 %, Steps: 3854, Current Learning Rate: 0.0009524, \u001b[96mTrain Loss: 1.158\n",
      "\u001b[0m\u001b[1mEpoch: [42/70], Progress: 44.09 %, Steps: 3855, Current Learning Rate: 0.0009526, \u001b[96mTrain Loss: 1.127\n",
      "\u001b[0m\u001b[1mEpoch: [42/70], Progress: 45.16 %, Steps: 3856, Current Learning Rate: 0.0009529, \u001b[96mTrain Loss: 1.019\n",
      "\u001b[0m\u001b[1mEpoch: [42/70], Progress: 46.24 %, Steps: 3857, Current Learning Rate: 0.0009531, \u001b[91mTrain Loss: 1.146\n",
      "\u001b[0m\u001b[1mEpoch: [42/70], Progress: 47.31 %, Steps: 3858, Current Learning Rate: 0.0009534, \u001b[96mTrain Loss: 1.135\n",
      "\u001b[0m\u001b[1mEpoch: [42/70], Progress: 48.39 %, Steps: 3859, Current Learning Rate: 0.0009536, \u001b[91mTrain Loss: 1.136\n",
      "\u001b[0m\u001b[1mEpoch: [42/70], Progress: 49.46 %, Steps: 3860, Current Learning Rate: 0.0009539, \u001b[91mTrain Loss: 1.145\n",
      "\u001b[0m\u001b[1mEpoch: [42/70], Progress: 50.54 %, Steps: 3861, Current Learning Rate: 0.0009541, \u001b[91mTrain Loss: 1.226\n",
      "\u001b[0m\u001b[1mEpoch: [42/70], Progress: 51.61 %, Steps: 3862, Current Learning Rate: 0.0009544, \u001b[96mTrain Loss: 1.068\n",
      "\u001b[0m\u001b[1mEpoch: [42/70], Progress: 52.69 %, Steps: 3863, Current Learning Rate: 0.0009546, \u001b[96mTrain Loss: 1.044\n",
      "\u001b[0m\u001b[1mEpoch: [42/70], Progress: 53.76 %, Steps: 3864, Current Learning Rate: 0.0009549, \u001b[91mTrain Loss: 1.167\n",
      "\u001b[0m\u001b[1mEpoch: [42/70], Progress: 54.84 %, Steps: 3865, Current Learning Rate: 0.0009551, \u001b[96mTrain Loss: 1.118\n",
      "\u001b[0m\u001b[1mEpoch: [42/70], Progress: 55.91 %, Steps: 3866, Current Learning Rate: 0.0009554, \u001b[91mTrain Loss: 1.137\n",
      "\u001b[0m\u001b[1mEpoch: [42/70], Progress: 56.99 %, Steps: 3867, Current Learning Rate: 0.0009556, \u001b[91mTrain Loss: 1.152\n",
      "\u001b[0m\u001b[1mEpoch: [42/70], Progress: 58.06 %, Steps: 3868, Current Learning Rate: 0.0009558, \u001b[91mTrain Loss: 1.256\n",
      "\u001b[0m\u001b[1mEpoch: [42/70], Progress: 59.14 %, Steps: 3869, Current Learning Rate: 0.0009561, \u001b[91mTrain Loss: 1.261\n",
      "\u001b[0m\u001b[1mEpoch: [42/70], Progress: 60.22 %, Steps: 3870, Current Learning Rate: 0.0009563, \u001b[96mTrain Loss: 1.216\n",
      "\u001b[0m\u001b[1mEpoch: [42/70], Progress: 61.29 %, Steps: 3871, Current Learning Rate: 0.0009566, \u001b[91mTrain Loss: 1.333\n",
      "\u001b[0m\u001b[1mEpoch: [42/70], Progress: 62.37 %, Steps: 3872, Current Learning Rate: 0.0009568, \u001b[96mTrain Loss: 1.112\n",
      "\u001b[0m\u001b[1mEpoch: [42/70], Progress: 63.44 %, Steps: 3873, Current Learning Rate: 0.0009571, \u001b[91mTrain Loss: 1.247\n",
      "\u001b[0m\u001b[1mEpoch: [42/70], Progress: 64.52 %, Steps: 3874, Current Learning Rate: 0.0009573, \u001b[96mTrain Loss: 1.187\n",
      "\u001b[0m\u001b[1mEpoch: [42/70], Progress: 65.59 %, Steps: 3875, Current Learning Rate: 0.0009576, \u001b[91mTrain Loss: 1.351\n",
      "\u001b[0m\u001b[1mEpoch: [42/70], Progress: 66.67 %, Steps: 3876, Current Learning Rate: 0.0009578, \u001b[96mTrain Loss: 1.079\n",
      "\u001b[0m\u001b[1mEpoch: [42/70], Progress: 67.74 %, Steps: 3877, Current Learning Rate: 0.0009581, \u001b[91mTrain Loss: 1.103\n",
      "\u001b[0m\u001b[1mEpoch: [42/70], Progress: 68.82 %, Steps: 3878, Current Learning Rate: 0.0009583, \u001b[91mTrain Loss: 1.124\n",
      "\u001b[0m\u001b[1mEpoch: [42/70], Progress: 69.89 %, Steps: 3879, Current Learning Rate: 0.0009586, \u001b[91mTrain Loss: 1.153\n",
      "\u001b[0m\u001b[1mEpoch: [42/70], Progress: 70.97 %, Steps: 3880, Current Learning Rate: 0.0009588, \u001b[91mTrain Loss: 1.220\n",
      "\u001b[0m\u001b[1mEpoch: [42/70], Progress: 72.04 %, Steps: 3881, Current Learning Rate: 0.0009591, \u001b[96mTrain Loss: 1.190\n",
      "\u001b[0m\u001b[1mEpoch: [42/70], Progress: 73.12 %, Steps: 3882, Current Learning Rate: 0.0009593, \u001b[96mTrain Loss: 1.085\n",
      "\u001b[0m\u001b[1mEpoch: [42/70], Progress: 74.19 %, Steps: 3883, Current Learning Rate: 0.0009596, \u001b[91mTrain Loss: 1.263\n",
      "\u001b[0m\u001b[1mEpoch: [42/70], Progress: 75.27 %, Steps: 3884, Current Learning Rate: 0.0009598, \u001b[91mTrain Loss: 1.305\n",
      "\u001b[0m\u001b[1mEpoch: [42/70], Progress: 76.34 %, Steps: 3885, Current Learning Rate: 0.0009600, \u001b[96mTrain Loss: 1.163\n",
      "\u001b[0m\u001b[1mEpoch: [42/70], Progress: 77.42 %, Steps: 3886, Current Learning Rate: 0.0009603, \u001b[91mTrain Loss: 1.272\n",
      "\u001b[0m\u001b[1mEpoch: [42/70], Progress: 78.49 %, Steps: 3887, Current Learning Rate: 0.0009605, \u001b[96mTrain Loss: 1.190\n",
      "\u001b[0m\u001b[1mEpoch: [42/70], Progress: 79.57 %, Steps: 3888, Current Learning Rate: 0.0009608, \u001b[96mTrain Loss: 1.152\n",
      "\u001b[0m\u001b[1mEpoch: [42/70], Progress: 80.65 %, Steps: 3889, Current Learning Rate: 0.0009610, \u001b[91mTrain Loss: 1.200\n",
      "\u001b[0m\u001b[1mEpoch: [42/70], Progress: 81.72 %, Steps: 3890, Current Learning Rate: 0.0009613, \u001b[91mTrain Loss: 1.306\n",
      "\u001b[0m\u001b[1mEpoch: [42/70], Progress: 82.80 %, Steps: 3891, Current Learning Rate: 0.0009615, \u001b[96mTrain Loss: 1.173\n",
      "\u001b[0m\u001b[1mEpoch: [42/70], Progress: 83.87 %, Steps: 3892, Current Learning Rate: 0.0009618, \u001b[91mTrain Loss: 1.267\n",
      "\u001b[0m\u001b[1mEpoch: [42/70], Progress: 84.95 %, Steps: 3893, Current Learning Rate: 0.0009620, \u001b[96mTrain Loss: 1.163\n",
      "\u001b[0m\u001b[1mEpoch: [42/70], Progress: 86.02 %, Steps: 3894, Current Learning Rate: 0.0009623, \u001b[96mTrain Loss: 1.104\n",
      "\u001b[0m\u001b[1mEpoch: [42/70], Progress: 87.10 %, Steps: 3895, Current Learning Rate: 0.0009625, \u001b[91mTrain Loss: 1.133\n",
      "\u001b[0m\u001b[1mEpoch: [42/70], Progress: 88.17 %, Steps: 3896, Current Learning Rate: 0.0009628, \u001b[91mTrain Loss: 1.249\n",
      "\u001b[0m\u001b[1mEpoch: [42/70], Progress: 89.25 %, Steps: 3897, Current Learning Rate: 0.0009630, \u001b[96mTrain Loss: 1.146\n",
      "\u001b[0m\u001b[1mEpoch: [42/70], Progress: 90.32 %, Steps: 3898, Current Learning Rate: 0.0009633, \u001b[91mTrain Loss: 1.212\n",
      "\u001b[0m\u001b[1mEpoch: [42/70], Progress: 91.40 %, Steps: 3899, Current Learning Rate: 0.0009635, \u001b[91mTrain Loss: 1.245\n",
      "\u001b[0m\u001b[1mEpoch: [42/70], Progress: 92.47 %, Steps: 3900, Current Learning Rate: 0.0009638, \u001b[96mTrain Loss: 1.151\n",
      "\u001b[0m\u001b[1mEpoch: [42/70], Progress: 93.55 %, Steps: 3901, Current Learning Rate: 0.0009640, \u001b[91mTrain Loss: 1.242\n",
      "\u001b[0m\u001b[1mEpoch: [42/70], Progress: 94.62 %, Steps: 3902, Current Learning Rate: 0.0009642, \u001b[96mTrain Loss: 1.221\n",
      "\u001b[0m\u001b[1mEpoch: [42/70], Progress: 95.70 %, Steps: 3903, Current Learning Rate: 0.0009645, \u001b[96mTrain Loss: 1.201\n",
      "\u001b[0m\u001b[1mEpoch: [42/70], Progress: 96.77 %, Steps: 3904, Current Learning Rate: 0.0009647, \u001b[91mTrain Loss: 1.282\n",
      "\u001b[0m\u001b[1mEpoch: [42/70], Progress: 97.85 %, Steps: 3905, Current Learning Rate: 0.0009650, \u001b[91mTrain Loss: 1.301\n",
      "\u001b[0m\u001b[1mEpoch: [42/70], Progress: 98.92 %, Steps: 3906, Current Learning Rate: 0.0009652, \u001b[96mTrain Loss: 1.187\n",
      "\u001b[1m\u001b[93mComputing Validation Loss...\n",
      "\u001b[1mEpoch 42 Completed! Average Train Loss: 1.131, Average Validation Loss: 0.535\n",
      "Best Model saved in best_model/best_model.pt.\n",
      "\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m= Epoch [43/70] ==========================================\n",
      "\u001b[0m\u001b[1mEpoch: [43/70], Progress: 0.00 %, Steps: 3907, Current Learning Rate: 0.0009655, \u001b[91mTrain Loss: 0.873\n",
      "\u001b[0m\u001b[1mEpoch: [43/70], Progress: 1.08 %, Steps: 3908, Current Learning Rate: 0.0009657, \u001b[91mTrain Loss: 0.956\n",
      "\u001b[0m\u001b[1mEpoch: [43/70], Progress: 2.15 %, Steps: 3909, Current Learning Rate: 0.0009660, \u001b[91mTrain Loss: 0.963\n",
      "\u001b[0m\u001b[1mEpoch: [43/70], Progress: 3.23 %, Steps: 3910, Current Learning Rate: 0.0009662, \u001b[96mTrain Loss: 0.932\n",
      "\u001b[0m\u001b[1mEpoch: [43/70], Progress: 4.30 %, Steps: 3911, Current Learning Rate: 0.0009665, \u001b[96mTrain Loss: 0.813\n",
      "\u001b[0m\u001b[1mEpoch: [43/70], Progress: 5.38 %, Steps: 3912, Current Learning Rate: 0.0009667, \u001b[91mTrain Loss: 0.908\n",
      "\u001b[0m\u001b[1mEpoch: [43/70], Progress: 6.45 %, Steps: 3913, Current Learning Rate: 0.0009670, \u001b[96mTrain Loss: 0.864\n",
      "\u001b[0m\u001b[1mEpoch: [43/70], Progress: 7.53 %, Steps: 3914, Current Learning Rate: 0.0009672, \u001b[96mTrain Loss: 0.802\n",
      "\u001b[0m\u001b[1mEpoch: [43/70], Progress: 8.60 %, Steps: 3915, Current Learning Rate: 0.0009675, \u001b[91mTrain Loss: 0.976\n",
      "\u001b[0m\u001b[1mEpoch: [43/70], Progress: 9.68 %, Steps: 3916, Current Learning Rate: 0.0009677, \u001b[91mTrain Loss: 1.061\n",
      "\u001b[0m\u001b[1mEpoch: [43/70], Progress: 10.75 %, Steps: 3917, Current Learning Rate: 0.0009680, \u001b[96mTrain Loss: 0.954\n",
      "\u001b[0m\u001b[1mEpoch: [43/70], Progress: 11.83 %, Steps: 3918, Current Learning Rate: 0.0009682, \u001b[91mTrain Loss: 0.995\n",
      "\u001b[0m\u001b[1mEpoch: [43/70], Progress: 12.90 %, Steps: 3919, Current Learning Rate: 0.0009684, \u001b[91mTrain Loss: 1.050\n",
      "\u001b[0m\u001b[1mEpoch: [43/70], Progress: 13.98 %, Steps: 3920, Current Learning Rate: 0.0009687, \u001b[96mTrain Loss: 0.976\n",
      "\u001b[0m\u001b[1mEpoch: [43/70], Progress: 15.05 %, Steps: 3921, Current Learning Rate: 0.0009689, \u001b[96mTrain Loss: 0.848\n",
      "\u001b[0m\u001b[1mEpoch: [43/70], Progress: 16.13 %, Steps: 3922, Current Learning Rate: 0.0009692, \u001b[96mTrain Loss: 0.841\n",
      "\u001b[0m\u001b[1mEpoch: [43/70], Progress: 17.20 %, Steps: 3923, Current Learning Rate: 0.0009694, \u001b[91mTrain Loss: 1.125\n",
      "\u001b[0m\u001b[1mEpoch: [43/70], Progress: 18.28 %, Steps: 3924, Current Learning Rate: 0.0009697, \u001b[96mTrain Loss: 0.905\n",
      "\u001b[0m\u001b[1mEpoch: [43/70], Progress: 19.35 %, Steps: 3925, Current Learning Rate: 0.0009699, \u001b[91mTrain Loss: 1.041\n",
      "\u001b[0m\u001b[1mEpoch: [43/70], Progress: 20.43 %, Steps: 3926, Current Learning Rate: 0.0009702, \u001b[96mTrain Loss: 0.983\n",
      "\u001b[0m\u001b[1mEpoch: [43/70], Progress: 21.51 %, Steps: 3927, Current Learning Rate: 0.0009704, \u001b[91mTrain Loss: 1.063\n",
      "\u001b[0m\u001b[1mEpoch: [43/70], Progress: 22.58 %, Steps: 3928, Current Learning Rate: 0.0009707, \u001b[91mTrain Loss: 1.083\n",
      "\u001b[0m\u001b[1mEpoch: [43/70], Progress: 23.66 %, Steps: 3929, Current Learning Rate: 0.0009709, \u001b[96mTrain Loss: 0.889\n",
      "\u001b[0m\u001b[1mEpoch: [43/70], Progress: 24.73 %, Steps: 3930, Current Learning Rate: 0.0009712, \u001b[91mTrain Loss: 0.915\n",
      "\u001b[0m\u001b[1mEpoch: [43/70], Progress: 25.81 %, Steps: 3931, Current Learning Rate: 0.0009714, \u001b[91mTrain Loss: 0.917\n",
      "\u001b[0m\u001b[1mEpoch: [43/70], Progress: 26.88 %, Steps: 3932, Current Learning Rate: 0.0009717, \u001b[91mTrain Loss: 0.940\n",
      "\u001b[0m\u001b[1mEpoch: [43/70], Progress: 27.96 %, Steps: 3933, Current Learning Rate: 0.0009719, \u001b[91mTrain Loss: 1.024\n",
      "\u001b[0m\u001b[1mEpoch: [43/70], Progress: 29.03 %, Steps: 3934, Current Learning Rate: 0.0009722, \u001b[96mTrain Loss: 0.952\n",
      "\u001b[0m\u001b[1mEpoch: [43/70], Progress: 30.11 %, Steps: 3935, Current Learning Rate: 0.0009724, \u001b[91mTrain Loss: 0.982\n",
      "\u001b[0m\u001b[1mEpoch: [43/70], Progress: 31.18 %, Steps: 3936, Current Learning Rate: 0.0009726, \u001b[96mTrain Loss: 0.943\n",
      "\u001b[0m\u001b[1mEpoch: [43/70], Progress: 32.26 %, Steps: 3937, Current Learning Rate: 0.0009729, \u001b[91mTrain Loss: 1.053\n",
      "\u001b[0m\u001b[1mEpoch: [43/70], Progress: 33.33 %, Steps: 3938, Current Learning Rate: 0.0009731, \u001b[96mTrain Loss: 0.924\n",
      "\u001b[0m\u001b[1mEpoch: [43/70], Progress: 34.41 %, Steps: 3939, Current Learning Rate: 0.0009734, \u001b[91mTrain Loss: 1.075\n",
      "\u001b[0m\u001b[1mEpoch: [43/70], Progress: 35.48 %, Steps: 3940, Current Learning Rate: 0.0009736, \u001b[96mTrain Loss: 0.961\n",
      "\u001b[0m\u001b[1mEpoch: [43/70], Progress: 36.56 %, Steps: 3941, Current Learning Rate: 0.0009739, \u001b[91mTrain Loss: 1.136\n",
      "\u001b[0m\u001b[1mEpoch: [43/70], Progress: 37.63 %, Steps: 3942, Current Learning Rate: 0.0009741, \u001b[96mTrain Loss: 0.978\n",
      "\u001b[0m\u001b[1mEpoch: [43/70], Progress: 38.71 %, Steps: 3943, Current Learning Rate: 0.0009744, \u001b[91mTrain Loss: 1.036\n",
      "\u001b[0m\u001b[1mEpoch: [43/70], Progress: 39.78 %, Steps: 3944, Current Learning Rate: 0.0009746, \u001b[91mTrain Loss: 1.177\n",
      "\u001b[0m\u001b[1mEpoch: [43/70], Progress: 40.86 %, Steps: 3945, Current Learning Rate: 0.0009749, \u001b[96mTrain Loss: 1.115\n",
      "\u001b[0m\u001b[1mEpoch: [43/70], Progress: 41.94 %, Steps: 3946, Current Learning Rate: 0.0009751, \u001b[96mTrain Loss: 1.039\n",
      "\u001b[0m\u001b[1mEpoch: [43/70], Progress: 43.01 %, Steps: 3947, Current Learning Rate: 0.0009754, \u001b[96mTrain Loss: 1.016\n",
      "\u001b[0m\u001b[1mEpoch: [43/70], Progress: 44.09 %, Steps: 3948, Current Learning Rate: 0.0009756, \u001b[91mTrain Loss: 1.139\n",
      "\u001b[0m\u001b[1mEpoch: [43/70], Progress: 45.16 %, Steps: 3949, Current Learning Rate: 0.0009759, \u001b[91mTrain Loss: 1.141\n",
      "\u001b[0m\u001b[1mEpoch: [43/70], Progress: 46.24 %, Steps: 3950, Current Learning Rate: 0.0009761, \u001b[96mTrain Loss: 1.038\n",
      "\u001b[0m\u001b[1mEpoch: [43/70], Progress: 47.31 %, Steps: 3951, Current Learning Rate: 0.0009764, \u001b[91mTrain Loss: 1.047\n",
      "\u001b[0m\u001b[1mEpoch: [43/70], Progress: 48.39 %, Steps: 3952, Current Learning Rate: 0.0009766, \u001b[96mTrain Loss: 1.033\n",
      "\u001b[0m\u001b[1mEpoch: [43/70], Progress: 49.46 %, Steps: 3953, Current Learning Rate: 0.0009768, \u001b[96mTrain Loss: 0.957\n",
      "\u001b[0m\u001b[1mEpoch: [43/70], Progress: 50.54 %, Steps: 3954, Current Learning Rate: 0.0009771, \u001b[91mTrain Loss: 1.056\n",
      "\u001b[0m\u001b[1mEpoch: [43/70], Progress: 51.61 %, Steps: 3955, Current Learning Rate: 0.0009773, \u001b[96mTrain Loss: 0.956\n",
      "\u001b[0m\u001b[1mEpoch: [43/70], Progress: 52.69 %, Steps: 3956, Current Learning Rate: 0.0009776, \u001b[91mTrain Loss: 1.231\n",
      "\u001b[0m\u001b[1mEpoch: [43/70], Progress: 53.76 %, Steps: 3957, Current Learning Rate: 0.0009778, \u001b[96mTrain Loss: 1.106\n",
      "\u001b[0m\u001b[1mEpoch: [43/70], Progress: 54.84 %, Steps: 3958, Current Learning Rate: 0.0009781, \u001b[96mTrain Loss: 1.085\n",
      "\u001b[0m\u001b[1mEpoch: [43/70], Progress: 55.91 %, Steps: 3959, Current Learning Rate: 0.0009783, \u001b[91mTrain Loss: 1.122\n",
      "\u001b[0m\u001b[1mEpoch: [43/70], Progress: 56.99 %, Steps: 3960, Current Learning Rate: 0.0009786, \u001b[96mTrain Loss: 1.108\n",
      "\u001b[0m\u001b[1mEpoch: [43/70], Progress: 58.06 %, Steps: 3961, Current Learning Rate: 0.0009788, \u001b[96mTrain Loss: 1.076\n",
      "\u001b[0m\u001b[1mEpoch: [43/70], Progress: 59.14 %, Steps: 3962, Current Learning Rate: 0.0009791, \u001b[91mTrain Loss: 1.105\n",
      "\u001b[0m\u001b[1mEpoch: [43/70], Progress: 60.22 %, Steps: 3963, Current Learning Rate: 0.0009793, \u001b[96mTrain Loss: 1.088\n",
      "\u001b[0m\u001b[1mEpoch: [43/70], Progress: 61.29 %, Steps: 3964, Current Learning Rate: 0.0009796, \u001b[96mTrain Loss: 1.080\n",
      "\u001b[0m\u001b[1mEpoch: [43/70], Progress: 62.37 %, Steps: 3965, Current Learning Rate: 0.0009798, \u001b[96mTrain Loss: 0.961\n",
      "\u001b[0m\u001b[1mEpoch: [43/70], Progress: 63.44 %, Steps: 3966, Current Learning Rate: 0.0009801, \u001b[96mTrain Loss: 0.954\n",
      "\u001b[0m\u001b[1mEpoch: [43/70], Progress: 64.52 %, Steps: 3967, Current Learning Rate: 0.0009803, \u001b[91mTrain Loss: 1.111\n",
      "\u001b[0m\u001b[1mEpoch: [43/70], Progress: 65.59 %, Steps: 3968, Current Learning Rate: 0.0009806, \u001b[96mTrain Loss: 1.057\n",
      "\u001b[0m\u001b[1mEpoch: [43/70], Progress: 66.67 %, Steps: 3969, Current Learning Rate: 0.0009808, \u001b[96mTrain Loss: 0.925\n",
      "\u001b[0m\u001b[1mEpoch: [43/70], Progress: 67.74 %, Steps: 3970, Current Learning Rate: 0.0009810, \u001b[91mTrain Loss: 1.148\n",
      "\u001b[0m\u001b[1mEpoch: [43/70], Progress: 68.82 %, Steps: 3971, Current Learning Rate: 0.0009813, \u001b[96mTrain Loss: 1.036\n",
      "\u001b[0m\u001b[1mEpoch: [43/70], Progress: 69.89 %, Steps: 3972, Current Learning Rate: 0.0009815, \u001b[91mTrain Loss: 1.118\n",
      "\u001b[0m\u001b[1mEpoch: [43/70], Progress: 70.97 %, Steps: 3973, Current Learning Rate: 0.0009818, \u001b[96mTrain Loss: 1.112\n",
      "\u001b[0m\u001b[1mEpoch: [43/70], Progress: 72.04 %, Steps: 3974, Current Learning Rate: 0.0009820, \u001b[96mTrain Loss: 1.079\n",
      "\u001b[0m\u001b[1mEpoch: [43/70], Progress: 73.12 %, Steps: 3975, Current Learning Rate: 0.0009823, \u001b[91mTrain Loss: 1.118\n",
      "\u001b[0m\u001b[1mEpoch: [43/70], Progress: 74.19 %, Steps: 3976, Current Learning Rate: 0.0009825, \u001b[91mTrain Loss: 1.125\n",
      "\u001b[0m\u001b[1mEpoch: [43/70], Progress: 75.27 %, Steps: 3977, Current Learning Rate: 0.0009828, \u001b[91mTrain Loss: 1.134\n",
      "\u001b[0m\u001b[1mEpoch: [43/70], Progress: 76.34 %, Steps: 3978, Current Learning Rate: 0.0009830, \u001b[96mTrain Loss: 1.096\n",
      "\u001b[0m\u001b[1mEpoch: [43/70], Progress: 77.42 %, Steps: 3979, Current Learning Rate: 0.0009833, \u001b[96mTrain Loss: 0.942\n",
      "\u001b[0m\u001b[1mEpoch: [43/70], Progress: 78.49 %, Steps: 3980, Current Learning Rate: 0.0009835, \u001b[91mTrain Loss: 1.080\n",
      "\u001b[0m\u001b[1mEpoch: [43/70], Progress: 79.57 %, Steps: 3981, Current Learning Rate: 0.0009838, \u001b[96mTrain Loss: 1.053\n",
      "\u001b[0m\u001b[1mEpoch: [43/70], Progress: 80.65 %, Steps: 3982, Current Learning Rate: 0.0009840, \u001b[96mTrain Loss: 1.005\n",
      "\u001b[0m\u001b[1mEpoch: [43/70], Progress: 81.72 %, Steps: 3983, Current Learning Rate: 0.0009843, \u001b[91mTrain Loss: 1.177\n",
      "\u001b[0m\u001b[1mEpoch: [43/70], Progress: 82.80 %, Steps: 3984, Current Learning Rate: 0.0009845, \u001b[91mTrain Loss: 1.251\n",
      "\u001b[0m\u001b[1mEpoch: [43/70], Progress: 83.87 %, Steps: 3985, Current Learning Rate: 0.0009848, \u001b[96mTrain Loss: 1.029\n",
      "\u001b[0m\u001b[1mEpoch: [43/70], Progress: 84.95 %, Steps: 3986, Current Learning Rate: 0.0009850, \u001b[91mTrain Loss: 1.127\n",
      "\u001b[0m\u001b[1mEpoch: [43/70], Progress: 86.02 %, Steps: 3987, Current Learning Rate: 0.0009852, \u001b[91mTrain Loss: 1.320\n",
      "\u001b[0m\u001b[1mEpoch: [43/70], Progress: 87.10 %, Steps: 3988, Current Learning Rate: 0.0009855, \u001b[96mTrain Loss: 1.145\n",
      "\u001b[0m\u001b[1mEpoch: [43/70], Progress: 88.17 %, Steps: 3989, Current Learning Rate: 0.0009857, \u001b[91mTrain Loss: 1.171\n",
      "\u001b[0m\u001b[1mEpoch: [43/70], Progress: 89.25 %, Steps: 3990, Current Learning Rate: 0.0009860, \u001b[96mTrain Loss: 1.071\n",
      "\u001b[0m\u001b[1mEpoch: [43/70], Progress: 90.32 %, Steps: 3991, Current Learning Rate: 0.0009862, \u001b[91mTrain Loss: 1.075\n",
      "\u001b[0m\u001b[1mEpoch: [43/70], Progress: 91.40 %, Steps: 3992, Current Learning Rate: 0.0009865, \u001b[91mTrain Loss: 1.159\n",
      "\u001b[0m\u001b[1mEpoch: [43/70], Progress: 92.47 %, Steps: 3993, Current Learning Rate: 0.0009867, \u001b[96mTrain Loss: 1.126\n",
      "\u001b[0m\u001b[1mEpoch: [43/70], Progress: 93.55 %, Steps: 3994, Current Learning Rate: 0.0009870, \u001b[91mTrain Loss: 1.165\n",
      "\u001b[0m\u001b[1mEpoch: [43/70], Progress: 94.62 %, Steps: 3995, Current Learning Rate: 0.0009872, \u001b[91mTrain Loss: 1.185\n",
      "\u001b[0m\u001b[1mEpoch: [43/70], Progress: 95.70 %, Steps: 3996, Current Learning Rate: 0.0009875, \u001b[96mTrain Loss: 1.043\n",
      "\u001b[0m\u001b[1mEpoch: [43/70], Progress: 96.77 %, Steps: 3997, Current Learning Rate: 0.0009877, \u001b[96mTrain Loss: 0.983\n",
      "\u001b[0m\u001b[1mEpoch: [43/70], Progress: 97.85 %, Steps: 3998, Current Learning Rate: 0.0009880, \u001b[91mTrain Loss: 1.198\n",
      "\u001b[0m\u001b[1mEpoch: [43/70], Progress: 98.92 %, Steps: 3999, Current Learning Rate: 0.0009882, \u001b[96mTrain Loss: 1.110\n",
      "\u001b[1m\u001b[93mComputing Validation Loss...\n",
      "\u001b[1mEpoch 43 Completed! Average Train Loss: 1.041, Average Validation Loss: 0.457\n",
      "Best Model saved in best_model/best_model.pt.\n",
      "\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m= Epoch [44/70] ==========================================\n",
      "\u001b[0m\u001b[1mEpoch: [44/70], Progress: 0.00 %, Steps: 4000, Current Learning Rate: 0.0009881, \u001b[91mTrain Loss: 0.838\n",
      "\u001b[0m\u001b[1mEpoch: [44/70], Progress: 1.08 %, Steps: 4001, Current Learning Rate: 0.0009880, \u001b[91mTrain Loss: 0.916\n",
      "\u001b[0m\u001b[1mEpoch: [44/70], Progress: 2.15 %, Steps: 4002, Current Learning Rate: 0.0009878, \u001b[96mTrain Loss: 0.836\n",
      "\u001b[0m\u001b[1mEpoch: [44/70], Progress: 3.23 %, Steps: 4003, Current Learning Rate: 0.0009877, \u001b[91mTrain Loss: 0.930\n",
      "\u001b[0m\u001b[1mEpoch: [44/70], Progress: 4.30 %, Steps: 4004, Current Learning Rate: 0.0009876, \u001b[96mTrain Loss: 0.808\n",
      "\u001b[0m\u001b[1mEpoch: [44/70], Progress: 5.38 %, Steps: 4005, Current Learning Rate: 0.0009875, \u001b[96mTrain Loss: 0.754\n",
      "\u001b[0m\u001b[1mEpoch: [44/70], Progress: 6.45 %, Steps: 4006, Current Learning Rate: 0.0009873, \u001b[91mTrain Loss: 0.871\n",
      "\u001b[0m\u001b[1mEpoch: [44/70], Progress: 7.53 %, Steps: 4007, Current Learning Rate: 0.0009872, \u001b[96mTrain Loss: 0.818\n",
      "\u001b[0m\u001b[1mEpoch: [44/70], Progress: 8.60 %, Steps: 4008, Current Learning Rate: 0.0009871, \u001b[91mTrain Loss: 0.856\n",
      "\u001b[0m\u001b[1mEpoch: [44/70], Progress: 9.68 %, Steps: 4009, Current Learning Rate: 0.0009870, \u001b[91mTrain Loss: 0.897\n",
      "\u001b[0m\u001b[1mEpoch: [44/70], Progress: 10.75 %, Steps: 4010, Current Learning Rate: 0.0009869, \u001b[96mTrain Loss: 0.775\n",
      "\u001b[0m\u001b[1mEpoch: [44/70], Progress: 11.83 %, Steps: 4011, Current Learning Rate: 0.0009867, \u001b[91mTrain Loss: 0.802\n",
      "\u001b[0m\u001b[1mEpoch: [44/70], Progress: 12.90 %, Steps: 4012, Current Learning Rate: 0.0009866, \u001b[91mTrain Loss: 0.879\n",
      "\u001b[0m\u001b[1mEpoch: [44/70], Progress: 13.98 %, Steps: 4013, Current Learning Rate: 0.0009865, \u001b[91mTrain Loss: 0.947\n",
      "\u001b[0m\u001b[1mEpoch: [44/70], Progress: 15.05 %, Steps: 4014, Current Learning Rate: 0.0009864, \u001b[96mTrain Loss: 0.882\n",
      "\u001b[0m\u001b[1mEpoch: [44/70], Progress: 16.13 %, Steps: 4015, Current Learning Rate: 0.0009862, \u001b[96mTrain Loss: 0.814\n",
      "\u001b[0m\u001b[1mEpoch: [44/70], Progress: 17.20 %, Steps: 4016, Current Learning Rate: 0.0009861, \u001b[91mTrain Loss: 0.893\n",
      "\u001b[0m\u001b[1mEpoch: [44/70], Progress: 18.28 %, Steps: 4017, Current Learning Rate: 0.0009860, \u001b[91mTrain Loss: 0.955\n",
      "\u001b[0m\u001b[1mEpoch: [44/70], Progress: 19.35 %, Steps: 4018, Current Learning Rate: 0.0009859, \u001b[96mTrain Loss: 0.893\n",
      "\u001b[0m\u001b[1mEpoch: [44/70], Progress: 20.43 %, Steps: 4019, Current Learning Rate: 0.0009858, \u001b[91mTrain Loss: 0.954\n",
      "\u001b[0m\u001b[1mEpoch: [44/70], Progress: 21.51 %, Steps: 4020, Current Learning Rate: 0.0009856, \u001b[96mTrain Loss: 0.940\n",
      "\u001b[0m\u001b[1mEpoch: [44/70], Progress: 22.58 %, Steps: 4021, Current Learning Rate: 0.0009855, \u001b[91mTrain Loss: 1.015\n",
      "\u001b[0m\u001b[1mEpoch: [44/70], Progress: 23.66 %, Steps: 4022, Current Learning Rate: 0.0009854, \u001b[96mTrain Loss: 0.932\n",
      "\u001b[0m\u001b[1mEpoch: [44/70], Progress: 24.73 %, Steps: 4023, Current Learning Rate: 0.0009853, \u001b[96mTrain Loss: 0.906\n",
      "\u001b[0m\u001b[1mEpoch: [44/70], Progress: 25.81 %, Steps: 4024, Current Learning Rate: 0.0009851, \u001b[91mTrain Loss: 0.922\n",
      "\u001b[0m\u001b[1mEpoch: [44/70], Progress: 26.88 %, Steps: 4025, Current Learning Rate: 0.0009850, \u001b[91mTrain Loss: 0.997\n",
      "\u001b[0m\u001b[1mEpoch: [44/70], Progress: 27.96 %, Steps: 4026, Current Learning Rate: 0.0009849, \u001b[96mTrain Loss: 0.992\n",
      "\u001b[0m\u001b[1mEpoch: [44/70], Progress: 29.03 %, Steps: 4027, Current Learning Rate: 0.0009848, \u001b[96mTrain Loss: 0.943\n",
      "\u001b[0m\u001b[1mEpoch: [44/70], Progress: 30.11 %, Steps: 4028, Current Learning Rate: 0.0009846, \u001b[91mTrain Loss: 0.950\n",
      "\u001b[0m\u001b[1mEpoch: [44/70], Progress: 31.18 %, Steps: 4029, Current Learning Rate: 0.0009845, \u001b[96mTrain Loss: 0.920\n",
      "\u001b[0m\u001b[1mEpoch: [44/70], Progress: 32.26 %, Steps: 4030, Current Learning Rate: 0.0009844, \u001b[91mTrain Loss: 0.927\n",
      "\u001b[0m\u001b[1mEpoch: [44/70], Progress: 33.33 %, Steps: 4031, Current Learning Rate: 0.0009843, \u001b[96mTrain Loss: 0.857\n",
      "\u001b[0m\u001b[1mEpoch: [44/70], Progress: 34.41 %, Steps: 4032, Current Learning Rate: 0.0009842, \u001b[91mTrain Loss: 0.947\n",
      "\u001b[0m\u001b[1mEpoch: [44/70], Progress: 35.48 %, Steps: 4033, Current Learning Rate: 0.0009840, \u001b[96mTrain Loss: 0.846\n",
      "\u001b[0m\u001b[1mEpoch: [44/70], Progress: 36.56 %, Steps: 4034, Current Learning Rate: 0.0009839, \u001b[91mTrain Loss: 1.046\n",
      "\u001b[0m\u001b[1mEpoch: [44/70], Progress: 37.63 %, Steps: 4035, Current Learning Rate: 0.0009838, \u001b[96mTrain Loss: 0.912\n",
      "\u001b[0m\u001b[1mEpoch: [44/70], Progress: 38.71 %, Steps: 4036, Current Learning Rate: 0.0009837, \u001b[91mTrain Loss: 0.954\n",
      "\u001b[0m\u001b[1mEpoch: [44/70], Progress: 39.78 %, Steps: 4037, Current Learning Rate: 0.0009836, \u001b[96mTrain Loss: 0.823\n",
      "\u001b[0m\u001b[1mEpoch: [44/70], Progress: 40.86 %, Steps: 4038, Current Learning Rate: 0.0009834, \u001b[91mTrain Loss: 0.880\n",
      "\u001b[0m\u001b[1mEpoch: [44/70], Progress: 41.94 %, Steps: 4039, Current Learning Rate: 0.0009833, \u001b[96mTrain Loss: 0.787\n",
      "\u001b[0m\u001b[1mEpoch: [44/70], Progress: 43.01 %, Steps: 4040, Current Learning Rate: 0.0009832, \u001b[91mTrain Loss: 1.008\n",
      "\u001b[0m\u001b[1mEpoch: [44/70], Progress: 44.09 %, Steps: 4041, Current Learning Rate: 0.0009831, \u001b[96mTrain Loss: 0.914\n",
      "\u001b[0m\u001b[1mEpoch: [44/70], Progress: 45.16 %, Steps: 4042, Current Learning Rate: 0.0009829, \u001b[91mTrain Loss: 0.998\n",
      "\u001b[0m\u001b[1mEpoch: [44/70], Progress: 46.24 %, Steps: 4043, Current Learning Rate: 0.0009828, \u001b[96mTrain Loss: 0.922\n",
      "\u001b[0m\u001b[1mEpoch: [44/70], Progress: 47.31 %, Steps: 4044, Current Learning Rate: 0.0009827, \u001b[91mTrain Loss: 1.042\n",
      "\u001b[0m\u001b[1mEpoch: [44/70], Progress: 48.39 %, Steps: 4045, Current Learning Rate: 0.0009826, \u001b[96mTrain Loss: 1.013\n",
      "\u001b[0m\u001b[1mEpoch: [44/70], Progress: 49.46 %, Steps: 4046, Current Learning Rate: 0.0009825, \u001b[96mTrain Loss: 0.958\n",
      "\u001b[0m\u001b[1mEpoch: [44/70], Progress: 50.54 %, Steps: 4047, Current Learning Rate: 0.0009823, \u001b[96mTrain Loss: 0.937\n",
      "\u001b[0m\u001b[1mEpoch: [44/70], Progress: 51.61 %, Steps: 4048, Current Learning Rate: 0.0009822, \u001b[91mTrain Loss: 0.938\n",
      "\u001b[0m\u001b[1mEpoch: [44/70], Progress: 52.69 %, Steps: 4049, Current Learning Rate: 0.0009821, \u001b[91mTrain Loss: 0.943\n",
      "\u001b[0m\u001b[1mEpoch: [44/70], Progress: 53.76 %, Steps: 4050, Current Learning Rate: 0.0009820, \u001b[96mTrain Loss: 0.889\n",
      "\u001b[0m\u001b[1mEpoch: [44/70], Progress: 54.84 %, Steps: 4051, Current Learning Rate: 0.0009819, \u001b[91mTrain Loss: 1.055\n",
      "\u001b[0m\u001b[1mEpoch: [44/70], Progress: 55.91 %, Steps: 4052, Current Learning Rate: 0.0009817, \u001b[96mTrain Loss: 0.954\n",
      "\u001b[0m\u001b[1mEpoch: [44/70], Progress: 56.99 %, Steps: 4053, Current Learning Rate: 0.0009816, \u001b[91mTrain Loss: 0.968\n",
      "\u001b[0m\u001b[1mEpoch: [44/70], Progress: 58.06 %, Steps: 4054, Current Learning Rate: 0.0009815, \u001b[91mTrain Loss: 0.992\n",
      "\u001b[0m\u001b[1mEpoch: [44/70], Progress: 59.14 %, Steps: 4055, Current Learning Rate: 0.0009814, \u001b[91mTrain Loss: 0.994\n",
      "\u001b[0m\u001b[1mEpoch: [44/70], Progress: 60.22 %, Steps: 4056, Current Learning Rate: 0.0009812, \u001b[96mTrain Loss: 0.891\n",
      "\u001b[0m\u001b[1mEpoch: [44/70], Progress: 61.29 %, Steps: 4057, Current Learning Rate: 0.0009811, \u001b[96mTrain Loss: 0.858\n",
      "\u001b[0m\u001b[1mEpoch: [44/70], Progress: 62.37 %, Steps: 4058, Current Learning Rate: 0.0009810, \u001b[91mTrain Loss: 0.929\n",
      "\u001b[0m\u001b[1mEpoch: [44/70], Progress: 63.44 %, Steps: 4059, Current Learning Rate: 0.0009809, \u001b[91mTrain Loss: 1.050\n",
      "\u001b[0m\u001b[1mEpoch: [44/70], Progress: 64.52 %, Steps: 4060, Current Learning Rate: 0.0009808, \u001b[91mTrain Loss: 1.086\n",
      "\u001b[0m\u001b[1mEpoch: [44/70], Progress: 65.59 %, Steps: 4061, Current Learning Rate: 0.0009806, \u001b[96mTrain Loss: 0.827\n",
      "\u001b[0m\u001b[1mEpoch: [44/70], Progress: 66.67 %, Steps: 4062, Current Learning Rate: 0.0009805, \u001b[91mTrain Loss: 1.034\n",
      "\u001b[0m\u001b[1mEpoch: [44/70], Progress: 67.74 %, Steps: 4063, Current Learning Rate: 0.0009804, \u001b[96mTrain Loss: 0.927\n",
      "\u001b[0m\u001b[1mEpoch: [44/70], Progress: 68.82 %, Steps: 4064, Current Learning Rate: 0.0009803, \u001b[96mTrain Loss: 0.911\n",
      "\u001b[0m\u001b[1mEpoch: [44/70], Progress: 69.89 %, Steps: 4065, Current Learning Rate: 0.0009802, \u001b[91mTrain Loss: 1.011\n",
      "\u001b[0m\u001b[1mEpoch: [44/70], Progress: 70.97 %, Steps: 4066, Current Learning Rate: 0.0009800, \u001b[96mTrain Loss: 0.995\n",
      "\u001b[0m\u001b[1mEpoch: [44/70], Progress: 72.04 %, Steps: 4067, Current Learning Rate: 0.0009799, \u001b[96mTrain Loss: 0.970\n",
      "\u001b[0m\u001b[1mEpoch: [44/70], Progress: 73.12 %, Steps: 4068, Current Learning Rate: 0.0009798, \u001b[91mTrain Loss: 0.990\n",
      "\u001b[0m\u001b[1mEpoch: [44/70], Progress: 74.19 %, Steps: 4069, Current Learning Rate: 0.0009797, \u001b[96mTrain Loss: 0.925\n",
      "\u001b[0m\u001b[1mEpoch: [44/70], Progress: 75.27 %, Steps: 4070, Current Learning Rate: 0.0009796, \u001b[91mTrain Loss: 1.080\n",
      "\u001b[0m\u001b[1mEpoch: [44/70], Progress: 76.34 %, Steps: 4071, Current Learning Rate: 0.0009794, \u001b[96mTrain Loss: 1.015\n",
      "\u001b[0m\u001b[1mEpoch: [44/70], Progress: 77.42 %, Steps: 4072, Current Learning Rate: 0.0009793, \u001b[96mTrain Loss: 1.001\n",
      "\u001b[0m\u001b[1mEpoch: [44/70], Progress: 78.49 %, Steps: 4073, Current Learning Rate: 0.0009792, \u001b[91mTrain Loss: 1.094\n",
      "\u001b[0m\u001b[1mEpoch: [44/70], Progress: 79.57 %, Steps: 4074, Current Learning Rate: 0.0009791, \u001b[91mTrain Loss: 1.095\n",
      "\u001b[0m\u001b[1mEpoch: [44/70], Progress: 80.65 %, Steps: 4075, Current Learning Rate: 0.0009790, \u001b[96mTrain Loss: 1.012\n",
      "\u001b[0m\u001b[1mEpoch: [44/70], Progress: 81.72 %, Steps: 4076, Current Learning Rate: 0.0009788, \u001b[91mTrain Loss: 1.027\n",
      "\u001b[0m\u001b[1mEpoch: [44/70], Progress: 82.80 %, Steps: 4077, Current Learning Rate: 0.0009787, \u001b[91mTrain Loss: 1.077\n",
      "\u001b[0m\u001b[1mEpoch: [44/70], Progress: 83.87 %, Steps: 4078, Current Learning Rate: 0.0009786, \u001b[96mTrain Loss: 0.958\n",
      "\u001b[0m\u001b[1mEpoch: [44/70], Progress: 84.95 %, Steps: 4079, Current Learning Rate: 0.0009785, \u001b[91mTrain Loss: 1.004\n",
      "\u001b[0m\u001b[1mEpoch: [44/70], Progress: 86.02 %, Steps: 4080, Current Learning Rate: 0.0009784, \u001b[91mTrain Loss: 1.059\n",
      "\u001b[0m\u001b[1mEpoch: [44/70], Progress: 87.10 %, Steps: 4081, Current Learning Rate: 0.0009782, \u001b[96mTrain Loss: 0.939\n",
      "\u001b[0m\u001b[1mEpoch: [44/70], Progress: 88.17 %, Steps: 4082, Current Learning Rate: 0.0009781, \u001b[91mTrain Loss: 1.039\n",
      "\u001b[0m\u001b[1mEpoch: [44/70], Progress: 89.25 %, Steps: 4083, Current Learning Rate: 0.0009780, \u001b[96mTrain Loss: 1.000\n",
      "\u001b[0m\u001b[1mEpoch: [44/70], Progress: 90.32 %, Steps: 4084, Current Learning Rate: 0.0009779, \u001b[96mTrain Loss: 0.971\n",
      "\u001b[0m\u001b[1mEpoch: [44/70], Progress: 91.40 %, Steps: 4085, Current Learning Rate: 0.0009778, \u001b[91mTrain Loss: 1.042\n",
      "\u001b[0m\u001b[1mEpoch: [44/70], Progress: 92.47 %, Steps: 4086, Current Learning Rate: 0.0009776, \u001b[96mTrain Loss: 0.969\n",
      "\u001b[0m\u001b[1mEpoch: [44/70], Progress: 93.55 %, Steps: 4087, Current Learning Rate: 0.0009775, \u001b[91mTrain Loss: 1.117\n",
      "\u001b[0m\u001b[1mEpoch: [44/70], Progress: 94.62 %, Steps: 4088, Current Learning Rate: 0.0009774, \u001b[96mTrain Loss: 1.073\n",
      "\u001b[0m\u001b[1mEpoch: [44/70], Progress: 95.70 %, Steps: 4089, Current Learning Rate: 0.0009773, \u001b[96mTrain Loss: 1.068\n",
      "\u001b[0m\u001b[1mEpoch: [44/70], Progress: 96.77 %, Steps: 4090, Current Learning Rate: 0.0009772, \u001b[96mTrain Loss: 1.051\n",
      "\u001b[0m\u001b[1mEpoch: [44/70], Progress: 97.85 %, Steps: 4091, Current Learning Rate: 0.0009770, \u001b[96mTrain Loss: 0.990\n",
      "\u001b[0m\u001b[1mEpoch: [44/70], Progress: 98.92 %, Steps: 4092, Current Learning Rate: 0.0009769, \u001b[91mTrain Loss: 1.151\n",
      "\u001b[1m\u001b[93mComputing Validation Loss...\n",
      "\u001b[1mEpoch 44 Completed! Average Train Loss: 0.952, Average Validation Loss: 0.371\n",
      "Best Model saved in best_model/best_model.pt.\n",
      "\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m= Epoch [45/70] ==========================================\n",
      "\u001b[0m\u001b[1mEpoch: [45/70], Progress: 0.00 %, Steps: 4093, Current Learning Rate: 0.0009768, \u001b[91mTrain Loss: 0.775\n",
      "\u001b[0m\u001b[1mEpoch: [45/70], Progress: 1.08 %, Steps: 4094, Current Learning Rate: 0.0009767, \u001b[96mTrain Loss: 0.671\n",
      "\u001b[0m\u001b[1mEpoch: [45/70], Progress: 2.15 %, Steps: 4095, Current Learning Rate: 0.0009766, \u001b[91mTrain Loss: 0.703\n",
      "\u001b[0m\u001b[1mEpoch: [45/70], Progress: 3.23 %, Steps: 4096, Current Learning Rate: 0.0009764, \u001b[91mTrain Loss: 0.717\n",
      "\u001b[0m\u001b[1mEpoch: [45/70], Progress: 4.30 %, Steps: 4097, Current Learning Rate: 0.0009763, \u001b[91mTrain Loss: 0.820\n",
      "\u001b[0m\u001b[1mEpoch: [45/70], Progress: 5.38 %, Steps: 4098, Current Learning Rate: 0.0009762, \u001b[96mTrain Loss: 0.781\n",
      "\u001b[0m\u001b[1mEpoch: [45/70], Progress: 6.45 %, Steps: 4099, Current Learning Rate: 0.0009761, \u001b[96mTrain Loss: 0.760\n",
      "\u001b[0m\u001b[1mEpoch: [45/70], Progress: 7.53 %, Steps: 4100, Current Learning Rate: 0.0009760, \u001b[96mTrain Loss: 0.725\n",
      "\u001b[0m\u001b[1mEpoch: [45/70], Progress: 8.60 %, Steps: 4101, Current Learning Rate: 0.0009758, \u001b[91mTrain Loss: 0.816\n",
      "\u001b[0m\u001b[1mEpoch: [45/70], Progress: 9.68 %, Steps: 4102, Current Learning Rate: 0.0009757, \u001b[96mTrain Loss: 0.667\n",
      "\u001b[0m\u001b[1mEpoch: [45/70], Progress: 10.75 %, Steps: 4103, Current Learning Rate: 0.0009756, \u001b[91mTrain Loss: 0.777\n",
      "\u001b[0m\u001b[1mEpoch: [45/70], Progress: 11.83 %, Steps: 4104, Current Learning Rate: 0.0009755, \u001b[96mTrain Loss: 0.734\n",
      "\u001b[0m\u001b[1mEpoch: [45/70], Progress: 12.90 %, Steps: 4105, Current Learning Rate: 0.0009754, \u001b[96mTrain Loss: 0.710\n",
      "\u001b[0m\u001b[1mEpoch: [45/70], Progress: 13.98 %, Steps: 4106, Current Learning Rate: 0.0009753, \u001b[91mTrain Loss: 0.727\n",
      "\u001b[0m\u001b[1mEpoch: [45/70], Progress: 15.05 %, Steps: 4107, Current Learning Rate: 0.0009751, \u001b[91mTrain Loss: 0.742\n",
      "\u001b[0m\u001b[1mEpoch: [45/70], Progress: 16.13 %, Steps: 4108, Current Learning Rate: 0.0009750, \u001b[91mTrain Loss: 0.747\n",
      "\u001b[0m\u001b[1mEpoch: [45/70], Progress: 17.20 %, Steps: 4109, Current Learning Rate: 0.0009749, \u001b[96mTrain Loss: 0.679\n",
      "\u001b[0m\u001b[1mEpoch: [45/70], Progress: 18.28 %, Steps: 4110, Current Learning Rate: 0.0009748, \u001b[91mTrain Loss: 0.754\n",
      "\u001b[0m\u001b[1mEpoch: [45/70], Progress: 19.35 %, Steps: 4111, Current Learning Rate: 0.0009747, \u001b[91mTrain Loss: 0.790\n",
      "\u001b[0m\u001b[1mEpoch: [45/70], Progress: 20.43 %, Steps: 4112, Current Learning Rate: 0.0009745, \u001b[96mTrain Loss: 0.778\n",
      "\u001b[0m\u001b[1mEpoch: [45/70], Progress: 21.51 %, Steps: 4113, Current Learning Rate: 0.0009744, \u001b[91mTrain Loss: 0.894\n",
      "\u001b[0m\u001b[1mEpoch: [45/70], Progress: 22.58 %, Steps: 4114, Current Learning Rate: 0.0009743, \u001b[96mTrain Loss: 0.741\n",
      "\u001b[0m\u001b[1mEpoch: [45/70], Progress: 23.66 %, Steps: 4115, Current Learning Rate: 0.0009742, \u001b[91mTrain Loss: 0.802\n",
      "\u001b[0m\u001b[1mEpoch: [45/70], Progress: 24.73 %, Steps: 4116, Current Learning Rate: 0.0009741, \u001b[91mTrain Loss: 0.834\n",
      "\u001b[0m\u001b[1mEpoch: [45/70], Progress: 25.81 %, Steps: 4117, Current Learning Rate: 0.0009740, \u001b[96mTrain Loss: 0.805\n",
      "\u001b[0m\u001b[1mEpoch: [45/70], Progress: 26.88 %, Steps: 4118, Current Learning Rate: 0.0009738, \u001b[91mTrain Loss: 0.834\n",
      "\u001b[0m\u001b[1mEpoch: [45/70], Progress: 27.96 %, Steps: 4119, Current Learning Rate: 0.0009737, \u001b[96mTrain Loss: 0.712\n",
      "\u001b[0m\u001b[1mEpoch: [45/70], Progress: 29.03 %, Steps: 4120, Current Learning Rate: 0.0009736, \u001b[91mTrain Loss: 0.906\n",
      "\u001b[0m\u001b[1mEpoch: [45/70], Progress: 30.11 %, Steps: 4121, Current Learning Rate: 0.0009735, \u001b[91mTrain Loss: 0.911\n",
      "\u001b[0m\u001b[1mEpoch: [45/70], Progress: 31.18 %, Steps: 4122, Current Learning Rate: 0.0009734, \u001b[96mTrain Loss: 0.835\n",
      "\u001b[0m\u001b[1mEpoch: [45/70], Progress: 32.26 %, Steps: 4123, Current Learning Rate: 0.0009732, \u001b[91mTrain Loss: 0.875\n",
      "\u001b[0m\u001b[1mEpoch: [45/70], Progress: 33.33 %, Steps: 4124, Current Learning Rate: 0.0009731, \u001b[96mTrain Loss: 0.822\n",
      "\u001b[0m\u001b[1mEpoch: [45/70], Progress: 34.41 %, Steps: 4125, Current Learning Rate: 0.0009730, \u001b[96mTrain Loss: 0.741\n",
      "\u001b[0m\u001b[1mEpoch: [45/70], Progress: 35.48 %, Steps: 4126, Current Learning Rate: 0.0009729, \u001b[91mTrain Loss: 0.763\n",
      "\u001b[0m\u001b[1mEpoch: [45/70], Progress: 36.56 %, Steps: 4127, Current Learning Rate: 0.0009728, \u001b[91mTrain Loss: 0.895\n",
      "\u001b[0m\u001b[1mEpoch: [45/70], Progress: 37.63 %, Steps: 4128, Current Learning Rate: 0.0009727, \u001b[96mTrain Loss: 0.857\n",
      "\u001b[0m\u001b[1mEpoch: [45/70], Progress: 38.71 %, Steps: 4129, Current Learning Rate: 0.0009725, \u001b[96mTrain Loss: 0.812\n",
      "\u001b[0m\u001b[1mEpoch: [45/70], Progress: 39.78 %, Steps: 4130, Current Learning Rate: 0.0009724, \u001b[96mTrain Loss: 0.756\n",
      "\u001b[0m\u001b[1mEpoch: [45/70], Progress: 40.86 %, Steps: 4131, Current Learning Rate: 0.0009723, \u001b[91mTrain Loss: 0.917\n",
      "\u001b[0m\u001b[1mEpoch: [45/70], Progress: 41.94 %, Steps: 4132, Current Learning Rate: 0.0009722, \u001b[96mTrain Loss: 0.777\n",
      "\u001b[0m\u001b[1mEpoch: [45/70], Progress: 43.01 %, Steps: 4133, Current Learning Rate: 0.0009721, \u001b[91mTrain Loss: 0.854\n",
      "\u001b[0m\u001b[1mEpoch: [45/70], Progress: 44.09 %, Steps: 4134, Current Learning Rate: 0.0009719, \u001b[91mTrain Loss: 0.871\n",
      "\u001b[0m\u001b[1mEpoch: [45/70], Progress: 45.16 %, Steps: 4135, Current Learning Rate: 0.0009718, \u001b[96mTrain Loss: 0.781\n",
      "\u001b[0m\u001b[1mEpoch: [45/70], Progress: 46.24 %, Steps: 4136, Current Learning Rate: 0.0009717, \u001b[91mTrain Loss: 0.803\n",
      "\u001b[0m\u001b[1mEpoch: [45/70], Progress: 47.31 %, Steps: 4137, Current Learning Rate: 0.0009716, \u001b[91mTrain Loss: 0.819\n",
      "\u001b[0m\u001b[1mEpoch: [45/70], Progress: 48.39 %, Steps: 4138, Current Learning Rate: 0.0009715, \u001b[91mTrain Loss: 1.007\n",
      "\u001b[0m\u001b[1mEpoch: [45/70], Progress: 49.46 %, Steps: 4139, Current Learning Rate: 0.0009714, \u001b[96mTrain Loss: 0.880\n",
      "\u001b[0m\u001b[1mEpoch: [45/70], Progress: 50.54 %, Steps: 4140, Current Learning Rate: 0.0009712, \u001b[96mTrain Loss: 0.858\n",
      "\u001b[0m\u001b[1mEpoch: [45/70], Progress: 51.61 %, Steps: 4141, Current Learning Rate: 0.0009711, \u001b[91mTrain Loss: 0.884\n",
      "\u001b[0m\u001b[1mEpoch: [45/70], Progress: 52.69 %, Steps: 4142, Current Learning Rate: 0.0009710, \u001b[96mTrain Loss: 0.832\n",
      "\u001b[0m\u001b[1mEpoch: [45/70], Progress: 53.76 %, Steps: 4143, Current Learning Rate: 0.0009709, \u001b[91mTrain Loss: 0.849\n",
      "\u001b[0m\u001b[1mEpoch: [45/70], Progress: 54.84 %, Steps: 4144, Current Learning Rate: 0.0009708, \u001b[96mTrain Loss: 0.838\n",
      "\u001b[0m\u001b[1mEpoch: [45/70], Progress: 55.91 %, Steps: 4145, Current Learning Rate: 0.0009707, \u001b[91mTrain Loss: 0.951\n",
      "\u001b[0m\u001b[1mEpoch: [45/70], Progress: 56.99 %, Steps: 4146, Current Learning Rate: 0.0009705, \u001b[91mTrain Loss: 0.962\n",
      "\u001b[0m\u001b[1mEpoch: [45/70], Progress: 58.06 %, Steps: 4147, Current Learning Rate: 0.0009704, \u001b[96mTrain Loss: 0.848\n",
      "\u001b[0m\u001b[1mEpoch: [45/70], Progress: 59.14 %, Steps: 4148, Current Learning Rate: 0.0009703, \u001b[91mTrain Loss: 0.930\n",
      "\u001b[0m\u001b[1mEpoch: [45/70], Progress: 60.22 %, Steps: 4149, Current Learning Rate: 0.0009702, \u001b[96mTrain Loss: 0.828\n",
      "\u001b[0m\u001b[1mEpoch: [45/70], Progress: 61.29 %, Steps: 4150, Current Learning Rate: 0.0009701, \u001b[91mTrain Loss: 0.938\n",
      "\u001b[0m\u001b[1mEpoch: [45/70], Progress: 62.37 %, Steps: 4151, Current Learning Rate: 0.0009700, \u001b[96mTrain Loss: 0.743\n",
      "\u001b[0m\u001b[1mEpoch: [45/70], Progress: 63.44 %, Steps: 4152, Current Learning Rate: 0.0009698, \u001b[91mTrain Loss: 0.778\n",
      "\u001b[0m\u001b[1mEpoch: [45/70], Progress: 64.52 %, Steps: 4153, Current Learning Rate: 0.0009697, \u001b[91mTrain Loss: 0.870\n",
      "\u001b[0m\u001b[1mEpoch: [45/70], Progress: 65.59 %, Steps: 4154, Current Learning Rate: 0.0009696, \u001b[96mTrain Loss: 0.801\n",
      "\u001b[0m\u001b[1mEpoch: [45/70], Progress: 66.67 %, Steps: 4155, Current Learning Rate: 0.0009695, \u001b[96mTrain Loss: 0.768\n",
      "\u001b[0m\u001b[1mEpoch: [45/70], Progress: 67.74 %, Steps: 4156, Current Learning Rate: 0.0009694, \u001b[91mTrain Loss: 0.805\n",
      "\u001b[0m\u001b[1mEpoch: [45/70], Progress: 68.82 %, Steps: 4157, Current Learning Rate: 0.0009693, \u001b[91mTrain Loss: 0.855\n",
      "\u001b[0m\u001b[1mEpoch: [45/70], Progress: 69.89 %, Steps: 4158, Current Learning Rate: 0.0009691, \u001b[96mTrain Loss: 0.765\n",
      "\u001b[0m\u001b[1mEpoch: [45/70], Progress: 70.97 %, Steps: 4159, Current Learning Rate: 0.0009690, \u001b[96mTrain Loss: 0.745\n",
      "\u001b[0m\u001b[1mEpoch: [45/70], Progress: 72.04 %, Steps: 4160, Current Learning Rate: 0.0009689, \u001b[91mTrain Loss: 0.854\n",
      "\u001b[0m\u001b[1mEpoch: [45/70], Progress: 73.12 %, Steps: 4161, Current Learning Rate: 0.0009688, \u001b[96mTrain Loss: 0.825\n",
      "\u001b[0m\u001b[1mEpoch: [45/70], Progress: 74.19 %, Steps: 4162, Current Learning Rate: 0.0009687, \u001b[91mTrain Loss: 0.991\n",
      "\u001b[0m\u001b[1mEpoch: [45/70], Progress: 75.27 %, Steps: 4163, Current Learning Rate: 0.0009686, \u001b[96mTrain Loss: 0.845\n",
      "\u001b[0m\u001b[1mEpoch: [45/70], Progress: 76.34 %, Steps: 4164, Current Learning Rate: 0.0009684, \u001b[91mTrain Loss: 0.886\n",
      "\u001b[0m\u001b[1mEpoch: [45/70], Progress: 77.42 %, Steps: 4165, Current Learning Rate: 0.0009683, \u001b[91mTrain Loss: 0.907\n",
      "\u001b[0m\u001b[1mEpoch: [45/70], Progress: 78.49 %, Steps: 4166, Current Learning Rate: 0.0009682, \u001b[91mTrain Loss: 0.938\n",
      "\u001b[0m\u001b[1mEpoch: [45/70], Progress: 79.57 %, Steps: 4167, Current Learning Rate: 0.0009681, \u001b[91mTrain Loss: 0.975\n",
      "\u001b[0m\u001b[1mEpoch: [45/70], Progress: 80.65 %, Steps: 4168, Current Learning Rate: 0.0009680, \u001b[96mTrain Loss: 0.800\n",
      "\u001b[0m\u001b[1mEpoch: [45/70], Progress: 81.72 %, Steps: 4169, Current Learning Rate: 0.0009679, \u001b[91mTrain Loss: 0.914\n",
      "\u001b[0m\u001b[1mEpoch: [45/70], Progress: 82.80 %, Steps: 4170, Current Learning Rate: 0.0009677, \u001b[96mTrain Loss: 0.804\n",
      "\u001b[0m\u001b[1mEpoch: [45/70], Progress: 83.87 %, Steps: 4171, Current Learning Rate: 0.0009676, \u001b[91mTrain Loss: 0.861\n",
      "\u001b[0m\u001b[1mEpoch: [45/70], Progress: 84.95 %, Steps: 4172, Current Learning Rate: 0.0009675, \u001b[96mTrain Loss: 0.805\n",
      "\u001b[0m\u001b[1mEpoch: [45/70], Progress: 86.02 %, Steps: 4173, Current Learning Rate: 0.0009674, \u001b[91mTrain Loss: 0.900\n",
      "\u001b[0m\u001b[1mEpoch: [45/70], Progress: 87.10 %, Steps: 4174, Current Learning Rate: 0.0009673, \u001b[91mTrain Loss: 1.115\n",
      "\u001b[0m\u001b[1mEpoch: [45/70], Progress: 88.17 %, Steps: 4175, Current Learning Rate: 0.0009672, \u001b[96mTrain Loss: 0.921\n",
      "\u001b[0m\u001b[1mEpoch: [45/70], Progress: 89.25 %, Steps: 4176, Current Learning Rate: 0.0009670, \u001b[96mTrain Loss: 0.845\n",
      "\u001b[0m\u001b[1mEpoch: [45/70], Progress: 90.32 %, Steps: 4177, Current Learning Rate: 0.0009669, \u001b[91mTrain Loss: 0.916\n",
      "\u001b[0m\u001b[1mEpoch: [45/70], Progress: 91.40 %, Steps: 4178, Current Learning Rate: 0.0009668, \u001b[96mTrain Loss: 0.889\n",
      "\u001b[0m\u001b[1mEpoch: [45/70], Progress: 92.47 %, Steps: 4179, Current Learning Rate: 0.0009667, \u001b[96mTrain Loss: 0.767\n",
      "\u001b[0m\u001b[1mEpoch: [45/70], Progress: 93.55 %, Steps: 4180, Current Learning Rate: 0.0009666, \u001b[91mTrain Loss: 0.794\n",
      "\u001b[0m\u001b[1mEpoch: [45/70], Progress: 94.62 %, Steps: 4181, Current Learning Rate: 0.0009665, \u001b[91mTrain Loss: 0.874\n",
      "\u001b[0m\u001b[1mEpoch: [45/70], Progress: 95.70 %, Steps: 4182, Current Learning Rate: 0.0009664, \u001b[91mTrain Loss: 0.971\n",
      "\u001b[0m\u001b[1mEpoch: [45/70], Progress: 96.77 %, Steps: 4183, Current Learning Rate: 0.0009662, \u001b[96mTrain Loss: 0.928\n",
      "\u001b[0m\u001b[1mEpoch: [45/70], Progress: 97.85 %, Steps: 4184, Current Learning Rate: 0.0009661, \u001b[96mTrain Loss: 0.868\n",
      "\u001b[0m\u001b[1mEpoch: [45/70], Progress: 98.92 %, Steps: 4185, Current Learning Rate: 0.0009660, \u001b[91mTrain Loss: 0.901\n",
      "\u001b[1m\u001b[93mComputing Validation Loss...\n",
      "\u001b[1mEpoch 45 Completed! Average Train Loss: 0.831, Average Validation Loss: 0.315\n",
      "Best Model saved in best_model/best_model.pt.\n",
      "\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m= Epoch [46/70] ==========================================\n",
      "\u001b[0m\u001b[1mEpoch: [46/70], Progress: 0.00 %, Steps: 4186, Current Learning Rate: 0.0009659, \u001b[91mTrain Loss: 0.632\n",
      "\u001b[0m\u001b[1mEpoch: [46/70], Progress: 1.08 %, Steps: 4187, Current Learning Rate: 0.0009658, \u001b[96mTrain Loss: 0.563\n",
      "\u001b[0m\u001b[1mEpoch: [46/70], Progress: 2.15 %, Steps: 4188, Current Learning Rate: 0.0009657, \u001b[96mTrain Loss: 0.556\n",
      "\u001b[0m\u001b[1mEpoch: [46/70], Progress: 3.23 %, Steps: 4189, Current Learning Rate: 0.0009655, \u001b[91mTrain Loss: 0.621\n",
      "\u001b[0m\u001b[1mEpoch: [46/70], Progress: 4.30 %, Steps: 4190, Current Learning Rate: 0.0009654, \u001b[91mTrain Loss: 0.684\n",
      "\u001b[0m\u001b[1mEpoch: [46/70], Progress: 5.38 %, Steps: 4191, Current Learning Rate: 0.0009653, \u001b[96mTrain Loss: 0.584\n",
      "\u001b[0m\u001b[1mEpoch: [46/70], Progress: 6.45 %, Steps: 4192, Current Learning Rate: 0.0009652, \u001b[91mTrain Loss: 0.723\n",
      "\u001b[0m\u001b[1mEpoch: [46/70], Progress: 7.53 %, Steps: 4193, Current Learning Rate: 0.0009651, \u001b[96mTrain Loss: 0.632\n",
      "\u001b[0m\u001b[1mEpoch: [46/70], Progress: 8.60 %, Steps: 4194, Current Learning Rate: 0.0009650, \u001b[91mTrain Loss: 0.682\n",
      "\u001b[0m\u001b[1mEpoch: [46/70], Progress: 9.68 %, Steps: 4195, Current Learning Rate: 0.0009649, \u001b[96mTrain Loss: 0.654\n",
      "\u001b[0m\u001b[1mEpoch: [46/70], Progress: 10.75 %, Steps: 4196, Current Learning Rate: 0.0009647, \u001b[91mTrain Loss: 0.740\n",
      "\u001b[0m\u001b[1mEpoch: [46/70], Progress: 11.83 %, Steps: 4197, Current Learning Rate: 0.0009646, \u001b[96mTrain Loss: 0.605\n",
      "\u001b[0m\u001b[1mEpoch: [46/70], Progress: 12.90 %, Steps: 4198, Current Learning Rate: 0.0009645, \u001b[91mTrain Loss: 0.811\n",
      "\u001b[0m\u001b[1mEpoch: [46/70], Progress: 13.98 %, Steps: 4199, Current Learning Rate: 0.0009644, \u001b[96mTrain Loss: 0.638\n",
      "\u001b[0m\u001b[1mEpoch: [46/70], Progress: 15.05 %, Steps: 4200, Current Learning Rate: 0.0009643, \u001b[91mTrain Loss: 0.721\n",
      "\u001b[0m\u001b[1mEpoch: [46/70], Progress: 16.13 %, Steps: 4201, Current Learning Rate: 0.0009642, \u001b[96mTrain Loss: 0.711\n",
      "\u001b[0m\u001b[1mEpoch: [46/70], Progress: 17.20 %, Steps: 4202, Current Learning Rate: 0.0009641, \u001b[96mTrain Loss: 0.663\n",
      "\u001b[0m\u001b[1mEpoch: [46/70], Progress: 18.28 %, Steps: 4203, Current Learning Rate: 0.0009639, \u001b[91mTrain Loss: 0.751\n",
      "\u001b[0m\u001b[1mEpoch: [46/70], Progress: 19.35 %, Steps: 4204, Current Learning Rate: 0.0009638, \u001b[96mTrain Loss: 0.691\n",
      "\u001b[0m\u001b[1mEpoch: [46/70], Progress: 20.43 %, Steps: 4205, Current Learning Rate: 0.0009637, \u001b[91mTrain Loss: 0.755\n",
      "\u001b[0m\u001b[1mEpoch: [46/70], Progress: 21.51 %, Steps: 4206, Current Learning Rate: 0.0009636, \u001b[96mTrain Loss: 0.647\n",
      "\u001b[0m\u001b[1mEpoch: [46/70], Progress: 22.58 %, Steps: 4207, Current Learning Rate: 0.0009635, \u001b[91mTrain Loss: 0.685\n",
      "\u001b[0m\u001b[1mEpoch: [46/70], Progress: 23.66 %, Steps: 4208, Current Learning Rate: 0.0009634, \u001b[91mTrain Loss: 0.711\n",
      "\u001b[0m\u001b[1mEpoch: [46/70], Progress: 24.73 %, Steps: 4209, Current Learning Rate: 0.0009632, \u001b[91mTrain Loss: 0.803\n",
      "\u001b[0m\u001b[1mEpoch: [46/70], Progress: 25.81 %, Steps: 4210, Current Learning Rate: 0.0009631, \u001b[96mTrain Loss: 0.712\n",
      "\u001b[0m\u001b[1mEpoch: [46/70], Progress: 26.88 %, Steps: 4211, Current Learning Rate: 0.0009630, \u001b[91mTrain Loss: 0.761\n",
      "\u001b[0m\u001b[1mEpoch: [46/70], Progress: 27.96 %, Steps: 4212, Current Learning Rate: 0.0009629, \u001b[96mTrain Loss: 0.707\n",
      "\u001b[0m\u001b[1mEpoch: [46/70], Progress: 29.03 %, Steps: 4213, Current Learning Rate: 0.0009628, \u001b[91mTrain Loss: 0.741\n",
      "\u001b[0m\u001b[1mEpoch: [46/70], Progress: 30.11 %, Steps: 4214, Current Learning Rate: 0.0009627, \u001b[96mTrain Loss: 0.677\n",
      "\u001b[0m\u001b[1mEpoch: [46/70], Progress: 31.18 %, Steps: 4215, Current Learning Rate: 0.0009626, \u001b[91mTrain Loss: 0.722\n",
      "\u001b[0m\u001b[1mEpoch: [46/70], Progress: 32.26 %, Steps: 4216, Current Learning Rate: 0.0009625, \u001b[91mTrain Loss: 0.766\n",
      "\u001b[0m\u001b[1mEpoch: [46/70], Progress: 33.33 %, Steps: 4217, Current Learning Rate: 0.0009623, \u001b[96mTrain Loss: 0.757\n",
      "\u001b[0m\u001b[1mEpoch: [46/70], Progress: 34.41 %, Steps: 4218, Current Learning Rate: 0.0009622, \u001b[96mTrain Loss: 0.719\n",
      "\u001b[0m\u001b[1mEpoch: [46/70], Progress: 35.48 %, Steps: 4219, Current Learning Rate: 0.0009621, \u001b[96mTrain Loss: 0.592\n",
      "\u001b[0m\u001b[1mEpoch: [46/70], Progress: 36.56 %, Steps: 4220, Current Learning Rate: 0.0009620, \u001b[91mTrain Loss: 0.802\n",
      "\u001b[0m\u001b[1mEpoch: [46/70], Progress: 37.63 %, Steps: 4221, Current Learning Rate: 0.0009619, \u001b[91mTrain Loss: 0.803\n",
      "\u001b[0m\u001b[1mEpoch: [46/70], Progress: 38.71 %, Steps: 4222, Current Learning Rate: 0.0009618, \u001b[96mTrain Loss: 0.695\n",
      "\u001b[0m\u001b[1mEpoch: [46/70], Progress: 39.78 %, Steps: 4223, Current Learning Rate: 0.0009617, \u001b[91mTrain Loss: 0.801\n",
      "\u001b[0m\u001b[1mEpoch: [46/70], Progress: 40.86 %, Steps: 4224, Current Learning Rate: 0.0009615, \u001b[91mTrain Loss: 0.813\n",
      "\u001b[0m\u001b[1mEpoch: [46/70], Progress: 41.94 %, Steps: 4225, Current Learning Rate: 0.0009614, \u001b[96mTrain Loss: 0.749\n",
      "\u001b[0m\u001b[1mEpoch: [46/70], Progress: 43.01 %, Steps: 4226, Current Learning Rate: 0.0009613, \u001b[96mTrain Loss: 0.635\n",
      "\u001b[0m\u001b[1mEpoch: [46/70], Progress: 44.09 %, Steps: 4227, Current Learning Rate: 0.0009612, \u001b[91mTrain Loss: 0.706\n",
      "\u001b[0m\u001b[1mEpoch: [46/70], Progress: 45.16 %, Steps: 4228, Current Learning Rate: 0.0009611, \u001b[96mTrain Loss: 0.701\n",
      "\u001b[0m\u001b[1mEpoch: [46/70], Progress: 46.24 %, Steps: 4229, Current Learning Rate: 0.0009610, \u001b[91mTrain Loss: 0.883\n",
      "\u001b[0m\u001b[1mEpoch: [46/70], Progress: 47.31 %, Steps: 4230, Current Learning Rate: 0.0009609, \u001b[96mTrain Loss: 0.724\n",
      "\u001b[0m\u001b[1mEpoch: [46/70], Progress: 48.39 %, Steps: 4231, Current Learning Rate: 0.0009607, \u001b[91mTrain Loss: 0.851\n",
      "\u001b[0m\u001b[1mEpoch: [46/70], Progress: 49.46 %, Steps: 4232, Current Learning Rate: 0.0009606, \u001b[96mTrain Loss: 0.820\n",
      "\u001b[0m\u001b[1mEpoch: [46/70], Progress: 50.54 %, Steps: 4233, Current Learning Rate: 0.0009605, \u001b[96mTrain Loss: 0.809\n",
      "\u001b[0m\u001b[1mEpoch: [46/70], Progress: 51.61 %, Steps: 4234, Current Learning Rate: 0.0009604, \u001b[96mTrain Loss: 0.718\n",
      "\u001b[0m\u001b[1mEpoch: [46/70], Progress: 52.69 %, Steps: 4235, Current Learning Rate: 0.0009603, \u001b[91mTrain Loss: 0.764\n",
      "\u001b[0m\u001b[1mEpoch: [46/70], Progress: 53.76 %, Steps: 4236, Current Learning Rate: 0.0009602, \u001b[96mTrain Loss: 0.742\n",
      "\u001b[0m\u001b[1mEpoch: [46/70], Progress: 54.84 %, Steps: 4237, Current Learning Rate: 0.0009601, \u001b[96mTrain Loss: 0.704\n",
      "\u001b[0m\u001b[1mEpoch: [46/70], Progress: 55.91 %, Steps: 4238, Current Learning Rate: 0.0009599, \u001b[96mTrain Loss: 0.700\n",
      "\u001b[0m\u001b[1mEpoch: [46/70], Progress: 56.99 %, Steps: 4239, Current Learning Rate: 0.0009598, \u001b[96mTrain Loss: 0.657\n",
      "\u001b[0m\u001b[1mEpoch: [46/70], Progress: 58.06 %, Steps: 4240, Current Learning Rate: 0.0009597, \u001b[91mTrain Loss: 0.779\n",
      "\u001b[0m\u001b[1mEpoch: [46/70], Progress: 59.14 %, Steps: 4241, Current Learning Rate: 0.0009596, \u001b[91mTrain Loss: 0.853\n",
      "\u001b[0m\u001b[1mEpoch: [46/70], Progress: 60.22 %, Steps: 4242, Current Learning Rate: 0.0009595, \u001b[96mTrain Loss: 0.768\n",
      "\u001b[0m\u001b[1mEpoch: [46/70], Progress: 61.29 %, Steps: 4243, Current Learning Rate: 0.0009594, \u001b[96mTrain Loss: 0.682\n",
      "\u001b[0m\u001b[1mEpoch: [46/70], Progress: 62.37 %, Steps: 4244, Current Learning Rate: 0.0009593, \u001b[91mTrain Loss: 0.794\n",
      "\u001b[0m\u001b[1mEpoch: [46/70], Progress: 63.44 %, Steps: 4245, Current Learning Rate: 0.0009592, \u001b[96mTrain Loss: 0.767\n",
      "\u001b[0m\u001b[1mEpoch: [46/70], Progress: 64.52 %, Steps: 4246, Current Learning Rate: 0.0009590, \u001b[96mTrain Loss: 0.680\n",
      "\u001b[0m\u001b[1mEpoch: [46/70], Progress: 65.59 %, Steps: 4247, Current Learning Rate: 0.0009589, \u001b[96mTrain Loss: 0.657\n",
      "\u001b[0m\u001b[1mEpoch: [46/70], Progress: 66.67 %, Steps: 4248, Current Learning Rate: 0.0009588, \u001b[91mTrain Loss: 0.781\n",
      "\u001b[0m\u001b[1mEpoch: [46/70], Progress: 67.74 %, Steps: 4249, Current Learning Rate: 0.0009587, \u001b[96mTrain Loss: 0.722\n",
      "\u001b[0m\u001b[1mEpoch: [46/70], Progress: 68.82 %, Steps: 4250, Current Learning Rate: 0.0009586, \u001b[91mTrain Loss: 0.795\n",
      "\u001b[0m\u001b[1mEpoch: [46/70], Progress: 69.89 %, Steps: 4251, Current Learning Rate: 0.0009585, \u001b[96mTrain Loss: 0.787\n",
      "\u001b[0m\u001b[1mEpoch: [46/70], Progress: 70.97 %, Steps: 4252, Current Learning Rate: 0.0009584, \u001b[96mTrain Loss: 0.768\n",
      "\u001b[0m\u001b[1mEpoch: [46/70], Progress: 72.04 %, Steps: 4253, Current Learning Rate: 0.0009583, \u001b[91mTrain Loss: 0.776\n",
      "\u001b[0m\u001b[1mEpoch: [46/70], Progress: 73.12 %, Steps: 4254, Current Learning Rate: 0.0009581, \u001b[91mTrain Loss: 0.800\n",
      "\u001b[0m\u001b[1mEpoch: [46/70], Progress: 74.19 %, Steps: 4255, Current Learning Rate: 0.0009580, \u001b[91mTrain Loss: 0.833\n",
      "\u001b[0m\u001b[1mEpoch: [46/70], Progress: 75.27 %, Steps: 4256, Current Learning Rate: 0.0009579, \u001b[96mTrain Loss: 0.816\n",
      "\u001b[0m\u001b[1mEpoch: [46/70], Progress: 76.34 %, Steps: 4257, Current Learning Rate: 0.0009578, \u001b[96mTrain Loss: 0.703\n",
      "\u001b[0m\u001b[1mEpoch: [46/70], Progress: 77.42 %, Steps: 4258, Current Learning Rate: 0.0009577, \u001b[91mTrain Loss: 0.862\n",
      "\u001b[0m\u001b[1mEpoch: [46/70], Progress: 78.49 %, Steps: 4259, Current Learning Rate: 0.0009576, \u001b[96mTrain Loss: 0.787\n",
      "\u001b[0m\u001b[1mEpoch: [46/70], Progress: 79.57 %, Steps: 4260, Current Learning Rate: 0.0009575, \u001b[91mTrain Loss: 0.803\n",
      "\u001b[0m\u001b[1mEpoch: [46/70], Progress: 80.65 %, Steps: 4261, Current Learning Rate: 0.0009574, \u001b[91mTrain Loss: 0.838\n",
      "\u001b[0m\u001b[1mEpoch: [46/70], Progress: 81.72 %, Steps: 4262, Current Learning Rate: 0.0009572, \u001b[96mTrain Loss: 0.751\n",
      "\u001b[0m\u001b[1mEpoch: [46/70], Progress: 82.80 %, Steps: 4263, Current Learning Rate: 0.0009571, \u001b[96mTrain Loss: 0.680\n",
      "\u001b[0m\u001b[1mEpoch: [46/70], Progress: 83.87 %, Steps: 4264, Current Learning Rate: 0.0009570, \u001b[91mTrain Loss: 0.758\n",
      "\u001b[0m\u001b[1mEpoch: [46/70], Progress: 84.95 %, Steps: 4265, Current Learning Rate: 0.0009569, \u001b[91mTrain Loss: 0.811\n",
      "\u001b[0m\u001b[1mEpoch: [46/70], Progress: 86.02 %, Steps: 4266, Current Learning Rate: 0.0009568, \u001b[91mTrain Loss: 0.828\n",
      "\u001b[0m\u001b[1mEpoch: [46/70], Progress: 87.10 %, Steps: 4267, Current Learning Rate: 0.0009567, \u001b[91mTrain Loss: 0.864\n",
      "\u001b[0m\u001b[1mEpoch: [46/70], Progress: 88.17 %, Steps: 4268, Current Learning Rate: 0.0009566, \u001b[96mTrain Loss: 0.812\n",
      "\u001b[0m\u001b[1mEpoch: [46/70], Progress: 89.25 %, Steps: 4269, Current Learning Rate: 0.0009565, \u001b[91mTrain Loss: 0.836\n",
      "\u001b[0m\u001b[1mEpoch: [46/70], Progress: 90.32 %, Steps: 4270, Current Learning Rate: 0.0009563, \u001b[96mTrain Loss: 0.769\n",
      "\u001b[0m\u001b[1mEpoch: [46/70], Progress: 91.40 %, Steps: 4271, Current Learning Rate: 0.0009562, \u001b[91mTrain Loss: 0.816\n",
      "\u001b[0m\u001b[1mEpoch: [46/70], Progress: 92.47 %, Steps: 4272, Current Learning Rate: 0.0009561, \u001b[91mTrain Loss: 0.885\n",
      "\u001b[0m\u001b[1mEpoch: [46/70], Progress: 93.55 %, Steps: 4273, Current Learning Rate: 0.0009560, \u001b[96mTrain Loss: 0.870\n",
      "\u001b[0m\u001b[1mEpoch: [46/70], Progress: 94.62 %, Steps: 4274, Current Learning Rate: 0.0009559, \u001b[96mTrain Loss: 0.782\n",
      "\u001b[0m\u001b[1mEpoch: [46/70], Progress: 95.70 %, Steps: 4275, Current Learning Rate: 0.0009558, \u001b[91mTrain Loss: 0.822\n",
      "\u001b[0m\u001b[1mEpoch: [46/70], Progress: 96.77 %, Steps: 4276, Current Learning Rate: 0.0009557, \u001b[91mTrain Loss: 0.895\n",
      "\u001b[0m\u001b[1mEpoch: [46/70], Progress: 97.85 %, Steps: 4277, Current Learning Rate: 0.0009556, \u001b[96mTrain Loss: 0.713\n",
      "\u001b[0m\u001b[1mEpoch: [46/70], Progress: 98.92 %, Steps: 4278, Current Learning Rate: 0.0009555, \u001b[91mTrain Loss: 0.826\n",
      "\u001b[1m\u001b[93mComputing Validation Loss...\n",
      "\u001b[1mEpoch 46 Completed! Average Train Loss: 0.745, Average Validation Loss: 0.285\n",
      "Best Model saved in best_model/best_model.pt.\n",
      "\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m= Epoch [47/70] ==========================================\n",
      "\u001b[0m\u001b[1mEpoch: [47/70], Progress: 0.00 %, Steps: 4279, Current Learning Rate: 0.0009553, \u001b[91mTrain Loss: 0.566\n",
      "\u001b[0m\u001b[1mEpoch: [47/70], Progress: 1.08 %, Steps: 4280, Current Learning Rate: 0.0009552, \u001b[91mTrain Loss: 0.587\n",
      "\u001b[0m\u001b[1mEpoch: [47/70], Progress: 2.15 %, Steps: 4281, Current Learning Rate: 0.0009551, \u001b[91mTrain Loss: 0.630\n",
      "\u001b[0m\u001b[1mEpoch: [47/70], Progress: 3.23 %, Steps: 4282, Current Learning Rate: 0.0009550, \u001b[91mTrain Loss: 0.737\n",
      "\u001b[0m\u001b[1mEpoch: [47/70], Progress: 4.30 %, Steps: 4283, Current Learning Rate: 0.0009549, \u001b[96mTrain Loss: 0.536\n",
      "\u001b[0m\u001b[1mEpoch: [47/70], Progress: 5.38 %, Steps: 4284, Current Learning Rate: 0.0009548, \u001b[91mTrain Loss: 0.602\n",
      "\u001b[0m\u001b[1mEpoch: [47/70], Progress: 6.45 %, Steps: 4285, Current Learning Rate: 0.0009547, \u001b[91mTrain Loss: 0.639\n",
      "\u001b[0m\u001b[1mEpoch: [47/70], Progress: 7.53 %, Steps: 4286, Current Learning Rate: 0.0009546, \u001b[91mTrain Loss: 0.662\n",
      "\u001b[0m\u001b[1mEpoch: [47/70], Progress: 8.60 %, Steps: 4287, Current Learning Rate: 0.0009544, \u001b[96mTrain Loss: 0.603\n",
      "\u001b[0m\u001b[1mEpoch: [47/70], Progress: 9.68 %, Steps: 4288, Current Learning Rate: 0.0009543, \u001b[91mTrain Loss: 0.631\n",
      "\u001b[0m\u001b[1mEpoch: [47/70], Progress: 10.75 %, Steps: 4289, Current Learning Rate: 0.0009542, \u001b[96mTrain Loss: 0.607\n",
      "\u001b[0m\u001b[1mEpoch: [47/70], Progress: 11.83 %, Steps: 4290, Current Learning Rate: 0.0009541, \u001b[91mTrain Loss: 0.689\n",
      "\u001b[0m\u001b[1mEpoch: [47/70], Progress: 12.90 %, Steps: 4291, Current Learning Rate: 0.0009540, \u001b[96mTrain Loss: 0.595\n",
      "\u001b[0m\u001b[1mEpoch: [47/70], Progress: 13.98 %, Steps: 4292, Current Learning Rate: 0.0009539, \u001b[96mTrain Loss: 0.543\n",
      "\u001b[0m\u001b[1mEpoch: [47/70], Progress: 15.05 %, Steps: 4293, Current Learning Rate: 0.0009538, \u001b[96mTrain Loss: 0.517\n",
      "\u001b[0m\u001b[1mEpoch: [47/70], Progress: 16.13 %, Steps: 4294, Current Learning Rate: 0.0009537, \u001b[91mTrain Loss: 0.578\n",
      "\u001b[0m\u001b[1mEpoch: [47/70], Progress: 17.20 %, Steps: 4295, Current Learning Rate: 0.0009536, \u001b[91mTrain Loss: 0.591\n",
      "\u001b[0m\u001b[1mEpoch: [47/70], Progress: 18.28 %, Steps: 4296, Current Learning Rate: 0.0009534, \u001b[96mTrain Loss: 0.455\n",
      "\u001b[0m\u001b[1mEpoch: [47/70], Progress: 19.35 %, Steps: 4297, Current Learning Rate: 0.0009533, \u001b[91mTrain Loss: 0.598\n",
      "\u001b[0m\u001b[1mEpoch: [47/70], Progress: 20.43 %, Steps: 4298, Current Learning Rate: 0.0009532, \u001b[96mTrain Loss: 0.551\n",
      "\u001b[0m\u001b[1mEpoch: [47/70], Progress: 21.51 %, Steps: 4299, Current Learning Rate: 0.0009531, \u001b[91mTrain Loss: 0.585\n",
      "\u001b[0m\u001b[1mEpoch: [47/70], Progress: 22.58 %, Steps: 4300, Current Learning Rate: 0.0009530, \u001b[91mTrain Loss: 0.665\n",
      "\u001b[0m\u001b[1mEpoch: [47/70], Progress: 23.66 %, Steps: 4301, Current Learning Rate: 0.0009529, \u001b[91mTrain Loss: 0.668\n",
      "\u001b[0m\u001b[1mEpoch: [47/70], Progress: 24.73 %, Steps: 4302, Current Learning Rate: 0.0009528, \u001b[96mTrain Loss: 0.638\n",
      "\u001b[0m\u001b[1mEpoch: [47/70], Progress: 25.81 %, Steps: 4303, Current Learning Rate: 0.0009527, \u001b[91mTrain Loss: 0.639\n",
      "\u001b[0m\u001b[1mEpoch: [47/70], Progress: 26.88 %, Steps: 4304, Current Learning Rate: 0.0009526, \u001b[91mTrain Loss: 0.687\n",
      "\u001b[0m\u001b[1mEpoch: [47/70], Progress: 27.96 %, Steps: 4305, Current Learning Rate: 0.0009525, \u001b[96mTrain Loss: 0.598\n",
      "\u001b[0m\u001b[1mEpoch: [47/70], Progress: 29.03 %, Steps: 4306, Current Learning Rate: 0.0009523, \u001b[91mTrain Loss: 0.732\n",
      "\u001b[0m\u001b[1mEpoch: [47/70], Progress: 30.11 %, Steps: 4307, Current Learning Rate: 0.0009522, \u001b[96mTrain Loss: 0.728\n",
      "\u001b[0m\u001b[1mEpoch: [47/70], Progress: 31.18 %, Steps: 4308, Current Learning Rate: 0.0009521, \u001b[96mTrain Loss: 0.666\n",
      "\u001b[0m\u001b[1mEpoch: [47/70], Progress: 32.26 %, Steps: 4309, Current Learning Rate: 0.0009520, \u001b[91mTrain Loss: 0.677\n",
      "\u001b[0m\u001b[1mEpoch: [47/70], Progress: 33.33 %, Steps: 4310, Current Learning Rate: 0.0009519, \u001b[96mTrain Loss: 0.616\n",
      "\u001b[0m\u001b[1mEpoch: [47/70], Progress: 34.41 %, Steps: 4311, Current Learning Rate: 0.0009518, \u001b[96mTrain Loss: 0.616\n",
      "\u001b[0m\u001b[1mEpoch: [47/70], Progress: 35.48 %, Steps: 4312, Current Learning Rate: 0.0009517, \u001b[91mTrain Loss: 0.648\n",
      "\u001b[0m\u001b[1mEpoch: [47/70], Progress: 36.56 %, Steps: 4313, Current Learning Rate: 0.0009516, \u001b[96mTrain Loss: 0.627\n",
      "\u001b[0m\u001b[1mEpoch: [47/70], Progress: 37.63 %, Steps: 4314, Current Learning Rate: 0.0009515, \u001b[91mTrain Loss: 0.647\n",
      "\u001b[0m\u001b[1mEpoch: [47/70], Progress: 38.71 %, Steps: 4315, Current Learning Rate: 0.0009513, \u001b[96mTrain Loss: 0.633\n",
      "\u001b[0m\u001b[1mEpoch: [47/70], Progress: 39.78 %, Steps: 4316, Current Learning Rate: 0.0009512, \u001b[91mTrain Loss: 0.671\n",
      "\u001b[0m\u001b[1mEpoch: [47/70], Progress: 40.86 %, Steps: 4317, Current Learning Rate: 0.0009511, \u001b[96mTrain Loss: 0.605\n",
      "\u001b[0m\u001b[1mEpoch: [47/70], Progress: 41.94 %, Steps: 4318, Current Learning Rate: 0.0009510, \u001b[91mTrain Loss: 0.641\n",
      "\u001b[0m\u001b[1mEpoch: [47/70], Progress: 43.01 %, Steps: 4319, Current Learning Rate: 0.0009509, \u001b[91mTrain Loss: 0.672\n",
      "\u001b[0m\u001b[1mEpoch: [47/70], Progress: 44.09 %, Steps: 4320, Current Learning Rate: 0.0009508, \u001b[96mTrain Loss: 0.656\n",
      "\u001b[0m\u001b[1mEpoch: [47/70], Progress: 45.16 %, Steps: 4321, Current Learning Rate: 0.0009507, \u001b[96mTrain Loss: 0.642\n",
      "\u001b[0m\u001b[1mEpoch: [47/70], Progress: 46.24 %, Steps: 4322, Current Learning Rate: 0.0009506, \u001b[91mTrain Loss: 0.660\n",
      "\u001b[0m\u001b[1mEpoch: [47/70], Progress: 47.31 %, Steps: 4323, Current Learning Rate: 0.0009505, \u001b[96mTrain Loss: 0.613\n",
      "\u001b[0m\u001b[1mEpoch: [47/70], Progress: 48.39 %, Steps: 4324, Current Learning Rate: 0.0009504, \u001b[91mTrain Loss: 0.681\n",
      "\u001b[0m\u001b[1mEpoch: [47/70], Progress: 49.46 %, Steps: 4325, Current Learning Rate: 0.0009502, \u001b[91mTrain Loss: 0.685\n",
      "\u001b[0m\u001b[1mEpoch: [47/70], Progress: 50.54 %, Steps: 4326, Current Learning Rate: 0.0009501, \u001b[91mTrain Loss: 0.747\n",
      "\u001b[0m\u001b[1mEpoch: [47/70], Progress: 51.61 %, Steps: 4327, Current Learning Rate: 0.0009500, \u001b[96mTrain Loss: 0.745\n",
      "\u001b[0m\u001b[1mEpoch: [47/70], Progress: 52.69 %, Steps: 4328, Current Learning Rate: 0.0009499, \u001b[96mTrain Loss: 0.666\n",
      "\u001b[0m\u001b[1mEpoch: [47/70], Progress: 53.76 %, Steps: 4329, Current Learning Rate: 0.0009498, \u001b[91mTrain Loss: 0.695\n",
      "\u001b[0m\u001b[1mEpoch: [47/70], Progress: 54.84 %, Steps: 4330, Current Learning Rate: 0.0009497, \u001b[91mTrain Loss: 0.708\n",
      "\u001b[0m\u001b[1mEpoch: [47/70], Progress: 55.91 %, Steps: 4331, Current Learning Rate: 0.0009496, \u001b[96mTrain Loss: 0.666\n",
      "\u001b[0m\u001b[1mEpoch: [47/70], Progress: 56.99 %, Steps: 4332, Current Learning Rate: 0.0009495, \u001b[91mTrain Loss: 0.679\n",
      "\u001b[0m\u001b[1mEpoch: [47/70], Progress: 58.06 %, Steps: 4333, Current Learning Rate: 0.0009494, \u001b[96mTrain Loss: 0.575\n",
      "\u001b[0m\u001b[1mEpoch: [47/70], Progress: 59.14 %, Steps: 4334, Current Learning Rate: 0.0009493, \u001b[91mTrain Loss: 0.667\n",
      "\u001b[0m\u001b[1mEpoch: [47/70], Progress: 60.22 %, Steps: 4335, Current Learning Rate: 0.0009492, \u001b[96mTrain Loss: 0.587\n",
      "\u001b[0m\u001b[1mEpoch: [47/70], Progress: 61.29 %, Steps: 4336, Current Learning Rate: 0.0009490, \u001b[91mTrain Loss: 0.718\n",
      "\u001b[0m\u001b[1mEpoch: [47/70], Progress: 62.37 %, Steps: 4337, Current Learning Rate: 0.0009489, \u001b[91mTrain Loss: 0.731\n",
      "\u001b[0m\u001b[1mEpoch: [47/70], Progress: 63.44 %, Steps: 4338, Current Learning Rate: 0.0009488, \u001b[96mTrain Loss: 0.693\n",
      "\u001b[0m\u001b[1mEpoch: [47/70], Progress: 64.52 %, Steps: 4339, Current Learning Rate: 0.0009487, \u001b[96mTrain Loss: 0.620\n",
      "\u001b[0m\u001b[1mEpoch: [47/70], Progress: 65.59 %, Steps: 4340, Current Learning Rate: 0.0009486, \u001b[91mTrain Loss: 0.654\n",
      "\u001b[0m\u001b[1mEpoch: [47/70], Progress: 66.67 %, Steps: 4341, Current Learning Rate: 0.0009485, \u001b[91mTrain Loss: 0.660\n",
      "\u001b[0m\u001b[1mEpoch: [47/70], Progress: 67.74 %, Steps: 4342, Current Learning Rate: 0.0009484, \u001b[91mTrain Loss: 0.699\n",
      "\u001b[0m\u001b[1mEpoch: [47/70], Progress: 68.82 %, Steps: 4343, Current Learning Rate: 0.0009483, \u001b[91mTrain Loss: 0.757\n",
      "\u001b[0m\u001b[1mEpoch: [47/70], Progress: 69.89 %, Steps: 4344, Current Learning Rate: 0.0009482, \u001b[96mTrain Loss: 0.648\n",
      "\u001b[0m\u001b[1mEpoch: [47/70], Progress: 70.97 %, Steps: 4345, Current Learning Rate: 0.0009481, \u001b[91mTrain Loss: 0.712\n",
      "\u001b[0m\u001b[1mEpoch: [47/70], Progress: 72.04 %, Steps: 4346, Current Learning Rate: 0.0009479, \u001b[96mTrain Loss: 0.684\n",
      "\u001b[0m\u001b[1mEpoch: [47/70], Progress: 73.12 %, Steps: 4347, Current Learning Rate: 0.0009478, \u001b[91mTrain Loss: 0.693\n",
      "\u001b[0m\u001b[1mEpoch: [47/70], Progress: 74.19 %, Steps: 4348, Current Learning Rate: 0.0009477, \u001b[91mTrain Loss: 0.706\n",
      "\u001b[0m\u001b[1mEpoch: [47/70], Progress: 75.27 %, Steps: 4349, Current Learning Rate: 0.0009476, \u001b[91mTrain Loss: 0.781\n",
      "\u001b[0m\u001b[1mEpoch: [47/70], Progress: 76.34 %, Steps: 4350, Current Learning Rate: 0.0009475, \u001b[96mTrain Loss: 0.711\n",
      "\u001b[0m\u001b[1mEpoch: [47/70], Progress: 77.42 %, Steps: 4351, Current Learning Rate: 0.0009474, \u001b[91mTrain Loss: 0.752\n",
      "\u001b[0m\u001b[1mEpoch: [47/70], Progress: 78.49 %, Steps: 4352, Current Learning Rate: 0.0009473, \u001b[96mTrain Loss: 0.616\n",
      "\u001b[0m\u001b[1mEpoch: [47/70], Progress: 79.57 %, Steps: 4353, Current Learning Rate: 0.0009472, \u001b[91mTrain Loss: 0.636\n",
      "\u001b[0m\u001b[1mEpoch: [47/70], Progress: 80.65 %, Steps: 4354, Current Learning Rate: 0.0009471, \u001b[91mTrain Loss: 0.700\n",
      "\u001b[0m\u001b[1mEpoch: [47/70], Progress: 81.72 %, Steps: 4355, Current Learning Rate: 0.0009470, \u001b[91mTrain Loss: 0.787\n",
      "\u001b[0m\u001b[1mEpoch: [47/70], Progress: 82.80 %, Steps: 4356, Current Learning Rate: 0.0009469, \u001b[96mTrain Loss: 0.774\n",
      "\u001b[0m\u001b[1mEpoch: [47/70], Progress: 83.87 %, Steps: 4357, Current Learning Rate: 0.0009468, \u001b[91mTrain Loss: 0.862\n",
      "\u001b[0m\u001b[1mEpoch: [47/70], Progress: 84.95 %, Steps: 4358, Current Learning Rate: 0.0009466, \u001b[96mTrain Loss: 0.614\n",
      "\u001b[0m\u001b[1mEpoch: [47/70], Progress: 86.02 %, Steps: 4359, Current Learning Rate: 0.0009465, \u001b[91mTrain Loss: 0.693\n",
      "\u001b[0m\u001b[1mEpoch: [47/70], Progress: 87.10 %, Steps: 4360, Current Learning Rate: 0.0009464, \u001b[91mTrain Loss: 0.705\n",
      "\u001b[0m\u001b[1mEpoch: [47/70], Progress: 88.17 %, Steps: 4361, Current Learning Rate: 0.0009463, \u001b[91mTrain Loss: 0.711\n",
      "\u001b[0m\u001b[1mEpoch: [47/70], Progress: 89.25 %, Steps: 4362, Current Learning Rate: 0.0009462, \u001b[96mTrain Loss: 0.687\n",
      "\u001b[0m\u001b[1mEpoch: [47/70], Progress: 90.32 %, Steps: 4363, Current Learning Rate: 0.0009461, \u001b[91mTrain Loss: 0.775\n",
      "\u001b[0m\u001b[1mEpoch: [47/70], Progress: 91.40 %, Steps: 4364, Current Learning Rate: 0.0009460, \u001b[91mTrain Loss: 0.795\n",
      "\u001b[0m\u001b[1mEpoch: [47/70], Progress: 92.47 %, Steps: 4365, Current Learning Rate: 0.0009459, \u001b[96mTrain Loss: 0.762\n",
      "\u001b[0m\u001b[1mEpoch: [47/70], Progress: 93.55 %, Steps: 4366, Current Learning Rate: 0.0009458, \u001b[96mTrain Loss: 0.741\n",
      "\u001b[0m\u001b[1mEpoch: [47/70], Progress: 94.62 %, Steps: 4367, Current Learning Rate: 0.0009457, \u001b[96mTrain Loss: 0.717\n",
      "\u001b[0m\u001b[1mEpoch: [47/70], Progress: 95.70 %, Steps: 4368, Current Learning Rate: 0.0009456, \u001b[91mTrain Loss: 0.738\n",
      "\u001b[0m\u001b[1mEpoch: [47/70], Progress: 96.77 %, Steps: 4369, Current Learning Rate: 0.0009455, \u001b[96mTrain Loss: 0.671\n",
      "\u001b[0m\u001b[1mEpoch: [47/70], Progress: 97.85 %, Steps: 4370, Current Learning Rate: 0.0009453, \u001b[91mTrain Loss: 0.804\n",
      "\u001b[0m\u001b[1mEpoch: [47/70], Progress: 98.92 %, Steps: 4371, Current Learning Rate: 0.0009452, \u001b[96mTrain Loss: 0.724\n",
      "\u001b[1m\u001b[93mComputing Validation Loss...\n",
      "\u001b[1mEpoch 47 Completed! Average Train Loss: 0.666, Average Validation Loss: 0.234\n",
      "Best Model saved in best_model/best_model.pt.\n",
      "\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m= Epoch [48/70] ==========================================\n",
      "\u001b[0m\u001b[1mEpoch: [48/70], Progress: 0.00 %, Steps: 4372, Current Learning Rate: 0.0009451, \u001b[91mTrain Loss: 0.619\n",
      "\u001b[0m\u001b[1mEpoch: [48/70], Progress: 1.08 %, Steps: 4373, Current Learning Rate: 0.0009450, \u001b[96mTrain Loss: 0.501\n",
      "\u001b[0m\u001b[1mEpoch: [48/70], Progress: 2.15 %, Steps: 4374, Current Learning Rate: 0.0009449, \u001b[91mTrain Loss: 0.547\n",
      "\u001b[0m\u001b[1mEpoch: [48/70], Progress: 3.23 %, Steps: 4375, Current Learning Rate: 0.0009448, \u001b[96mTrain Loss: 0.514\n",
      "\u001b[0m\u001b[1mEpoch: [48/70], Progress: 4.30 %, Steps: 4376, Current Learning Rate: 0.0009447, \u001b[96mTrain Loss: 0.402\n",
      "\u001b[0m\u001b[1mEpoch: [48/70], Progress: 5.38 %, Steps: 4377, Current Learning Rate: 0.0009446, \u001b[91mTrain Loss: 0.529\n",
      "\u001b[0m\u001b[1mEpoch: [48/70], Progress: 6.45 %, Steps: 4378, Current Learning Rate: 0.0009445, \u001b[91mTrain Loss: 0.536\n",
      "\u001b[0m\u001b[1mEpoch: [48/70], Progress: 7.53 %, Steps: 4379, Current Learning Rate: 0.0009444, \u001b[91mTrain Loss: 0.549\n",
      "\u001b[0m\u001b[1mEpoch: [48/70], Progress: 8.60 %, Steps: 4380, Current Learning Rate: 0.0009443, \u001b[96mTrain Loss: 0.511\n",
      "\u001b[0m\u001b[1mEpoch: [48/70], Progress: 9.68 %, Steps: 4381, Current Learning Rate: 0.0009442, \u001b[96mTrain Loss: 0.505\n",
      "\u001b[0m\u001b[1mEpoch: [48/70], Progress: 10.75 %, Steps: 4382, Current Learning Rate: 0.0009440, \u001b[96mTrain Loss: 0.450\n",
      "\u001b[0m\u001b[1mEpoch: [48/70], Progress: 11.83 %, Steps: 4383, Current Learning Rate: 0.0009439, \u001b[91mTrain Loss: 0.452\n",
      "\u001b[0m\u001b[1mEpoch: [48/70], Progress: 12.90 %, Steps: 4384, Current Learning Rate: 0.0009438, \u001b[91mTrain Loss: 0.562\n",
      "\u001b[0m\u001b[1mEpoch: [48/70], Progress: 13.98 %, Steps: 4385, Current Learning Rate: 0.0009437, \u001b[91mTrain Loss: 0.615\n",
      "\u001b[0m\u001b[1mEpoch: [48/70], Progress: 15.05 %, Steps: 4386, Current Learning Rate: 0.0009436, \u001b[96mTrain Loss: 0.504\n",
      "\u001b[0m\u001b[1mEpoch: [48/70], Progress: 16.13 %, Steps: 4387, Current Learning Rate: 0.0009435, \u001b[91mTrain Loss: 0.577\n",
      "\u001b[0m\u001b[1mEpoch: [48/70], Progress: 17.20 %, Steps: 4388, Current Learning Rate: 0.0009434, \u001b[96mTrain Loss: 0.449\n",
      "\u001b[0m\u001b[1mEpoch: [48/70], Progress: 18.28 %, Steps: 4389, Current Learning Rate: 0.0009433, \u001b[91mTrain Loss: 0.520\n",
      "\u001b[0m\u001b[1mEpoch: [48/70], Progress: 19.35 %, Steps: 4390, Current Learning Rate: 0.0009432, \u001b[96mTrain Loss: 0.495\n",
      "\u001b[0m\u001b[1mEpoch: [48/70], Progress: 20.43 %, Steps: 4391, Current Learning Rate: 0.0009431, \u001b[91mTrain Loss: 0.552\n",
      "\u001b[0m\u001b[1mEpoch: [48/70], Progress: 21.51 %, Steps: 4392, Current Learning Rate: 0.0009430, \u001b[96mTrain Loss: 0.525\n",
      "\u001b[0m\u001b[1mEpoch: [48/70], Progress: 22.58 %, Steps: 4393, Current Learning Rate: 0.0009429, \u001b[91mTrain Loss: 0.590\n",
      "\u001b[0m\u001b[1mEpoch: [48/70], Progress: 23.66 %, Steps: 4394, Current Learning Rate: 0.0009428, \u001b[91mTrain Loss: 0.593\n",
      "\u001b[0m\u001b[1mEpoch: [48/70], Progress: 24.73 %, Steps: 4395, Current Learning Rate: 0.0009427, \u001b[96mTrain Loss: 0.500\n",
      "\u001b[0m\u001b[1mEpoch: [48/70], Progress: 25.81 %, Steps: 4396, Current Learning Rate: 0.0009425, \u001b[91mTrain Loss: 0.560\n",
      "\u001b[0m\u001b[1mEpoch: [48/70], Progress: 26.88 %, Steps: 4397, Current Learning Rate: 0.0009424, \u001b[96mTrain Loss: 0.503\n",
      "\u001b[0m\u001b[1mEpoch: [48/70], Progress: 27.96 %, Steps: 4398, Current Learning Rate: 0.0009423, \u001b[91mTrain Loss: 0.539\n",
      "\u001b[0m\u001b[1mEpoch: [48/70], Progress: 29.03 %, Steps: 4399, Current Learning Rate: 0.0009422, \u001b[96mTrain Loss: 0.508\n",
      "\u001b[0m\u001b[1mEpoch: [48/70], Progress: 30.11 %, Steps: 4400, Current Learning Rate: 0.0009421, \u001b[91mTrain Loss: 0.520\n",
      "\u001b[0m\u001b[1mEpoch: [48/70], Progress: 31.18 %, Steps: 4401, Current Learning Rate: 0.0009420, \u001b[91mTrain Loss: 0.585\n",
      "\u001b[0m\u001b[1mEpoch: [48/70], Progress: 32.26 %, Steps: 4402, Current Learning Rate: 0.0009419, \u001b[96mTrain Loss: 0.573\n",
      "\u001b[0m\u001b[1mEpoch: [48/70], Progress: 33.33 %, Steps: 4403, Current Learning Rate: 0.0009418, \u001b[91mTrain Loss: 0.639\n",
      "\u001b[0m\u001b[1mEpoch: [48/70], Progress: 34.41 %, Steps: 4404, Current Learning Rate: 0.0009417, \u001b[96mTrain Loss: 0.583\n",
      "\u001b[0m\u001b[1mEpoch: [48/70], Progress: 35.48 %, Steps: 4405, Current Learning Rate: 0.0009416, \u001b[91mTrain Loss: 0.666\n",
      "\u001b[0m\u001b[1mEpoch: [48/70], Progress: 36.56 %, Steps: 4406, Current Learning Rate: 0.0009415, \u001b[96mTrain Loss: 0.554\n",
      "\u001b[0m\u001b[1mEpoch: [48/70], Progress: 37.63 %, Steps: 4407, Current Learning Rate: 0.0009414, \u001b[91mTrain Loss: 0.613\n",
      "\u001b[0m\u001b[1mEpoch: [48/70], Progress: 38.71 %, Steps: 4408, Current Learning Rate: 0.0009413, \u001b[91mTrain Loss: 0.694\n",
      "\u001b[0m\u001b[1mEpoch: [48/70], Progress: 39.78 %, Steps: 4409, Current Learning Rate: 0.0009412, \u001b[96mTrain Loss: 0.671\n",
      "\u001b[0m\u001b[1mEpoch: [48/70], Progress: 40.86 %, Steps: 4410, Current Learning Rate: 0.0009410, \u001b[91mTrain Loss: 0.674\n",
      "\u001b[0m\u001b[1mEpoch: [48/70], Progress: 41.94 %, Steps: 4411, Current Learning Rate: 0.0009409, \u001b[91mTrain Loss: 0.712\n",
      "\u001b[0m\u001b[1mEpoch: [48/70], Progress: 43.01 %, Steps: 4412, Current Learning Rate: 0.0009408, \u001b[96mTrain Loss: 0.675\n",
      "\u001b[0m\u001b[1mEpoch: [48/70], Progress: 44.09 %, Steps: 4413, Current Learning Rate: 0.0009407, \u001b[91mTrain Loss: 0.746\n",
      "\u001b[0m\u001b[1mEpoch: [48/70], Progress: 45.16 %, Steps: 4414, Current Learning Rate: 0.0009406, \u001b[96mTrain Loss: 0.539\n",
      "\u001b[0m\u001b[1mEpoch: [48/70], Progress: 46.24 %, Steps: 4415, Current Learning Rate: 0.0009405, \u001b[91mTrain Loss: 0.604\n",
      "\u001b[0m\u001b[1mEpoch: [48/70], Progress: 47.31 %, Steps: 4416, Current Learning Rate: 0.0009404, \u001b[91mTrain Loss: 0.703\n",
      "\u001b[0m\u001b[1mEpoch: [48/70], Progress: 48.39 %, Steps: 4417, Current Learning Rate: 0.0009403, \u001b[96mTrain Loss: 0.636\n",
      "\u001b[0m\u001b[1mEpoch: [48/70], Progress: 49.46 %, Steps: 4418, Current Learning Rate: 0.0009402, \u001b[91mTrain Loss: 0.644\n",
      "\u001b[0m\u001b[1mEpoch: [48/70], Progress: 50.54 %, Steps: 4419, Current Learning Rate: 0.0009401, \u001b[96mTrain Loss: 0.610\n",
      "\u001b[0m\u001b[1mEpoch: [48/70], Progress: 51.61 %, Steps: 4420, Current Learning Rate: 0.0009400, \u001b[91mTrain Loss: 0.715\n",
      "\u001b[0m\u001b[1mEpoch: [48/70], Progress: 52.69 %, Steps: 4421, Current Learning Rate: 0.0009399, \u001b[96mTrain Loss: 0.608\n",
      "\u001b[0m\u001b[1mEpoch: [48/70], Progress: 53.76 %, Steps: 4422, Current Learning Rate: 0.0009398, \u001b[96mTrain Loss: 0.584\n",
      "\u001b[0m\u001b[1mEpoch: [48/70], Progress: 54.84 %, Steps: 4423, Current Learning Rate: 0.0009397, \u001b[91mTrain Loss: 0.661\n",
      "\u001b[0m\u001b[1mEpoch: [48/70], Progress: 55.91 %, Steps: 4424, Current Learning Rate: 0.0009396, \u001b[91mTrain Loss: 0.669\n",
      "\u001b[0m\u001b[1mEpoch: [48/70], Progress: 56.99 %, Steps: 4425, Current Learning Rate: 0.0009395, \u001b[96mTrain Loss: 0.623\n",
      "\u001b[0m\u001b[1mEpoch: [48/70], Progress: 58.06 %, Steps: 4426, Current Learning Rate: 0.0009393, \u001b[91mTrain Loss: 0.647\n",
      "\u001b[0m\u001b[1mEpoch: [48/70], Progress: 59.14 %, Steps: 4427, Current Learning Rate: 0.0009392, \u001b[96mTrain Loss: 0.602\n",
      "\u001b[0m\u001b[1mEpoch: [48/70], Progress: 60.22 %, Steps: 4428, Current Learning Rate: 0.0009391, \u001b[96mTrain Loss: 0.575\n",
      "\u001b[0m\u001b[1mEpoch: [48/70], Progress: 61.29 %, Steps: 4429, Current Learning Rate: 0.0009390, \u001b[91mTrain Loss: 0.616\n",
      "\u001b[0m\u001b[1mEpoch: [48/70], Progress: 62.37 %, Steps: 4430, Current Learning Rate: 0.0009389, \u001b[96mTrain Loss: 0.556\n",
      "\u001b[0m\u001b[1mEpoch: [48/70], Progress: 63.44 %, Steps: 4431, Current Learning Rate: 0.0009388, \u001b[91mTrain Loss: 0.604\n",
      "\u001b[0m\u001b[1mEpoch: [48/70], Progress: 64.52 %, Steps: 4432, Current Learning Rate: 0.0009387, \u001b[91mTrain Loss: 0.607\n",
      "\u001b[0m\u001b[1mEpoch: [48/70], Progress: 65.59 %, Steps: 4433, Current Learning Rate: 0.0009386, \u001b[91mTrain Loss: 0.674\n",
      "\u001b[0m\u001b[1mEpoch: [48/70], Progress: 66.67 %, Steps: 4434, Current Learning Rate: 0.0009385, \u001b[96mTrain Loss: 0.575\n",
      "\u001b[0m\u001b[1mEpoch: [48/70], Progress: 67.74 %, Steps: 4435, Current Learning Rate: 0.0009384, \u001b[91mTrain Loss: 0.640\n",
      "\u001b[0m\u001b[1mEpoch: [48/70], Progress: 68.82 %, Steps: 4436, Current Learning Rate: 0.0009383, \u001b[96mTrain Loss: 0.595\n",
      "\u001b[0m\u001b[1mEpoch: [48/70], Progress: 69.89 %, Steps: 4437, Current Learning Rate: 0.0009382, \u001b[91mTrain Loss: 0.599\n",
      "\u001b[0m\u001b[1mEpoch: [48/70], Progress: 70.97 %, Steps: 4438, Current Learning Rate: 0.0009381, \u001b[96mTrain Loss: 0.536\n",
      "\u001b[0m\u001b[1mEpoch: [48/70], Progress: 72.04 %, Steps: 4439, Current Learning Rate: 0.0009380, \u001b[91mTrain Loss: 0.630\n",
      "\u001b[0m\u001b[1mEpoch: [48/70], Progress: 73.12 %, Steps: 4440, Current Learning Rate: 0.0009379, \u001b[96mTrain Loss: 0.613\n",
      "\u001b[0m\u001b[1mEpoch: [48/70], Progress: 74.19 %, Steps: 4441, Current Learning Rate: 0.0009378, \u001b[96mTrain Loss: 0.562\n",
      "\u001b[0m\u001b[1mEpoch: [48/70], Progress: 75.27 %, Steps: 4442, Current Learning Rate: 0.0009377, \u001b[91mTrain Loss: 0.662\n",
      "\u001b[0m\u001b[1mEpoch: [48/70], Progress: 76.34 %, Steps: 4443, Current Learning Rate: 0.0009375, \u001b[91mTrain Loss: 0.715\n",
      "\u001b[0m\u001b[1mEpoch: [48/70], Progress: 77.42 %, Steps: 4444, Current Learning Rate: 0.0009374, \u001b[96mTrain Loss: 0.533\n",
      "\u001b[0m\u001b[1mEpoch: [48/70], Progress: 78.49 %, Steps: 4445, Current Learning Rate: 0.0009373, \u001b[91mTrain Loss: 0.653\n",
      "\u001b[0m\u001b[1mEpoch: [48/70], Progress: 79.57 %, Steps: 4446, Current Learning Rate: 0.0009372, \u001b[91mTrain Loss: 0.748\n",
      "\u001b[0m\u001b[1mEpoch: [48/70], Progress: 80.65 %, Steps: 4447, Current Learning Rate: 0.0009371, \u001b[96mTrain Loss: 0.671\n",
      "\u001b[0m\u001b[1mEpoch: [48/70], Progress: 81.72 %, Steps: 4448, Current Learning Rate: 0.0009370, \u001b[91mTrain Loss: 0.693\n",
      "\u001b[0m\u001b[1mEpoch: [48/70], Progress: 82.80 %, Steps: 4449, Current Learning Rate: 0.0009369, \u001b[96mTrain Loss: 0.685\n",
      "\u001b[0m\u001b[1mEpoch: [48/70], Progress: 83.87 %, Steps: 4450, Current Learning Rate: 0.0009368, \u001b[91mTrain Loss: 0.715\n",
      "\u001b[0m\u001b[1mEpoch: [48/70], Progress: 84.95 %, Steps: 4451, Current Learning Rate: 0.0009367, \u001b[91mTrain Loss: 0.742\n",
      "\u001b[0m\u001b[1mEpoch: [48/70], Progress: 86.02 %, Steps: 4452, Current Learning Rate: 0.0009366, \u001b[96mTrain Loss: 0.548\n",
      "\u001b[0m\u001b[1mEpoch: [48/70], Progress: 87.10 %, Steps: 4453, Current Learning Rate: 0.0009365, \u001b[91mTrain Loss: 0.629\n",
      "\u001b[0m\u001b[1mEpoch: [48/70], Progress: 88.17 %, Steps: 4454, Current Learning Rate: 0.0009364, \u001b[91mTrain Loss: 0.660\n",
      "\u001b[0m\u001b[1mEpoch: [48/70], Progress: 89.25 %, Steps: 4455, Current Learning Rate: 0.0009363, \u001b[96mTrain Loss: 0.589\n",
      "\u001b[0m\u001b[1mEpoch: [48/70], Progress: 90.32 %, Steps: 4456, Current Learning Rate: 0.0009362, \u001b[91mTrain Loss: 0.591\n",
      "\u001b[0m\u001b[1mEpoch: [48/70], Progress: 91.40 %, Steps: 4457, Current Learning Rate: 0.0009361, \u001b[91mTrain Loss: 0.668\n",
      "\u001b[0m\u001b[1mEpoch: [48/70], Progress: 92.47 %, Steps: 4458, Current Learning Rate: 0.0009360, \u001b[96mTrain Loss: 0.624\n",
      "\u001b[0m\u001b[1mEpoch: [48/70], Progress: 93.55 %, Steps: 4459, Current Learning Rate: 0.0009359, \u001b[96mTrain Loss: 0.606\n",
      "\u001b[0m\u001b[1mEpoch: [48/70], Progress: 94.62 %, Steps: 4460, Current Learning Rate: 0.0009358, \u001b[91mTrain Loss: 0.797\n",
      "\u001b[0m\u001b[1mEpoch: [48/70], Progress: 95.70 %, Steps: 4461, Current Learning Rate: 0.0009357, \u001b[96mTrain Loss: 0.667\n",
      "\u001b[0m\u001b[1mEpoch: [48/70], Progress: 96.77 %, Steps: 4462, Current Learning Rate: 0.0009355, \u001b[91mTrain Loss: 0.678\n",
      "\u001b[0m\u001b[1mEpoch: [48/70], Progress: 97.85 %, Steps: 4463, Current Learning Rate: 0.0009354, \u001b[91mTrain Loss: 0.721\n",
      "\u001b[0m\u001b[1mEpoch: [48/70], Progress: 98.92 %, Steps: 4464, Current Learning Rate: 0.0009353, \u001b[96mTrain Loss: 0.530\n",
      "\u001b[1m\u001b[93mComputing Validation Loss...\n",
      "\u001b[1mEpoch 48 Completed! Average Train Loss: 0.602, Average Validation Loss: 0.193\n",
      "Best Model saved in best_model/best_model.pt.\n",
      "\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m= Epoch [49/70] ==========================================\n",
      "\u001b[0m\u001b[1mEpoch: [49/70], Progress: 0.00 %, Steps: 4465, Current Learning Rate: 0.0009352, \u001b[91mTrain Loss: 0.434\n",
      "\u001b[0m\u001b[1mEpoch: [49/70], Progress: 1.08 %, Steps: 4466, Current Learning Rate: 0.0009351, \u001b[91mTrain Loss: 0.459\n",
      "\u001b[0m\u001b[1mEpoch: [49/70], Progress: 2.15 %, Steps: 4467, Current Learning Rate: 0.0009350, \u001b[96mTrain Loss: 0.434\n",
      "\u001b[0m\u001b[1mEpoch: [49/70], Progress: 3.23 %, Steps: 4468, Current Learning Rate: 0.0009349, \u001b[91mTrain Loss: 0.468\n",
      "\u001b[0m\u001b[1mEpoch: [49/70], Progress: 4.30 %, Steps: 4469, Current Learning Rate: 0.0009348, \u001b[96mTrain Loss: 0.410\n",
      "\u001b[0m\u001b[1mEpoch: [49/70], Progress: 5.38 %, Steps: 4470, Current Learning Rate: 0.0009347, \u001b[91mTrain Loss: 0.423\n",
      "\u001b[0m\u001b[1mEpoch: [49/70], Progress: 6.45 %, Steps: 4471, Current Learning Rate: 0.0009346, \u001b[91mTrain Loss: 0.463\n",
      "\u001b[0m\u001b[1mEpoch: [49/70], Progress: 7.53 %, Steps: 4472, Current Learning Rate: 0.0009345, \u001b[91mTrain Loss: 0.533\n",
      "\u001b[0m\u001b[1mEpoch: [49/70], Progress: 8.60 %, Steps: 4473, Current Learning Rate: 0.0009344, \u001b[96mTrain Loss: 0.497\n",
      "\u001b[0m\u001b[1mEpoch: [49/70], Progress: 9.68 %, Steps: 4474, Current Learning Rate: 0.0009343, \u001b[96mTrain Loss: 0.484\n",
      "\u001b[0m\u001b[1mEpoch: [49/70], Progress: 10.75 %, Steps: 4475, Current Learning Rate: 0.0009342, \u001b[96mTrain Loss: 0.427\n",
      "\u001b[0m\u001b[1mEpoch: [49/70], Progress: 11.83 %, Steps: 4476, Current Learning Rate: 0.0009341, \u001b[91mTrain Loss: 0.490\n",
      "\u001b[0m\u001b[1mEpoch: [49/70], Progress: 12.90 %, Steps: 4477, Current Learning Rate: 0.0009340, \u001b[96mTrain Loss: 0.399\n",
      "\u001b[0m\u001b[1mEpoch: [49/70], Progress: 13.98 %, Steps: 4478, Current Learning Rate: 0.0009339, \u001b[91mTrain Loss: 0.452\n",
      "\u001b[0m\u001b[1mEpoch: [49/70], Progress: 15.05 %, Steps: 4479, Current Learning Rate: 0.0009338, \u001b[96mTrain Loss: 0.443\n",
      "\u001b[0m\u001b[1mEpoch: [49/70], Progress: 16.13 %, Steps: 4480, Current Learning Rate: 0.0009337, \u001b[91mTrain Loss: 0.505\n",
      "\u001b[0m\u001b[1mEpoch: [49/70], Progress: 17.20 %, Steps: 4481, Current Learning Rate: 0.0009336, \u001b[91mTrain Loss: 0.509\n",
      "\u001b[0m\u001b[1mEpoch: [49/70], Progress: 18.28 %, Steps: 4482, Current Learning Rate: 0.0009335, \u001b[96mTrain Loss: 0.454\n",
      "\u001b[0m\u001b[1mEpoch: [49/70], Progress: 19.35 %, Steps: 4483, Current Learning Rate: 0.0009334, \u001b[96mTrain Loss: 0.453\n",
      "\u001b[0m\u001b[1mEpoch: [49/70], Progress: 20.43 %, Steps: 4484, Current Learning Rate: 0.0009333, \u001b[91mTrain Loss: 0.508\n",
      "\u001b[0m\u001b[1mEpoch: [49/70], Progress: 21.51 %, Steps: 4485, Current Learning Rate: 0.0009331, \u001b[91mTrain Loss: 0.544\n",
      "\u001b[0m\u001b[1mEpoch: [49/70], Progress: 22.58 %, Steps: 4486, Current Learning Rate: 0.0009330, \u001b[96mTrain Loss: 0.541\n",
      "\u001b[0m\u001b[1mEpoch: [49/70], Progress: 23.66 %, Steps: 4487, Current Learning Rate: 0.0009329, \u001b[96mTrain Loss: 0.460\n",
      "\u001b[0m\u001b[1mEpoch: [49/70], Progress: 24.73 %, Steps: 4488, Current Learning Rate: 0.0009328, \u001b[91mTrain Loss: 0.502\n",
      "\u001b[0m\u001b[1mEpoch: [49/70], Progress: 25.81 %, Steps: 4489, Current Learning Rate: 0.0009327, \u001b[91mTrain Loss: 0.643\n",
      "\u001b[0m\u001b[1mEpoch: [49/70], Progress: 26.88 %, Steps: 4490, Current Learning Rate: 0.0009326, \u001b[96mTrain Loss: 0.453\n",
      "\u001b[0m\u001b[1mEpoch: [49/70], Progress: 27.96 %, Steps: 4491, Current Learning Rate: 0.0009325, \u001b[91mTrain Loss: 0.497\n",
      "\u001b[0m\u001b[1mEpoch: [49/70], Progress: 29.03 %, Steps: 4492, Current Learning Rate: 0.0009324, \u001b[91mTrain Loss: 0.570\n",
      "\u001b[0m\u001b[1mEpoch: [49/70], Progress: 30.11 %, Steps: 4493, Current Learning Rate: 0.0009323, \u001b[91mTrain Loss: 0.617\n",
      "\u001b[0m\u001b[1mEpoch: [49/70], Progress: 31.18 %, Steps: 4494, Current Learning Rate: 0.0009322, \u001b[96mTrain Loss: 0.419\n",
      "\u001b[0m\u001b[1mEpoch: [49/70], Progress: 32.26 %, Steps: 4495, Current Learning Rate: 0.0009321, \u001b[91mTrain Loss: 0.591\n",
      "\u001b[0m\u001b[1mEpoch: [49/70], Progress: 33.33 %, Steps: 4496, Current Learning Rate: 0.0009320, \u001b[96mTrain Loss: 0.431\n",
      "\u001b[0m\u001b[1mEpoch: [49/70], Progress: 34.41 %, Steps: 4497, Current Learning Rate: 0.0009319, \u001b[96mTrain Loss: 0.424\n",
      "\u001b[0m\u001b[1mEpoch: [49/70], Progress: 35.48 %, Steps: 4498, Current Learning Rate: 0.0009318, \u001b[91mTrain Loss: 0.537\n",
      "\u001b[0m\u001b[1mEpoch: [49/70], Progress: 36.56 %, Steps: 4499, Current Learning Rate: 0.0009317, \u001b[96mTrain Loss: 0.525\n",
      "\u001b[0m\u001b[1mEpoch: [49/70], Progress: 37.63 %, Steps: 4500, Current Learning Rate: 0.0009316, \u001b[91mTrain Loss: 0.550\n",
      "\u001b[0m\u001b[1mEpoch: [49/70], Progress: 38.71 %, Steps: 4501, Current Learning Rate: 0.0009315, \u001b[91mTrain Loss: 0.572\n",
      "\u001b[0m\u001b[1mEpoch: [49/70], Progress: 39.78 %, Steps: 4502, Current Learning Rate: 0.0009314, \u001b[91mTrain Loss: 0.714\n",
      "\u001b[0m\u001b[1mEpoch: [49/70], Progress: 40.86 %, Steps: 4503, Current Learning Rate: 0.0009313, \u001b[96mTrain Loss: 0.533\n",
      "\u001b[0m\u001b[1mEpoch: [49/70], Progress: 41.94 %, Steps: 4504, Current Learning Rate: 0.0009312, \u001b[91mTrain Loss: 0.544\n",
      "\u001b[0m\u001b[1mEpoch: [49/70], Progress: 43.01 %, Steps: 4505, Current Learning Rate: 0.0009311, \u001b[96mTrain Loss: 0.540\n",
      "\u001b[0m\u001b[1mEpoch: [49/70], Progress: 44.09 %, Steps: 4506, Current Learning Rate: 0.0009310, \u001b[96mTrain Loss: 0.501\n",
      "\u001b[0m\u001b[1mEpoch: [49/70], Progress: 45.16 %, Steps: 4507, Current Learning Rate: 0.0009309, \u001b[91mTrain Loss: 0.512\n",
      "\u001b[0m\u001b[1mEpoch: [49/70], Progress: 46.24 %, Steps: 4508, Current Learning Rate: 0.0009308, \u001b[91mTrain Loss: 0.577\n",
      "\u001b[0m\u001b[1mEpoch: [49/70], Progress: 47.31 %, Steps: 4509, Current Learning Rate: 0.0009307, \u001b[96mTrain Loss: 0.468\n",
      "\u001b[0m\u001b[1mEpoch: [49/70], Progress: 48.39 %, Steps: 4510, Current Learning Rate: 0.0009306, \u001b[91mTrain Loss: 0.502\n",
      "\u001b[0m\u001b[1mEpoch: [49/70], Progress: 49.46 %, Steps: 4511, Current Learning Rate: 0.0009305, \u001b[91mTrain Loss: 0.569\n",
      "\u001b[0m\u001b[1mEpoch: [49/70], Progress: 50.54 %, Steps: 4512, Current Learning Rate: 0.0009304, \u001b[96mTrain Loss: 0.568\n",
      "\u001b[0m\u001b[1mEpoch: [49/70], Progress: 51.61 %, Steps: 4513, Current Learning Rate: 0.0009302, \u001b[96mTrain Loss: 0.557\n",
      "\u001b[0m\u001b[1mEpoch: [49/70], Progress: 52.69 %, Steps: 4514, Current Learning Rate: 0.0009301, \u001b[91mTrain Loss: 0.681\n",
      "\u001b[0m\u001b[1mEpoch: [49/70], Progress: 53.76 %, Steps: 4515, Current Learning Rate: 0.0009300, \u001b[96mTrain Loss: 0.462\n",
      "\u001b[0m\u001b[1mEpoch: [49/70], Progress: 54.84 %, Steps: 4516, Current Learning Rate: 0.0009299, \u001b[91mTrain Loss: 0.600\n",
      "\u001b[0m\u001b[1mEpoch: [49/70], Progress: 55.91 %, Steps: 4517, Current Learning Rate: 0.0009298, \u001b[96mTrain Loss: 0.591\n",
      "\u001b[0m\u001b[1mEpoch: [49/70], Progress: 56.99 %, Steps: 4518, Current Learning Rate: 0.0009297, \u001b[91mTrain Loss: 0.670\n",
      "\u001b[0m\u001b[1mEpoch: [49/70], Progress: 58.06 %, Steps: 4519, Current Learning Rate: 0.0009296, \u001b[96mTrain Loss: 0.648\n",
      "\u001b[0m\u001b[1mEpoch: [49/70], Progress: 59.14 %, Steps: 4520, Current Learning Rate: 0.0009295, \u001b[96mTrain Loss: 0.570\n",
      "\u001b[0m\u001b[1mEpoch: [49/70], Progress: 60.22 %, Steps: 4521, Current Learning Rate: 0.0009294, \u001b[96mTrain Loss: 0.535\n",
      "\u001b[0m\u001b[1mEpoch: [49/70], Progress: 61.29 %, Steps: 4522, Current Learning Rate: 0.0009293, \u001b[91mTrain Loss: 0.578\n",
      "\u001b[0m\u001b[1mEpoch: [49/70], Progress: 62.37 %, Steps: 4523, Current Learning Rate: 0.0009292, \u001b[96mTrain Loss: 0.489\n",
      "\u001b[0m\u001b[1mEpoch: [49/70], Progress: 63.44 %, Steps: 4524, Current Learning Rate: 0.0009291, \u001b[91mTrain Loss: 0.527\n",
      "\u001b[0m\u001b[1mEpoch: [49/70], Progress: 64.52 %, Steps: 4525, Current Learning Rate: 0.0009290, \u001b[91mTrain Loss: 0.659\n",
      "\u001b[0m\u001b[1mEpoch: [49/70], Progress: 65.59 %, Steps: 4526, Current Learning Rate: 0.0009289, \u001b[96mTrain Loss: 0.566\n",
      "\u001b[0m\u001b[1mEpoch: [49/70], Progress: 66.67 %, Steps: 4527, Current Learning Rate: 0.0009288, \u001b[96mTrain Loss: 0.539\n",
      "\u001b[0m\u001b[1mEpoch: [49/70], Progress: 67.74 %, Steps: 4528, Current Learning Rate: 0.0009287, \u001b[96mTrain Loss: 0.515\n",
      "\u001b[0m\u001b[1mEpoch: [49/70], Progress: 68.82 %, Steps: 4529, Current Learning Rate: 0.0009286, \u001b[91mTrain Loss: 0.611\n",
      "\u001b[0m\u001b[1mEpoch: [49/70], Progress: 69.89 %, Steps: 4530, Current Learning Rate: 0.0009285, \u001b[96mTrain Loss: 0.549\n",
      "\u001b[0m\u001b[1mEpoch: [49/70], Progress: 70.97 %, Steps: 4531, Current Learning Rate: 0.0009284, \u001b[91mTrain Loss: 0.585\n",
      "\u001b[0m\u001b[1mEpoch: [49/70], Progress: 72.04 %, Steps: 4532, Current Learning Rate: 0.0009283, \u001b[91mTrain Loss: 0.666\n",
      "\u001b[0m\u001b[1mEpoch: [49/70], Progress: 73.12 %, Steps: 4533, Current Learning Rate: 0.0009282, \u001b[96mTrain Loss: 0.595\n",
      "\u001b[0m\u001b[1mEpoch: [49/70], Progress: 74.19 %, Steps: 4534, Current Learning Rate: 0.0009281, \u001b[96mTrain Loss: 0.508\n",
      "\u001b[0m\u001b[1mEpoch: [49/70], Progress: 75.27 %, Steps: 4535, Current Learning Rate: 0.0009280, \u001b[91mTrain Loss: 0.637\n",
      "\u001b[0m\u001b[1mEpoch: [49/70], Progress: 76.34 %, Steps: 4536, Current Learning Rate: 0.0009279, \u001b[91mTrain Loss: 0.653\n",
      "\u001b[0m\u001b[1mEpoch: [49/70], Progress: 77.42 %, Steps: 4537, Current Learning Rate: 0.0009278, \u001b[96mTrain Loss: 0.563\n",
      "\u001b[0m\u001b[1mEpoch: [49/70], Progress: 78.49 %, Steps: 4538, Current Learning Rate: 0.0009277, \u001b[96mTrain Loss: 0.555\n",
      "\u001b[0m\u001b[1mEpoch: [49/70], Progress: 79.57 %, Steps: 4539, Current Learning Rate: 0.0009276, \u001b[91mTrain Loss: 0.591\n",
      "\u001b[0m\u001b[1mEpoch: [49/70], Progress: 80.65 %, Steps: 4540, Current Learning Rate: 0.0009275, \u001b[91mTrain Loss: 0.677\n",
      "\u001b[0m\u001b[1mEpoch: [49/70], Progress: 81.72 %, Steps: 4541, Current Learning Rate: 0.0009274, \u001b[91mTrain Loss: 0.681\n",
      "\u001b[0m\u001b[1mEpoch: [49/70], Progress: 82.80 %, Steps: 4542, Current Learning Rate: 0.0009273, \u001b[91mTrain Loss: 0.730\n",
      "\u001b[0m\u001b[1mEpoch: [49/70], Progress: 83.87 %, Steps: 4543, Current Learning Rate: 0.0009272, \u001b[96mTrain Loss: 0.547\n",
      "\u001b[0m\u001b[1mEpoch: [49/70], Progress: 84.95 %, Steps: 4544, Current Learning Rate: 0.0009271, \u001b[91mTrain Loss: 0.619\n",
      "\u001b[0m\u001b[1mEpoch: [49/70], Progress: 86.02 %, Steps: 4545, Current Learning Rate: 0.0009270, \u001b[91mTrain Loss: 0.664\n",
      "\u001b[0m\u001b[1mEpoch: [49/70], Progress: 87.10 %, Steps: 4546, Current Learning Rate: 0.0009269, \u001b[96mTrain Loss: 0.599\n",
      "\u001b[0m\u001b[1mEpoch: [49/70], Progress: 88.17 %, Steps: 4547, Current Learning Rate: 0.0009268, \u001b[96mTrain Loss: 0.541\n",
      "\u001b[0m\u001b[1mEpoch: [49/70], Progress: 89.25 %, Steps: 4548, Current Learning Rate: 0.0009267, \u001b[91mTrain Loss: 0.626\n",
      "\u001b[0m\u001b[1mEpoch: [49/70], Progress: 90.32 %, Steps: 4549, Current Learning Rate: 0.0009266, \u001b[96mTrain Loss: 0.575\n",
      "\u001b[0m\u001b[1mEpoch: [49/70], Progress: 91.40 %, Steps: 4550, Current Learning Rate: 0.0009265, \u001b[91mTrain Loss: 0.629\n",
      "\u001b[0m\u001b[1mEpoch: [49/70], Progress: 92.47 %, Steps: 4551, Current Learning Rate: 0.0009264, \u001b[96mTrain Loss: 0.602\n",
      "\u001b[0m\u001b[1mEpoch: [49/70], Progress: 93.55 %, Steps: 4552, Current Learning Rate: 0.0009263, \u001b[96mTrain Loss: 0.585\n",
      "\u001b[0m\u001b[1mEpoch: [49/70], Progress: 94.62 %, Steps: 4553, Current Learning Rate: 0.0009262, \u001b[91mTrain Loss: 0.610\n",
      "\u001b[0m\u001b[1mEpoch: [49/70], Progress: 95.70 %, Steps: 4554, Current Learning Rate: 0.0009261, \u001b[96mTrain Loss: 0.544\n",
      "\u001b[0m\u001b[1mEpoch: [49/70], Progress: 96.77 %, Steps: 4555, Current Learning Rate: 0.0009260, \u001b[91mTrain Loss: 0.647\n",
      "\u001b[0m\u001b[1mEpoch: [49/70], Progress: 97.85 %, Steps: 4556, Current Learning Rate: 0.0009258, \u001b[96mTrain Loss: 0.436\n",
      "\u001b[0m\u001b[1mEpoch: [49/70], Progress: 98.92 %, Steps: 4557, Current Learning Rate: 0.0009257, \u001b[91mTrain Loss: 0.456\n",
      "\u001b[1m\u001b[93mComputing Validation Loss...\n",
      "\u001b[1mEpoch 49 Completed! Average Train Loss: 0.544, Average Validation Loss: 0.160\n",
      "Best Model saved in best_model/best_model.pt.\n",
      "\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m= Epoch [50/70] ==========================================\n",
      "\u001b[0m\u001b[1mEpoch: [50/70], Progress: 0.00 %, Steps: 4558, Current Learning Rate: 0.0009256, \u001b[91mTrain Loss: 0.399\n",
      "\u001b[0m\u001b[1mEpoch: [50/70], Progress: 1.08 %, Steps: 4559, Current Learning Rate: 0.0009255, \u001b[91mTrain Loss: 0.418\n",
      "\u001b[0m\u001b[1mEpoch: [50/70], Progress: 2.15 %, Steps: 4560, Current Learning Rate: 0.0009254, \u001b[91mTrain Loss: 0.422\n",
      "\u001b[0m\u001b[1mEpoch: [50/70], Progress: 3.23 %, Steps: 4561, Current Learning Rate: 0.0009253, \u001b[96mTrain Loss: 0.381\n",
      "\u001b[0m\u001b[1mEpoch: [50/70], Progress: 4.30 %, Steps: 4562, Current Learning Rate: 0.0009252, \u001b[91mTrain Loss: 0.488\n",
      "\u001b[0m\u001b[1mEpoch: [50/70], Progress: 5.38 %, Steps: 4563, Current Learning Rate: 0.0009251, \u001b[96mTrain Loss: 0.401\n",
      "\u001b[0m\u001b[1mEpoch: [50/70], Progress: 6.45 %, Steps: 4564, Current Learning Rate: 0.0009250, \u001b[91mTrain Loss: 0.523\n",
      "\u001b[0m\u001b[1mEpoch: [50/70], Progress: 7.53 %, Steps: 4565, Current Learning Rate: 0.0009249, \u001b[96mTrain Loss: 0.383\n",
      "\u001b[0m\u001b[1mEpoch: [50/70], Progress: 8.60 %, Steps: 4566, Current Learning Rate: 0.0009248, \u001b[96mTrain Loss: 0.371\n",
      "\u001b[0m\u001b[1mEpoch: [50/70], Progress: 9.68 %, Steps: 4567, Current Learning Rate: 0.0009247, \u001b[91mTrain Loss: 0.425\n",
      "\u001b[0m\u001b[1mEpoch: [50/70], Progress: 10.75 %, Steps: 4568, Current Learning Rate: 0.0009246, \u001b[96mTrain Loss: 0.350\n",
      "\u001b[0m\u001b[1mEpoch: [50/70], Progress: 11.83 %, Steps: 4569, Current Learning Rate: 0.0009245, \u001b[91mTrain Loss: 0.395\n",
      "\u001b[0m\u001b[1mEpoch: [50/70], Progress: 12.90 %, Steps: 4570, Current Learning Rate: 0.0009244, \u001b[91mTrain Loss: 0.507\n",
      "\u001b[0m\u001b[1mEpoch: [50/70], Progress: 13.98 %, Steps: 4571, Current Learning Rate: 0.0009243, \u001b[96mTrain Loss: 0.400\n",
      "\u001b[0m\u001b[1mEpoch: [50/70], Progress: 15.05 %, Steps: 4572, Current Learning Rate: 0.0009242, \u001b[96mTrain Loss: 0.395\n",
      "\u001b[0m\u001b[1mEpoch: [50/70], Progress: 16.13 %, Steps: 4573, Current Learning Rate: 0.0009241, \u001b[91mTrain Loss: 0.429\n",
      "\u001b[0m\u001b[1mEpoch: [50/70], Progress: 17.20 %, Steps: 4574, Current Learning Rate: 0.0009240, \u001b[96mTrain Loss: 0.414\n",
      "\u001b[0m\u001b[1mEpoch: [50/70], Progress: 18.28 %, Steps: 4575, Current Learning Rate: 0.0009239, \u001b[91mTrain Loss: 0.431\n",
      "\u001b[0m\u001b[1mEpoch: [50/70], Progress: 19.35 %, Steps: 4576, Current Learning Rate: 0.0009238, \u001b[91mTrain Loss: 0.459\n",
      "\u001b[0m\u001b[1mEpoch: [50/70], Progress: 20.43 %, Steps: 4577, Current Learning Rate: 0.0009237, \u001b[91mTrain Loss: 0.501\n",
      "\u001b[0m\u001b[1mEpoch: [50/70], Progress: 21.51 %, Steps: 4578, Current Learning Rate: 0.0009236, \u001b[96mTrain Loss: 0.442\n",
      "\u001b[0m\u001b[1mEpoch: [50/70], Progress: 22.58 %, Steps: 4579, Current Learning Rate: 0.0009235, \u001b[96mTrain Loss: 0.405\n",
      "\u001b[0m\u001b[1mEpoch: [50/70], Progress: 23.66 %, Steps: 4580, Current Learning Rate: 0.0009234, \u001b[91mTrain Loss: 0.482\n",
      "\u001b[0m\u001b[1mEpoch: [50/70], Progress: 24.73 %, Steps: 4581, Current Learning Rate: 0.0009233, \u001b[91mTrain Loss: 0.510\n",
      "\u001b[0m\u001b[1mEpoch: [50/70], Progress: 25.81 %, Steps: 4582, Current Learning Rate: 0.0009232, \u001b[96mTrain Loss: 0.469\n",
      "\u001b[0m\u001b[1mEpoch: [50/70], Progress: 26.88 %, Steps: 4583, Current Learning Rate: 0.0009231, \u001b[96mTrain Loss: 0.432\n",
      "\u001b[0m\u001b[1mEpoch: [50/70], Progress: 27.96 %, Steps: 4584, Current Learning Rate: 0.0009230, \u001b[91mTrain Loss: 0.527\n",
      "\u001b[0m\u001b[1mEpoch: [50/70], Progress: 29.03 %, Steps: 4585, Current Learning Rate: 0.0009229, \u001b[91mTrain Loss: 0.539\n",
      "\u001b[0m\u001b[1mEpoch: [50/70], Progress: 30.11 %, Steps: 4586, Current Learning Rate: 0.0009228, \u001b[96mTrain Loss: 0.507\n",
      "\u001b[0m\u001b[1mEpoch: [50/70], Progress: 31.18 %, Steps: 4587, Current Learning Rate: 0.0009227, \u001b[96mTrain Loss: 0.503\n",
      "\u001b[0m\u001b[1mEpoch: [50/70], Progress: 32.26 %, Steps: 4588, Current Learning Rate: 0.0009226, \u001b[96mTrain Loss: 0.495\n",
      "\u001b[0m\u001b[1mEpoch: [50/70], Progress: 33.33 %, Steps: 4589, Current Learning Rate: 0.0009225, \u001b[96mTrain Loss: 0.481\n",
      "\u001b[0m\u001b[1mEpoch: [50/70], Progress: 34.41 %, Steps: 4590, Current Learning Rate: 0.0009224, \u001b[96mTrain Loss: 0.472\n",
      "\u001b[0m\u001b[1mEpoch: [50/70], Progress: 35.48 %, Steps: 4591, Current Learning Rate: 0.0009223, \u001b[91mTrain Loss: 0.544\n",
      "\u001b[0m\u001b[1mEpoch: [50/70], Progress: 36.56 %, Steps: 4592, Current Learning Rate: 0.0009222, \u001b[91mTrain Loss: 0.557\n",
      "\u001b[0m\u001b[1mEpoch: [50/70], Progress: 37.63 %, Steps: 4593, Current Learning Rate: 0.0009221, \u001b[91mTrain Loss: 0.571\n",
      "\u001b[0m\u001b[1mEpoch: [50/70], Progress: 38.71 %, Steps: 4594, Current Learning Rate: 0.0009220, \u001b[96mTrain Loss: 0.453\n",
      "\u001b[0m\u001b[1mEpoch: [50/70], Progress: 39.78 %, Steps: 4595, Current Learning Rate: 0.0009219, \u001b[91mTrain Loss: 0.470\n",
      "\u001b[0m\u001b[1mEpoch: [50/70], Progress: 40.86 %, Steps: 4596, Current Learning Rate: 0.0009218, \u001b[96mTrain Loss: 0.457\n",
      "\u001b[0m\u001b[1mEpoch: [50/70], Progress: 41.94 %, Steps: 4597, Current Learning Rate: 0.0009217, \u001b[91mTrain Loss: 0.521\n",
      "\u001b[0m\u001b[1mEpoch: [50/70], Progress: 43.01 %, Steps: 4598, Current Learning Rate: 0.0009216, \u001b[96mTrain Loss: 0.482\n",
      "\u001b[0m\u001b[1mEpoch: [50/70], Progress: 44.09 %, Steps: 4599, Current Learning Rate: 0.0009215, \u001b[91mTrain Loss: 0.517\n",
      "\u001b[0m\u001b[1mEpoch: [50/70], Progress: 45.16 %, Steps: 4600, Current Learning Rate: 0.0009214, \u001b[96mTrain Loss: 0.479\n",
      "\u001b[0m\u001b[1mEpoch: [50/70], Progress: 46.24 %, Steps: 4601, Current Learning Rate: 0.0009213, \u001b[91mTrain Loss: 0.484\n",
      "\u001b[0m\u001b[1mEpoch: [50/70], Progress: 47.31 %, Steps: 4602, Current Learning Rate: 0.0009212, \u001b[96mTrain Loss: 0.408\n",
      "\u001b[0m\u001b[1mEpoch: [50/70], Progress: 48.39 %, Steps: 4603, Current Learning Rate: 0.0009211, \u001b[91mTrain Loss: 0.535\n",
      "\u001b[0m\u001b[1mEpoch: [50/70], Progress: 49.46 %, Steps: 4604, Current Learning Rate: 0.0009210, \u001b[96mTrain Loss: 0.443\n",
      "\u001b[0m\u001b[1mEpoch: [50/70], Progress: 50.54 %, Steps: 4605, Current Learning Rate: 0.0009209, \u001b[91mTrain Loss: 0.540\n",
      "\u001b[0m\u001b[1mEpoch: [50/70], Progress: 51.61 %, Steps: 4606, Current Learning Rate: 0.0009208, \u001b[96mTrain Loss: 0.448\n",
      "\u001b[0m\u001b[1mEpoch: [50/70], Progress: 52.69 %, Steps: 4607, Current Learning Rate: 0.0009207, \u001b[91mTrain Loss: 0.563\n",
      "\u001b[0m\u001b[1mEpoch: [50/70], Progress: 53.76 %, Steps: 4608, Current Learning Rate: 0.0009206, \u001b[96mTrain Loss: 0.456\n",
      "\u001b[0m\u001b[1mEpoch: [50/70], Progress: 54.84 %, Steps: 4609, Current Learning Rate: 0.0009205, \u001b[91mTrain Loss: 0.636\n",
      "\u001b[0m\u001b[1mEpoch: [50/70], Progress: 55.91 %, Steps: 4610, Current Learning Rate: 0.0009204, \u001b[96mTrain Loss: 0.518\n",
      "\u001b[0m\u001b[1mEpoch: [50/70], Progress: 56.99 %, Steps: 4611, Current Learning Rate: 0.0009203, \u001b[96mTrain Loss: 0.459\n",
      "\u001b[0m\u001b[1mEpoch: [50/70], Progress: 58.06 %, Steps: 4612, Current Learning Rate: 0.0009202, \u001b[91mTrain Loss: 0.541\n",
      "\u001b[0m\u001b[1mEpoch: [50/70], Progress: 59.14 %, Steps: 4613, Current Learning Rate: 0.0009201, \u001b[96mTrain Loss: 0.435\n",
      "\u001b[0m\u001b[1mEpoch: [50/70], Progress: 60.22 %, Steps: 4614, Current Learning Rate: 0.0009200, \u001b[91mTrain Loss: 0.441\n",
      "\u001b[0m\u001b[1mEpoch: [50/70], Progress: 61.29 %, Steps: 4615, Current Learning Rate: 0.0009199, \u001b[91mTrain Loss: 0.519\n",
      "\u001b[0m\u001b[1mEpoch: [50/70], Progress: 62.37 %, Steps: 4616, Current Learning Rate: 0.0009198, \u001b[96mTrain Loss: 0.510\n",
      "\u001b[0m\u001b[1mEpoch: [50/70], Progress: 63.44 %, Steps: 4617, Current Learning Rate: 0.0009197, \u001b[91mTrain Loss: 0.566\n",
      "\u001b[0m\u001b[1mEpoch: [50/70], Progress: 64.52 %, Steps: 4618, Current Learning Rate: 0.0009196, \u001b[96mTrain Loss: 0.455\n",
      "\u001b[0m\u001b[1mEpoch: [50/70], Progress: 65.59 %, Steps: 4619, Current Learning Rate: 0.0009195, \u001b[91mTrain Loss: 0.575\n",
      "\u001b[0m\u001b[1mEpoch: [50/70], Progress: 66.67 %, Steps: 4620, Current Learning Rate: 0.0009194, \u001b[96mTrain Loss: 0.492\n",
      "\u001b[0m\u001b[1mEpoch: [50/70], Progress: 67.74 %, Steps: 4621, Current Learning Rate: 0.0009193, \u001b[96mTrain Loss: 0.479\n",
      "\u001b[0m\u001b[1mEpoch: [50/70], Progress: 68.82 %, Steps: 4622, Current Learning Rate: 0.0009192, \u001b[96mTrain Loss: 0.455\n",
      "\u001b[0m\u001b[1mEpoch: [50/70], Progress: 69.89 %, Steps: 4623, Current Learning Rate: 0.0009191, \u001b[91mTrain Loss: 0.520\n",
      "\u001b[0m\u001b[1mEpoch: [50/70], Progress: 70.97 %, Steps: 4624, Current Learning Rate: 0.0009190, \u001b[96mTrain Loss: 0.452\n",
      "\u001b[0m\u001b[1mEpoch: [50/70], Progress: 72.04 %, Steps: 4625, Current Learning Rate: 0.0009189, \u001b[91mTrain Loss: 0.461\n",
      "\u001b[0m\u001b[1mEpoch: [50/70], Progress: 73.12 %, Steps: 4626, Current Learning Rate: 0.0009188, \u001b[91mTrain Loss: 0.566\n",
      "\u001b[0m\u001b[1mEpoch: [50/70], Progress: 74.19 %, Steps: 4627, Current Learning Rate: 0.0009187, \u001b[96mTrain Loss: 0.494\n",
      "\u001b[0m\u001b[1mEpoch: [50/70], Progress: 75.27 %, Steps: 4628, Current Learning Rate: 0.0009186, \u001b[91mTrain Loss: 0.549\n",
      "\u001b[0m\u001b[1mEpoch: [50/70], Progress: 76.34 %, Steps: 4629, Current Learning Rate: 0.0009185, \u001b[96mTrain Loss: 0.521\n",
      "\u001b[0m\u001b[1mEpoch: [50/70], Progress: 77.42 %, Steps: 4630, Current Learning Rate: 0.0009184, \u001b[96mTrain Loss: 0.516\n",
      "\u001b[0m\u001b[1mEpoch: [50/70], Progress: 78.49 %, Steps: 4631, Current Learning Rate: 0.0009183, \u001b[91mTrain Loss: 0.548\n",
      "\u001b[0m\u001b[1mEpoch: [50/70], Progress: 79.57 %, Steps: 4632, Current Learning Rate: 0.0009182, \u001b[91mTrain Loss: 0.584\n",
      "\u001b[0m\u001b[1mEpoch: [50/70], Progress: 80.65 %, Steps: 4633, Current Learning Rate: 0.0009181, \u001b[96mTrain Loss: 0.519\n",
      "\u001b[0m\u001b[1mEpoch: [50/70], Progress: 81.72 %, Steps: 4634, Current Learning Rate: 0.0009180, \u001b[96mTrain Loss: 0.475\n",
      "\u001b[0m\u001b[1mEpoch: [50/70], Progress: 82.80 %, Steps: 4635, Current Learning Rate: 0.0009179, \u001b[91mTrain Loss: 0.556\n",
      "\u001b[0m\u001b[1mEpoch: [50/70], Progress: 83.87 %, Steps: 4636, Current Learning Rate: 0.0009178, \u001b[96mTrain Loss: 0.489\n",
      "\u001b[0m\u001b[1mEpoch: [50/70], Progress: 84.95 %, Steps: 4637, Current Learning Rate: 0.0009177, \u001b[91mTrain Loss: 0.513\n",
      "\u001b[0m\u001b[1mEpoch: [50/70], Progress: 86.02 %, Steps: 4638, Current Learning Rate: 0.0009176, \u001b[91mTrain Loss: 0.546\n",
      "\u001b[0m\u001b[1mEpoch: [50/70], Progress: 87.10 %, Steps: 4639, Current Learning Rate: 0.0009175, \u001b[91mTrain Loss: 0.619\n",
      "\u001b[0m\u001b[1mEpoch: [50/70], Progress: 88.17 %, Steps: 4640, Current Learning Rate: 0.0009174, \u001b[96mTrain Loss: 0.542\n",
      "\u001b[0m\u001b[1mEpoch: [50/70], Progress: 89.25 %, Steps: 4641, Current Learning Rate: 0.0009173, \u001b[96mTrain Loss: 0.522\n",
      "\u001b[0m\u001b[1mEpoch: [50/70], Progress: 90.32 %, Steps: 4642, Current Learning Rate: 0.0009172, \u001b[91mTrain Loss: 0.593\n",
      "\u001b[0m\u001b[1mEpoch: [50/70], Progress: 91.40 %, Steps: 4643, Current Learning Rate: 0.0009171, \u001b[96mTrain Loss: 0.553\n",
      "\u001b[0m\u001b[1mEpoch: [50/70], Progress: 92.47 %, Steps: 4644, Current Learning Rate: 0.0009170, \u001b[91mTrain Loss: 0.613\n",
      "\u001b[0m\u001b[1mEpoch: [50/70], Progress: 93.55 %, Steps: 4645, Current Learning Rate: 0.0009169, \u001b[96mTrain Loss: 0.530\n",
      "\u001b[0m\u001b[1mEpoch: [50/70], Progress: 94.62 %, Steps: 4646, Current Learning Rate: 0.0009168, \u001b[96mTrain Loss: 0.496\n",
      "\u001b[0m\u001b[1mEpoch: [50/70], Progress: 95.70 %, Steps: 4647, Current Learning Rate: 0.0009167, \u001b[91mTrain Loss: 0.573\n",
      "\u001b[0m\u001b[1mEpoch: [50/70], Progress: 96.77 %, Steps: 4648, Current Learning Rate: 0.0009166, \u001b[96mTrain Loss: 0.508\n",
      "\u001b[0m\u001b[1mEpoch: [50/70], Progress: 97.85 %, Steps: 4649, Current Learning Rate: 0.0009165, \u001b[91mTrain Loss: 0.590\n",
      "\u001b[0m\u001b[1mEpoch: [50/70], Progress: 98.92 %, Steps: 4650, Current Learning Rate: 0.0009164, \u001b[96mTrain Loss: 0.513\n",
      "\u001b[1m\u001b[93mComputing Validation Loss...\n",
      "\u001b[1mEpoch 50 Completed! Average Train Loss: 0.492, Average Validation Loss: 0.144\n",
      "Best Model saved in best_model/best_model.pt.\n",
      "\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m= Epoch [51/70] ==========================================\n",
      "\u001b[0m\u001b[1mEpoch: [51/70], Progress: 0.00 %, Steps: 4651, Current Learning Rate: 0.0009163, \u001b[91mTrain Loss: 0.389\n",
      "\u001b[0m\u001b[1mEpoch: [51/70], Progress: 1.08 %, Steps: 4652, Current Learning Rate: 0.0009162, \u001b[96mTrain Loss: 0.374\n",
      "\u001b[0m\u001b[1mEpoch: [51/70], Progress: 2.15 %, Steps: 4653, Current Learning Rate: 0.0009162, \u001b[96mTrain Loss: 0.356\n",
      "\u001b[0m\u001b[1mEpoch: [51/70], Progress: 3.23 %, Steps: 4654, Current Learning Rate: 0.0009161, \u001b[96mTrain Loss: 0.348\n",
      "\u001b[0m\u001b[1mEpoch: [51/70], Progress: 4.30 %, Steps: 4655, Current Learning Rate: 0.0009160, \u001b[91mTrain Loss: 0.366\n",
      "\u001b[0m\u001b[1mEpoch: [51/70], Progress: 5.38 %, Steps: 4656, Current Learning Rate: 0.0009159, \u001b[96mTrain Loss: 0.364\n",
      "\u001b[0m\u001b[1mEpoch: [51/70], Progress: 6.45 %, Steps: 4657, Current Learning Rate: 0.0009158, \u001b[96mTrain Loss: 0.344\n",
      "\u001b[0m\u001b[1mEpoch: [51/70], Progress: 7.53 %, Steps: 4658, Current Learning Rate: 0.0009157, \u001b[91mTrain Loss: 0.391\n",
      "\u001b[0m\u001b[1mEpoch: [51/70], Progress: 8.60 %, Steps: 4659, Current Learning Rate: 0.0009156, \u001b[96mTrain Loss: 0.368\n",
      "\u001b[0m\u001b[1mEpoch: [51/70], Progress: 9.68 %, Steps: 4660, Current Learning Rate: 0.0009155, \u001b[96mTrain Loss: 0.333\n",
      "\u001b[0m\u001b[1mEpoch: [51/70], Progress: 10.75 %, Steps: 4661, Current Learning Rate: 0.0009154, \u001b[91mTrain Loss: 0.351\n",
      "\u001b[0m\u001b[1mEpoch: [51/70], Progress: 11.83 %, Steps: 4662, Current Learning Rate: 0.0009153, \u001b[91mTrain Loss: 0.388\n",
      "\u001b[0m\u001b[1mEpoch: [51/70], Progress: 12.90 %, Steps: 4663, Current Learning Rate: 0.0009152, \u001b[91mTrain Loss: 0.395\n",
      "\u001b[0m\u001b[1mEpoch: [51/70], Progress: 13.98 %, Steps: 4664, Current Learning Rate: 0.0009151, \u001b[91mTrain Loss: 0.398\n",
      "\u001b[0m\u001b[1mEpoch: [51/70], Progress: 15.05 %, Steps: 4665, Current Learning Rate: 0.0009150, \u001b[96mTrain Loss: 0.360\n",
      "\u001b[0m\u001b[1mEpoch: [51/70], Progress: 16.13 %, Steps: 4666, Current Learning Rate: 0.0009149, \u001b[96mTrain Loss: 0.299\n",
      "\u001b[0m\u001b[1mEpoch: [51/70], Progress: 17.20 %, Steps: 4667, Current Learning Rate: 0.0009148, \u001b[91mTrain Loss: 0.369\n",
      "\u001b[0m\u001b[1mEpoch: [51/70], Progress: 18.28 %, Steps: 4668, Current Learning Rate: 0.0009147, \u001b[91mTrain Loss: 0.483\n",
      "\u001b[0m\u001b[1mEpoch: [51/70], Progress: 19.35 %, Steps: 4669, Current Learning Rate: 0.0009146, \u001b[96mTrain Loss: 0.395\n",
      "\u001b[0m\u001b[1mEpoch: [51/70], Progress: 20.43 %, Steps: 4670, Current Learning Rate: 0.0009145, \u001b[91mTrain Loss: 0.501\n",
      "\u001b[0m\u001b[1mEpoch: [51/70], Progress: 21.51 %, Steps: 4671, Current Learning Rate: 0.0009144, \u001b[96mTrain Loss: 0.426\n",
      "\u001b[0m\u001b[1mEpoch: [51/70], Progress: 22.58 %, Steps: 4672, Current Learning Rate: 0.0009143, \u001b[96mTrain Loss: 0.413\n",
      "\u001b[0m\u001b[1mEpoch: [51/70], Progress: 23.66 %, Steps: 4673, Current Learning Rate: 0.0009142, \u001b[96mTrain Loss: 0.363\n",
      "\u001b[0m\u001b[1mEpoch: [51/70], Progress: 24.73 %, Steps: 4674, Current Learning Rate: 0.0009141, \u001b[91mTrain Loss: 0.405\n",
      "\u001b[0m\u001b[1mEpoch: [51/70], Progress: 25.81 %, Steps: 4675, Current Learning Rate: 0.0009140, \u001b[96mTrain Loss: 0.387\n",
      "\u001b[0m\u001b[1mEpoch: [51/70], Progress: 26.88 %, Steps: 4676, Current Learning Rate: 0.0009139, \u001b[96mTrain Loss: 0.387\n",
      "\u001b[0m\u001b[1mEpoch: [51/70], Progress: 27.96 %, Steps: 4677, Current Learning Rate: 0.0009138, \u001b[91mTrain Loss: 0.389\n",
      "\u001b[0m\u001b[1mEpoch: [51/70], Progress: 29.03 %, Steps: 4678, Current Learning Rate: 0.0009137, \u001b[91mTrain Loss: 0.457\n",
      "\u001b[0m\u001b[1mEpoch: [51/70], Progress: 30.11 %, Steps: 4679, Current Learning Rate: 0.0009136, \u001b[96mTrain Loss: 0.443\n",
      "\u001b[0m\u001b[1mEpoch: [51/70], Progress: 31.18 %, Steps: 4680, Current Learning Rate: 0.0009135, \u001b[91mTrain Loss: 0.444\n",
      "\u001b[0m\u001b[1mEpoch: [51/70], Progress: 32.26 %, Steps: 4681, Current Learning Rate: 0.0009134, \u001b[91mTrain Loss: 0.463\n",
      "\u001b[0m\u001b[1mEpoch: [51/70], Progress: 33.33 %, Steps: 4682, Current Learning Rate: 0.0009133, \u001b[96mTrain Loss: 0.354\n",
      "\u001b[0m\u001b[1mEpoch: [51/70], Progress: 34.41 %, Steps: 4683, Current Learning Rate: 0.0009132, \u001b[91mTrain Loss: 0.397\n",
      "\u001b[0m\u001b[1mEpoch: [51/70], Progress: 35.48 %, Steps: 4684, Current Learning Rate: 0.0009131, \u001b[91mTrain Loss: 0.405\n",
      "\u001b[0m\u001b[1mEpoch: [51/70], Progress: 36.56 %, Steps: 4685, Current Learning Rate: 0.0009130, \u001b[91mTrain Loss: 0.424\n",
      "\u001b[0m\u001b[1mEpoch: [51/70], Progress: 37.63 %, Steps: 4686, Current Learning Rate: 0.0009129, \u001b[91mTrain Loss: 0.446\n",
      "\u001b[0m\u001b[1mEpoch: [51/70], Progress: 38.71 %, Steps: 4687, Current Learning Rate: 0.0009128, \u001b[91mTrain Loss: 0.465\n",
      "\u001b[0m\u001b[1mEpoch: [51/70], Progress: 39.78 %, Steps: 4688, Current Learning Rate: 0.0009127, \u001b[91mTrain Loss: 0.473\n",
      "\u001b[0m\u001b[1mEpoch: [51/70], Progress: 40.86 %, Steps: 4689, Current Learning Rate: 0.0009126, \u001b[96mTrain Loss: 0.442\n",
      "\u001b[0m\u001b[1mEpoch: [51/70], Progress: 41.94 %, Steps: 4690, Current Learning Rate: 0.0009125, \u001b[96mTrain Loss: 0.442\n",
      "\u001b[0m\u001b[1mEpoch: [51/70], Progress: 43.01 %, Steps: 4691, Current Learning Rate: 0.0009124, \u001b[91mTrain Loss: 0.461\n",
      "\u001b[0m\u001b[1mEpoch: [51/70], Progress: 44.09 %, Steps: 4692, Current Learning Rate: 0.0009123, \u001b[96mTrain Loss: 0.433\n",
      "\u001b[0m\u001b[1mEpoch: [51/70], Progress: 45.16 %, Steps: 4693, Current Learning Rate: 0.0009122, \u001b[96mTrain Loss: 0.421\n",
      "\u001b[0m\u001b[1mEpoch: [51/70], Progress: 46.24 %, Steps: 4694, Current Learning Rate: 0.0009121, \u001b[91mTrain Loss: 0.472\n",
      "\u001b[0m\u001b[1mEpoch: [51/70], Progress: 47.31 %, Steps: 4695, Current Learning Rate: 0.0009120, \u001b[96mTrain Loss: 0.459\n",
      "\u001b[0m\u001b[1mEpoch: [51/70], Progress: 48.39 %, Steps: 4696, Current Learning Rate: 0.0009119, \u001b[91mTrain Loss: 0.462\n",
      "\u001b[0m\u001b[1mEpoch: [51/70], Progress: 49.46 %, Steps: 4697, Current Learning Rate: 0.0009119, \u001b[96mTrain Loss: 0.460\n",
      "\u001b[0m\u001b[1mEpoch: [51/70], Progress: 50.54 %, Steps: 4698, Current Learning Rate: 0.0009118, \u001b[96mTrain Loss: 0.416\n",
      "\u001b[0m\u001b[1mEpoch: [51/70], Progress: 51.61 %, Steps: 4699, Current Learning Rate: 0.0009117, \u001b[96mTrain Loss: 0.400\n",
      "\u001b[0m\u001b[1mEpoch: [51/70], Progress: 52.69 %, Steps: 4700, Current Learning Rate: 0.0009116, \u001b[91mTrain Loss: 0.460\n",
      "\u001b[0m\u001b[1mEpoch: [51/70], Progress: 53.76 %, Steps: 4701, Current Learning Rate: 0.0009115, \u001b[91mTrain Loss: 0.481\n",
      "\u001b[0m\u001b[1mEpoch: [51/70], Progress: 54.84 %, Steps: 4702, Current Learning Rate: 0.0009114, \u001b[96mTrain Loss: 0.354\n",
      "\u001b[0m\u001b[1mEpoch: [51/70], Progress: 55.91 %, Steps: 4703, Current Learning Rate: 0.0009113, \u001b[91mTrain Loss: 0.572\n",
      "\u001b[0m\u001b[1mEpoch: [51/70], Progress: 56.99 %, Steps: 4704, Current Learning Rate: 0.0009112, \u001b[96mTrain Loss: 0.469\n",
      "\u001b[0m\u001b[1mEpoch: [51/70], Progress: 58.06 %, Steps: 4705, Current Learning Rate: 0.0009111, \u001b[96mTrain Loss: 0.468\n",
      "\u001b[0m\u001b[1mEpoch: [51/70], Progress: 59.14 %, Steps: 4706, Current Learning Rate: 0.0009110, \u001b[96mTrain Loss: 0.467\n",
      "\u001b[0m\u001b[1mEpoch: [51/70], Progress: 60.22 %, Steps: 4707, Current Learning Rate: 0.0009109, \u001b[96mTrain Loss: 0.406\n",
      "\u001b[0m\u001b[1mEpoch: [51/70], Progress: 61.29 %, Steps: 4708, Current Learning Rate: 0.0009108, \u001b[96mTrain Loss: 0.401\n",
      "\u001b[0m\u001b[1mEpoch: [51/70], Progress: 62.37 %, Steps: 4709, Current Learning Rate: 0.0009107, \u001b[91mTrain Loss: 0.459\n",
      "\u001b[0m\u001b[1mEpoch: [51/70], Progress: 63.44 %, Steps: 4710, Current Learning Rate: 0.0009106, \u001b[96mTrain Loss: 0.353\n",
      "\u001b[0m\u001b[1mEpoch: [51/70], Progress: 64.52 %, Steps: 4711, Current Learning Rate: 0.0009105, \u001b[91mTrain Loss: 0.454\n",
      "\u001b[0m\u001b[1mEpoch: [51/70], Progress: 65.59 %, Steps: 4712, Current Learning Rate: 0.0009104, \u001b[96mTrain Loss: 0.425\n",
      "\u001b[0m\u001b[1mEpoch: [51/70], Progress: 66.67 %, Steps: 4713, Current Learning Rate: 0.0009103, \u001b[91mTrain Loss: 0.480\n",
      "\u001b[0m\u001b[1mEpoch: [51/70], Progress: 67.74 %, Steps: 4714, Current Learning Rate: 0.0009102, \u001b[96mTrain Loss: 0.474\n",
      "\u001b[0m\u001b[1mEpoch: [51/70], Progress: 68.82 %, Steps: 4715, Current Learning Rate: 0.0009101, \u001b[91mTrain Loss: 0.505\n",
      "\u001b[0m\u001b[1mEpoch: [51/70], Progress: 69.89 %, Steps: 4716, Current Learning Rate: 0.0009100, \u001b[96mTrain Loss: 0.490\n",
      "\u001b[0m\u001b[1mEpoch: [51/70], Progress: 70.97 %, Steps: 4717, Current Learning Rate: 0.0009099, \u001b[91mTrain Loss: 0.506\n",
      "\u001b[0m\u001b[1mEpoch: [51/70], Progress: 72.04 %, Steps: 4718, Current Learning Rate: 0.0009098, \u001b[96mTrain Loss: 0.444\n",
      "\u001b[0m\u001b[1mEpoch: [51/70], Progress: 73.12 %, Steps: 4719, Current Learning Rate: 0.0009097, \u001b[91mTrain Loss: 0.494\n",
      "\u001b[0m\u001b[1mEpoch: [51/70], Progress: 74.19 %, Steps: 4720, Current Learning Rate: 0.0009096, \u001b[91mTrain Loss: 0.552\n",
      "\u001b[0m\u001b[1mEpoch: [51/70], Progress: 75.27 %, Steps: 4721, Current Learning Rate: 0.0009095, \u001b[96mTrain Loss: 0.489\n",
      "\u001b[0m\u001b[1mEpoch: [51/70], Progress: 76.34 %, Steps: 4722, Current Learning Rate: 0.0009094, \u001b[91mTrain Loss: 0.493\n",
      "\u001b[0m\u001b[1mEpoch: [51/70], Progress: 77.42 %, Steps: 4723, Current Learning Rate: 0.0009093, \u001b[96mTrain Loss: 0.484\n",
      "\u001b[0m\u001b[1mEpoch: [51/70], Progress: 78.49 %, Steps: 4724, Current Learning Rate: 0.0009092, \u001b[96mTrain Loss: 0.435\n",
      "\u001b[0m\u001b[1mEpoch: [51/70], Progress: 79.57 %, Steps: 4725, Current Learning Rate: 0.0009091, \u001b[91mTrain Loss: 0.508\n",
      "\u001b[0m\u001b[1mEpoch: [51/70], Progress: 80.65 %, Steps: 4726, Current Learning Rate: 0.0009090, \u001b[96mTrain Loss: 0.478\n",
      "\u001b[0m\u001b[1mEpoch: [51/70], Progress: 81.72 %, Steps: 4727, Current Learning Rate: 0.0009090, \u001b[91mTrain Loss: 0.505\n",
      "\u001b[0m\u001b[1mEpoch: [51/70], Progress: 82.80 %, Steps: 4728, Current Learning Rate: 0.0009089, \u001b[96mTrain Loss: 0.444\n",
      "\u001b[0m\u001b[1mEpoch: [51/70], Progress: 83.87 %, Steps: 4729, Current Learning Rate: 0.0009088, \u001b[96mTrain Loss: 0.436\n",
      "\u001b[0m\u001b[1mEpoch: [51/70], Progress: 84.95 %, Steps: 4730, Current Learning Rate: 0.0009087, \u001b[91mTrain Loss: 0.469\n",
      "\u001b[0m\u001b[1mEpoch: [51/70], Progress: 86.02 %, Steps: 4731, Current Learning Rate: 0.0009086, \u001b[96mTrain Loss: 0.438\n",
      "\u001b[0m\u001b[1mEpoch: [51/70], Progress: 87.10 %, Steps: 4732, Current Learning Rate: 0.0009085, \u001b[96mTrain Loss: 0.435\n",
      "\u001b[0m\u001b[1mEpoch: [51/70], Progress: 88.17 %, Steps: 4733, Current Learning Rate: 0.0009084, \u001b[91mTrain Loss: 0.448\n",
      "\u001b[0m\u001b[1mEpoch: [51/70], Progress: 89.25 %, Steps: 4734, Current Learning Rate: 0.0009083, \u001b[91mTrain Loss: 0.485\n",
      "\u001b[0m\u001b[1mEpoch: [51/70], Progress: 90.32 %, Steps: 4735, Current Learning Rate: 0.0009082, \u001b[91mTrain Loss: 0.488\n",
      "\u001b[0m\u001b[1mEpoch: [51/70], Progress: 91.40 %, Steps: 4736, Current Learning Rate: 0.0009081, \u001b[91mTrain Loss: 0.510\n",
      "\u001b[0m\u001b[1mEpoch: [51/70], Progress: 92.47 %, Steps: 4737, Current Learning Rate: 0.0009080, \u001b[91mTrain Loss: 0.555\n",
      "\u001b[0m\u001b[1mEpoch: [51/70], Progress: 93.55 %, Steps: 4738, Current Learning Rate: 0.0009079, \u001b[96mTrain Loss: 0.485\n",
      "\u001b[0m\u001b[1mEpoch: [51/70], Progress: 94.62 %, Steps: 4739, Current Learning Rate: 0.0009078, \u001b[91mTrain Loss: 0.506\n",
      "\u001b[0m\u001b[1mEpoch: [51/70], Progress: 95.70 %, Steps: 4740, Current Learning Rate: 0.0009077, \u001b[91mTrain Loss: 0.525\n",
      "\u001b[0m\u001b[1mEpoch: [51/70], Progress: 96.77 %, Steps: 4741, Current Learning Rate: 0.0009076, \u001b[96mTrain Loss: 0.449\n",
      "\u001b[0m\u001b[1mEpoch: [51/70], Progress: 97.85 %, Steps: 4742, Current Learning Rate: 0.0009075, \u001b[91mTrain Loss: 0.487\n",
      "\u001b[0m\u001b[1mEpoch: [51/70], Progress: 98.92 %, Steps: 4743, Current Learning Rate: 0.0009074, \u001b[96mTrain Loss: 0.446\n",
      "\u001b[1m\u001b[93mComputing Validation Loss...\n",
      "\u001b[1mEpoch 51 Completed! Average Train Loss: 0.438, Average Validation Loss: 0.122\n",
      "Best Model saved in best_model/best_model.pt.\n",
      "\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m= Epoch [52/70] ==========================================\n",
      "\u001b[0m\u001b[1mEpoch: [52/70], Progress: 0.00 %, Steps: 4744, Current Learning Rate: 0.0009073, \u001b[91mTrain Loss: 0.307\n",
      "\u001b[0m\u001b[1mEpoch: [52/70], Progress: 1.08 %, Steps: 4745, Current Learning Rate: 0.0009072, \u001b[96mTrain Loss: 0.305\n",
      "\u001b[0m\u001b[1mEpoch: [52/70], Progress: 2.15 %, Steps: 4746, Current Learning Rate: 0.0009071, \u001b[91mTrain Loss: 0.317\n",
      "\u001b[0m\u001b[1mEpoch: [52/70], Progress: 3.23 %, Steps: 4747, Current Learning Rate: 0.0009070, \u001b[96mTrain Loss: 0.300\n",
      "\u001b[0m\u001b[1mEpoch: [52/70], Progress: 4.30 %, Steps: 4748, Current Learning Rate: 0.0009069, \u001b[91mTrain Loss: 0.341\n",
      "\u001b[0m\u001b[1mEpoch: [52/70], Progress: 5.38 %, Steps: 4749, Current Learning Rate: 0.0009068, \u001b[96mTrain Loss: 0.303\n",
      "\u001b[0m\u001b[1mEpoch: [52/70], Progress: 6.45 %, Steps: 4750, Current Learning Rate: 0.0009067, \u001b[91mTrain Loss: 0.313\n",
      "\u001b[0m\u001b[1mEpoch: [52/70], Progress: 7.53 %, Steps: 4751, Current Learning Rate: 0.0009067, \u001b[91mTrain Loss: 0.360\n",
      "\u001b[0m\u001b[1mEpoch: [52/70], Progress: 8.60 %, Steps: 4752, Current Learning Rate: 0.0009066, \u001b[96mTrain Loss: 0.252\n",
      "\u001b[0m\u001b[1mEpoch: [52/70], Progress: 9.68 %, Steps: 4753, Current Learning Rate: 0.0009065, \u001b[91mTrain Loss: 0.345\n",
      "\u001b[0m\u001b[1mEpoch: [52/70], Progress: 10.75 %, Steps: 4754, Current Learning Rate: 0.0009064, \u001b[91mTrain Loss: 0.388\n",
      "\u001b[0m\u001b[1mEpoch: [52/70], Progress: 11.83 %, Steps: 4755, Current Learning Rate: 0.0009063, \u001b[91mTrain Loss: 0.394\n",
      "\u001b[0m\u001b[1mEpoch: [52/70], Progress: 12.90 %, Steps: 4756, Current Learning Rate: 0.0009062, \u001b[96mTrain Loss: 0.370\n",
      "\u001b[0m\u001b[1mEpoch: [52/70], Progress: 13.98 %, Steps: 4757, Current Learning Rate: 0.0009061, \u001b[96mTrain Loss: 0.342\n",
      "\u001b[0m\u001b[1mEpoch: [52/70], Progress: 15.05 %, Steps: 4758, Current Learning Rate: 0.0009060, \u001b[91mTrain Loss: 0.448\n",
      "\u001b[0m\u001b[1mEpoch: [52/70], Progress: 16.13 %, Steps: 4759, Current Learning Rate: 0.0009059, \u001b[96mTrain Loss: 0.337\n",
      "\u001b[0m\u001b[1mEpoch: [52/70], Progress: 17.20 %, Steps: 4760, Current Learning Rate: 0.0009058, \u001b[96mTrain Loss: 0.289\n",
      "\u001b[0m\u001b[1mEpoch: [52/70], Progress: 18.28 %, Steps: 4761, Current Learning Rate: 0.0009057, \u001b[96mTrain Loss: 0.287\n",
      "\u001b[0m\u001b[1mEpoch: [52/70], Progress: 19.35 %, Steps: 4762, Current Learning Rate: 0.0009056, \u001b[91mTrain Loss: 0.454\n",
      "\u001b[0m\u001b[1mEpoch: [52/70], Progress: 20.43 %, Steps: 4763, Current Learning Rate: 0.0009055, \u001b[96mTrain Loss: 0.337\n",
      "\u001b[0m\u001b[1mEpoch: [52/70], Progress: 21.51 %, Steps: 4764, Current Learning Rate: 0.0009054, \u001b[91mTrain Loss: 0.365\n",
      "\u001b[0m\u001b[1mEpoch: [52/70], Progress: 22.58 %, Steps: 4765, Current Learning Rate: 0.0009053, \u001b[96mTrain Loss: 0.342\n",
      "\u001b[0m\u001b[1mEpoch: [52/70], Progress: 23.66 %, Steps: 4766, Current Learning Rate: 0.0009052, \u001b[91mTrain Loss: 0.365\n",
      "\u001b[0m\u001b[1mEpoch: [52/70], Progress: 24.73 %, Steps: 4767, Current Learning Rate: 0.0009051, \u001b[96mTrain Loss: 0.363\n",
      "\u001b[0m\u001b[1mEpoch: [52/70], Progress: 25.81 %, Steps: 4768, Current Learning Rate: 0.0009050, \u001b[91mTrain Loss: 0.376\n",
      "\u001b[0m\u001b[1mEpoch: [52/70], Progress: 26.88 %, Steps: 4769, Current Learning Rate: 0.0009049, \u001b[96mTrain Loss: 0.360\n",
      "\u001b[0m\u001b[1mEpoch: [52/70], Progress: 27.96 %, Steps: 4770, Current Learning Rate: 0.0009048, \u001b[91mTrain Loss: 0.382\n",
      "\u001b[0m\u001b[1mEpoch: [52/70], Progress: 29.03 %, Steps: 4771, Current Learning Rate: 0.0009048, \u001b[91mTrain Loss: 0.421\n",
      "\u001b[0m\u001b[1mEpoch: [52/70], Progress: 30.11 %, Steps: 4772, Current Learning Rate: 0.0009047, \u001b[96mTrain Loss: 0.289\n",
      "\u001b[0m\u001b[1mEpoch: [52/70], Progress: 31.18 %, Steps: 4773, Current Learning Rate: 0.0009046, \u001b[91mTrain Loss: 0.336\n",
      "\u001b[0m\u001b[1mEpoch: [52/70], Progress: 32.26 %, Steps: 4774, Current Learning Rate: 0.0009045, \u001b[91mTrain Loss: 0.394\n",
      "\u001b[0m\u001b[1mEpoch: [52/70], Progress: 33.33 %, Steps: 4775, Current Learning Rate: 0.0009044, \u001b[91mTrain Loss: 0.479\n",
      "\u001b[0m\u001b[1mEpoch: [52/70], Progress: 34.41 %, Steps: 4776, Current Learning Rate: 0.0009043, \u001b[96mTrain Loss: 0.389\n",
      "\u001b[0m\u001b[1mEpoch: [52/70], Progress: 35.48 %, Steps: 4777, Current Learning Rate: 0.0009042, \u001b[96mTrain Loss: 0.326\n",
      "\u001b[0m\u001b[1mEpoch: [52/70], Progress: 36.56 %, Steps: 4778, Current Learning Rate: 0.0009041, \u001b[91mTrain Loss: 0.462\n",
      "\u001b[0m\u001b[1mEpoch: [52/70], Progress: 37.63 %, Steps: 4779, Current Learning Rate: 0.0009040, \u001b[96mTrain Loss: 0.371\n",
      "\u001b[0m\u001b[1mEpoch: [52/70], Progress: 38.71 %, Steps: 4780, Current Learning Rate: 0.0009039, \u001b[91mTrain Loss: 0.436\n",
      "\u001b[0m\u001b[1mEpoch: [52/70], Progress: 39.78 %, Steps: 4781, Current Learning Rate: 0.0009038, \u001b[96mTrain Loss: 0.354\n",
      "\u001b[0m\u001b[1mEpoch: [52/70], Progress: 40.86 %, Steps: 4782, Current Learning Rate: 0.0009037, \u001b[91mTrain Loss: 0.358\n",
      "\u001b[0m\u001b[1mEpoch: [52/70], Progress: 41.94 %, Steps: 4783, Current Learning Rate: 0.0009036, \u001b[91mTrain Loss: 0.461\n",
      "\u001b[0m\u001b[1mEpoch: [52/70], Progress: 43.01 %, Steps: 4784, Current Learning Rate: 0.0009035, \u001b[96mTrain Loss: 0.362\n",
      "\u001b[0m\u001b[1mEpoch: [52/70], Progress: 44.09 %, Steps: 4785, Current Learning Rate: 0.0009034, \u001b[91mTrain Loss: 0.400\n",
      "\u001b[0m\u001b[1mEpoch: [52/70], Progress: 45.16 %, Steps: 4786, Current Learning Rate: 0.0009033, \u001b[91mTrain Loss: 0.407\n",
      "\u001b[0m\u001b[1mEpoch: [52/70], Progress: 46.24 %, Steps: 4787, Current Learning Rate: 0.0009032, \u001b[96mTrain Loss: 0.318\n",
      "\u001b[0m\u001b[1mEpoch: [52/70], Progress: 47.31 %, Steps: 4788, Current Learning Rate: 0.0009031, \u001b[91mTrain Loss: 0.397\n",
      "\u001b[0m\u001b[1mEpoch: [52/70], Progress: 48.39 %, Steps: 4789, Current Learning Rate: 0.0009031, \u001b[91mTrain Loss: 0.400\n",
      "\u001b[0m\u001b[1mEpoch: [52/70], Progress: 49.46 %, Steps: 4790, Current Learning Rate: 0.0009030, \u001b[91mTrain Loss: 0.400\n",
      "\u001b[0m\u001b[1mEpoch: [52/70], Progress: 50.54 %, Steps: 4791, Current Learning Rate: 0.0009029, \u001b[91mTrain Loss: 0.409\n",
      "\u001b[0m\u001b[1mEpoch: [52/70], Progress: 51.61 %, Steps: 4792, Current Learning Rate: 0.0009028, \u001b[96mTrain Loss: 0.370\n",
      "\u001b[0m\u001b[1mEpoch: [52/70], Progress: 52.69 %, Steps: 4793, Current Learning Rate: 0.0009027, \u001b[91mTrain Loss: 0.376\n",
      "\u001b[0m\u001b[1mEpoch: [52/70], Progress: 53.76 %, Steps: 4794, Current Learning Rate: 0.0009026, \u001b[91mTrain Loss: 0.410\n",
      "\u001b[0m\u001b[1mEpoch: [52/70], Progress: 54.84 %, Steps: 4795, Current Learning Rate: 0.0009025, \u001b[96mTrain Loss: 0.359\n",
      "\u001b[0m\u001b[1mEpoch: [52/70], Progress: 55.91 %, Steps: 4796, Current Learning Rate: 0.0009024, \u001b[96mTrain Loss: 0.355\n",
      "\u001b[0m\u001b[1mEpoch: [52/70], Progress: 56.99 %, Steps: 4797, Current Learning Rate: 0.0009023, \u001b[91mTrain Loss: 0.398\n",
      "\u001b[0m\u001b[1mEpoch: [52/70], Progress: 58.06 %, Steps: 4798, Current Learning Rate: 0.0009022, \u001b[96mTrain Loss: 0.340\n",
      "\u001b[0m\u001b[1mEpoch: [52/70], Progress: 59.14 %, Steps: 4799, Current Learning Rate: 0.0009021, \u001b[91mTrain Loss: 0.384\n",
      "\u001b[0m\u001b[1mEpoch: [52/70], Progress: 60.22 %, Steps: 4800, Current Learning Rate: 0.0009020, \u001b[91mTrain Loss: 0.433\n",
      "\u001b[0m\u001b[1mEpoch: [52/70], Progress: 61.29 %, Steps: 4801, Current Learning Rate: 0.0009019, \u001b[96mTrain Loss: 0.367\n",
      "\u001b[0m\u001b[1mEpoch: [52/70], Progress: 62.37 %, Steps: 4802, Current Learning Rate: 0.0009018, \u001b[91mTrain Loss: 0.406\n",
      "\u001b[0m\u001b[1mEpoch: [52/70], Progress: 63.44 %, Steps: 4803, Current Learning Rate: 0.0009017, \u001b[91mTrain Loss: 0.497\n",
      "\u001b[0m\u001b[1mEpoch: [52/70], Progress: 64.52 %, Steps: 4804, Current Learning Rate: 0.0009016, \u001b[96mTrain Loss: 0.435\n",
      "\u001b[0m\u001b[1mEpoch: [52/70], Progress: 65.59 %, Steps: 4805, Current Learning Rate: 0.0009015, \u001b[96mTrain Loss: 0.419\n",
      "\u001b[0m\u001b[1mEpoch: [52/70], Progress: 66.67 %, Steps: 4806, Current Learning Rate: 0.0009015, \u001b[91mTrain Loss: 0.464\n",
      "\u001b[0m\u001b[1mEpoch: [52/70], Progress: 67.74 %, Steps: 4807, Current Learning Rate: 0.0009014, \u001b[96mTrain Loss: 0.451\n",
      "\u001b[0m\u001b[1mEpoch: [52/70], Progress: 68.82 %, Steps: 4808, Current Learning Rate: 0.0009013, \u001b[91mTrain Loss: 0.505\n",
      "\u001b[0m\u001b[1mEpoch: [52/70], Progress: 69.89 %, Steps: 4809, Current Learning Rate: 0.0009012, \u001b[96mTrain Loss: 0.458\n",
      "\u001b[0m\u001b[1mEpoch: [52/70], Progress: 70.97 %, Steps: 4810, Current Learning Rate: 0.0009011, \u001b[91mTrain Loss: 0.472\n",
      "\u001b[0m\u001b[1mEpoch: [52/70], Progress: 72.04 %, Steps: 4811, Current Learning Rate: 0.0009010, \u001b[96mTrain Loss: 0.452\n",
      "\u001b[0m\u001b[1mEpoch: [52/70], Progress: 73.12 %, Steps: 4812, Current Learning Rate: 0.0009009, \u001b[91mTrain Loss: 0.457\n",
      "\u001b[0m\u001b[1mEpoch: [52/70], Progress: 74.19 %, Steps: 4813, Current Learning Rate: 0.0009008, \u001b[91mTrain Loss: 0.473\n",
      "\u001b[0m\u001b[1mEpoch: [52/70], Progress: 75.27 %, Steps: 4814, Current Learning Rate: 0.0009007, \u001b[96mTrain Loss: 0.437\n",
      "\u001b[0m\u001b[1mEpoch: [52/70], Progress: 76.34 %, Steps: 4815, Current Learning Rate: 0.0009006, \u001b[96mTrain Loss: 0.361\n",
      "\u001b[0m\u001b[1mEpoch: [52/70], Progress: 77.42 %, Steps: 4816, Current Learning Rate: 0.0009005, \u001b[91mTrain Loss: 0.494\n",
      "\u001b[0m\u001b[1mEpoch: [52/70], Progress: 78.49 %, Steps: 4817, Current Learning Rate: 0.0009004, \u001b[96mTrain Loss: 0.381\n",
      "\u001b[0m\u001b[1mEpoch: [52/70], Progress: 79.57 %, Steps: 4818, Current Learning Rate: 0.0009003, \u001b[96mTrain Loss: 0.367\n",
      "\u001b[0m\u001b[1mEpoch: [52/70], Progress: 80.65 %, Steps: 4819, Current Learning Rate: 0.0009002, \u001b[91mTrain Loss: 0.461\n",
      "\u001b[0m\u001b[1mEpoch: [52/70], Progress: 81.72 %, Steps: 4820, Current Learning Rate: 0.0009001, \u001b[96mTrain Loss: 0.433\n",
      "\u001b[0m\u001b[1mEpoch: [52/70], Progress: 82.80 %, Steps: 4821, Current Learning Rate: 0.0009000, \u001b[91mTrain Loss: 0.455\n",
      "\u001b[0m\u001b[1mEpoch: [52/70], Progress: 83.87 %, Steps: 4822, Current Learning Rate: 0.0009000, \u001b[96mTrain Loss: 0.429\n",
      "\u001b[0m\u001b[1mEpoch: [52/70], Progress: 84.95 %, Steps: 4823, Current Learning Rate: 0.0008999, \u001b[91mTrain Loss: 0.489\n",
      "\u001b[0m\u001b[1mEpoch: [52/70], Progress: 86.02 %, Steps: 4824, Current Learning Rate: 0.0008998, \u001b[96mTrain Loss: 0.456\n",
      "\u001b[0m\u001b[1mEpoch: [52/70], Progress: 87.10 %, Steps: 4825, Current Learning Rate: 0.0008997, \u001b[96mTrain Loss: 0.361\n",
      "\u001b[0m\u001b[1mEpoch: [52/70], Progress: 88.17 %, Steps: 4826, Current Learning Rate: 0.0008996, \u001b[91mTrain Loss: 0.428\n",
      "\u001b[0m\u001b[1mEpoch: [52/70], Progress: 89.25 %, Steps: 4827, Current Learning Rate: 0.0008995, \u001b[91mTrain Loss: 0.490\n",
      "\u001b[0m\u001b[1mEpoch: [52/70], Progress: 90.32 %, Steps: 4828, Current Learning Rate: 0.0008994, \u001b[96mTrain Loss: 0.440\n",
      "\u001b[0m\u001b[1mEpoch: [52/70], Progress: 91.40 %, Steps: 4829, Current Learning Rate: 0.0008993, \u001b[91mTrain Loss: 0.446\n",
      "\u001b[0m\u001b[1mEpoch: [52/70], Progress: 92.47 %, Steps: 4830, Current Learning Rate: 0.0008992, \u001b[96mTrain Loss: 0.427\n",
      "\u001b[0m\u001b[1mEpoch: [52/70], Progress: 93.55 %, Steps: 4831, Current Learning Rate: 0.0008991, \u001b[91mTrain Loss: 0.517\n",
      "\u001b[0m\u001b[1mEpoch: [52/70], Progress: 94.62 %, Steps: 4832, Current Learning Rate: 0.0008990, \u001b[96mTrain Loss: 0.458\n",
      "\u001b[0m\u001b[1mEpoch: [52/70], Progress: 95.70 %, Steps: 4833, Current Learning Rate: 0.0008989, \u001b[91mTrain Loss: 0.471\n",
      "\u001b[0m\u001b[1mEpoch: [52/70], Progress: 96.77 %, Steps: 4834, Current Learning Rate: 0.0008988, \u001b[96mTrain Loss: 0.377\n",
      "\u001b[0m\u001b[1mEpoch: [52/70], Progress: 97.85 %, Steps: 4835, Current Learning Rate: 0.0008987, \u001b[91mTrain Loss: 0.388\n",
      "\u001b[0m\u001b[1mEpoch: [52/70], Progress: 98.92 %, Steps: 4836, Current Learning Rate: 0.0008987, \u001b[91mTrain Loss: 0.514\n",
      "\u001b[1m\u001b[93mComputing Validation Loss...\n",
      "\u001b[1mEpoch 52 Completed! Average Train Loss: 0.396, Average Validation Loss: 0.104\n",
      "Best Model saved in best_model/best_model.pt.\n",
      "\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m= Epoch [53/70] ==========================================\n",
      "\u001b[0m\u001b[1mEpoch: [53/70], Progress: 0.00 %, Steps: 4837, Current Learning Rate: 0.0008986, \u001b[91mTrain Loss: 0.276\n",
      "\u001b[0m\u001b[1mEpoch: [53/70], Progress: 1.08 %, Steps: 4838, Current Learning Rate: 0.0008985, \u001b[91mTrain Loss: 0.312\n",
      "\u001b[0m\u001b[1mEpoch: [53/70], Progress: 2.15 %, Steps: 4839, Current Learning Rate: 0.0008984, \u001b[91mTrain Loss: 0.323\n",
      "\u001b[0m\u001b[1mEpoch: [53/70], Progress: 3.23 %, Steps: 4840, Current Learning Rate: 0.0008983, \u001b[96mTrain Loss: 0.286\n",
      "\u001b[0m\u001b[1mEpoch: [53/70], Progress: 4.30 %, Steps: 4841, Current Learning Rate: 0.0008982, \u001b[91mTrain Loss: 0.331\n",
      "\u001b[0m\u001b[1mEpoch: [53/70], Progress: 5.38 %, Steps: 4842, Current Learning Rate: 0.0008981, \u001b[96mTrain Loss: 0.301\n",
      "\u001b[0m\u001b[1mEpoch: [53/70], Progress: 6.45 %, Steps: 4843, Current Learning Rate: 0.0008980, \u001b[96mTrain Loss: 0.288\n",
      "\u001b[0m\u001b[1mEpoch: [53/70], Progress: 7.53 %, Steps: 4844, Current Learning Rate: 0.0008979, \u001b[91mTrain Loss: 0.297\n",
      "\u001b[0m\u001b[1mEpoch: [53/70], Progress: 8.60 %, Steps: 4845, Current Learning Rate: 0.0008978, \u001b[96mTrain Loss: 0.290\n",
      "\u001b[0m\u001b[1mEpoch: [53/70], Progress: 9.68 %, Steps: 4846, Current Learning Rate: 0.0008977, \u001b[96mTrain Loss: 0.259\n",
      "\u001b[0m\u001b[1mEpoch: [53/70], Progress: 10.75 %, Steps: 4847, Current Learning Rate: 0.0008976, \u001b[91mTrain Loss: 0.362\n",
      "\u001b[0m\u001b[1mEpoch: [53/70], Progress: 11.83 %, Steps: 4848, Current Learning Rate: 0.0008975, \u001b[96mTrain Loss: 0.263\n",
      "\u001b[0m\u001b[1mEpoch: [53/70], Progress: 12.90 %, Steps: 4849, Current Learning Rate: 0.0008974, \u001b[91mTrain Loss: 0.345\n",
      "\u001b[0m\u001b[1mEpoch: [53/70], Progress: 13.98 %, Steps: 4850, Current Learning Rate: 0.0008974, \u001b[96mTrain Loss: 0.228\n",
      "\u001b[0m\u001b[1mEpoch: [53/70], Progress: 15.05 %, Steps: 4851, Current Learning Rate: 0.0008973, \u001b[91mTrain Loss: 0.264\n",
      "\u001b[0m\u001b[1mEpoch: [53/70], Progress: 16.13 %, Steps: 4852, Current Learning Rate: 0.0008972, \u001b[91mTrain Loss: 0.298\n",
      "\u001b[0m\u001b[1mEpoch: [53/70], Progress: 17.20 %, Steps: 4853, Current Learning Rate: 0.0008971, \u001b[91mTrain Loss: 0.331\n",
      "\u001b[0m\u001b[1mEpoch: [53/70], Progress: 18.28 %, Steps: 4854, Current Learning Rate: 0.0008970, \u001b[91mTrain Loss: 0.385\n",
      "\u001b[0m\u001b[1mEpoch: [53/70], Progress: 19.35 %, Steps: 4855, Current Learning Rate: 0.0008969, \u001b[96mTrain Loss: 0.342\n",
      "\u001b[0m\u001b[1mEpoch: [53/70], Progress: 20.43 %, Steps: 4856, Current Learning Rate: 0.0008968, \u001b[91mTrain Loss: 0.395\n",
      "\u001b[0m\u001b[1mEpoch: [53/70], Progress: 21.51 %, Steps: 4857, Current Learning Rate: 0.0008967, \u001b[96mTrain Loss: 0.333\n",
      "\u001b[0m\u001b[1mEpoch: [53/70], Progress: 22.58 %, Steps: 4858, Current Learning Rate: 0.0008966, \u001b[91mTrain Loss: 0.363\n",
      "\u001b[0m\u001b[1mEpoch: [53/70], Progress: 23.66 %, Steps: 4859, Current Learning Rate: 0.0008965, \u001b[96mTrain Loss: 0.358\n",
      "\u001b[0m\u001b[1mEpoch: [53/70], Progress: 24.73 %, Steps: 4860, Current Learning Rate: 0.0008964, \u001b[96mTrain Loss: 0.306\n",
      "\u001b[0m\u001b[1mEpoch: [53/70], Progress: 25.81 %, Steps: 4861, Current Learning Rate: 0.0008963, \u001b[96mTrain Loss: 0.278\n",
      "\u001b[0m\u001b[1mEpoch: [53/70], Progress: 26.88 %, Steps: 4862, Current Learning Rate: 0.0008962, \u001b[91mTrain Loss: 0.327\n",
      "\u001b[0m\u001b[1mEpoch: [53/70], Progress: 27.96 %, Steps: 4863, Current Learning Rate: 0.0008962, \u001b[96mTrain Loss: 0.324\n",
      "\u001b[0m\u001b[1mEpoch: [53/70], Progress: 29.03 %, Steps: 4864, Current Learning Rate: 0.0008961, \u001b[91mTrain Loss: 0.341\n",
      "\u001b[0m\u001b[1mEpoch: [53/70], Progress: 30.11 %, Steps: 4865, Current Learning Rate: 0.0008960, \u001b[96mTrain Loss: 0.321\n",
      "\u001b[0m\u001b[1mEpoch: [53/70], Progress: 31.18 %, Steps: 4866, Current Learning Rate: 0.0008959, \u001b[91mTrain Loss: 0.323\n",
      "\u001b[0m\u001b[1mEpoch: [53/70], Progress: 32.26 %, Steps: 4867, Current Learning Rate: 0.0008958, \u001b[91mTrain Loss: 0.411\n",
      "\u001b[0m\u001b[1mEpoch: [53/70], Progress: 33.33 %, Steps: 4868, Current Learning Rate: 0.0008957, \u001b[96mTrain Loss: 0.285\n",
      "\u001b[0m\u001b[1mEpoch: [53/70], Progress: 34.41 %, Steps: 4869, Current Learning Rate: 0.0008956, \u001b[91mTrain Loss: 0.392\n",
      "\u001b[0m\u001b[1mEpoch: [53/70], Progress: 35.48 %, Steps: 4870, Current Learning Rate: 0.0008955, \u001b[96mTrain Loss: 0.328\n",
      "\u001b[0m\u001b[1mEpoch: [53/70], Progress: 36.56 %, Steps: 4871, Current Learning Rate: 0.0008954, \u001b[91mTrain Loss: 0.341\n",
      "\u001b[0m\u001b[1mEpoch: [53/70], Progress: 37.63 %, Steps: 4872, Current Learning Rate: 0.0008953, \u001b[96mTrain Loss: 0.297\n",
      "\u001b[0m\u001b[1mEpoch: [53/70], Progress: 38.71 %, Steps: 4873, Current Learning Rate: 0.0008952, \u001b[96mTrain Loss: 0.279\n",
      "\u001b[0m\u001b[1mEpoch: [53/70], Progress: 39.78 %, Steps: 4874, Current Learning Rate: 0.0008951, \u001b[91mTrain Loss: 0.323\n",
      "\u001b[0m\u001b[1mEpoch: [53/70], Progress: 40.86 %, Steps: 4875, Current Learning Rate: 0.0008951, \u001b[96mTrain Loss: 0.291\n",
      "\u001b[0m\u001b[1mEpoch: [53/70], Progress: 41.94 %, Steps: 4876, Current Learning Rate: 0.0008950, \u001b[91mTrain Loss: 0.395\n",
      "\u001b[0m\u001b[1mEpoch: [53/70], Progress: 43.01 %, Steps: 4877, Current Learning Rate: 0.0008949, \u001b[96mTrain Loss: 0.371\n",
      "\u001b[0m\u001b[1mEpoch: [53/70], Progress: 44.09 %, Steps: 4878, Current Learning Rate: 0.0008948, \u001b[96mTrain Loss: 0.345\n",
      "\u001b[0m\u001b[1mEpoch: [53/70], Progress: 45.16 %, Steps: 4879, Current Learning Rate: 0.0008947, \u001b[96mTrain Loss: 0.286\n",
      "\u001b[0m\u001b[1mEpoch: [53/70], Progress: 46.24 %, Steps: 4880, Current Learning Rate: 0.0008946, \u001b[91mTrain Loss: 0.428\n",
      "\u001b[0m\u001b[1mEpoch: [53/70], Progress: 47.31 %, Steps: 4881, Current Learning Rate: 0.0008945, \u001b[96mTrain Loss: 0.392\n",
      "\u001b[0m\u001b[1mEpoch: [53/70], Progress: 48.39 %, Steps: 4882, Current Learning Rate: 0.0008944, \u001b[96mTrain Loss: 0.339\n",
      "\u001b[0m\u001b[1mEpoch: [53/70], Progress: 49.46 %, Steps: 4883, Current Learning Rate: 0.0008943, \u001b[96mTrain Loss: 0.313\n",
      "\u001b[0m\u001b[1mEpoch: [53/70], Progress: 50.54 %, Steps: 4884, Current Learning Rate: 0.0008942, \u001b[91mTrain Loss: 0.404\n",
      "\u001b[0m\u001b[1mEpoch: [53/70], Progress: 51.61 %, Steps: 4885, Current Learning Rate: 0.0008941, \u001b[96mTrain Loss: 0.328\n",
      "\u001b[0m\u001b[1mEpoch: [53/70], Progress: 52.69 %, Steps: 4886, Current Learning Rate: 0.0008940, \u001b[91mTrain Loss: 0.337\n",
      "\u001b[0m\u001b[1mEpoch: [53/70], Progress: 53.76 %, Steps: 4887, Current Learning Rate: 0.0008940, \u001b[91mTrain Loss: 0.387\n",
      "\u001b[0m\u001b[1mEpoch: [53/70], Progress: 54.84 %, Steps: 4888, Current Learning Rate: 0.0008939, \u001b[96mTrain Loss: 0.371\n",
      "\u001b[0m\u001b[1mEpoch: [53/70], Progress: 55.91 %, Steps: 4889, Current Learning Rate: 0.0008938, \u001b[96mTrain Loss: 0.358\n",
      "\u001b[0m\u001b[1mEpoch: [53/70], Progress: 56.99 %, Steps: 4890, Current Learning Rate: 0.0008937, \u001b[91mTrain Loss: 0.373\n",
      "\u001b[0m\u001b[1mEpoch: [53/70], Progress: 58.06 %, Steps: 4891, Current Learning Rate: 0.0008936, \u001b[91mTrain Loss: 0.467\n",
      "\u001b[0m\u001b[1mEpoch: [53/70], Progress: 59.14 %, Steps: 4892, Current Learning Rate: 0.0008935, \u001b[96mTrain Loss: 0.433\n",
      "\u001b[0m\u001b[1mEpoch: [53/70], Progress: 60.22 %, Steps: 4893, Current Learning Rate: 0.0008934, \u001b[96mTrain Loss: 0.371\n",
      "\u001b[0m\u001b[1mEpoch: [53/70], Progress: 61.29 %, Steps: 4894, Current Learning Rate: 0.0008933, \u001b[91mTrain Loss: 0.410\n",
      "\u001b[0m\u001b[1mEpoch: [53/70], Progress: 62.37 %, Steps: 4895, Current Learning Rate: 0.0008932, \u001b[91mTrain Loss: 0.490\n",
      "\u001b[0m\u001b[1mEpoch: [53/70], Progress: 63.44 %, Steps: 4896, Current Learning Rate: 0.0008931, \u001b[96mTrain Loss: 0.396\n",
      "\u001b[0m\u001b[1mEpoch: [53/70], Progress: 64.52 %, Steps: 4897, Current Learning Rate: 0.0008930, \u001b[96mTrain Loss: 0.386\n",
      "\u001b[0m\u001b[1mEpoch: [53/70], Progress: 65.59 %, Steps: 4898, Current Learning Rate: 0.0008929, \u001b[96mTrain Loss: 0.314\n",
      "\u001b[0m\u001b[1mEpoch: [53/70], Progress: 66.67 %, Steps: 4899, Current Learning Rate: 0.0008929, \u001b[91mTrain Loss: 0.484\n",
      "\u001b[0m\u001b[1mEpoch: [53/70], Progress: 67.74 %, Steps: 4900, Current Learning Rate: 0.0008928, \u001b[96mTrain Loss: 0.325\n",
      "\u001b[0m\u001b[1mEpoch: [53/70], Progress: 68.82 %, Steps: 4901, Current Learning Rate: 0.0008927, \u001b[96mTrain Loss: 0.308\n",
      "\u001b[0m\u001b[1mEpoch: [53/70], Progress: 69.89 %, Steps: 4902, Current Learning Rate: 0.0008926, \u001b[91mTrain Loss: 0.324\n",
      "\u001b[0m\u001b[1mEpoch: [53/70], Progress: 70.97 %, Steps: 4903, Current Learning Rate: 0.0008925, \u001b[91mTrain Loss: 0.395\n",
      "\u001b[0m\u001b[1mEpoch: [53/70], Progress: 72.04 %, Steps: 4904, Current Learning Rate: 0.0008924, \u001b[96mTrain Loss: 0.363\n",
      "\u001b[0m\u001b[1mEpoch: [53/70], Progress: 73.12 %, Steps: 4905, Current Learning Rate: 0.0008923, \u001b[91mTrain Loss: 0.456\n",
      "\u001b[0m\u001b[1mEpoch: [53/70], Progress: 74.19 %, Steps: 4906, Current Learning Rate: 0.0008922, \u001b[96mTrain Loss: 0.358\n",
      "\u001b[0m\u001b[1mEpoch: [53/70], Progress: 75.27 %, Steps: 4907, Current Learning Rate: 0.0008921, \u001b[91mTrain Loss: 0.460\n",
      "\u001b[0m\u001b[1mEpoch: [53/70], Progress: 76.34 %, Steps: 4908, Current Learning Rate: 0.0008920, \u001b[96mTrain Loss: 0.395\n",
      "\u001b[0m\u001b[1mEpoch: [53/70], Progress: 77.42 %, Steps: 4909, Current Learning Rate: 0.0008919, \u001b[96mTrain Loss: 0.393\n",
      "\u001b[0m\u001b[1mEpoch: [53/70], Progress: 78.49 %, Steps: 4910, Current Learning Rate: 0.0008919, \u001b[91mTrain Loss: 0.408\n",
      "\u001b[0m\u001b[1mEpoch: [53/70], Progress: 79.57 %, Steps: 4911, Current Learning Rate: 0.0008918, \u001b[96mTrain Loss: 0.359\n",
      "\u001b[0m\u001b[1mEpoch: [53/70], Progress: 80.65 %, Steps: 4912, Current Learning Rate: 0.0008917, \u001b[91mTrain Loss: 0.369\n",
      "\u001b[0m\u001b[1mEpoch: [53/70], Progress: 81.72 %, Steps: 4913, Current Learning Rate: 0.0008916, \u001b[91mTrain Loss: 0.371\n",
      "\u001b[0m\u001b[1mEpoch: [53/70], Progress: 82.80 %, Steps: 4914, Current Learning Rate: 0.0008915, \u001b[91mTrain Loss: 0.406\n",
      "\u001b[0m\u001b[1mEpoch: [53/70], Progress: 83.87 %, Steps: 4915, Current Learning Rate: 0.0008914, \u001b[91mTrain Loss: 0.410\n",
      "\u001b[0m\u001b[1mEpoch: [53/70], Progress: 84.95 %, Steps: 4916, Current Learning Rate: 0.0008913, \u001b[91mTrain Loss: 0.430\n",
      "\u001b[0m\u001b[1mEpoch: [53/70], Progress: 86.02 %, Steps: 4917, Current Learning Rate: 0.0008912, \u001b[96mTrain Loss: 0.410\n",
      "\u001b[0m\u001b[1mEpoch: [53/70], Progress: 87.10 %, Steps: 4918, Current Learning Rate: 0.0008911, \u001b[91mTrain Loss: 0.417\n",
      "\u001b[0m\u001b[1mEpoch: [53/70], Progress: 88.17 %, Steps: 4919, Current Learning Rate: 0.0008910, \u001b[91mTrain Loss: 0.460\n",
      "\u001b[0m\u001b[1mEpoch: [53/70], Progress: 89.25 %, Steps: 4920, Current Learning Rate: 0.0008910, \u001b[96mTrain Loss: 0.398\n",
      "\u001b[0m\u001b[1mEpoch: [53/70], Progress: 90.32 %, Steps: 4921, Current Learning Rate: 0.0008909, \u001b[96mTrain Loss: 0.361\n",
      "\u001b[0m\u001b[1mEpoch: [53/70], Progress: 91.40 %, Steps: 4922, Current Learning Rate: 0.0008908, \u001b[91mTrain Loss: 0.434\n",
      "\u001b[0m\u001b[1mEpoch: [53/70], Progress: 92.47 %, Steps: 4923, Current Learning Rate: 0.0008907, \u001b[96mTrain Loss: 0.384\n",
      "\u001b[0m\u001b[1mEpoch: [53/70], Progress: 93.55 %, Steps: 4924, Current Learning Rate: 0.0008906, \u001b[91mTrain Loss: 0.401\n",
      "\u001b[0m\u001b[1mEpoch: [53/70], Progress: 94.62 %, Steps: 4925, Current Learning Rate: 0.0008905, \u001b[91mTrain Loss: 0.439\n",
      "\u001b[0m\u001b[1mEpoch: [53/70], Progress: 95.70 %, Steps: 4926, Current Learning Rate: 0.0008904, \u001b[96mTrain Loss: 0.366\n",
      "\u001b[0m\u001b[1mEpoch: [53/70], Progress: 96.77 %, Steps: 4927, Current Learning Rate: 0.0008903, \u001b[96mTrain Loss: 0.321\n",
      "\u001b[0m\u001b[1mEpoch: [53/70], Progress: 97.85 %, Steps: 4928, Current Learning Rate: 0.0008902, \u001b[91mTrain Loss: 0.409\n",
      "\u001b[0m\u001b[1mEpoch: [53/70], Progress: 98.92 %, Steps: 4929, Current Learning Rate: 0.0008901, \u001b[96mTrain Loss: 0.335\n",
      "\u001b[1m\u001b[93mComputing Validation Loss...\n",
      "\u001b[1mEpoch 53 Completed! Average Train Loss: 0.357, Average Validation Loss: 0.090\n",
      "Best Model saved in best_model/best_model.pt.\n",
      "\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m= Epoch [54/70] ==========================================\n",
      "\u001b[0m\u001b[1mEpoch: [54/70], Progress: 0.00 %, Steps: 4930, Current Learning Rate: 0.0008900, \u001b[91mTrain Loss: 0.251\n",
      "\u001b[0m\u001b[1mEpoch: [54/70], Progress: 1.08 %, Steps: 4931, Current Learning Rate: 0.0008900, \u001b[91mTrain Loss: 0.263\n",
      "\u001b[0m\u001b[1mEpoch: [54/70], Progress: 2.15 %, Steps: 4932, Current Learning Rate: 0.0008899, \u001b[91mTrain Loss: 0.263\n",
      "\u001b[0m\u001b[1mEpoch: [54/70], Progress: 3.23 %, Steps: 4933, Current Learning Rate: 0.0008898, \u001b[91mTrain Loss: 0.279\n",
      "\u001b[0m\u001b[1mEpoch: [54/70], Progress: 4.30 %, Steps: 4934, Current Learning Rate: 0.0008897, \u001b[96mTrain Loss: 0.256\n",
      "\u001b[0m\u001b[1mEpoch: [54/70], Progress: 5.38 %, Steps: 4935, Current Learning Rate: 0.0008896, \u001b[96mTrain Loss: 0.231\n",
      "\u001b[0m\u001b[1mEpoch: [54/70], Progress: 6.45 %, Steps: 4936, Current Learning Rate: 0.0008895, \u001b[91mTrain Loss: 0.291\n",
      "\u001b[0m\u001b[1mEpoch: [54/70], Progress: 7.53 %, Steps: 4937, Current Learning Rate: 0.0008894, \u001b[91mTrain Loss: 0.298\n",
      "\u001b[0m\u001b[1mEpoch: [54/70], Progress: 8.60 %, Steps: 4938, Current Learning Rate: 0.0008893, \u001b[96mTrain Loss: 0.260\n",
      "\u001b[0m\u001b[1mEpoch: [54/70], Progress: 9.68 %, Steps: 4939, Current Learning Rate: 0.0008892, \u001b[96mTrain Loss: 0.248\n",
      "\u001b[0m\u001b[1mEpoch: [54/70], Progress: 10.75 %, Steps: 4940, Current Learning Rate: 0.0008891, \u001b[91mTrain Loss: 0.268\n",
      "\u001b[0m\u001b[1mEpoch: [54/70], Progress: 11.83 %, Steps: 4941, Current Learning Rate: 0.0008891, \u001b[96mTrain Loss: 0.253\n",
      "\u001b[0m\u001b[1mEpoch: [54/70], Progress: 12.90 %, Steps: 4942, Current Learning Rate: 0.0008890, \u001b[96mTrain Loss: 0.243\n",
      "\u001b[0m\u001b[1mEpoch: [54/70], Progress: 13.98 %, Steps: 4943, Current Learning Rate: 0.0008889, \u001b[91mTrain Loss: 0.282\n",
      "\u001b[0m\u001b[1mEpoch: [54/70], Progress: 15.05 %, Steps: 4944, Current Learning Rate: 0.0008888, \u001b[96mTrain Loss: 0.264\n",
      "\u001b[0m\u001b[1mEpoch: [54/70], Progress: 16.13 %, Steps: 4945, Current Learning Rate: 0.0008887, \u001b[91mTrain Loss: 0.323\n",
      "\u001b[0m\u001b[1mEpoch: [54/70], Progress: 17.20 %, Steps: 4946, Current Learning Rate: 0.0008886, \u001b[96mTrain Loss: 0.274\n",
      "\u001b[0m\u001b[1mEpoch: [54/70], Progress: 18.28 %, Steps: 4947, Current Learning Rate: 0.0008885, \u001b[96mTrain Loss: 0.266\n",
      "\u001b[0m\u001b[1mEpoch: [54/70], Progress: 19.35 %, Steps: 4948, Current Learning Rate: 0.0008884, \u001b[91mTrain Loss: 0.267\n",
      "\u001b[0m\u001b[1mEpoch: [54/70], Progress: 20.43 %, Steps: 4949, Current Learning Rate: 0.0008883, \u001b[91mTrain Loss: 0.273\n",
      "\u001b[0m\u001b[1mEpoch: [54/70], Progress: 21.51 %, Steps: 4950, Current Learning Rate: 0.0008882, \u001b[91mTrain Loss: 0.350\n",
      "\u001b[0m\u001b[1mEpoch: [54/70], Progress: 22.58 %, Steps: 4951, Current Learning Rate: 0.0008882, \u001b[96mTrain Loss: 0.275\n",
      "\u001b[0m\u001b[1mEpoch: [54/70], Progress: 23.66 %, Steps: 4952, Current Learning Rate: 0.0008881, \u001b[91mTrain Loss: 0.302\n",
      "\u001b[0m\u001b[1mEpoch: [54/70], Progress: 24.73 %, Steps: 4953, Current Learning Rate: 0.0008880, \u001b[91mTrain Loss: 0.322\n",
      "\u001b[0m\u001b[1mEpoch: [54/70], Progress: 25.81 %, Steps: 4954, Current Learning Rate: 0.0008879, \u001b[96mTrain Loss: 0.249\n",
      "\u001b[0m\u001b[1mEpoch: [54/70], Progress: 26.88 %, Steps: 4955, Current Learning Rate: 0.0008878, \u001b[91mTrain Loss: 0.318\n",
      "\u001b[0m\u001b[1mEpoch: [54/70], Progress: 27.96 %, Steps: 4956, Current Learning Rate: 0.0008877, \u001b[91mTrain Loss: 0.327\n",
      "\u001b[0m\u001b[1mEpoch: [54/70], Progress: 29.03 %, Steps: 4957, Current Learning Rate: 0.0008876, \u001b[96mTrain Loss: 0.279\n",
      "\u001b[0m\u001b[1mEpoch: [54/70], Progress: 30.11 %, Steps: 4958, Current Learning Rate: 0.0008875, \u001b[91mTrain Loss: 0.353\n",
      "\u001b[0m\u001b[1mEpoch: [54/70], Progress: 31.18 %, Steps: 4959, Current Learning Rate: 0.0008874, \u001b[96mTrain Loss: 0.292\n",
      "\u001b[0m\u001b[1mEpoch: [54/70], Progress: 32.26 %, Steps: 4960, Current Learning Rate: 0.0008874, \u001b[96mTrain Loss: 0.289\n",
      "\u001b[0m\u001b[1mEpoch: [54/70], Progress: 33.33 %, Steps: 4961, Current Learning Rate: 0.0008873, \u001b[91mTrain Loss: 0.309\n",
      "\u001b[0m\u001b[1mEpoch: [54/70], Progress: 34.41 %, Steps: 4962, Current Learning Rate: 0.0008872, \u001b[91mTrain Loss: 0.359\n",
      "\u001b[0m\u001b[1mEpoch: [54/70], Progress: 35.48 %, Steps: 4963, Current Learning Rate: 0.0008871, \u001b[96mTrain Loss: 0.304\n",
      "\u001b[0m\u001b[1mEpoch: [54/70], Progress: 36.56 %, Steps: 4964, Current Learning Rate: 0.0008870, \u001b[91mTrain Loss: 0.381\n",
      "\u001b[0m\u001b[1mEpoch: [54/70], Progress: 37.63 %, Steps: 4965, Current Learning Rate: 0.0008869, \u001b[96mTrain Loss: 0.317\n",
      "\u001b[0m\u001b[1mEpoch: [54/70], Progress: 38.71 %, Steps: 4966, Current Learning Rate: 0.0008868, \u001b[96mTrain Loss: 0.259\n",
      "\u001b[0m\u001b[1mEpoch: [54/70], Progress: 39.78 %, Steps: 4967, Current Learning Rate: 0.0008867, \u001b[91mTrain Loss: 0.345\n",
      "\u001b[0m\u001b[1mEpoch: [54/70], Progress: 40.86 %, Steps: 4968, Current Learning Rate: 0.0008866, \u001b[96mTrain Loss: 0.290\n",
      "\u001b[0m\u001b[1mEpoch: [54/70], Progress: 41.94 %, Steps: 4969, Current Learning Rate: 0.0008865, \u001b[91mTrain Loss: 0.301\n",
      "\u001b[0m\u001b[1mEpoch: [54/70], Progress: 43.01 %, Steps: 4970, Current Learning Rate: 0.0008865, \u001b[91mTrain Loss: 0.361\n",
      "\u001b[0m\u001b[1mEpoch: [54/70], Progress: 44.09 %, Steps: 4971, Current Learning Rate: 0.0008864, \u001b[96mTrain Loss: 0.267\n",
      "\u001b[0m\u001b[1mEpoch: [54/70], Progress: 45.16 %, Steps: 4972, Current Learning Rate: 0.0008863, \u001b[91mTrain Loss: 0.322\n",
      "\u001b[0m\u001b[1mEpoch: [54/70], Progress: 46.24 %, Steps: 4973, Current Learning Rate: 0.0008862, \u001b[91mTrain Loss: 0.412\n",
      "\u001b[0m\u001b[1mEpoch: [54/70], Progress: 47.31 %, Steps: 4974, Current Learning Rate: 0.0008861, \u001b[91mTrain Loss: 0.415\n",
      "\u001b[0m\u001b[1mEpoch: [54/70], Progress: 48.39 %, Steps: 4975, Current Learning Rate: 0.0008860, \u001b[96mTrain Loss: 0.362\n",
      "\u001b[0m\u001b[1mEpoch: [54/70], Progress: 49.46 %, Steps: 4976, Current Learning Rate: 0.0008859, \u001b[96mTrain Loss: 0.317\n",
      "\u001b[0m\u001b[1mEpoch: [54/70], Progress: 50.54 %, Steps: 4977, Current Learning Rate: 0.0008858, \u001b[96mTrain Loss: 0.285\n",
      "\u001b[0m\u001b[1mEpoch: [54/70], Progress: 51.61 %, Steps: 4978, Current Learning Rate: 0.0008857, \u001b[91mTrain Loss: 0.331\n",
      "\u001b[0m\u001b[1mEpoch: [54/70], Progress: 52.69 %, Steps: 4979, Current Learning Rate: 0.0008857, \u001b[91mTrain Loss: 0.359\n",
      "\u001b[0m\u001b[1mEpoch: [54/70], Progress: 53.76 %, Steps: 4980, Current Learning Rate: 0.0008856, \u001b[91mTrain Loss: 0.371\n",
      "\u001b[0m\u001b[1mEpoch: [54/70], Progress: 54.84 %, Steps: 4981, Current Learning Rate: 0.0008855, \u001b[96mTrain Loss: 0.347\n",
      "\u001b[0m\u001b[1mEpoch: [54/70], Progress: 55.91 %, Steps: 4982, Current Learning Rate: 0.0008854, \u001b[91mTrain Loss: 0.371\n",
      "\u001b[0m\u001b[1mEpoch: [54/70], Progress: 56.99 %, Steps: 4983, Current Learning Rate: 0.0008853, \u001b[96mTrain Loss: 0.351\n",
      "\u001b[0m\u001b[1mEpoch: [54/70], Progress: 58.06 %, Steps: 4984, Current Learning Rate: 0.0008852, \u001b[96mTrain Loss: 0.284\n",
      "\u001b[0m\u001b[1mEpoch: [54/70], Progress: 59.14 %, Steps: 4985, Current Learning Rate: 0.0008851, \u001b[91mTrain Loss: 0.390\n",
      "\u001b[0m\u001b[1mEpoch: [54/70], Progress: 60.22 %, Steps: 4986, Current Learning Rate: 0.0008850, \u001b[96mTrain Loss: 0.342\n",
      "\u001b[0m\u001b[1mEpoch: [54/70], Progress: 61.29 %, Steps: 4987, Current Learning Rate: 0.0008849, \u001b[96mTrain Loss: 0.292\n",
      "\u001b[0m\u001b[1mEpoch: [54/70], Progress: 62.37 %, Steps: 4988, Current Learning Rate: 0.0008849, \u001b[91mTrain Loss: 0.380\n",
      "\u001b[0m\u001b[1mEpoch: [54/70], Progress: 63.44 %, Steps: 4989, Current Learning Rate: 0.0008848, \u001b[96mTrain Loss: 0.340\n",
      "\u001b[0m\u001b[1mEpoch: [54/70], Progress: 64.52 %, Steps: 4990, Current Learning Rate: 0.0008847, \u001b[91mTrain Loss: 0.368\n",
      "\u001b[0m\u001b[1mEpoch: [54/70], Progress: 65.59 %, Steps: 4991, Current Learning Rate: 0.0008846, \u001b[91mTrain Loss: 0.388\n",
      "\u001b[0m\u001b[1mEpoch: [54/70], Progress: 66.67 %, Steps: 4992, Current Learning Rate: 0.0008845, \u001b[96mTrain Loss: 0.308\n",
      "\u001b[0m\u001b[1mEpoch: [54/70], Progress: 67.74 %, Steps: 4993, Current Learning Rate: 0.0008844, \u001b[91mTrain Loss: 0.335\n",
      "\u001b[0m\u001b[1mEpoch: [54/70], Progress: 68.82 %, Steps: 4994, Current Learning Rate: 0.0008843, \u001b[91mTrain Loss: 0.365\n",
      "\u001b[0m\u001b[1mEpoch: [54/70], Progress: 69.89 %, Steps: 4995, Current Learning Rate: 0.0008842, \u001b[96mTrain Loss: 0.348\n",
      "\u001b[0m\u001b[1mEpoch: [54/70], Progress: 70.97 %, Steps: 4996, Current Learning Rate: 0.0008841, \u001b[96mTrain Loss: 0.304\n",
      "\u001b[0m\u001b[1mEpoch: [54/70], Progress: 72.04 %, Steps: 4997, Current Learning Rate: 0.0008841, \u001b[91mTrain Loss: 0.364\n",
      "\u001b[0m\u001b[1mEpoch: [54/70], Progress: 73.12 %, Steps: 4998, Current Learning Rate: 0.0008840, \u001b[91mTrain Loss: 0.381\n",
      "\u001b[0m\u001b[1mEpoch: [54/70], Progress: 74.19 %, Steps: 4999, Current Learning Rate: 0.0008839, \u001b[91mTrain Loss: 0.395\n",
      "\u001b[0m\u001b[1mEpoch: [54/70], Progress: 75.27 %, Steps: 5000, Current Learning Rate: 0.0008838, \u001b[96mTrain Loss: 0.301\n",
      "\u001b[0m\u001b[1mEpoch: [54/70], Progress: 76.34 %, Steps: 5001, Current Learning Rate: 0.0008837, \u001b[96mTrain Loss: 0.294\n",
      "\u001b[0m\u001b[1mEpoch: [54/70], Progress: 77.42 %, Steps: 5002, Current Learning Rate: 0.0008836, \u001b[91mTrain Loss: 0.347\n",
      "\u001b[0m\u001b[1mEpoch: [54/70], Progress: 78.49 %, Steps: 5003, Current Learning Rate: 0.0008835, \u001b[91mTrain Loss: 0.355\n",
      "\u001b[0m\u001b[1mEpoch: [54/70], Progress: 79.57 %, Steps: 5004, Current Learning Rate: 0.0008834, \u001b[96mTrain Loss: 0.349\n",
      "\u001b[0m\u001b[1mEpoch: [54/70], Progress: 80.65 %, Steps: 5005, Current Learning Rate: 0.0008834, \u001b[96mTrain Loss: 0.309\n",
      "\u001b[0m\u001b[1mEpoch: [54/70], Progress: 81.72 %, Steps: 5006, Current Learning Rate: 0.0008833, \u001b[91mTrain Loss: 0.341\n",
      "\u001b[0m\u001b[1mEpoch: [54/70], Progress: 82.80 %, Steps: 5007, Current Learning Rate: 0.0008832, \u001b[91mTrain Loss: 0.378\n",
      "\u001b[0m\u001b[1mEpoch: [54/70], Progress: 83.87 %, Steps: 5008, Current Learning Rate: 0.0008831, \u001b[91mTrain Loss: 0.379\n",
      "\u001b[0m\u001b[1mEpoch: [54/70], Progress: 84.95 %, Steps: 5009, Current Learning Rate: 0.0008830, \u001b[96mTrain Loss: 0.359\n",
      "\u001b[0m\u001b[1mEpoch: [54/70], Progress: 86.02 %, Steps: 5010, Current Learning Rate: 0.0008829, \u001b[91mTrain Loss: 0.404\n",
      "\u001b[0m\u001b[1mEpoch: [54/70], Progress: 87.10 %, Steps: 5011, Current Learning Rate: 0.0008828, \u001b[96mTrain Loss: 0.331\n",
      "\u001b[0m\u001b[1mEpoch: [54/70], Progress: 88.17 %, Steps: 5012, Current Learning Rate: 0.0008827, \u001b[91mTrain Loss: 0.361\n",
      "\u001b[0m\u001b[1mEpoch: [54/70], Progress: 89.25 %, Steps: 5013, Current Learning Rate: 0.0008826, \u001b[91mTrain Loss: 0.370\n",
      "\u001b[0m\u001b[1mEpoch: [54/70], Progress: 90.32 %, Steps: 5014, Current Learning Rate: 0.0008826, \u001b[96mTrain Loss: 0.346\n",
      "\u001b[0m\u001b[1mEpoch: [54/70], Progress: 91.40 %, Steps: 5015, Current Learning Rate: 0.0008825, \u001b[91mTrain Loss: 0.392\n",
      "\u001b[0m\u001b[1mEpoch: [54/70], Progress: 92.47 %, Steps: 5016, Current Learning Rate: 0.0008824, \u001b[96mTrain Loss: 0.300\n",
      "\u001b[0m\u001b[1mEpoch: [54/70], Progress: 93.55 %, Steps: 5017, Current Learning Rate: 0.0008823, \u001b[91mTrain Loss: 0.427\n",
      "\u001b[0m\u001b[1mEpoch: [54/70], Progress: 94.62 %, Steps: 5018, Current Learning Rate: 0.0008822, \u001b[96mTrain Loss: 0.359\n",
      "\u001b[0m\u001b[1mEpoch: [54/70], Progress: 95.70 %, Steps: 5019, Current Learning Rate: 0.0008821, \u001b[96mTrain Loss: 0.350\n",
      "\u001b[0m\u001b[1mEpoch: [54/70], Progress: 96.77 %, Steps: 5020, Current Learning Rate: 0.0008820, \u001b[91mTrain Loss: 0.409\n",
      "\u001b[0m\u001b[1mEpoch: [54/70], Progress: 97.85 %, Steps: 5021, Current Learning Rate: 0.0008819, \u001b[91mTrain Loss: 0.429\n",
      "\u001b[0m\u001b[1mEpoch: [54/70], Progress: 98.92 %, Steps: 5022, Current Learning Rate: 0.0008819, \u001b[96mTrain Loss: 0.356\n",
      "\u001b[1m\u001b[93mComputing Validation Loss...\n",
      "\u001b[1mEpoch 54 Completed! Average Train Loss: 0.326, Average Validation Loss: 0.076\n",
      "Best Model saved in best_model/best_model.pt.\n",
      "\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m= Epoch [55/70] ==========================================\n",
      "\u001b[0m\u001b[1mEpoch: [55/70], Progress: 0.00 %, Steps: 5023, Current Learning Rate: 0.0008818, \u001b[91mTrain Loss: 0.243\n",
      "\u001b[0m\u001b[1mEpoch: [55/70], Progress: 1.08 %, Steps: 5024, Current Learning Rate: 0.0008817, \u001b[96mTrain Loss: 0.206\n",
      "\u001b[0m\u001b[1mEpoch: [55/70], Progress: 2.15 %, Steps: 5025, Current Learning Rate: 0.0008816, \u001b[91mTrain Loss: 0.262\n",
      "\u001b[0m\u001b[1mEpoch: [55/70], Progress: 3.23 %, Steps: 5026, Current Learning Rate: 0.0008815, \u001b[91mTrain Loss: 0.310\n",
      "\u001b[0m\u001b[1mEpoch: [55/70], Progress: 4.30 %, Steps: 5027, Current Learning Rate: 0.0008814, \u001b[96mTrain Loss: 0.284\n",
      "\u001b[0m\u001b[1mEpoch: [55/70], Progress: 5.38 %, Steps: 5028, Current Learning Rate: 0.0008813, \u001b[96mTrain Loss: 0.221\n",
      "\u001b[0m\u001b[1mEpoch: [55/70], Progress: 6.45 %, Steps: 5029, Current Learning Rate: 0.0008812, \u001b[91mTrain Loss: 0.263\n",
      "\u001b[0m\u001b[1mEpoch: [55/70], Progress: 7.53 %, Steps: 5030, Current Learning Rate: 0.0008812, \u001b[96mTrain Loss: 0.251\n",
      "\u001b[0m\u001b[1mEpoch: [55/70], Progress: 8.60 %, Steps: 5031, Current Learning Rate: 0.0008811, \u001b[91mTrain Loss: 0.257\n",
      "\u001b[0m\u001b[1mEpoch: [55/70], Progress: 9.68 %, Steps: 5032, Current Learning Rate: 0.0008810, \u001b[96mTrain Loss: 0.256\n",
      "\u001b[0m\u001b[1mEpoch: [55/70], Progress: 10.75 %, Steps: 5033, Current Learning Rate: 0.0008809, \u001b[96mTrain Loss: 0.240\n",
      "\u001b[0m\u001b[1mEpoch: [55/70], Progress: 11.83 %, Steps: 5034, Current Learning Rate: 0.0008808, \u001b[91mTrain Loss: 0.284\n",
      "\u001b[0m\u001b[1mEpoch: [55/70], Progress: 12.90 %, Steps: 5035, Current Learning Rate: 0.0008807, \u001b[91mTrain Loss: 0.334\n",
      "\u001b[0m\u001b[1mEpoch: [55/70], Progress: 13.98 %, Steps: 5036, Current Learning Rate: 0.0008806, \u001b[96mTrain Loss: 0.239\n",
      "\u001b[0m\u001b[1mEpoch: [55/70], Progress: 15.05 %, Steps: 5037, Current Learning Rate: 0.0008805, \u001b[91mTrain Loss: 0.292\n",
      "\u001b[0m\u001b[1mEpoch: [55/70], Progress: 16.13 %, Steps: 5038, Current Learning Rate: 0.0008805, \u001b[91mTrain Loss: 0.298\n",
      "\u001b[0m\u001b[1mEpoch: [55/70], Progress: 17.20 %, Steps: 5039, Current Learning Rate: 0.0008804, \u001b[91mTrain Loss: 0.349\n",
      "\u001b[0m\u001b[1mEpoch: [55/70], Progress: 18.28 %, Steps: 5040, Current Learning Rate: 0.0008803, \u001b[96mTrain Loss: 0.348\n",
      "\u001b[0m\u001b[1mEpoch: [55/70], Progress: 19.35 %, Steps: 5041, Current Learning Rate: 0.0008802, \u001b[96mTrain Loss: 0.223\n",
      "\u001b[0m\u001b[1mEpoch: [55/70], Progress: 20.43 %, Steps: 5042, Current Learning Rate: 0.0008801, \u001b[91mTrain Loss: 0.262\n",
      "\u001b[0m\u001b[1mEpoch: [55/70], Progress: 21.51 %, Steps: 5043, Current Learning Rate: 0.0008800, \u001b[96mTrain Loss: 0.260\n",
      "\u001b[0m\u001b[1mEpoch: [55/70], Progress: 22.58 %, Steps: 5044, Current Learning Rate: 0.0008799, \u001b[91mTrain Loss: 0.300\n",
      "\u001b[0m\u001b[1mEpoch: [55/70], Progress: 23.66 %, Steps: 5045, Current Learning Rate: 0.0008798, \u001b[96mTrain Loss: 0.271\n",
      "\u001b[0m\u001b[1mEpoch: [55/70], Progress: 24.73 %, Steps: 5046, Current Learning Rate: 0.0008798, \u001b[96mTrain Loss: 0.264\n",
      "\u001b[0m\u001b[1mEpoch: [55/70], Progress: 25.81 %, Steps: 5047, Current Learning Rate: 0.0008797, \u001b[91mTrain Loss: 0.299\n",
      "\u001b[0m\u001b[1mEpoch: [55/70], Progress: 26.88 %, Steps: 5048, Current Learning Rate: 0.0008796, \u001b[91mTrain Loss: 0.305\n",
      "\u001b[0m\u001b[1mEpoch: [55/70], Progress: 27.96 %, Steps: 5049, Current Learning Rate: 0.0008795, \u001b[91mTrain Loss: 0.322\n",
      "\u001b[0m\u001b[1mEpoch: [55/70], Progress: 29.03 %, Steps: 5050, Current Learning Rate: 0.0008794, \u001b[96mTrain Loss: 0.279\n",
      "\u001b[0m\u001b[1mEpoch: [55/70], Progress: 30.11 %, Steps: 5051, Current Learning Rate: 0.0008793, \u001b[91mTrain Loss: 0.298\n",
      "\u001b[0m\u001b[1mEpoch: [55/70], Progress: 31.18 %, Steps: 5052, Current Learning Rate: 0.0008792, \u001b[91mTrain Loss: 0.319\n",
      "\u001b[0m\u001b[1mEpoch: [55/70], Progress: 32.26 %, Steps: 5053, Current Learning Rate: 0.0008791, \u001b[96mTrain Loss: 0.314\n",
      "\u001b[0m\u001b[1mEpoch: [55/70], Progress: 33.33 %, Steps: 5054, Current Learning Rate: 0.0008791, \u001b[96mTrain Loss: 0.269\n",
      "\u001b[0m\u001b[1mEpoch: [55/70], Progress: 34.41 %, Steps: 5055, Current Learning Rate: 0.0008790, \u001b[96mTrain Loss: 0.210\n",
      "\u001b[0m\u001b[1mEpoch: [55/70], Progress: 35.48 %, Steps: 5056, Current Learning Rate: 0.0008789, \u001b[91mTrain Loss: 0.296\n",
      "\u001b[0m\u001b[1mEpoch: [55/70], Progress: 36.56 %, Steps: 5057, Current Learning Rate: 0.0008788, \u001b[91mTrain Loss: 0.347\n",
      "\u001b[0m\u001b[1mEpoch: [55/70], Progress: 37.63 %, Steps: 5058, Current Learning Rate: 0.0008787, \u001b[91mTrain Loss: 0.384\n",
      "\u001b[0m\u001b[1mEpoch: [55/70], Progress: 38.71 %, Steps: 5059, Current Learning Rate: 0.0008786, \u001b[96mTrain Loss: 0.251\n",
      "\u001b[0m\u001b[1mEpoch: [55/70], Progress: 39.78 %, Steps: 5060, Current Learning Rate: 0.0008785, \u001b[91mTrain Loss: 0.278\n",
      "\u001b[0m\u001b[1mEpoch: [55/70], Progress: 40.86 %, Steps: 5061, Current Learning Rate: 0.0008785, \u001b[91mTrain Loss: 0.310\n",
      "\u001b[0m\u001b[1mEpoch: [55/70], Progress: 41.94 %, Steps: 5062, Current Learning Rate: 0.0008784, \u001b[96mTrain Loss: 0.201\n",
      "\u001b[0m\u001b[1mEpoch: [55/70], Progress: 43.01 %, Steps: 5063, Current Learning Rate: 0.0008783, \u001b[91mTrain Loss: 0.262\n",
      "\u001b[0m\u001b[1mEpoch: [55/70], Progress: 44.09 %, Steps: 5064, Current Learning Rate: 0.0008782, \u001b[91mTrain Loss: 0.283\n",
      "\u001b[0m\u001b[1mEpoch: [55/70], Progress: 45.16 %, Steps: 5065, Current Learning Rate: 0.0008781, \u001b[96mTrain Loss: 0.239\n",
      "\u001b[0m\u001b[1mEpoch: [55/70], Progress: 46.24 %, Steps: 5066, Current Learning Rate: 0.0008780, \u001b[91mTrain Loss: 0.290\n",
      "\u001b[0m\u001b[1mEpoch: [55/70], Progress: 47.31 %, Steps: 5067, Current Learning Rate: 0.0008779, \u001b[91mTrain Loss: 0.308\n",
      "\u001b[0m\u001b[1mEpoch: [55/70], Progress: 48.39 %, Steps: 5068, Current Learning Rate: 0.0008778, \u001b[91mTrain Loss: 0.323\n",
      "\u001b[0m\u001b[1mEpoch: [55/70], Progress: 49.46 %, Steps: 5069, Current Learning Rate: 0.0008778, \u001b[91mTrain Loss: 0.359\n",
      "\u001b[0m\u001b[1mEpoch: [55/70], Progress: 50.54 %, Steps: 5070, Current Learning Rate: 0.0008777, \u001b[96mTrain Loss: 0.286\n",
      "\u001b[0m\u001b[1mEpoch: [55/70], Progress: 51.61 %, Steps: 5071, Current Learning Rate: 0.0008776, \u001b[91mTrain Loss: 0.335\n",
      "\u001b[0m\u001b[1mEpoch: [55/70], Progress: 52.69 %, Steps: 5072, Current Learning Rate: 0.0008775, \u001b[91mTrain Loss: 0.341\n",
      "\u001b[0m\u001b[1mEpoch: [55/70], Progress: 53.76 %, Steps: 5073, Current Learning Rate: 0.0008774, \u001b[96mTrain Loss: 0.290\n",
      "\u001b[0m\u001b[1mEpoch: [55/70], Progress: 54.84 %, Steps: 5074, Current Learning Rate: 0.0008773, \u001b[96mTrain Loss: 0.256\n",
      "\u001b[0m\u001b[1mEpoch: [55/70], Progress: 55.91 %, Steps: 5075, Current Learning Rate: 0.0008772, \u001b[91mTrain Loss: 0.315\n",
      "\u001b[0m\u001b[1mEpoch: [55/70], Progress: 56.99 %, Steps: 5076, Current Learning Rate: 0.0008772, \u001b[96mTrain Loss: 0.290\n",
      "\u001b[0m\u001b[1mEpoch: [55/70], Progress: 58.06 %, Steps: 5077, Current Learning Rate: 0.0008771, \u001b[96mTrain Loss: 0.287\n",
      "\u001b[0m\u001b[1mEpoch: [55/70], Progress: 59.14 %, Steps: 5078, Current Learning Rate: 0.0008770, \u001b[91mTrain Loss: 0.288\n",
      "\u001b[0m\u001b[1mEpoch: [55/70], Progress: 60.22 %, Steps: 5079, Current Learning Rate: 0.0008769, \u001b[96mTrain Loss: 0.274\n",
      "\u001b[0m\u001b[1mEpoch: [55/70], Progress: 61.29 %, Steps: 5080, Current Learning Rate: 0.0008768, \u001b[91mTrain Loss: 0.329\n",
      "\u001b[0m\u001b[1mEpoch: [55/70], Progress: 62.37 %, Steps: 5081, Current Learning Rate: 0.0008767, \u001b[96mTrain Loss: 0.302\n",
      "\u001b[0m\u001b[1mEpoch: [55/70], Progress: 63.44 %, Steps: 5082, Current Learning Rate: 0.0008766, \u001b[91mTrain Loss: 0.334\n",
      "\u001b[0m\u001b[1mEpoch: [55/70], Progress: 64.52 %, Steps: 5083, Current Learning Rate: 0.0008766, \u001b[96mTrain Loss: 0.285\n",
      "\u001b[0m\u001b[1mEpoch: [55/70], Progress: 65.59 %, Steps: 5084, Current Learning Rate: 0.0008765, \u001b[91mTrain Loss: 0.334\n",
      "\u001b[0m\u001b[1mEpoch: [55/70], Progress: 66.67 %, Steps: 5085, Current Learning Rate: 0.0008764, \u001b[96mTrain Loss: 0.332\n",
      "\u001b[0m\u001b[1mEpoch: [55/70], Progress: 67.74 %, Steps: 5086, Current Learning Rate: 0.0008763, \u001b[96mTrain Loss: 0.262\n",
      "\u001b[0m\u001b[1mEpoch: [55/70], Progress: 68.82 %, Steps: 5087, Current Learning Rate: 0.0008762, \u001b[91mTrain Loss: 0.344\n",
      "\u001b[0m\u001b[1mEpoch: [55/70], Progress: 69.89 %, Steps: 5088, Current Learning Rate: 0.0008761, \u001b[96mTrain Loss: 0.289\n",
      "\u001b[0m\u001b[1mEpoch: [55/70], Progress: 70.97 %, Steps: 5089, Current Learning Rate: 0.0008760, \u001b[96mTrain Loss: 0.254\n",
      "\u001b[0m\u001b[1mEpoch: [55/70], Progress: 72.04 %, Steps: 5090, Current Learning Rate: 0.0008759, \u001b[91mTrain Loss: 0.359\n",
      "\u001b[0m\u001b[1mEpoch: [55/70], Progress: 73.12 %, Steps: 5091, Current Learning Rate: 0.0008759, \u001b[91mTrain Loss: 0.365\n",
      "\u001b[0m\u001b[1mEpoch: [55/70], Progress: 74.19 %, Steps: 5092, Current Learning Rate: 0.0008758, \u001b[96mTrain Loss: 0.335\n",
      "\u001b[0m\u001b[1mEpoch: [55/70], Progress: 75.27 %, Steps: 5093, Current Learning Rate: 0.0008757, \u001b[96mTrain Loss: 0.264\n",
      "\u001b[0m\u001b[1mEpoch: [55/70], Progress: 76.34 %, Steps: 5094, Current Learning Rate: 0.0008756, \u001b[91mTrain Loss: 0.337\n",
      "\u001b[0m\u001b[1mEpoch: [55/70], Progress: 77.42 %, Steps: 5095, Current Learning Rate: 0.0008755, \u001b[91mTrain Loss: 0.353\n",
      "\u001b[0m\u001b[1mEpoch: [55/70], Progress: 78.49 %, Steps: 5096, Current Learning Rate: 0.0008754, \u001b[91mTrain Loss: 0.355\n",
      "\u001b[0m\u001b[1mEpoch: [55/70], Progress: 79.57 %, Steps: 5097, Current Learning Rate: 0.0008753, \u001b[96mTrain Loss: 0.320\n",
      "\u001b[0m\u001b[1mEpoch: [55/70], Progress: 80.65 %, Steps: 5098, Current Learning Rate: 0.0008753, \u001b[91mTrain Loss: 0.343\n",
      "\u001b[0m\u001b[1mEpoch: [55/70], Progress: 81.72 %, Steps: 5099, Current Learning Rate: 0.0008752, \u001b[96mTrain Loss: 0.315\n",
      "\u001b[0m\u001b[1mEpoch: [55/70], Progress: 82.80 %, Steps: 5100, Current Learning Rate: 0.0008751, \u001b[91mTrain Loss: 0.328\n",
      "\u001b[0m\u001b[1mEpoch: [55/70], Progress: 83.87 %, Steps: 5101, Current Learning Rate: 0.0008750, \u001b[91mTrain Loss: 0.355\n",
      "\u001b[0m\u001b[1mEpoch: [55/70], Progress: 84.95 %, Steps: 5102, Current Learning Rate: 0.0008749, \u001b[96mTrain Loss: 0.288\n",
      "\u001b[0m\u001b[1mEpoch: [55/70], Progress: 86.02 %, Steps: 5103, Current Learning Rate: 0.0008748, \u001b[91mTrain Loss: 0.432\n",
      "\u001b[0m\u001b[1mEpoch: [55/70], Progress: 87.10 %, Steps: 5104, Current Learning Rate: 0.0008747, \u001b[96mTrain Loss: 0.272\n",
      "\u001b[0m\u001b[1mEpoch: [55/70], Progress: 88.17 %, Steps: 5105, Current Learning Rate: 0.0008747, \u001b[91mTrain Loss: 0.397\n",
      "\u001b[0m\u001b[1mEpoch: [55/70], Progress: 89.25 %, Steps: 5106, Current Learning Rate: 0.0008746, \u001b[96mTrain Loss: 0.259\n",
      "\u001b[0m\u001b[1mEpoch: [55/70], Progress: 90.32 %, Steps: 5107, Current Learning Rate: 0.0008745, \u001b[91mTrain Loss: 0.388\n",
      "\u001b[0m\u001b[1mEpoch: [55/70], Progress: 91.40 %, Steps: 5108, Current Learning Rate: 0.0008744, \u001b[96mTrain Loss: 0.343\n",
      "\u001b[0m\u001b[1mEpoch: [55/70], Progress: 92.47 %, Steps: 5109, Current Learning Rate: 0.0008743, \u001b[96mTrain Loss: 0.322\n",
      "\u001b[0m\u001b[1mEpoch: [55/70], Progress: 93.55 %, Steps: 5110, Current Learning Rate: 0.0008742, \u001b[96mTrain Loss: 0.315\n",
      "\u001b[0m\u001b[1mEpoch: [55/70], Progress: 94.62 %, Steps: 5111, Current Learning Rate: 0.0008741, \u001b[96mTrain Loss: 0.285\n",
      "\u001b[0m\u001b[1mEpoch: [55/70], Progress: 95.70 %, Steps: 5112, Current Learning Rate: 0.0008741, \u001b[91mTrain Loss: 0.328\n",
      "\u001b[0m\u001b[1mEpoch: [55/70], Progress: 96.77 %, Steps: 5113, Current Learning Rate: 0.0008740, \u001b[91mTrain Loss: 0.365\n",
      "\u001b[0m\u001b[1mEpoch: [55/70], Progress: 97.85 %, Steps: 5114, Current Learning Rate: 0.0008739, \u001b[96mTrain Loss: 0.350\n",
      "\u001b[0m\u001b[1mEpoch: [55/70], Progress: 98.92 %, Steps: 5115, Current Learning Rate: 0.0008738, \u001b[96mTrain Loss: 0.305\n",
      "\u001b[1m\u001b[93mComputing Validation Loss...\n",
      "\u001b[1mEpoch 55 Completed! Average Train Loss: 0.301, Average Validation Loss: 0.073\n",
      "Best Model saved in best_model/best_model.pt.\n",
      "\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m= Epoch [56/70] ==========================================\n",
      "\u001b[0m\u001b[1mEpoch: [56/70], Progress: 0.00 %, Steps: 5116, Current Learning Rate: 0.0008737, \u001b[91mTrain Loss: 0.234\n",
      "\u001b[0m\u001b[1mEpoch: [56/70], Progress: 1.08 %, Steps: 5117, Current Learning Rate: 0.0008736, \u001b[96mTrain Loss: 0.217\n",
      "\u001b[0m\u001b[1mEpoch: [56/70], Progress: 2.15 %, Steps: 5118, Current Learning Rate: 0.0008735, \u001b[96mTrain Loss: 0.208\n",
      "\u001b[0m\u001b[1mEpoch: [56/70], Progress: 3.23 %, Steps: 5119, Current Learning Rate: 0.0008735, \u001b[91mTrain Loss: 0.218\n",
      "\u001b[0m\u001b[1mEpoch: [56/70], Progress: 4.30 %, Steps: 5120, Current Learning Rate: 0.0008734, \u001b[91mTrain Loss: 0.329\n",
      "\u001b[0m\u001b[1mEpoch: [56/70], Progress: 5.38 %, Steps: 5121, Current Learning Rate: 0.0008733, \u001b[96mTrain Loss: 0.205\n",
      "\u001b[0m\u001b[1mEpoch: [56/70], Progress: 6.45 %, Steps: 5122, Current Learning Rate: 0.0008732, \u001b[91mTrain Loss: 0.288\n",
      "\u001b[0m\u001b[1mEpoch: [56/70], Progress: 7.53 %, Steps: 5123, Current Learning Rate: 0.0008731, \u001b[96mTrain Loss: 0.247\n",
      "\u001b[0m\u001b[1mEpoch: [56/70], Progress: 8.60 %, Steps: 5124, Current Learning Rate: 0.0008730, \u001b[96mTrain Loss: 0.208\n",
      "\u001b[0m\u001b[1mEpoch: [56/70], Progress: 9.68 %, Steps: 5125, Current Learning Rate: 0.0008730, \u001b[96mTrain Loss: 0.197\n",
      "\u001b[0m\u001b[1mEpoch: [56/70], Progress: 10.75 %, Steps: 5126, Current Learning Rate: 0.0008729, \u001b[91mTrain Loss: 0.240\n",
      "\u001b[0m\u001b[1mEpoch: [56/70], Progress: 11.83 %, Steps: 5127, Current Learning Rate: 0.0008728, \u001b[96mTrain Loss: 0.227\n",
      "\u001b[0m\u001b[1mEpoch: [56/70], Progress: 12.90 %, Steps: 5128, Current Learning Rate: 0.0008727, \u001b[91mTrain Loss: 0.267\n",
      "\u001b[0m\u001b[1mEpoch: [56/70], Progress: 13.98 %, Steps: 5129, Current Learning Rate: 0.0008726, \u001b[91mTrain Loss: 0.275\n",
      "\u001b[0m\u001b[1mEpoch: [56/70], Progress: 15.05 %, Steps: 5130, Current Learning Rate: 0.0008725, \u001b[96mTrain Loss: 0.267\n",
      "\u001b[0m\u001b[1mEpoch: [56/70], Progress: 16.13 %, Steps: 5131, Current Learning Rate: 0.0008724, \u001b[96mTrain Loss: 0.242\n",
      "\u001b[0m\u001b[1mEpoch: [56/70], Progress: 17.20 %, Steps: 5132, Current Learning Rate: 0.0008724, \u001b[96mTrain Loss: 0.196\n",
      "\u001b[0m\u001b[1mEpoch: [56/70], Progress: 18.28 %, Steps: 5133, Current Learning Rate: 0.0008723, \u001b[91mTrain Loss: 0.229\n",
      "\u001b[0m\u001b[1mEpoch: [56/70], Progress: 19.35 %, Steps: 5134, Current Learning Rate: 0.0008722, \u001b[91mTrain Loss: 0.262\n",
      "\u001b[0m\u001b[1mEpoch: [56/70], Progress: 20.43 %, Steps: 5135, Current Learning Rate: 0.0008721, \u001b[96mTrain Loss: 0.259\n",
      "\u001b[0m\u001b[1mEpoch: [56/70], Progress: 21.51 %, Steps: 5136, Current Learning Rate: 0.0008720, \u001b[96mTrain Loss: 0.231\n",
      "\u001b[0m\u001b[1mEpoch: [56/70], Progress: 22.58 %, Steps: 5137, Current Learning Rate: 0.0008719, \u001b[96mTrain Loss: 0.215\n",
      "\u001b[0m\u001b[1mEpoch: [56/70], Progress: 23.66 %, Steps: 5138, Current Learning Rate: 0.0008718, \u001b[91mTrain Loss: 0.249\n",
      "\u001b[0m\u001b[1mEpoch: [56/70], Progress: 24.73 %, Steps: 5139, Current Learning Rate: 0.0008718, \u001b[96mTrain Loss: 0.224\n",
      "\u001b[0m\u001b[1mEpoch: [56/70], Progress: 25.81 %, Steps: 5140, Current Learning Rate: 0.0008717, \u001b[91mTrain Loss: 0.243\n",
      "\u001b[0m\u001b[1mEpoch: [56/70], Progress: 26.88 %, Steps: 5141, Current Learning Rate: 0.0008716, \u001b[91mTrain Loss: 0.247\n",
      "\u001b[0m\u001b[1mEpoch: [56/70], Progress: 27.96 %, Steps: 5142, Current Learning Rate: 0.0008715, \u001b[91mTrain Loss: 0.291\n",
      "\u001b[0m\u001b[1mEpoch: [56/70], Progress: 29.03 %, Steps: 5143, Current Learning Rate: 0.0008714, \u001b[96mTrain Loss: 0.208\n",
      "\u001b[0m\u001b[1mEpoch: [56/70], Progress: 30.11 %, Steps: 5144, Current Learning Rate: 0.0008713, \u001b[91mTrain Loss: 0.250\n",
      "\u001b[0m\u001b[1mEpoch: [56/70], Progress: 31.18 %, Steps: 5145, Current Learning Rate: 0.0008713, \u001b[91mTrain Loss: 0.271\n",
      "\u001b[0m\u001b[1mEpoch: [56/70], Progress: 32.26 %, Steps: 5146, Current Learning Rate: 0.0008712, \u001b[91mTrain Loss: 0.319\n",
      "\u001b[0m\u001b[1mEpoch: [56/70], Progress: 33.33 %, Steps: 5147, Current Learning Rate: 0.0008711, \u001b[96mTrain Loss: 0.205\n",
      "\u001b[0m\u001b[1mEpoch: [56/70], Progress: 34.41 %, Steps: 5148, Current Learning Rate: 0.0008710, \u001b[91mTrain Loss: 0.278\n",
      "\u001b[0m\u001b[1mEpoch: [56/70], Progress: 35.48 %, Steps: 5149, Current Learning Rate: 0.0008709, \u001b[91mTrain Loss: 0.282\n",
      "\u001b[0m\u001b[1mEpoch: [56/70], Progress: 36.56 %, Steps: 5150, Current Learning Rate: 0.0008708, \u001b[91mTrain Loss: 0.291\n",
      "\u001b[0m\u001b[1mEpoch: [56/70], Progress: 37.63 %, Steps: 5151, Current Learning Rate: 0.0008707, \u001b[96mTrain Loss: 0.184\n",
      "\u001b[0m\u001b[1mEpoch: [56/70], Progress: 38.71 %, Steps: 5152, Current Learning Rate: 0.0008707, \u001b[91mTrain Loss: 0.248\n",
      "\u001b[0m\u001b[1mEpoch: [56/70], Progress: 39.78 %, Steps: 5153, Current Learning Rate: 0.0008706, \u001b[91mTrain Loss: 0.251\n",
      "\u001b[0m\u001b[1mEpoch: [56/70], Progress: 40.86 %, Steps: 5154, Current Learning Rate: 0.0008705, \u001b[96mTrain Loss: 0.231\n",
      "\u001b[0m\u001b[1mEpoch: [56/70], Progress: 41.94 %, Steps: 5155, Current Learning Rate: 0.0008704, \u001b[91mTrain Loss: 0.284\n",
      "\u001b[0m\u001b[1mEpoch: [56/70], Progress: 43.01 %, Steps: 5156, Current Learning Rate: 0.0008703, \u001b[96mTrain Loss: 0.271\n",
      "\u001b[0m\u001b[1mEpoch: [56/70], Progress: 44.09 %, Steps: 5157, Current Learning Rate: 0.0008702, \u001b[96mTrain Loss: 0.256\n",
      "\u001b[0m\u001b[1mEpoch: [56/70], Progress: 45.16 %, Steps: 5158, Current Learning Rate: 0.0008702, \u001b[91mTrain Loss: 0.280\n",
      "\u001b[0m\u001b[1mEpoch: [56/70], Progress: 46.24 %, Steps: 5159, Current Learning Rate: 0.0008701, \u001b[96mTrain Loss: 0.231\n",
      "\u001b[0m\u001b[1mEpoch: [56/70], Progress: 47.31 %, Steps: 5160, Current Learning Rate: 0.0008700, \u001b[91mTrain Loss: 0.302\n",
      "\u001b[0m\u001b[1mEpoch: [56/70], Progress: 48.39 %, Steps: 5161, Current Learning Rate: 0.0008699, \u001b[96mTrain Loss: 0.258\n",
      "\u001b[0m\u001b[1mEpoch: [56/70], Progress: 49.46 %, Steps: 5162, Current Learning Rate: 0.0008698, \u001b[91mTrain Loss: 0.294\n",
      "\u001b[0m\u001b[1mEpoch: [56/70], Progress: 50.54 %, Steps: 5163, Current Learning Rate: 0.0008697, \u001b[96mTrain Loss: 0.230\n",
      "\u001b[0m\u001b[1mEpoch: [56/70], Progress: 51.61 %, Steps: 5164, Current Learning Rate: 0.0008697, \u001b[91mTrain Loss: 0.256\n",
      "\u001b[0m\u001b[1mEpoch: [56/70], Progress: 52.69 %, Steps: 5165, Current Learning Rate: 0.0008696, \u001b[91mTrain Loss: 0.272\n",
      "\u001b[0m\u001b[1mEpoch: [56/70], Progress: 53.76 %, Steps: 5166, Current Learning Rate: 0.0008695, \u001b[96mTrain Loss: 0.271\n",
      "\u001b[0m\u001b[1mEpoch: [56/70], Progress: 54.84 %, Steps: 5167, Current Learning Rate: 0.0008694, \u001b[96mTrain Loss: 0.265\n",
      "\u001b[0m\u001b[1mEpoch: [56/70], Progress: 55.91 %, Steps: 5168, Current Learning Rate: 0.0008693, \u001b[91mTrain Loss: 0.266\n",
      "\u001b[0m\u001b[1mEpoch: [56/70], Progress: 56.99 %, Steps: 5169, Current Learning Rate: 0.0008692, \u001b[91mTrain Loss: 0.284\n",
      "\u001b[0m\u001b[1mEpoch: [56/70], Progress: 58.06 %, Steps: 5170, Current Learning Rate: 0.0008691, \u001b[91mTrain Loss: 0.296\n",
      "\u001b[0m\u001b[1mEpoch: [56/70], Progress: 59.14 %, Steps: 5171, Current Learning Rate: 0.0008691, \u001b[96mTrain Loss: 0.268\n",
      "\u001b[0m\u001b[1mEpoch: [56/70], Progress: 60.22 %, Steps: 5172, Current Learning Rate: 0.0008690, \u001b[91mTrain Loss: 0.333\n",
      "\u001b[0m\u001b[1mEpoch: [56/70], Progress: 61.29 %, Steps: 5173, Current Learning Rate: 0.0008689, \u001b[96mTrain Loss: 0.249\n",
      "\u001b[0m\u001b[1mEpoch: [56/70], Progress: 62.37 %, Steps: 5174, Current Learning Rate: 0.0008688, \u001b[91mTrain Loss: 0.254\n",
      "\u001b[0m\u001b[1mEpoch: [56/70], Progress: 63.44 %, Steps: 5175, Current Learning Rate: 0.0008687, \u001b[91mTrain Loss: 0.297\n",
      "\u001b[0m\u001b[1mEpoch: [56/70], Progress: 64.52 %, Steps: 5176, Current Learning Rate: 0.0008686, \u001b[91mTrain Loss: 0.319\n",
      "\u001b[0m\u001b[1mEpoch: [56/70], Progress: 65.59 %, Steps: 5177, Current Learning Rate: 0.0008686, \u001b[96mTrain Loss: 0.309\n",
      "\u001b[0m\u001b[1mEpoch: [56/70], Progress: 66.67 %, Steps: 5178, Current Learning Rate: 0.0008685, \u001b[96mTrain Loss: 0.281\n",
      "\u001b[0m\u001b[1mEpoch: [56/70], Progress: 67.74 %, Steps: 5179, Current Learning Rate: 0.0008684, \u001b[91mTrain Loss: 0.286\n",
      "\u001b[0m\u001b[1mEpoch: [56/70], Progress: 68.82 %, Steps: 5180, Current Learning Rate: 0.0008683, \u001b[91mTrain Loss: 0.308\n",
      "\u001b[0m\u001b[1mEpoch: [56/70], Progress: 69.89 %, Steps: 5181, Current Learning Rate: 0.0008682, \u001b[96mTrain Loss: 0.223\n",
      "\u001b[0m\u001b[1mEpoch: [56/70], Progress: 70.97 %, Steps: 5182, Current Learning Rate: 0.0008681, \u001b[91mTrain Loss: 0.395\n",
      "\u001b[0m\u001b[1mEpoch: [56/70], Progress: 72.04 %, Steps: 5183, Current Learning Rate: 0.0008681, \u001b[96mTrain Loss: 0.327\n",
      "\u001b[0m\u001b[1mEpoch: [56/70], Progress: 73.12 %, Steps: 5184, Current Learning Rate: 0.0008680, \u001b[91mTrain Loss: 0.352\n",
      "\u001b[0m\u001b[1mEpoch: [56/70], Progress: 74.19 %, Steps: 5185, Current Learning Rate: 0.0008679, \u001b[96mTrain Loss: 0.349\n",
      "\u001b[0m\u001b[1mEpoch: [56/70], Progress: 75.27 %, Steps: 5186, Current Learning Rate: 0.0008678, \u001b[96mTrain Loss: 0.294\n",
      "\u001b[0m\u001b[1mEpoch: [56/70], Progress: 76.34 %, Steps: 5187, Current Learning Rate: 0.0008677, \u001b[96mTrain Loss: 0.253\n",
      "\u001b[0m\u001b[1mEpoch: [56/70], Progress: 77.42 %, Steps: 5188, Current Learning Rate: 0.0008676, \u001b[91mTrain Loss: 0.320\n",
      "\u001b[0m\u001b[1mEpoch: [56/70], Progress: 78.49 %, Steps: 5189, Current Learning Rate: 0.0008676, \u001b[96mTrain Loss: 0.295\n",
      "\u001b[0m\u001b[1mEpoch: [56/70], Progress: 79.57 %, Steps: 5190, Current Learning Rate: 0.0008675, \u001b[96mTrain Loss: 0.280\n",
      "\u001b[0m\u001b[1mEpoch: [56/70], Progress: 80.65 %, Steps: 5191, Current Learning Rate: 0.0008674, \u001b[96mTrain Loss: 0.271\n",
      "\u001b[0m\u001b[1mEpoch: [56/70], Progress: 81.72 %, Steps: 5192, Current Learning Rate: 0.0008673, \u001b[91mTrain Loss: 0.274\n",
      "\u001b[0m\u001b[1mEpoch: [56/70], Progress: 82.80 %, Steps: 5193, Current Learning Rate: 0.0008672, \u001b[91mTrain Loss: 0.324\n",
      "\u001b[0m\u001b[1mEpoch: [56/70], Progress: 83.87 %, Steps: 5194, Current Learning Rate: 0.0008671, \u001b[96mTrain Loss: 0.309\n",
      "\u001b[0m\u001b[1mEpoch: [56/70], Progress: 84.95 %, Steps: 5195, Current Learning Rate: 0.0008671, \u001b[96mTrain Loss: 0.296\n",
      "\u001b[0m\u001b[1mEpoch: [56/70], Progress: 86.02 %, Steps: 5196, Current Learning Rate: 0.0008670, \u001b[96mTrain Loss: 0.243\n",
      "\u001b[0m\u001b[1mEpoch: [56/70], Progress: 87.10 %, Steps: 5197, Current Learning Rate: 0.0008669, \u001b[91mTrain Loss: 0.269\n",
      "\u001b[0m\u001b[1mEpoch: [56/70], Progress: 88.17 %, Steps: 5198, Current Learning Rate: 0.0008668, \u001b[91mTrain Loss: 0.376\n",
      "\u001b[0m\u001b[1mEpoch: [56/70], Progress: 89.25 %, Steps: 5199, Current Learning Rate: 0.0008667, \u001b[96mTrain Loss: 0.321\n",
      "\u001b[0m\u001b[1mEpoch: [56/70], Progress: 90.32 %, Steps: 5200, Current Learning Rate: 0.0008666, \u001b[91mTrain Loss: 0.324\n",
      "\u001b[0m\u001b[1mEpoch: [56/70], Progress: 91.40 %, Steps: 5201, Current Learning Rate: 0.0008666, \u001b[96mTrain Loss: 0.300\n",
      "\u001b[0m\u001b[1mEpoch: [56/70], Progress: 92.47 %, Steps: 5202, Current Learning Rate: 0.0008665, \u001b[96mTrain Loss: 0.270\n",
      "\u001b[0m\u001b[1mEpoch: [56/70], Progress: 93.55 %, Steps: 5203, Current Learning Rate: 0.0008664, \u001b[91mTrain Loss: 0.282\n",
      "\u001b[0m\u001b[1mEpoch: [56/70], Progress: 94.62 %, Steps: 5204, Current Learning Rate: 0.0008663, \u001b[91mTrain Loss: 0.300\n",
      "\u001b[0m\u001b[1mEpoch: [56/70], Progress: 95.70 %, Steps: 5205, Current Learning Rate: 0.0008662, \u001b[96mTrain Loss: 0.299\n",
      "\u001b[0m\u001b[1mEpoch: [56/70], Progress: 96.77 %, Steps: 5206, Current Learning Rate: 0.0008661, \u001b[91mTrain Loss: 0.332\n",
      "\u001b[0m\u001b[1mEpoch: [56/70], Progress: 97.85 %, Steps: 5207, Current Learning Rate: 0.0008661, \u001b[91mTrain Loss: 0.421\n",
      "\u001b[0m\u001b[1mEpoch: [56/70], Progress: 98.92 %, Steps: 5208, Current Learning Rate: 0.0008660, \u001b[96mTrain Loss: 0.245\n",
      "\u001b[1m\u001b[93mComputing Validation Loss...\n",
      "\u001b[1mEpoch 56 Completed! Average Train Loss: 0.272, Average Validation Loss: 0.061\n",
      "Best Model saved in best_model/best_model.pt.\n",
      "\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m= Epoch [57/70] ==========================================\n",
      "\u001b[0m\u001b[1mEpoch: [57/70], Progress: 0.00 %, Steps: 5209, Current Learning Rate: 0.0008659, \u001b[91mTrain Loss: 0.225\n",
      "\u001b[0m\u001b[1mEpoch: [57/70], Progress: 1.08 %, Steps: 5210, Current Learning Rate: 0.0008658, \u001b[96mTrain Loss: 0.206\n",
      "\u001b[0m\u001b[1mEpoch: [57/70], Progress: 2.15 %, Steps: 5211, Current Learning Rate: 0.0008657, \u001b[96mTrain Loss: 0.189\n",
      "\u001b[0m\u001b[1mEpoch: [57/70], Progress: 3.23 %, Steps: 5212, Current Learning Rate: 0.0008656, \u001b[91mTrain Loss: 0.259\n",
      "\u001b[0m\u001b[1mEpoch: [57/70], Progress: 4.30 %, Steps: 5213, Current Learning Rate: 0.0008656, \u001b[96mTrain Loss: 0.236\n",
      "\u001b[0m\u001b[1mEpoch: [57/70], Progress: 5.38 %, Steps: 5214, Current Learning Rate: 0.0008655, \u001b[96mTrain Loss: 0.205\n",
      "\u001b[0m\u001b[1mEpoch: [57/70], Progress: 6.45 %, Steps: 5215, Current Learning Rate: 0.0008654, \u001b[96mTrain Loss: 0.164\n",
      "\u001b[0m\u001b[1mEpoch: [57/70], Progress: 7.53 %, Steps: 5216, Current Learning Rate: 0.0008653, \u001b[91mTrain Loss: 0.165\n",
      "\u001b[0m\u001b[1mEpoch: [57/70], Progress: 8.60 %, Steps: 5217, Current Learning Rate: 0.0008652, \u001b[91mTrain Loss: 0.276\n",
      "\u001b[0m\u001b[1mEpoch: [57/70], Progress: 9.68 %, Steps: 5218, Current Learning Rate: 0.0008651, \u001b[96mTrain Loss: 0.206\n",
      "\u001b[0m\u001b[1mEpoch: [57/70], Progress: 10.75 %, Steps: 5219, Current Learning Rate: 0.0008651, \u001b[96mTrain Loss: 0.180\n",
      "\u001b[0m\u001b[1mEpoch: [57/70], Progress: 11.83 %, Steps: 5220, Current Learning Rate: 0.0008650, \u001b[91mTrain Loss: 0.228\n",
      "\u001b[0m\u001b[1mEpoch: [57/70], Progress: 12.90 %, Steps: 5221, Current Learning Rate: 0.0008649, \u001b[91mTrain Loss: 0.274\n",
      "\u001b[0m\u001b[1mEpoch: [57/70], Progress: 13.98 %, Steps: 5222, Current Learning Rate: 0.0008648, \u001b[96mTrain Loss: 0.222\n",
      "\u001b[0m\u001b[1mEpoch: [57/70], Progress: 15.05 %, Steps: 5223, Current Learning Rate: 0.0008647, \u001b[91mTrain Loss: 0.252\n",
      "\u001b[0m\u001b[1mEpoch: [57/70], Progress: 16.13 %, Steps: 5224, Current Learning Rate: 0.0008646, \u001b[96mTrain Loss: 0.235\n",
      "\u001b[0m\u001b[1mEpoch: [57/70], Progress: 17.20 %, Steps: 5225, Current Learning Rate: 0.0008646, \u001b[96mTrain Loss: 0.211\n",
      "\u001b[0m\u001b[1mEpoch: [57/70], Progress: 18.28 %, Steps: 5226, Current Learning Rate: 0.0008645, \u001b[96mTrain Loss: 0.195\n",
      "\u001b[0m\u001b[1mEpoch: [57/70], Progress: 19.35 %, Steps: 5227, Current Learning Rate: 0.0008644, \u001b[91mTrain Loss: 0.272\n",
      "\u001b[0m\u001b[1mEpoch: [57/70], Progress: 20.43 %, Steps: 5228, Current Learning Rate: 0.0008643, \u001b[96mTrain Loss: 0.210\n",
      "\u001b[0m\u001b[1mEpoch: [57/70], Progress: 21.51 %, Steps: 5229, Current Learning Rate: 0.0008642, \u001b[91mTrain Loss: 0.240\n",
      "\u001b[0m\u001b[1mEpoch: [57/70], Progress: 22.58 %, Steps: 5230, Current Learning Rate: 0.0008641, \u001b[96mTrain Loss: 0.229\n",
      "\u001b[0m\u001b[1mEpoch: [57/70], Progress: 23.66 %, Steps: 5231, Current Learning Rate: 0.0008641, \u001b[91mTrain Loss: 0.247\n",
      "\u001b[0m\u001b[1mEpoch: [57/70], Progress: 24.73 %, Steps: 5232, Current Learning Rate: 0.0008640, \u001b[91mTrain Loss: 0.284\n",
      "\u001b[0m\u001b[1mEpoch: [57/70], Progress: 25.81 %, Steps: 5233, Current Learning Rate: 0.0008639, \u001b[96mTrain Loss: 0.207\n",
      "\u001b[0m\u001b[1mEpoch: [57/70], Progress: 26.88 %, Steps: 5234, Current Learning Rate: 0.0008638, \u001b[91mTrain Loss: 0.238\n",
      "\u001b[0m\u001b[1mEpoch: [57/70], Progress: 27.96 %, Steps: 5235, Current Learning Rate: 0.0008637, \u001b[91mTrain Loss: 0.271\n",
      "\u001b[0m\u001b[1mEpoch: [57/70], Progress: 29.03 %, Steps: 5236, Current Learning Rate: 0.0008637, \u001b[96mTrain Loss: 0.196\n",
      "\u001b[0m\u001b[1mEpoch: [57/70], Progress: 30.11 %, Steps: 5237, Current Learning Rate: 0.0008636, \u001b[91mTrain Loss: 0.254\n",
      "\u001b[0m\u001b[1mEpoch: [57/70], Progress: 31.18 %, Steps: 5238, Current Learning Rate: 0.0008635, \u001b[91mTrain Loss: 0.265\n",
      "\u001b[0m\u001b[1mEpoch: [57/70], Progress: 32.26 %, Steps: 5239, Current Learning Rate: 0.0008634, \u001b[96mTrain Loss: 0.210\n",
      "\u001b[0m\u001b[1mEpoch: [57/70], Progress: 33.33 %, Steps: 5240, Current Learning Rate: 0.0008633, \u001b[91mTrain Loss: 0.281\n",
      "\u001b[0m\u001b[1mEpoch: [57/70], Progress: 34.41 %, Steps: 5241, Current Learning Rate: 0.0008632, \u001b[96mTrain Loss: 0.189\n",
      "\u001b[0m\u001b[1mEpoch: [57/70], Progress: 35.48 %, Steps: 5242, Current Learning Rate: 0.0008632, \u001b[91mTrain Loss: 0.226\n",
      "\u001b[0m\u001b[1mEpoch: [57/70], Progress: 36.56 %, Steps: 5243, Current Learning Rate: 0.0008631, \u001b[91mTrain Loss: 0.260\n",
      "\u001b[0m\u001b[1mEpoch: [57/70], Progress: 37.63 %, Steps: 5244, Current Learning Rate: 0.0008630, \u001b[96mTrain Loss: 0.248\n",
      "\u001b[0m\u001b[1mEpoch: [57/70], Progress: 38.71 %, Steps: 5245, Current Learning Rate: 0.0008629, \u001b[96mTrain Loss: 0.181\n",
      "\u001b[0m\u001b[1mEpoch: [57/70], Progress: 39.78 %, Steps: 5246, Current Learning Rate: 0.0008628, \u001b[91mTrain Loss: 0.250\n",
      "\u001b[0m\u001b[1mEpoch: [57/70], Progress: 40.86 %, Steps: 5247, Current Learning Rate: 0.0008627, \u001b[91mTrain Loss: 0.255\n",
      "\u001b[0m\u001b[1mEpoch: [57/70], Progress: 41.94 %, Steps: 5248, Current Learning Rate: 0.0008627, \u001b[96mTrain Loss: 0.220\n",
      "\u001b[0m\u001b[1mEpoch: [57/70], Progress: 43.01 %, Steps: 5249, Current Learning Rate: 0.0008626, \u001b[96mTrain Loss: 0.209\n",
      "\u001b[0m\u001b[1mEpoch: [57/70], Progress: 44.09 %, Steps: 5250, Current Learning Rate: 0.0008625, \u001b[91mTrain Loss: 0.211\n",
      "\u001b[0m\u001b[1mEpoch: [57/70], Progress: 45.16 %, Steps: 5251, Current Learning Rate: 0.0008624, \u001b[91mTrain Loss: 0.254\n",
      "\u001b[0m\u001b[1mEpoch: [57/70], Progress: 46.24 %, Steps: 5252, Current Learning Rate: 0.0008623, \u001b[91mTrain Loss: 0.258\n",
      "\u001b[0m\u001b[1mEpoch: [57/70], Progress: 47.31 %, Steps: 5253, Current Learning Rate: 0.0008623, \u001b[96mTrain Loss: 0.228\n",
      "\u001b[0m\u001b[1mEpoch: [57/70], Progress: 48.39 %, Steps: 5254, Current Learning Rate: 0.0008622, \u001b[91mTrain Loss: 0.321\n",
      "\u001b[0m\u001b[1mEpoch: [57/70], Progress: 49.46 %, Steps: 5255, Current Learning Rate: 0.0008621, \u001b[96mTrain Loss: 0.229\n",
      "\u001b[0m\u001b[1mEpoch: [57/70], Progress: 50.54 %, Steps: 5256, Current Learning Rate: 0.0008620, \u001b[91mTrain Loss: 0.314\n",
      "\u001b[0m\u001b[1mEpoch: [57/70], Progress: 51.61 %, Steps: 5257, Current Learning Rate: 0.0008619, \u001b[91mTrain Loss: 0.321\n",
      "\u001b[0m\u001b[1mEpoch: [57/70], Progress: 52.69 %, Steps: 5258, Current Learning Rate: 0.0008618, \u001b[96mTrain Loss: 0.235\n",
      "\u001b[0m\u001b[1mEpoch: [57/70], Progress: 53.76 %, Steps: 5259, Current Learning Rate: 0.0008618, \u001b[96mTrain Loss: 0.228\n",
      "\u001b[0m\u001b[1mEpoch: [57/70], Progress: 54.84 %, Steps: 5260, Current Learning Rate: 0.0008617, \u001b[91mTrain Loss: 0.286\n",
      "\u001b[0m\u001b[1mEpoch: [57/70], Progress: 55.91 %, Steps: 5261, Current Learning Rate: 0.0008616, \u001b[96mTrain Loss: 0.240\n",
      "\u001b[0m\u001b[1mEpoch: [57/70], Progress: 56.99 %, Steps: 5262, Current Learning Rate: 0.0008615, \u001b[91mTrain Loss: 0.247\n",
      "\u001b[0m\u001b[1mEpoch: [57/70], Progress: 58.06 %, Steps: 5263, Current Learning Rate: 0.0008614, \u001b[91mTrain Loss: 0.257\n",
      "\u001b[0m\u001b[1mEpoch: [57/70], Progress: 59.14 %, Steps: 5264, Current Learning Rate: 0.0008614, \u001b[91mTrain Loss: 0.283\n",
      "\u001b[0m\u001b[1mEpoch: [57/70], Progress: 60.22 %, Steps: 5265, Current Learning Rate: 0.0008613, \u001b[96mTrain Loss: 0.240\n",
      "\u001b[0m\u001b[1mEpoch: [57/70], Progress: 61.29 %, Steps: 5266, Current Learning Rate: 0.0008612, \u001b[91mTrain Loss: 0.277\n",
      "\u001b[0m\u001b[1mEpoch: [57/70], Progress: 62.37 %, Steps: 5267, Current Learning Rate: 0.0008611, \u001b[96mTrain Loss: 0.225\n",
      "\u001b[0m\u001b[1mEpoch: [57/70], Progress: 63.44 %, Steps: 5268, Current Learning Rate: 0.0008610, \u001b[91mTrain Loss: 0.234\n",
      "\u001b[0m\u001b[1mEpoch: [57/70], Progress: 64.52 %, Steps: 5269, Current Learning Rate: 0.0008609, \u001b[96mTrain Loss: 0.225\n",
      "\u001b[0m\u001b[1mEpoch: [57/70], Progress: 65.59 %, Steps: 5270, Current Learning Rate: 0.0008609, \u001b[96mTrain Loss: 0.203\n",
      "\u001b[0m\u001b[1mEpoch: [57/70], Progress: 66.67 %, Steps: 5271, Current Learning Rate: 0.0008608, \u001b[91mTrain Loss: 0.275\n",
      "\u001b[0m\u001b[1mEpoch: [57/70], Progress: 67.74 %, Steps: 5272, Current Learning Rate: 0.0008607, \u001b[96mTrain Loss: 0.270\n",
      "\u001b[0m\u001b[1mEpoch: [57/70], Progress: 68.82 %, Steps: 5273, Current Learning Rate: 0.0008606, \u001b[91mTrain Loss: 0.301\n",
      "\u001b[0m\u001b[1mEpoch: [57/70], Progress: 69.89 %, Steps: 5274, Current Learning Rate: 0.0008605, \u001b[96mTrain Loss: 0.284\n",
      "\u001b[0m\u001b[1mEpoch: [57/70], Progress: 70.97 %, Steps: 5275, Current Learning Rate: 0.0008605, \u001b[96mTrain Loss: 0.283\n",
      "\u001b[0m\u001b[1mEpoch: [57/70], Progress: 72.04 %, Steps: 5276, Current Learning Rate: 0.0008604, \u001b[91mTrain Loss: 0.296\n",
      "\u001b[0m\u001b[1mEpoch: [57/70], Progress: 73.12 %, Steps: 5277, Current Learning Rate: 0.0008603, \u001b[96mTrain Loss: 0.207\n",
      "\u001b[0m\u001b[1mEpoch: [57/70], Progress: 74.19 %, Steps: 5278, Current Learning Rate: 0.0008602, \u001b[91mTrain Loss: 0.230\n",
      "\u001b[0m\u001b[1mEpoch: [57/70], Progress: 75.27 %, Steps: 5279, Current Learning Rate: 0.0008601, \u001b[91mTrain Loss: 0.240\n",
      "\u001b[0m\u001b[1mEpoch: [57/70], Progress: 76.34 %, Steps: 5280, Current Learning Rate: 0.0008600, \u001b[96mTrain Loss: 0.218\n",
      "\u001b[0m\u001b[1mEpoch: [57/70], Progress: 77.42 %, Steps: 5281, Current Learning Rate: 0.0008600, \u001b[91mTrain Loss: 0.310\n",
      "\u001b[0m\u001b[1mEpoch: [57/70], Progress: 78.49 %, Steps: 5282, Current Learning Rate: 0.0008599, \u001b[96mTrain Loss: 0.308\n",
      "\u001b[0m\u001b[1mEpoch: [57/70], Progress: 79.57 %, Steps: 5283, Current Learning Rate: 0.0008598, \u001b[96mTrain Loss: 0.255\n",
      "\u001b[0m\u001b[1mEpoch: [57/70], Progress: 80.65 %, Steps: 5284, Current Learning Rate: 0.0008597, \u001b[91mTrain Loss: 0.268\n",
      "\u001b[0m\u001b[1mEpoch: [57/70], Progress: 81.72 %, Steps: 5285, Current Learning Rate: 0.0008596, \u001b[91mTrain Loss: 0.338\n",
      "\u001b[0m\u001b[1mEpoch: [57/70], Progress: 82.80 %, Steps: 5286, Current Learning Rate: 0.0008596, \u001b[96mTrain Loss: 0.294\n",
      "\u001b[0m\u001b[1mEpoch: [57/70], Progress: 83.87 %, Steps: 5287, Current Learning Rate: 0.0008595, \u001b[91mTrain Loss: 0.299\n",
      "\u001b[0m\u001b[1mEpoch: [57/70], Progress: 84.95 %, Steps: 5288, Current Learning Rate: 0.0008594, \u001b[96mTrain Loss: 0.276\n",
      "\u001b[0m\u001b[1mEpoch: [57/70], Progress: 86.02 %, Steps: 5289, Current Learning Rate: 0.0008593, \u001b[96mTrain Loss: 0.273\n",
      "\u001b[0m\u001b[1mEpoch: [57/70], Progress: 87.10 %, Steps: 5290, Current Learning Rate: 0.0008592, \u001b[91mTrain Loss: 0.289\n",
      "\u001b[0m\u001b[1mEpoch: [57/70], Progress: 88.17 %, Steps: 5291, Current Learning Rate: 0.0008592, \u001b[96mTrain Loss: 0.268\n",
      "\u001b[0m\u001b[1mEpoch: [57/70], Progress: 89.25 %, Steps: 5292, Current Learning Rate: 0.0008591, \u001b[91mTrain Loss: 0.304\n",
      "\u001b[0m\u001b[1mEpoch: [57/70], Progress: 90.32 %, Steps: 5293, Current Learning Rate: 0.0008590, \u001b[96mTrain Loss: 0.290\n",
      "\u001b[0m\u001b[1mEpoch: [57/70], Progress: 91.40 %, Steps: 5294, Current Learning Rate: 0.0008589, \u001b[91mTrain Loss: 0.299\n",
      "\u001b[0m\u001b[1mEpoch: [57/70], Progress: 92.47 %, Steps: 5295, Current Learning Rate: 0.0008588, \u001b[96mTrain Loss: 0.266\n",
      "\u001b[0m\u001b[1mEpoch: [57/70], Progress: 93.55 %, Steps: 5296, Current Learning Rate: 0.0008587, \u001b[91mTrain Loss: 0.270\n",
      "\u001b[0m\u001b[1mEpoch: [57/70], Progress: 94.62 %, Steps: 5297, Current Learning Rate: 0.0008587, \u001b[96mTrain Loss: 0.243\n",
      "\u001b[0m\u001b[1mEpoch: [57/70], Progress: 95.70 %, Steps: 5298, Current Learning Rate: 0.0008586, \u001b[96mTrain Loss: 0.217\n",
      "\u001b[0m\u001b[1mEpoch: [57/70], Progress: 96.77 %, Steps: 5299, Current Learning Rate: 0.0008585, \u001b[91mTrain Loss: 0.325\n",
      "\u001b[0m\u001b[1mEpoch: [57/70], Progress: 97.85 %, Steps: 5300, Current Learning Rate: 0.0008584, \u001b[96mTrain Loss: 0.311\n",
      "\u001b[0m\u001b[1mEpoch: [57/70], Progress: 98.92 %, Steps: 5301, Current Learning Rate: 0.0008583, \u001b[96mTrain Loss: 0.308\n",
      "\u001b[1m\u001b[93mComputing Validation Loss...\n",
      "\u001b[1mEpoch 57 Completed! Average Train Loss: 0.251, Average Validation Loss: 0.050\n",
      "Best Model saved in best_model/best_model.pt.\n",
      "\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m= Epoch [58/70] ==========================================\n",
      "\u001b[0m\u001b[1mEpoch: [58/70], Progress: 0.00 %, Steps: 5302, Current Learning Rate: 0.0008583, \u001b[91mTrain Loss: 0.189\n",
      "\u001b[0m\u001b[1mEpoch: [58/70], Progress: 1.08 %, Steps: 5303, Current Learning Rate: 0.0008582, \u001b[91mTrain Loss: 0.191\n",
      "\u001b[0m\u001b[1mEpoch: [58/70], Progress: 2.15 %, Steps: 5304, Current Learning Rate: 0.0008581, \u001b[91mTrain Loss: 0.211\n",
      "\u001b[0m\u001b[1mEpoch: [58/70], Progress: 3.23 %, Steps: 5305, Current Learning Rate: 0.0008580, \u001b[96mTrain Loss: 0.205\n",
      "\u001b[0m\u001b[1mEpoch: [58/70], Progress: 4.30 %, Steps: 5306, Current Learning Rate: 0.0008579, \u001b[96mTrain Loss: 0.170\n",
      "\u001b[0m\u001b[1mEpoch: [58/70], Progress: 5.38 %, Steps: 5307, Current Learning Rate: 0.0008579, \u001b[91mTrain Loss: 0.212\n",
      "\u001b[0m\u001b[1mEpoch: [58/70], Progress: 6.45 %, Steps: 5308, Current Learning Rate: 0.0008578, \u001b[96mTrain Loss: 0.164\n",
      "\u001b[0m\u001b[1mEpoch: [58/70], Progress: 7.53 %, Steps: 5309, Current Learning Rate: 0.0008577, \u001b[91mTrain Loss: 0.192\n",
      "\u001b[0m\u001b[1mEpoch: [58/70], Progress: 8.60 %, Steps: 5310, Current Learning Rate: 0.0008576, \u001b[96mTrain Loss: 0.178\n",
      "\u001b[0m\u001b[1mEpoch: [58/70], Progress: 9.68 %, Steps: 5311, Current Learning Rate: 0.0008575, \u001b[91mTrain Loss: 0.191\n",
      "\u001b[0m\u001b[1mEpoch: [58/70], Progress: 10.75 %, Steps: 5312, Current Learning Rate: 0.0008575, \u001b[96mTrain Loss: 0.175\n",
      "\u001b[0m\u001b[1mEpoch: [58/70], Progress: 11.83 %, Steps: 5313, Current Learning Rate: 0.0008574, \u001b[91mTrain Loss: 0.196\n",
      "\u001b[0m\u001b[1mEpoch: [58/70], Progress: 12.90 %, Steps: 5314, Current Learning Rate: 0.0008573, \u001b[96mTrain Loss: 0.180\n",
      "\u001b[0m\u001b[1mEpoch: [58/70], Progress: 13.98 %, Steps: 5315, Current Learning Rate: 0.0008572, \u001b[91mTrain Loss: 0.195\n",
      "\u001b[0m\u001b[1mEpoch: [58/70], Progress: 15.05 %, Steps: 5316, Current Learning Rate: 0.0008571, \u001b[91mTrain Loss: 0.208\n",
      "\u001b[0m\u001b[1mEpoch: [58/70], Progress: 16.13 %, Steps: 5317, Current Learning Rate: 0.0008570, \u001b[96mTrain Loss: 0.197\n",
      "\u001b[0m\u001b[1mEpoch: [58/70], Progress: 17.20 %, Steps: 5318, Current Learning Rate: 0.0008570, \u001b[96mTrain Loss: 0.190\n",
      "\u001b[0m\u001b[1mEpoch: [58/70], Progress: 18.28 %, Steps: 5319, Current Learning Rate: 0.0008569, \u001b[96mTrain Loss: 0.189\n",
      "\u001b[0m\u001b[1mEpoch: [58/70], Progress: 19.35 %, Steps: 5320, Current Learning Rate: 0.0008568, \u001b[91mTrain Loss: 0.245\n",
      "\u001b[0m\u001b[1mEpoch: [58/70], Progress: 20.43 %, Steps: 5321, Current Learning Rate: 0.0008567, \u001b[96mTrain Loss: 0.202\n",
      "\u001b[0m\u001b[1mEpoch: [58/70], Progress: 21.51 %, Steps: 5322, Current Learning Rate: 0.0008566, \u001b[91mTrain Loss: 0.203\n",
      "\u001b[0m\u001b[1mEpoch: [58/70], Progress: 22.58 %, Steps: 5323, Current Learning Rate: 0.0008566, \u001b[96mTrain Loss: 0.182\n",
      "\u001b[0m\u001b[1mEpoch: [58/70], Progress: 23.66 %, Steps: 5324, Current Learning Rate: 0.0008565, \u001b[91mTrain Loss: 0.223\n",
      "\u001b[0m\u001b[1mEpoch: [58/70], Progress: 24.73 %, Steps: 5325, Current Learning Rate: 0.0008564, \u001b[91mTrain Loss: 0.250\n",
      "\u001b[0m\u001b[1mEpoch: [58/70], Progress: 25.81 %, Steps: 5326, Current Learning Rate: 0.0008563, \u001b[96mTrain Loss: 0.239\n",
      "\u001b[0m\u001b[1mEpoch: [58/70], Progress: 26.88 %, Steps: 5327, Current Learning Rate: 0.0008562, \u001b[96mTrain Loss: 0.185\n",
      "\u001b[0m\u001b[1mEpoch: [58/70], Progress: 27.96 %, Steps: 5328, Current Learning Rate: 0.0008562, \u001b[91mTrain Loss: 0.206\n",
      "\u001b[0m\u001b[1mEpoch: [58/70], Progress: 29.03 %, Steps: 5329, Current Learning Rate: 0.0008561, \u001b[96mTrain Loss: 0.184\n",
      "\u001b[0m\u001b[1mEpoch: [58/70], Progress: 30.11 %, Steps: 5330, Current Learning Rate: 0.0008560, \u001b[91mTrain Loss: 0.224\n",
      "\u001b[0m\u001b[1mEpoch: [58/70], Progress: 31.18 %, Steps: 5331, Current Learning Rate: 0.0008559, \u001b[96mTrain Loss: 0.212\n",
      "\u001b[0m\u001b[1mEpoch: [58/70], Progress: 32.26 %, Steps: 5332, Current Learning Rate: 0.0008558, \u001b[96mTrain Loss: 0.177\n",
      "\u001b[0m\u001b[1mEpoch: [58/70], Progress: 33.33 %, Steps: 5333, Current Learning Rate: 0.0008558, \u001b[91mTrain Loss: 0.281\n",
      "\u001b[0m\u001b[1mEpoch: [58/70], Progress: 34.41 %, Steps: 5334, Current Learning Rate: 0.0008557, \u001b[96mTrain Loss: 0.191\n",
      "\u001b[0m\u001b[1mEpoch: [58/70], Progress: 35.48 %, Steps: 5335, Current Learning Rate: 0.0008556, \u001b[91mTrain Loss: 0.255\n",
      "\u001b[0m\u001b[1mEpoch: [58/70], Progress: 36.56 %, Steps: 5336, Current Learning Rate: 0.0008555, \u001b[96mTrain Loss: 0.222\n",
      "\u001b[0m\u001b[1mEpoch: [58/70], Progress: 37.63 %, Steps: 5337, Current Learning Rate: 0.0008554, \u001b[96mTrain Loss: 0.182\n",
      "\u001b[0m\u001b[1mEpoch: [58/70], Progress: 38.71 %, Steps: 5338, Current Learning Rate: 0.0008554, \u001b[96mTrain Loss: 0.165\n",
      "\u001b[0m\u001b[1mEpoch: [58/70], Progress: 39.78 %, Steps: 5339, Current Learning Rate: 0.0008553, \u001b[91mTrain Loss: 0.204\n",
      "\u001b[0m\u001b[1mEpoch: [58/70], Progress: 40.86 %, Steps: 5340, Current Learning Rate: 0.0008552, \u001b[91mTrain Loss: 0.255\n",
      "\u001b[0m\u001b[1mEpoch: [58/70], Progress: 41.94 %, Steps: 5341, Current Learning Rate: 0.0008551, \u001b[96mTrain Loss: 0.244\n",
      "\u001b[0m\u001b[1mEpoch: [58/70], Progress: 43.01 %, Steps: 5342, Current Learning Rate: 0.0008550, \u001b[96mTrain Loss: 0.205\n",
      "\u001b[0m\u001b[1mEpoch: [58/70], Progress: 44.09 %, Steps: 5343, Current Learning Rate: 0.0008550, \u001b[91mTrain Loss: 0.212\n",
      "\u001b[0m\u001b[1mEpoch: [58/70], Progress: 45.16 %, Steps: 5344, Current Learning Rate: 0.0008549, \u001b[91mTrain Loss: 0.223\n",
      "\u001b[0m\u001b[1mEpoch: [58/70], Progress: 46.24 %, Steps: 5345, Current Learning Rate: 0.0008548, \u001b[96mTrain Loss: 0.200\n",
      "\u001b[0m\u001b[1mEpoch: [58/70], Progress: 47.31 %, Steps: 5346, Current Learning Rate: 0.0008547, \u001b[91mTrain Loss: 0.225\n",
      "\u001b[0m\u001b[1mEpoch: [58/70], Progress: 48.39 %, Steps: 5347, Current Learning Rate: 0.0008546, \u001b[96mTrain Loss: 0.183\n",
      "\u001b[0m\u001b[1mEpoch: [58/70], Progress: 49.46 %, Steps: 5348, Current Learning Rate: 0.0008546, \u001b[91mTrain Loss: 0.271\n",
      "\u001b[0m\u001b[1mEpoch: [58/70], Progress: 50.54 %, Steps: 5349, Current Learning Rate: 0.0008545, \u001b[96mTrain Loss: 0.206\n",
      "\u001b[0m\u001b[1mEpoch: [58/70], Progress: 51.61 %, Steps: 5350, Current Learning Rate: 0.0008544, \u001b[96mTrain Loss: 0.188\n",
      "\u001b[0m\u001b[1mEpoch: [58/70], Progress: 52.69 %, Steps: 5351, Current Learning Rate: 0.0008543, \u001b[91mTrain Loss: 0.264\n",
      "\u001b[0m\u001b[1mEpoch: [58/70], Progress: 53.76 %, Steps: 5352, Current Learning Rate: 0.0008542, \u001b[96mTrain Loss: 0.206\n",
      "\u001b[0m\u001b[1mEpoch: [58/70], Progress: 54.84 %, Steps: 5353, Current Learning Rate: 0.0008542, \u001b[91mTrain Loss: 0.263\n",
      "\u001b[0m\u001b[1mEpoch: [58/70], Progress: 55.91 %, Steps: 5354, Current Learning Rate: 0.0008541, \u001b[91mTrain Loss: 0.307\n",
      "\u001b[0m\u001b[1mEpoch: [58/70], Progress: 56.99 %, Steps: 5355, Current Learning Rate: 0.0008540, \u001b[96mTrain Loss: 0.280\n",
      "\u001b[0m\u001b[1mEpoch: [58/70], Progress: 58.06 %, Steps: 5356, Current Learning Rate: 0.0008539, \u001b[96mTrain Loss: 0.173\n",
      "\u001b[0m\u001b[1mEpoch: [58/70], Progress: 59.14 %, Steps: 5357, Current Learning Rate: 0.0008538, \u001b[91mTrain Loss: 0.260\n",
      "\u001b[0m\u001b[1mEpoch: [58/70], Progress: 60.22 %, Steps: 5358, Current Learning Rate: 0.0008538, \u001b[96mTrain Loss: 0.228\n",
      "\u001b[0m\u001b[1mEpoch: [58/70], Progress: 61.29 %, Steps: 5359, Current Learning Rate: 0.0008537, \u001b[91mTrain Loss: 0.279\n",
      "\u001b[0m\u001b[1mEpoch: [58/70], Progress: 62.37 %, Steps: 5360, Current Learning Rate: 0.0008536, \u001b[91mTrain Loss: 0.286\n",
      "\u001b[0m\u001b[1mEpoch: [58/70], Progress: 63.44 %, Steps: 5361, Current Learning Rate: 0.0008535, \u001b[91mTrain Loss: 0.323\n",
      "\u001b[0m\u001b[1mEpoch: [58/70], Progress: 64.52 %, Steps: 5362, Current Learning Rate: 0.0008534, \u001b[96mTrain Loss: 0.276\n",
      "\u001b[0m\u001b[1mEpoch: [58/70], Progress: 65.59 %, Steps: 5363, Current Learning Rate: 0.0008534, \u001b[91mTrain Loss: 0.285\n",
      "\u001b[0m\u001b[1mEpoch: [58/70], Progress: 66.67 %, Steps: 5364, Current Learning Rate: 0.0008533, \u001b[96mTrain Loss: 0.251\n",
      "\u001b[0m\u001b[1mEpoch: [58/70], Progress: 67.74 %, Steps: 5365, Current Learning Rate: 0.0008532, \u001b[91mTrain Loss: 0.276\n",
      "\u001b[0m\u001b[1mEpoch: [58/70], Progress: 68.82 %, Steps: 5366, Current Learning Rate: 0.0008531, \u001b[96mTrain Loss: 0.222\n",
      "\u001b[0m\u001b[1mEpoch: [58/70], Progress: 69.89 %, Steps: 5367, Current Learning Rate: 0.0008530, \u001b[91mTrain Loss: 0.266\n",
      "\u001b[0m\u001b[1mEpoch: [58/70], Progress: 70.97 %, Steps: 5368, Current Learning Rate: 0.0008530, \u001b[96mTrain Loss: 0.231\n",
      "\u001b[0m\u001b[1mEpoch: [58/70], Progress: 72.04 %, Steps: 5369, Current Learning Rate: 0.0008529, \u001b[96mTrain Loss: 0.229\n",
      "\u001b[0m\u001b[1mEpoch: [58/70], Progress: 73.12 %, Steps: 5370, Current Learning Rate: 0.0008528, \u001b[91mTrain Loss: 0.306\n",
      "\u001b[0m\u001b[1mEpoch: [58/70], Progress: 74.19 %, Steps: 5371, Current Learning Rate: 0.0008527, \u001b[96mTrain Loss: 0.220\n",
      "\u001b[0m\u001b[1mEpoch: [58/70], Progress: 75.27 %, Steps: 5372, Current Learning Rate: 0.0008527, \u001b[91mTrain Loss: 0.277\n",
      "\u001b[0m\u001b[1mEpoch: [58/70], Progress: 76.34 %, Steps: 5373, Current Learning Rate: 0.0008526, \u001b[91mTrain Loss: 0.295\n",
      "\u001b[0m\u001b[1mEpoch: [58/70], Progress: 77.42 %, Steps: 5374, Current Learning Rate: 0.0008525, \u001b[91mTrain Loss: 0.366\n",
      "\u001b[0m\u001b[1mEpoch: [58/70], Progress: 78.49 %, Steps: 5375, Current Learning Rate: 0.0008524, \u001b[96mTrain Loss: 0.233\n",
      "\u001b[0m\u001b[1mEpoch: [58/70], Progress: 79.57 %, Steps: 5376, Current Learning Rate: 0.0008523, \u001b[91mTrain Loss: 0.320\n",
      "\u001b[0m\u001b[1mEpoch: [58/70], Progress: 80.65 %, Steps: 5377, Current Learning Rate: 0.0008523, \u001b[96mTrain Loss: 0.240\n",
      "\u001b[0m\u001b[1mEpoch: [58/70], Progress: 81.72 %, Steps: 5378, Current Learning Rate: 0.0008522, \u001b[96mTrain Loss: 0.220\n",
      "\u001b[0m\u001b[1mEpoch: [58/70], Progress: 82.80 %, Steps: 5379, Current Learning Rate: 0.0008521, \u001b[96mTrain Loss: 0.208\n",
      "\u001b[0m\u001b[1mEpoch: [58/70], Progress: 83.87 %, Steps: 5380, Current Learning Rate: 0.0008520, \u001b[91mTrain Loss: 0.257\n",
      "\u001b[0m\u001b[1mEpoch: [58/70], Progress: 84.95 %, Steps: 5381, Current Learning Rate: 0.0008519, \u001b[96mTrain Loss: 0.243\n",
      "\u001b[0m\u001b[1mEpoch: [58/70], Progress: 86.02 %, Steps: 5382, Current Learning Rate: 0.0008519, \u001b[96mTrain Loss: 0.205\n",
      "\u001b[0m\u001b[1mEpoch: [58/70], Progress: 87.10 %, Steps: 5383, Current Learning Rate: 0.0008518, \u001b[91mTrain Loss: 0.232\n",
      "\u001b[0m\u001b[1mEpoch: [58/70], Progress: 88.17 %, Steps: 5384, Current Learning Rate: 0.0008517, \u001b[91mTrain Loss: 0.293\n",
      "\u001b[0m\u001b[1mEpoch: [58/70], Progress: 89.25 %, Steps: 5385, Current Learning Rate: 0.0008516, \u001b[96mTrain Loss: 0.236\n",
      "\u001b[0m\u001b[1mEpoch: [58/70], Progress: 90.32 %, Steps: 5386, Current Learning Rate: 0.0008515, \u001b[91mTrain Loss: 0.248\n",
      "\u001b[0m\u001b[1mEpoch: [58/70], Progress: 91.40 %, Steps: 5387, Current Learning Rate: 0.0008515, \u001b[96mTrain Loss: 0.222\n",
      "\u001b[0m\u001b[1mEpoch: [58/70], Progress: 92.47 %, Steps: 5388, Current Learning Rate: 0.0008514, \u001b[96mTrain Loss: 0.219\n",
      "\u001b[0m\u001b[1mEpoch: [58/70], Progress: 93.55 %, Steps: 5389, Current Learning Rate: 0.0008513, \u001b[91mTrain Loss: 0.229\n",
      "\u001b[0m\u001b[1mEpoch: [58/70], Progress: 94.62 %, Steps: 5390, Current Learning Rate: 0.0008512, \u001b[96mTrain Loss: 0.219\n",
      "\u001b[0m\u001b[1mEpoch: [58/70], Progress: 95.70 %, Steps: 5391, Current Learning Rate: 0.0008511, \u001b[91mTrain Loss: 0.289\n",
      "\u001b[0m\u001b[1mEpoch: [58/70], Progress: 96.77 %, Steps: 5392, Current Learning Rate: 0.0008511, \u001b[96mTrain Loss: 0.267\n",
      "\u001b[0m\u001b[1mEpoch: [58/70], Progress: 97.85 %, Steps: 5393, Current Learning Rate: 0.0008510, \u001b[96mTrain Loss: 0.267\n",
      "\u001b[0m\u001b[1mEpoch: [58/70], Progress: 98.92 %, Steps: 5394, Current Learning Rate: 0.0008509, \u001b[96mTrain Loss: 0.243\n",
      "\u001b[1m\u001b[93mComputing Validation Loss...\n",
      "\u001b[1mEpoch 58 Completed! Average Train Loss: 0.229, Average Validation Loss: 0.044\n",
      "Best Model saved in best_model/best_model.pt.\n",
      "\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m= Epoch [59/70] ==========================================\n",
      "\u001b[0m\u001b[1mEpoch: [59/70], Progress: 0.00 %, Steps: 5395, Current Learning Rate: 0.0008508, \u001b[91mTrain Loss: 0.155\n",
      "\u001b[0m\u001b[1mEpoch: [59/70], Progress: 1.08 %, Steps: 5396, Current Learning Rate: 0.0008508, \u001b[91mTrain Loss: 0.201\n",
      "\u001b[0m\u001b[1mEpoch: [59/70], Progress: 2.15 %, Steps: 5397, Current Learning Rate: 0.0008507, \u001b[91mTrain Loss: 0.202\n",
      "\u001b[0m\u001b[1mEpoch: [59/70], Progress: 3.23 %, Steps: 5398, Current Learning Rate: 0.0008506, \u001b[96mTrain Loss: 0.183\n",
      "\u001b[0m\u001b[1mEpoch: [59/70], Progress: 4.30 %, Steps: 5399, Current Learning Rate: 0.0008505, \u001b[91mTrain Loss: 0.187\n",
      "\u001b[0m\u001b[1mEpoch: [59/70], Progress: 5.38 %, Steps: 5400, Current Learning Rate: 0.0008504, \u001b[96mTrain Loss: 0.176\n",
      "\u001b[0m\u001b[1mEpoch: [59/70], Progress: 6.45 %, Steps: 5401, Current Learning Rate: 0.0008504, \u001b[91mTrain Loss: 0.209\n",
      "\u001b[0m\u001b[1mEpoch: [59/70], Progress: 7.53 %, Steps: 5402, Current Learning Rate: 0.0008503, \u001b[96mTrain Loss: 0.157\n",
      "\u001b[0m\u001b[1mEpoch: [59/70], Progress: 8.60 %, Steps: 5403, Current Learning Rate: 0.0008502, \u001b[91mTrain Loss: 0.189\n",
      "\u001b[0m\u001b[1mEpoch: [59/70], Progress: 9.68 %, Steps: 5404, Current Learning Rate: 0.0008501, \u001b[96mTrain Loss: 0.152\n",
      "\u001b[0m\u001b[1mEpoch: [59/70], Progress: 10.75 %, Steps: 5405, Current Learning Rate: 0.0008500, \u001b[91mTrain Loss: 0.161\n",
      "\u001b[0m\u001b[1mEpoch: [59/70], Progress: 11.83 %, Steps: 5406, Current Learning Rate: 0.0008500, \u001b[96mTrain Loss: 0.123\n",
      "\u001b[0m\u001b[1mEpoch: [59/70], Progress: 12.90 %, Steps: 5407, Current Learning Rate: 0.0008499, \u001b[91mTrain Loss: 0.170\n",
      "\u001b[0m\u001b[1mEpoch: [59/70], Progress: 13.98 %, Steps: 5408, Current Learning Rate: 0.0008498, \u001b[96mTrain Loss: 0.158\n",
      "\u001b[0m\u001b[1mEpoch: [59/70], Progress: 15.05 %, Steps: 5409, Current Learning Rate: 0.0008497, \u001b[91mTrain Loss: 0.198\n",
      "\u001b[0m\u001b[1mEpoch: [59/70], Progress: 16.13 %, Steps: 5410, Current Learning Rate: 0.0008497, \u001b[96mTrain Loss: 0.155\n",
      "\u001b[0m\u001b[1mEpoch: [59/70], Progress: 17.20 %, Steps: 5411, Current Learning Rate: 0.0008496, \u001b[91mTrain Loss: 0.162\n",
      "\u001b[0m\u001b[1mEpoch: [59/70], Progress: 18.28 %, Steps: 5412, Current Learning Rate: 0.0008495, \u001b[91mTrain Loss: 0.176\n",
      "\u001b[0m\u001b[1mEpoch: [59/70], Progress: 19.35 %, Steps: 5413, Current Learning Rate: 0.0008494, \u001b[91mTrain Loss: 0.203\n",
      "\u001b[0m\u001b[1mEpoch: [59/70], Progress: 20.43 %, Steps: 5414, Current Learning Rate: 0.0008493, \u001b[91mTrain Loss: 0.210\n",
      "\u001b[0m\u001b[1mEpoch: [59/70], Progress: 21.51 %, Steps: 5415, Current Learning Rate: 0.0008493, \u001b[96mTrain Loss: 0.206\n",
      "\u001b[0m\u001b[1mEpoch: [59/70], Progress: 22.58 %, Steps: 5416, Current Learning Rate: 0.0008492, \u001b[91mTrain Loss: 0.237\n",
      "\u001b[0m\u001b[1mEpoch: [59/70], Progress: 23.66 %, Steps: 5417, Current Learning Rate: 0.0008491, \u001b[96mTrain Loss: 0.170\n",
      "\u001b[0m\u001b[1mEpoch: [59/70], Progress: 24.73 %, Steps: 5418, Current Learning Rate: 0.0008490, \u001b[91mTrain Loss: 0.200\n",
      "\u001b[0m\u001b[1mEpoch: [59/70], Progress: 25.81 %, Steps: 5419, Current Learning Rate: 0.0008489, \u001b[96mTrain Loss: 0.183\n",
      "\u001b[0m\u001b[1mEpoch: [59/70], Progress: 26.88 %, Steps: 5420, Current Learning Rate: 0.0008489, \u001b[96mTrain Loss: 0.158\n",
      "\u001b[0m\u001b[1mEpoch: [59/70], Progress: 27.96 %, Steps: 5421, Current Learning Rate: 0.0008488, \u001b[91mTrain Loss: 0.223\n",
      "\u001b[0m\u001b[1mEpoch: [59/70], Progress: 29.03 %, Steps: 5422, Current Learning Rate: 0.0008487, \u001b[96mTrain Loss: 0.219\n",
      "\u001b[0m\u001b[1mEpoch: [59/70], Progress: 30.11 %, Steps: 5423, Current Learning Rate: 0.0008486, \u001b[96mTrain Loss: 0.211\n",
      "\u001b[0m\u001b[1mEpoch: [59/70], Progress: 31.18 %, Steps: 5424, Current Learning Rate: 0.0008486, \u001b[91mTrain Loss: 0.247\n",
      "\u001b[0m\u001b[1mEpoch: [59/70], Progress: 32.26 %, Steps: 5425, Current Learning Rate: 0.0008485, \u001b[96mTrain Loss: 0.247\n",
      "\u001b[0m\u001b[1mEpoch: [59/70], Progress: 33.33 %, Steps: 5426, Current Learning Rate: 0.0008484, \u001b[96mTrain Loss: 0.235\n",
      "\u001b[0m\u001b[1mEpoch: [59/70], Progress: 34.41 %, Steps: 5427, Current Learning Rate: 0.0008483, \u001b[96mTrain Loss: 0.208\n",
      "\u001b[0m\u001b[1mEpoch: [59/70], Progress: 35.48 %, Steps: 5428, Current Learning Rate: 0.0008482, \u001b[91mTrain Loss: 0.247\n",
      "\u001b[0m\u001b[1mEpoch: [59/70], Progress: 36.56 %, Steps: 5429, Current Learning Rate: 0.0008482, \u001b[96mTrain Loss: 0.238\n",
      "\u001b[0m\u001b[1mEpoch: [59/70], Progress: 37.63 %, Steps: 5430, Current Learning Rate: 0.0008481, \u001b[96mTrain Loss: 0.224\n",
      "\u001b[0m\u001b[1mEpoch: [59/70], Progress: 38.71 %, Steps: 5431, Current Learning Rate: 0.0008480, \u001b[96mTrain Loss: 0.212\n",
      "\u001b[0m\u001b[1mEpoch: [59/70], Progress: 39.78 %, Steps: 5432, Current Learning Rate: 0.0008479, \u001b[96mTrain Loss: 0.180\n",
      "\u001b[0m\u001b[1mEpoch: [59/70], Progress: 40.86 %, Steps: 5433, Current Learning Rate: 0.0008479, \u001b[91mTrain Loss: 0.204\n",
      "\u001b[0m\u001b[1mEpoch: [59/70], Progress: 41.94 %, Steps: 5434, Current Learning Rate: 0.0008478, \u001b[91mTrain Loss: 0.223\n",
      "\u001b[0m\u001b[1mEpoch: [59/70], Progress: 43.01 %, Steps: 5435, Current Learning Rate: 0.0008477, \u001b[91mTrain Loss: 0.298\n",
      "\u001b[0m\u001b[1mEpoch: [59/70], Progress: 44.09 %, Steps: 5436, Current Learning Rate: 0.0008476, \u001b[96mTrain Loss: 0.204\n",
      "\u001b[0m\u001b[1mEpoch: [59/70], Progress: 45.16 %, Steps: 5437, Current Learning Rate: 0.0008475, \u001b[96mTrain Loss: 0.183\n",
      "\u001b[0m\u001b[1mEpoch: [59/70], Progress: 46.24 %, Steps: 5438, Current Learning Rate: 0.0008475, \u001b[91mTrain Loss: 0.203\n",
      "\u001b[0m\u001b[1mEpoch: [59/70], Progress: 47.31 %, Steps: 5439, Current Learning Rate: 0.0008474, \u001b[96mTrain Loss: 0.187\n",
      "\u001b[0m\u001b[1mEpoch: [59/70], Progress: 48.39 %, Steps: 5440, Current Learning Rate: 0.0008473, \u001b[91mTrain Loss: 0.196\n",
      "\u001b[0m\u001b[1mEpoch: [59/70], Progress: 49.46 %, Steps: 5441, Current Learning Rate: 0.0008472, \u001b[91mTrain Loss: 0.284\n",
      "\u001b[0m\u001b[1mEpoch: [59/70], Progress: 50.54 %, Steps: 5442, Current Learning Rate: 0.0008472, \u001b[96mTrain Loss: 0.230\n",
      "\u001b[0m\u001b[1mEpoch: [59/70], Progress: 51.61 %, Steps: 5443, Current Learning Rate: 0.0008471, \u001b[91mTrain Loss: 0.255\n",
      "\u001b[0m\u001b[1mEpoch: [59/70], Progress: 52.69 %, Steps: 5444, Current Learning Rate: 0.0008470, \u001b[96mTrain Loss: 0.225\n",
      "\u001b[0m\u001b[1mEpoch: [59/70], Progress: 53.76 %, Steps: 5445, Current Learning Rate: 0.0008469, \u001b[96mTrain Loss: 0.184\n",
      "\u001b[0m\u001b[1mEpoch: [59/70], Progress: 54.84 %, Steps: 5446, Current Learning Rate: 0.0008468, \u001b[91mTrain Loss: 0.191\n",
      "\u001b[0m\u001b[1mEpoch: [59/70], Progress: 55.91 %, Steps: 5447, Current Learning Rate: 0.0008468, \u001b[96mTrain Loss: 0.161\n",
      "\u001b[0m\u001b[1mEpoch: [59/70], Progress: 56.99 %, Steps: 5448, Current Learning Rate: 0.0008467, \u001b[91mTrain Loss: 0.312\n",
      "\u001b[0m\u001b[1mEpoch: [59/70], Progress: 58.06 %, Steps: 5449, Current Learning Rate: 0.0008466, \u001b[96mTrain Loss: 0.186\n",
      "\u001b[0m\u001b[1mEpoch: [59/70], Progress: 59.14 %, Steps: 5450, Current Learning Rate: 0.0008465, \u001b[91mTrain Loss: 0.238\n",
      "\u001b[0m\u001b[1mEpoch: [59/70], Progress: 60.22 %, Steps: 5451, Current Learning Rate: 0.0008465, \u001b[96mTrain Loss: 0.188\n",
      "\u001b[0m\u001b[1mEpoch: [59/70], Progress: 61.29 %, Steps: 5452, Current Learning Rate: 0.0008464, \u001b[91mTrain Loss: 0.225\n",
      "\u001b[0m\u001b[1mEpoch: [59/70], Progress: 62.37 %, Steps: 5453, Current Learning Rate: 0.0008463, \u001b[96mTrain Loss: 0.195\n",
      "\u001b[0m\u001b[1mEpoch: [59/70], Progress: 63.44 %, Steps: 5454, Current Learning Rate: 0.0008462, \u001b[91mTrain Loss: 0.219\n",
      "\u001b[0m\u001b[1mEpoch: [59/70], Progress: 64.52 %, Steps: 5455, Current Learning Rate: 0.0008461, \u001b[91mTrain Loss: 0.230\n",
      "\u001b[0m\u001b[1mEpoch: [59/70], Progress: 65.59 %, Steps: 5456, Current Learning Rate: 0.0008461, \u001b[96mTrain Loss: 0.193\n",
      "\u001b[0m\u001b[1mEpoch: [59/70], Progress: 66.67 %, Steps: 5457, Current Learning Rate: 0.0008460, \u001b[91mTrain Loss: 0.229\n",
      "\u001b[0m\u001b[1mEpoch: [59/70], Progress: 67.74 %, Steps: 5458, Current Learning Rate: 0.0008459, \u001b[96mTrain Loss: 0.223\n",
      "\u001b[0m\u001b[1mEpoch: [59/70], Progress: 68.82 %, Steps: 5459, Current Learning Rate: 0.0008458, \u001b[96mTrain Loss: 0.209\n",
      "\u001b[0m\u001b[1mEpoch: [59/70], Progress: 69.89 %, Steps: 5460, Current Learning Rate: 0.0008458, \u001b[91mTrain Loss: 0.273\n",
      "\u001b[0m\u001b[1mEpoch: [59/70], Progress: 70.97 %, Steps: 5461, Current Learning Rate: 0.0008457, \u001b[96mTrain Loss: 0.187\n",
      "\u001b[0m\u001b[1mEpoch: [59/70], Progress: 72.04 %, Steps: 5462, Current Learning Rate: 0.0008456, \u001b[91mTrain Loss: 0.262\n",
      "\u001b[0m\u001b[1mEpoch: [59/70], Progress: 73.12 %, Steps: 5463, Current Learning Rate: 0.0008455, \u001b[96mTrain Loss: 0.232\n",
      "\u001b[0m\u001b[1mEpoch: [59/70], Progress: 74.19 %, Steps: 5464, Current Learning Rate: 0.0008454, \u001b[91mTrain Loss: 0.249\n",
      "\u001b[0m\u001b[1mEpoch: [59/70], Progress: 75.27 %, Steps: 5465, Current Learning Rate: 0.0008454, \u001b[96mTrain Loss: 0.208\n",
      "\u001b[0m\u001b[1mEpoch: [59/70], Progress: 76.34 %, Steps: 5466, Current Learning Rate: 0.0008453, \u001b[91mTrain Loss: 0.284\n",
      "\u001b[0m\u001b[1mEpoch: [59/70], Progress: 77.42 %, Steps: 5467, Current Learning Rate: 0.0008452, \u001b[96mTrain Loss: 0.230\n",
      "\u001b[0m\u001b[1mEpoch: [59/70], Progress: 78.49 %, Steps: 5468, Current Learning Rate: 0.0008451, \u001b[91mTrain Loss: 0.244\n",
      "\u001b[0m\u001b[1mEpoch: [59/70], Progress: 79.57 %, Steps: 5469, Current Learning Rate: 0.0008451, \u001b[96mTrain Loss: 0.207\n",
      "\u001b[0m\u001b[1mEpoch: [59/70], Progress: 80.65 %, Steps: 5470, Current Learning Rate: 0.0008450, \u001b[91mTrain Loss: 0.244\n",
      "\u001b[0m\u001b[1mEpoch: [59/70], Progress: 81.72 %, Steps: 5471, Current Learning Rate: 0.0008449, \u001b[96mTrain Loss: 0.190\n",
      "\u001b[0m\u001b[1mEpoch: [59/70], Progress: 82.80 %, Steps: 5472, Current Learning Rate: 0.0008448, \u001b[91mTrain Loss: 0.248\n",
      "\u001b[0m\u001b[1mEpoch: [59/70], Progress: 83.87 %, Steps: 5473, Current Learning Rate: 0.0008447, \u001b[96mTrain Loss: 0.196\n",
      "\u001b[0m\u001b[1mEpoch: [59/70], Progress: 84.95 %, Steps: 5474, Current Learning Rate: 0.0008447, \u001b[96mTrain Loss: 0.162\n",
      "\u001b[0m\u001b[1mEpoch: [59/70], Progress: 86.02 %, Steps: 5475, Current Learning Rate: 0.0008446, \u001b[91mTrain Loss: 0.204\n",
      "\u001b[0m\u001b[1mEpoch: [59/70], Progress: 87.10 %, Steps: 5476, Current Learning Rate: 0.0008445, \u001b[91mTrain Loss: 0.269\n",
      "\u001b[0m\u001b[1mEpoch: [59/70], Progress: 88.17 %, Steps: 5477, Current Learning Rate: 0.0008444, \u001b[91mTrain Loss: 0.277\n",
      "\u001b[0m\u001b[1mEpoch: [59/70], Progress: 89.25 %, Steps: 5478, Current Learning Rate: 0.0008444, \u001b[96mTrain Loss: 0.219\n",
      "\u001b[0m\u001b[1mEpoch: [59/70], Progress: 90.32 %, Steps: 5479, Current Learning Rate: 0.0008443, \u001b[96mTrain Loss: 0.194\n",
      "\u001b[0m\u001b[1mEpoch: [59/70], Progress: 91.40 %, Steps: 5480, Current Learning Rate: 0.0008442, \u001b[91mTrain Loss: 0.332\n",
      "\u001b[0m\u001b[1mEpoch: [59/70], Progress: 92.47 %, Steps: 5481, Current Learning Rate: 0.0008441, \u001b[96mTrain Loss: 0.242\n",
      "\u001b[0m\u001b[1mEpoch: [59/70], Progress: 93.55 %, Steps: 5482, Current Learning Rate: 0.0008441, \u001b[96mTrain Loss: 0.189\n",
      "\u001b[0m\u001b[1mEpoch: [59/70], Progress: 94.62 %, Steps: 5483, Current Learning Rate: 0.0008440, \u001b[91mTrain Loss: 0.233\n",
      "\u001b[0m\u001b[1mEpoch: [59/70], Progress: 95.70 %, Steps: 5484, Current Learning Rate: 0.0008439, \u001b[91mTrain Loss: 0.256\n",
      "\u001b[0m\u001b[1mEpoch: [59/70], Progress: 96.77 %, Steps: 5485, Current Learning Rate: 0.0008438, \u001b[96mTrain Loss: 0.217\n",
      "\u001b[0m\u001b[1mEpoch: [59/70], Progress: 97.85 %, Steps: 5486, Current Learning Rate: 0.0008437, \u001b[91mTrain Loss: 0.227\n",
      "\u001b[0m\u001b[1mEpoch: [59/70], Progress: 98.92 %, Steps: 5487, Current Learning Rate: 0.0008437, \u001b[96mTrain Loss: 0.129\n",
      "\u001b[1m\u001b[93mComputing Validation Loss...\n",
      "\u001b[1mEpoch 59 Completed! Average Train Loss: 0.211, Average Validation Loss: 0.041\n",
      "Best Model saved in best_model/best_model.pt.\n",
      "\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m= Epoch [60/70] ==========================================\n",
      "\u001b[0m\u001b[1mEpoch: [60/70], Progress: 0.00 %, Steps: 5488, Current Learning Rate: 0.0008436, \u001b[91mTrain Loss: 0.166\n",
      "\u001b[0m\u001b[1mEpoch: [60/70], Progress: 1.08 %, Steps: 5489, Current Learning Rate: 0.0008435, \u001b[96mTrain Loss: 0.154\n",
      "\u001b[0m\u001b[1mEpoch: [60/70], Progress: 2.15 %, Steps: 5490, Current Learning Rate: 0.0008434, \u001b[96mTrain Loss: 0.132\n",
      "\u001b[0m\u001b[1mEpoch: [60/70], Progress: 3.23 %, Steps: 5491, Current Learning Rate: 0.0008434, \u001b[91mTrain Loss: 0.142\n",
      "\u001b[0m\u001b[1mEpoch: [60/70], Progress: 4.30 %, Steps: 5492, Current Learning Rate: 0.0008433, \u001b[91mTrain Loss: 0.169\n",
      "\u001b[0m\u001b[1mEpoch: [60/70], Progress: 5.38 %, Steps: 5493, Current Learning Rate: 0.0008432, \u001b[91mTrain Loss: 0.208\n",
      "\u001b[0m\u001b[1mEpoch: [60/70], Progress: 6.45 %, Steps: 5494, Current Learning Rate: 0.0008431, \u001b[96mTrain Loss: 0.120\n",
      "\u001b[0m\u001b[1mEpoch: [60/70], Progress: 7.53 %, Steps: 5495, Current Learning Rate: 0.0008431, \u001b[91mTrain Loss: 0.173\n",
      "\u001b[0m\u001b[1mEpoch: [60/70], Progress: 8.60 %, Steps: 5496, Current Learning Rate: 0.0008430, \u001b[96mTrain Loss: 0.140\n",
      "\u001b[0m\u001b[1mEpoch: [60/70], Progress: 9.68 %, Steps: 5497, Current Learning Rate: 0.0008429, \u001b[91mTrain Loss: 0.147\n",
      "\u001b[0m\u001b[1mEpoch: [60/70], Progress: 10.75 %, Steps: 5498, Current Learning Rate: 0.0008428, \u001b[91mTrain Loss: 0.167\n",
      "\u001b[0m\u001b[1mEpoch: [60/70], Progress: 11.83 %, Steps: 5499, Current Learning Rate: 0.0008427, \u001b[96mTrain Loss: 0.157\n",
      "\u001b[0m\u001b[1mEpoch: [60/70], Progress: 12.90 %, Steps: 5500, Current Learning Rate: 0.0008427, \u001b[91mTrain Loss: 0.181\n",
      "\u001b[0m\u001b[1mEpoch: [60/70], Progress: 13.98 %, Steps: 5501, Current Learning Rate: 0.0008426, \u001b[96mTrain Loss: 0.145\n",
      "\u001b[0m\u001b[1mEpoch: [60/70], Progress: 15.05 %, Steps: 5502, Current Learning Rate: 0.0008425, \u001b[91mTrain Loss: 0.203\n",
      "\u001b[0m\u001b[1mEpoch: [60/70], Progress: 16.13 %, Steps: 5503, Current Learning Rate: 0.0008424, \u001b[96mTrain Loss: 0.192\n",
      "\u001b[0m\u001b[1mEpoch: [60/70], Progress: 17.20 %, Steps: 5504, Current Learning Rate: 0.0008424, \u001b[96mTrain Loss: 0.178\n",
      "\u001b[0m\u001b[1mEpoch: [60/70], Progress: 18.28 %, Steps: 5505, Current Learning Rate: 0.0008423, \u001b[91mTrain Loss: 0.222\n",
      "\u001b[0m\u001b[1mEpoch: [60/70], Progress: 19.35 %, Steps: 5506, Current Learning Rate: 0.0008422, \u001b[96mTrain Loss: 0.211\n",
      "\u001b[0m\u001b[1mEpoch: [60/70], Progress: 20.43 %, Steps: 5507, Current Learning Rate: 0.0008421, \u001b[96mTrain Loss: 0.204\n",
      "\u001b[0m\u001b[1mEpoch: [60/70], Progress: 21.51 %, Steps: 5508, Current Learning Rate: 0.0008421, \u001b[96mTrain Loss: 0.194\n",
      "\u001b[0m\u001b[1mEpoch: [60/70], Progress: 22.58 %, Steps: 5509, Current Learning Rate: 0.0008420, \u001b[96mTrain Loss: 0.163\n",
      "\u001b[0m\u001b[1mEpoch: [60/70], Progress: 23.66 %, Steps: 5510, Current Learning Rate: 0.0008419, \u001b[91mTrain Loss: 0.194\n",
      "\u001b[0m\u001b[1mEpoch: [60/70], Progress: 24.73 %, Steps: 5511, Current Learning Rate: 0.0008418, \u001b[91mTrain Loss: 0.229\n",
      "\u001b[0m\u001b[1mEpoch: [60/70], Progress: 25.81 %, Steps: 5512, Current Learning Rate: 0.0008418, \u001b[96mTrain Loss: 0.218\n",
      "\u001b[0m\u001b[1mEpoch: [60/70], Progress: 26.88 %, Steps: 5513, Current Learning Rate: 0.0008417, \u001b[91mTrain Loss: 0.262\n",
      "\u001b[0m\u001b[1mEpoch: [60/70], Progress: 27.96 %, Steps: 5514, Current Learning Rate: 0.0008416, \u001b[96mTrain Loss: 0.154\n",
      "\u001b[0m\u001b[1mEpoch: [60/70], Progress: 29.03 %, Steps: 5515, Current Learning Rate: 0.0008415, \u001b[91mTrain Loss: 0.201\n",
      "\u001b[0m\u001b[1mEpoch: [60/70], Progress: 30.11 %, Steps: 5516, Current Learning Rate: 0.0008415, \u001b[96mTrain Loss: 0.181\n",
      "\u001b[0m\u001b[1mEpoch: [60/70], Progress: 31.18 %, Steps: 5517, Current Learning Rate: 0.0008414, \u001b[91mTrain Loss: 0.207\n",
      "\u001b[0m\u001b[1mEpoch: [60/70], Progress: 32.26 %, Steps: 5518, Current Learning Rate: 0.0008413, \u001b[96mTrain Loss: 0.184\n",
      "\u001b[0m\u001b[1mEpoch: [60/70], Progress: 33.33 %, Steps: 5519, Current Learning Rate: 0.0008412, \u001b[96mTrain Loss: 0.163\n",
      "\u001b[0m\u001b[1mEpoch: [60/70], Progress: 34.41 %, Steps: 5520, Current Learning Rate: 0.0008411, \u001b[91mTrain Loss: 0.169\n",
      "\u001b[0m\u001b[1mEpoch: [60/70], Progress: 35.48 %, Steps: 5521, Current Learning Rate: 0.0008411, \u001b[96mTrain Loss: 0.163\n",
      "\u001b[0m\u001b[1mEpoch: [60/70], Progress: 36.56 %, Steps: 5522, Current Learning Rate: 0.0008410, \u001b[91mTrain Loss: 0.217\n",
      "\u001b[0m\u001b[1mEpoch: [60/70], Progress: 37.63 %, Steps: 5523, Current Learning Rate: 0.0008409, \u001b[91mTrain Loss: 0.232\n",
      "\u001b[0m\u001b[1mEpoch: [60/70], Progress: 38.71 %, Steps: 5524, Current Learning Rate: 0.0008408, \u001b[96mTrain Loss: 0.138\n",
      "\u001b[0m\u001b[1mEpoch: [60/70], Progress: 39.78 %, Steps: 5525, Current Learning Rate: 0.0008408, \u001b[91mTrain Loss: 0.222\n",
      "\u001b[0m\u001b[1mEpoch: [60/70], Progress: 40.86 %, Steps: 5526, Current Learning Rate: 0.0008407, \u001b[91mTrain Loss: 0.259\n",
      "\u001b[0m\u001b[1mEpoch: [60/70], Progress: 41.94 %, Steps: 5527, Current Learning Rate: 0.0008406, \u001b[96mTrain Loss: 0.218\n",
      "\u001b[0m\u001b[1mEpoch: [60/70], Progress: 43.01 %, Steps: 5528, Current Learning Rate: 0.0008405, \u001b[96mTrain Loss: 0.194\n",
      "\u001b[0m\u001b[1mEpoch: [60/70], Progress: 44.09 %, Steps: 5529, Current Learning Rate: 0.0008405, \u001b[91mTrain Loss: 0.254\n",
      "\u001b[0m\u001b[1mEpoch: [60/70], Progress: 45.16 %, Steps: 5530, Current Learning Rate: 0.0008404, \u001b[96mTrain Loss: 0.162\n",
      "\u001b[0m\u001b[1mEpoch: [60/70], Progress: 46.24 %, Steps: 5531, Current Learning Rate: 0.0008403, \u001b[91mTrain Loss: 0.229\n",
      "\u001b[0m\u001b[1mEpoch: [60/70], Progress: 47.31 %, Steps: 5532, Current Learning Rate: 0.0008402, \u001b[96mTrain Loss: 0.176\n",
      "\u001b[0m\u001b[1mEpoch: [60/70], Progress: 48.39 %, Steps: 5533, Current Learning Rate: 0.0008402, \u001b[91mTrain Loss: 0.184\n",
      "\u001b[0m\u001b[1mEpoch: [60/70], Progress: 49.46 %, Steps: 5534, Current Learning Rate: 0.0008401, \u001b[96mTrain Loss: 0.181\n",
      "\u001b[0m\u001b[1mEpoch: [60/70], Progress: 50.54 %, Steps: 5535, Current Learning Rate: 0.0008400, \u001b[91mTrain Loss: 0.188\n",
      "\u001b[0m\u001b[1mEpoch: [60/70], Progress: 51.61 %, Steps: 5536, Current Learning Rate: 0.0008399, \u001b[96mTrain Loss: 0.169\n",
      "\u001b[0m\u001b[1mEpoch: [60/70], Progress: 52.69 %, Steps: 5537, Current Learning Rate: 0.0008399, \u001b[91mTrain Loss: 0.242\n",
      "\u001b[0m\u001b[1mEpoch: [60/70], Progress: 53.76 %, Steps: 5538, Current Learning Rate: 0.0008398, \u001b[96mTrain Loss: 0.236\n",
      "\u001b[0m\u001b[1mEpoch: [60/70], Progress: 54.84 %, Steps: 5539, Current Learning Rate: 0.0008397, \u001b[96mTrain Loss: 0.221\n",
      "\u001b[0m\u001b[1mEpoch: [60/70], Progress: 55.91 %, Steps: 5540, Current Learning Rate: 0.0008396, \u001b[91mTrain Loss: 0.264\n",
      "\u001b[0m\u001b[1mEpoch: [60/70], Progress: 56.99 %, Steps: 5541, Current Learning Rate: 0.0008396, \u001b[96mTrain Loss: 0.149\n",
      "\u001b[0m\u001b[1mEpoch: [60/70], Progress: 58.06 %, Steps: 5542, Current Learning Rate: 0.0008395, \u001b[91mTrain Loss: 0.192\n",
      "\u001b[0m\u001b[1mEpoch: [60/70], Progress: 59.14 %, Steps: 5543, Current Learning Rate: 0.0008394, \u001b[91mTrain Loss: 0.228\n",
      "\u001b[0m\u001b[1mEpoch: [60/70], Progress: 60.22 %, Steps: 5544, Current Learning Rate: 0.0008393, \u001b[96mTrain Loss: 0.223\n",
      "\u001b[0m\u001b[1mEpoch: [60/70], Progress: 61.29 %, Steps: 5545, Current Learning Rate: 0.0008392, \u001b[96mTrain Loss: 0.173\n",
      "\u001b[0m\u001b[1mEpoch: [60/70], Progress: 62.37 %, Steps: 5546, Current Learning Rate: 0.0008392, \u001b[91mTrain Loss: 0.187\n",
      "\u001b[0m\u001b[1mEpoch: [60/70], Progress: 63.44 %, Steps: 5547, Current Learning Rate: 0.0008391, \u001b[91mTrain Loss: 0.245\n",
      "\u001b[0m\u001b[1mEpoch: [60/70], Progress: 64.52 %, Steps: 5548, Current Learning Rate: 0.0008390, \u001b[96mTrain Loss: 0.175\n",
      "\u001b[0m\u001b[1mEpoch: [60/70], Progress: 65.59 %, Steps: 5549, Current Learning Rate: 0.0008389, \u001b[91mTrain Loss: 0.198\n",
      "\u001b[0m\u001b[1mEpoch: [60/70], Progress: 66.67 %, Steps: 5550, Current Learning Rate: 0.0008389, \u001b[96mTrain Loss: 0.182\n",
      "\u001b[0m\u001b[1mEpoch: [60/70], Progress: 67.74 %, Steps: 5551, Current Learning Rate: 0.0008388, \u001b[91mTrain Loss: 0.199\n",
      "\u001b[0m\u001b[1mEpoch: [60/70], Progress: 68.82 %, Steps: 5552, Current Learning Rate: 0.0008387, \u001b[91mTrain Loss: 0.246\n",
      "\u001b[0m\u001b[1mEpoch: [60/70], Progress: 69.89 %, Steps: 5553, Current Learning Rate: 0.0008386, \u001b[96mTrain Loss: 0.175\n",
      "\u001b[0m\u001b[1mEpoch: [60/70], Progress: 70.97 %, Steps: 5554, Current Learning Rate: 0.0008386, \u001b[91mTrain Loss: 0.228\n",
      "\u001b[0m\u001b[1mEpoch: [60/70], Progress: 72.04 %, Steps: 5555, Current Learning Rate: 0.0008385, \u001b[96mTrain Loss: 0.203\n",
      "\u001b[0m\u001b[1mEpoch: [60/70], Progress: 73.12 %, Steps: 5556, Current Learning Rate: 0.0008384, \u001b[96mTrain Loss: 0.158\n",
      "\u001b[0m\u001b[1mEpoch: [60/70], Progress: 74.19 %, Steps: 5557, Current Learning Rate: 0.0008383, \u001b[91mTrain Loss: 0.260\n",
      "\u001b[0m\u001b[1mEpoch: [60/70], Progress: 75.27 %, Steps: 5558, Current Learning Rate: 0.0008383, \u001b[96mTrain Loss: 0.196\n",
      "\u001b[0m\u001b[1mEpoch: [60/70], Progress: 76.34 %, Steps: 5559, Current Learning Rate: 0.0008382, \u001b[91mTrain Loss: 0.251\n",
      "\u001b[0m\u001b[1mEpoch: [60/70], Progress: 77.42 %, Steps: 5560, Current Learning Rate: 0.0008381, \u001b[96mTrain Loss: 0.243\n",
      "\u001b[0m\u001b[1mEpoch: [60/70], Progress: 78.49 %, Steps: 5561, Current Learning Rate: 0.0008380, \u001b[96mTrain Loss: 0.208\n",
      "\u001b[0m\u001b[1mEpoch: [60/70], Progress: 79.57 %, Steps: 5562, Current Learning Rate: 0.0008380, \u001b[91mTrain Loss: 0.221\n",
      "\u001b[0m\u001b[1mEpoch: [60/70], Progress: 80.65 %, Steps: 5563, Current Learning Rate: 0.0008379, \u001b[91mTrain Loss: 0.307\n",
      "\u001b[0m\u001b[1mEpoch: [60/70], Progress: 81.72 %, Steps: 5564, Current Learning Rate: 0.0008378, \u001b[96mTrain Loss: 0.228\n",
      "\u001b[0m\u001b[1mEpoch: [60/70], Progress: 82.80 %, Steps: 5565, Current Learning Rate: 0.0008377, \u001b[96mTrain Loss: 0.219\n",
      "\u001b[0m\u001b[1mEpoch: [60/70], Progress: 83.87 %, Steps: 5566, Current Learning Rate: 0.0008377, \u001b[96mTrain Loss: 0.201\n",
      "\u001b[0m\u001b[1mEpoch: [60/70], Progress: 84.95 %, Steps: 5567, Current Learning Rate: 0.0008376, \u001b[96mTrain Loss: 0.176\n",
      "\u001b[0m\u001b[1mEpoch: [60/70], Progress: 86.02 %, Steps: 5568, Current Learning Rate: 0.0008375, \u001b[91mTrain Loss: 0.207\n",
      "\u001b[0m\u001b[1mEpoch: [60/70], Progress: 87.10 %, Steps: 5569, Current Learning Rate: 0.0008374, \u001b[96mTrain Loss: 0.168\n",
      "\u001b[0m\u001b[1mEpoch: [60/70], Progress: 88.17 %, Steps: 5570, Current Learning Rate: 0.0008374, \u001b[96mTrain Loss: 0.116\n",
      "\u001b[0m\u001b[1mEpoch: [60/70], Progress: 89.25 %, Steps: 5571, Current Learning Rate: 0.0008373, \u001b[91mTrain Loss: 0.211\n",
      "\u001b[0m\u001b[1mEpoch: [60/70], Progress: 90.32 %, Steps: 5572, Current Learning Rate: 0.0008372, \u001b[96mTrain Loss: 0.197\n",
      "\u001b[0m\u001b[1mEpoch: [60/70], Progress: 91.40 %, Steps: 5573, Current Learning Rate: 0.0008371, \u001b[91mTrain Loss: 0.219\n",
      "\u001b[0m\u001b[1mEpoch: [60/70], Progress: 92.47 %, Steps: 5574, Current Learning Rate: 0.0008371, \u001b[91mTrain Loss: 0.255\n",
      "\u001b[0m\u001b[1mEpoch: [60/70], Progress: 93.55 %, Steps: 5575, Current Learning Rate: 0.0008370, \u001b[91mTrain Loss: 0.284\n",
      "\u001b[0m\u001b[1mEpoch: [60/70], Progress: 94.62 %, Steps: 5576, Current Learning Rate: 0.0008369, \u001b[96mTrain Loss: 0.175\n",
      "\u001b[0m\u001b[1mEpoch: [60/70], Progress: 95.70 %, Steps: 5577, Current Learning Rate: 0.0008368, \u001b[91mTrain Loss: 0.201\n",
      "\u001b[0m\u001b[1mEpoch: [60/70], Progress: 96.77 %, Steps: 5578, Current Learning Rate: 0.0008368, \u001b[91mTrain Loss: 0.250\n",
      "\u001b[0m\u001b[1mEpoch: [60/70], Progress: 97.85 %, Steps: 5579, Current Learning Rate: 0.0008367, \u001b[96mTrain Loss: 0.201\n",
      "\u001b[0m\u001b[1mEpoch: [60/70], Progress: 98.92 %, Steps: 5580, Current Learning Rate: 0.0008366, \u001b[91mTrain Loss: 0.273\n",
      "\u001b[1m\u001b[93mComputing Validation Loss...\n",
      "\u001b[1mEpoch 60 Completed! Average Train Loss: 0.199, Average Validation Loss: 0.035\n",
      "Best Model saved in best_model/best_model.pt.\n",
      "\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m= Epoch [61/70] ==========================================\n",
      "\u001b[0m\u001b[1mEpoch: [61/70], Progress: 0.00 %, Steps: 5581, Current Learning Rate: 0.0008365, \u001b[91mTrain Loss: 0.181\n",
      "\u001b[0m\u001b[1mEpoch: [61/70], Progress: 1.08 %, Steps: 5582, Current Learning Rate: 0.0008365, \u001b[96mTrain Loss: 0.126\n",
      "\u001b[0m\u001b[1mEpoch: [61/70], Progress: 2.15 %, Steps: 5583, Current Learning Rate: 0.0008364, \u001b[96mTrain Loss: 0.101\n",
      "\u001b[0m\u001b[1mEpoch: [61/70], Progress: 3.23 %, Steps: 5584, Current Learning Rate: 0.0008363, \u001b[91mTrain Loss: 0.123\n",
      "\u001b[0m\u001b[1mEpoch: [61/70], Progress: 4.30 %, Steps: 5585, Current Learning Rate: 0.0008362, \u001b[96mTrain Loss: 0.118\n",
      "\u001b[0m\u001b[1mEpoch: [61/70], Progress: 5.38 %, Steps: 5586, Current Learning Rate: 0.0008362, \u001b[91mTrain Loss: 0.135\n",
      "\u001b[0m\u001b[1mEpoch: [61/70], Progress: 6.45 %, Steps: 5587, Current Learning Rate: 0.0008361, \u001b[91mTrain Loss: 0.145\n",
      "\u001b[0m\u001b[1mEpoch: [61/70], Progress: 7.53 %, Steps: 5588, Current Learning Rate: 0.0008360, \u001b[91mTrain Loss: 0.167\n",
      "\u001b[0m\u001b[1mEpoch: [61/70], Progress: 8.60 %, Steps: 5589, Current Learning Rate: 0.0008359, \u001b[96mTrain Loss: 0.132\n",
      "\u001b[0m\u001b[1mEpoch: [61/70], Progress: 9.68 %, Steps: 5590, Current Learning Rate: 0.0008359, \u001b[91mTrain Loss: 0.141\n",
      "\u001b[0m\u001b[1mEpoch: [61/70], Progress: 10.75 %, Steps: 5591, Current Learning Rate: 0.0008358, \u001b[91mTrain Loss: 0.182\n",
      "\u001b[0m\u001b[1mEpoch: [61/70], Progress: 11.83 %, Steps: 5592, Current Learning Rate: 0.0008357, \u001b[96mTrain Loss: 0.134\n",
      "\u001b[0m\u001b[1mEpoch: [61/70], Progress: 12.90 %, Steps: 5593, Current Learning Rate: 0.0008356, \u001b[91mTrain Loss: 0.150\n",
      "\u001b[0m\u001b[1mEpoch: [61/70], Progress: 13.98 %, Steps: 5594, Current Learning Rate: 0.0008356, \u001b[96mTrain Loss: 0.144\n",
      "\u001b[0m\u001b[1mEpoch: [61/70], Progress: 15.05 %, Steps: 5595, Current Learning Rate: 0.0008355, \u001b[91mTrain Loss: 0.181\n",
      "\u001b[0m\u001b[1mEpoch: [61/70], Progress: 16.13 %, Steps: 5596, Current Learning Rate: 0.0008354, \u001b[96mTrain Loss: 0.168\n",
      "\u001b[0m\u001b[1mEpoch: [61/70], Progress: 17.20 %, Steps: 5597, Current Learning Rate: 0.0008353, \u001b[96mTrain Loss: 0.128\n",
      "\u001b[0m\u001b[1mEpoch: [61/70], Progress: 18.28 %, Steps: 5598, Current Learning Rate: 0.0008353, \u001b[91mTrain Loss: 0.139\n",
      "\u001b[0m\u001b[1mEpoch: [61/70], Progress: 19.35 %, Steps: 5599, Current Learning Rate: 0.0008352, \u001b[91mTrain Loss: 0.199\n",
      "\u001b[0m\u001b[1mEpoch: [61/70], Progress: 20.43 %, Steps: 5600, Current Learning Rate: 0.0008351, \u001b[96mTrain Loss: 0.157\n",
      "\u001b[0m\u001b[1mEpoch: [61/70], Progress: 21.51 %, Steps: 5601, Current Learning Rate: 0.0008350, \u001b[91mTrain Loss: 0.214\n",
      "\u001b[0m\u001b[1mEpoch: [61/70], Progress: 22.58 %, Steps: 5602, Current Learning Rate: 0.0008350, \u001b[96mTrain Loss: 0.153\n",
      "\u001b[0m\u001b[1mEpoch: [61/70], Progress: 23.66 %, Steps: 5603, Current Learning Rate: 0.0008349, \u001b[91mTrain Loss: 0.197\n",
      "\u001b[0m\u001b[1mEpoch: [61/70], Progress: 24.73 %, Steps: 5604, Current Learning Rate: 0.0008348, \u001b[96mTrain Loss: 0.162\n",
      "\u001b[0m\u001b[1mEpoch: [61/70], Progress: 25.81 %, Steps: 5605, Current Learning Rate: 0.0008347, \u001b[91mTrain Loss: 0.208\n",
      "\u001b[0m\u001b[1mEpoch: [61/70], Progress: 26.88 %, Steps: 5606, Current Learning Rate: 0.0008347, \u001b[96mTrain Loss: 0.162\n",
      "\u001b[0m\u001b[1mEpoch: [61/70], Progress: 27.96 %, Steps: 5607, Current Learning Rate: 0.0008346, \u001b[91mTrain Loss: 0.171\n",
      "\u001b[0m\u001b[1mEpoch: [61/70], Progress: 29.03 %, Steps: 5608, Current Learning Rate: 0.0008345, \u001b[96mTrain Loss: 0.171\n",
      "\u001b[0m\u001b[1mEpoch: [61/70], Progress: 30.11 %, Steps: 5609, Current Learning Rate: 0.0008344, \u001b[96mTrain Loss: 0.141\n",
      "\u001b[0m\u001b[1mEpoch: [61/70], Progress: 31.18 %, Steps: 5610, Current Learning Rate: 0.0008344, \u001b[91mTrain Loss: 0.143\n",
      "\u001b[0m\u001b[1mEpoch: [61/70], Progress: 32.26 %, Steps: 5611, Current Learning Rate: 0.0008343, \u001b[91mTrain Loss: 0.202\n",
      "\u001b[0m\u001b[1mEpoch: [61/70], Progress: 33.33 %, Steps: 5612, Current Learning Rate: 0.0008342, \u001b[96mTrain Loss: 0.151\n",
      "\u001b[0m\u001b[1mEpoch: [61/70], Progress: 34.41 %, Steps: 5613, Current Learning Rate: 0.0008341, \u001b[91mTrain Loss: 0.214\n",
      "\u001b[0m\u001b[1mEpoch: [61/70], Progress: 35.48 %, Steps: 5614, Current Learning Rate: 0.0008341, \u001b[96mTrain Loss: 0.193\n",
      "\u001b[0m\u001b[1mEpoch: [61/70], Progress: 36.56 %, Steps: 5615, Current Learning Rate: 0.0008340, \u001b[96mTrain Loss: 0.190\n",
      "\u001b[0m\u001b[1mEpoch: [61/70], Progress: 37.63 %, Steps: 5616, Current Learning Rate: 0.0008339, \u001b[96mTrain Loss: 0.177\n",
      "\u001b[0m\u001b[1mEpoch: [61/70], Progress: 38.71 %, Steps: 5617, Current Learning Rate: 0.0008339, \u001b[91mTrain Loss: 0.185\n",
      "\u001b[0m\u001b[1mEpoch: [61/70], Progress: 39.78 %, Steps: 5618, Current Learning Rate: 0.0008338, \u001b[96mTrain Loss: 0.179\n",
      "\u001b[0m\u001b[1mEpoch: [61/70], Progress: 40.86 %, Steps: 5619, Current Learning Rate: 0.0008337, \u001b[96mTrain Loss: 0.153\n",
      "\u001b[0m\u001b[1mEpoch: [61/70], Progress: 41.94 %, Steps: 5620, Current Learning Rate: 0.0008336, \u001b[96mTrain Loss: 0.143\n",
      "\u001b[0m\u001b[1mEpoch: [61/70], Progress: 43.01 %, Steps: 5621, Current Learning Rate: 0.0008336, \u001b[91mTrain Loss: 0.161\n",
      "\u001b[0m\u001b[1mEpoch: [61/70], Progress: 44.09 %, Steps: 5622, Current Learning Rate: 0.0008335, \u001b[91mTrain Loss: 0.170\n",
      "\u001b[0m\u001b[1mEpoch: [61/70], Progress: 45.16 %, Steps: 5623, Current Learning Rate: 0.0008334, \u001b[91mTrain Loss: 0.197\n",
      "\u001b[0m\u001b[1mEpoch: [61/70], Progress: 46.24 %, Steps: 5624, Current Learning Rate: 0.0008333, \u001b[96mTrain Loss: 0.188\n",
      "\u001b[0m\u001b[1mEpoch: [61/70], Progress: 47.31 %, Steps: 5625, Current Learning Rate: 0.0008333, \u001b[91mTrain Loss: 0.230\n",
      "\u001b[0m\u001b[1mEpoch: [61/70], Progress: 48.39 %, Steps: 5626, Current Learning Rate: 0.0008332, \u001b[96mTrain Loss: 0.227\n",
      "\u001b[0m\u001b[1mEpoch: [61/70], Progress: 49.46 %, Steps: 5627, Current Learning Rate: 0.0008331, \u001b[91mTrain Loss: 0.233\n",
      "\u001b[0m\u001b[1mEpoch: [61/70], Progress: 50.54 %, Steps: 5628, Current Learning Rate: 0.0008330, \u001b[96mTrain Loss: 0.182\n",
      "\u001b[0m\u001b[1mEpoch: [61/70], Progress: 51.61 %, Steps: 5629, Current Learning Rate: 0.0008330, \u001b[96mTrain Loss: 0.164\n",
      "\u001b[0m\u001b[1mEpoch: [61/70], Progress: 52.69 %, Steps: 5630, Current Learning Rate: 0.0008329, \u001b[91mTrain Loss: 0.218\n",
      "\u001b[0m\u001b[1mEpoch: [61/70], Progress: 53.76 %, Steps: 5631, Current Learning Rate: 0.0008328, \u001b[91mTrain Loss: 0.231\n",
      "\u001b[0m\u001b[1mEpoch: [61/70], Progress: 54.84 %, Steps: 5632, Current Learning Rate: 0.0008327, \u001b[96mTrain Loss: 0.204\n",
      "\u001b[0m\u001b[1mEpoch: [61/70], Progress: 55.91 %, Steps: 5633, Current Learning Rate: 0.0008327, \u001b[96mTrain Loss: 0.181\n",
      "\u001b[0m\u001b[1mEpoch: [61/70], Progress: 56.99 %, Steps: 5634, Current Learning Rate: 0.0008326, \u001b[91mTrain Loss: 0.201\n",
      "\u001b[0m\u001b[1mEpoch: [61/70], Progress: 58.06 %, Steps: 5635, Current Learning Rate: 0.0008325, \u001b[91mTrain Loss: 0.241\n",
      "\u001b[0m\u001b[1mEpoch: [61/70], Progress: 59.14 %, Steps: 5636, Current Learning Rate: 0.0008324, \u001b[96mTrain Loss: 0.162\n",
      "\u001b[0m\u001b[1mEpoch: [61/70], Progress: 60.22 %, Steps: 5637, Current Learning Rate: 0.0008324, \u001b[96mTrain Loss: 0.159\n",
      "\u001b[0m\u001b[1mEpoch: [61/70], Progress: 61.29 %, Steps: 5638, Current Learning Rate: 0.0008323, \u001b[91mTrain Loss: 0.201\n",
      "\u001b[0m\u001b[1mEpoch: [61/70], Progress: 62.37 %, Steps: 5639, Current Learning Rate: 0.0008322, \u001b[96mTrain Loss: 0.199\n",
      "\u001b[0m\u001b[1mEpoch: [61/70], Progress: 63.44 %, Steps: 5640, Current Learning Rate: 0.0008322, \u001b[91mTrain Loss: 0.208\n",
      "\u001b[0m\u001b[1mEpoch: [61/70], Progress: 64.52 %, Steps: 5641, Current Learning Rate: 0.0008321, \u001b[96mTrain Loss: 0.182\n",
      "\u001b[0m\u001b[1mEpoch: [61/70], Progress: 65.59 %, Steps: 5642, Current Learning Rate: 0.0008320, \u001b[91mTrain Loss: 0.185\n",
      "\u001b[0m\u001b[1mEpoch: [61/70], Progress: 66.67 %, Steps: 5643, Current Learning Rate: 0.0008319, \u001b[91mTrain Loss: 0.249\n",
      "\u001b[0m\u001b[1mEpoch: [61/70], Progress: 67.74 %, Steps: 5644, Current Learning Rate: 0.0008319, \u001b[96mTrain Loss: 0.201\n",
      "\u001b[0m\u001b[1mEpoch: [61/70], Progress: 68.82 %, Steps: 5645, Current Learning Rate: 0.0008318, \u001b[91mTrain Loss: 0.211\n",
      "\u001b[0m\u001b[1mEpoch: [61/70], Progress: 69.89 %, Steps: 5646, Current Learning Rate: 0.0008317, \u001b[96mTrain Loss: 0.164\n",
      "\u001b[0m\u001b[1mEpoch: [61/70], Progress: 70.97 %, Steps: 5647, Current Learning Rate: 0.0008316, \u001b[91mTrain Loss: 0.225\n",
      "\u001b[0m\u001b[1mEpoch: [61/70], Progress: 72.04 %, Steps: 5648, Current Learning Rate: 0.0008316, \u001b[96mTrain Loss: 0.171\n",
      "\u001b[0m\u001b[1mEpoch: [61/70], Progress: 73.12 %, Steps: 5649, Current Learning Rate: 0.0008315, \u001b[91mTrain Loss: 0.175\n",
      "\u001b[0m\u001b[1mEpoch: [61/70], Progress: 74.19 %, Steps: 5650, Current Learning Rate: 0.0008314, \u001b[91mTrain Loss: 0.234\n",
      "\u001b[0m\u001b[1mEpoch: [61/70], Progress: 75.27 %, Steps: 5651, Current Learning Rate: 0.0008313, \u001b[96mTrain Loss: 0.171\n",
      "\u001b[0m\u001b[1mEpoch: [61/70], Progress: 76.34 %, Steps: 5652, Current Learning Rate: 0.0008313, \u001b[91mTrain Loss: 0.176\n",
      "\u001b[0m\u001b[1mEpoch: [61/70], Progress: 77.42 %, Steps: 5653, Current Learning Rate: 0.0008312, \u001b[91mTrain Loss: 0.207\n",
      "\u001b[0m\u001b[1mEpoch: [61/70], Progress: 78.49 %, Steps: 5654, Current Learning Rate: 0.0008311, \u001b[96mTrain Loss: 0.200\n",
      "\u001b[0m\u001b[1mEpoch: [61/70], Progress: 79.57 %, Steps: 5655, Current Learning Rate: 0.0008310, \u001b[91mTrain Loss: 0.204\n",
      "\u001b[0m\u001b[1mEpoch: [61/70], Progress: 80.65 %, Steps: 5656, Current Learning Rate: 0.0008310, \u001b[91mTrain Loss: 0.207\n",
      "\u001b[0m\u001b[1mEpoch: [61/70], Progress: 81.72 %, Steps: 5657, Current Learning Rate: 0.0008309, \u001b[96mTrain Loss: 0.189\n",
      "\u001b[0m\u001b[1mEpoch: [61/70], Progress: 82.80 %, Steps: 5658, Current Learning Rate: 0.0008308, \u001b[91mTrain Loss: 0.252\n",
      "\u001b[0m\u001b[1mEpoch: [61/70], Progress: 83.87 %, Steps: 5659, Current Learning Rate: 0.0008308, \u001b[96mTrain Loss: 0.206\n",
      "\u001b[0m\u001b[1mEpoch: [61/70], Progress: 84.95 %, Steps: 5660, Current Learning Rate: 0.0008307, \u001b[96mTrain Loss: 0.201\n",
      "\u001b[0m\u001b[1mEpoch: [61/70], Progress: 86.02 %, Steps: 5661, Current Learning Rate: 0.0008306, \u001b[96mTrain Loss: 0.182\n",
      "\u001b[0m\u001b[1mEpoch: [61/70], Progress: 87.10 %, Steps: 5662, Current Learning Rate: 0.0008305, \u001b[91mTrain Loss: 0.183\n",
      "\u001b[0m\u001b[1mEpoch: [61/70], Progress: 88.17 %, Steps: 5663, Current Learning Rate: 0.0008305, \u001b[91mTrain Loss: 0.222\n",
      "\u001b[0m\u001b[1mEpoch: [61/70], Progress: 89.25 %, Steps: 5664, Current Learning Rate: 0.0008304, \u001b[96mTrain Loss: 0.205\n",
      "\u001b[0m\u001b[1mEpoch: [61/70], Progress: 90.32 %, Steps: 5665, Current Learning Rate: 0.0008303, \u001b[91mTrain Loss: 0.208\n",
      "\u001b[0m\u001b[1mEpoch: [61/70], Progress: 91.40 %, Steps: 5666, Current Learning Rate: 0.0008302, \u001b[91mTrain Loss: 0.239\n",
      "\u001b[0m\u001b[1mEpoch: [61/70], Progress: 92.47 %, Steps: 5667, Current Learning Rate: 0.0008302, \u001b[96mTrain Loss: 0.202\n",
      "\u001b[0m\u001b[1mEpoch: [61/70], Progress: 93.55 %, Steps: 5668, Current Learning Rate: 0.0008301, \u001b[96mTrain Loss: 0.166\n",
      "\u001b[0m\u001b[1mEpoch: [61/70], Progress: 94.62 %, Steps: 5669, Current Learning Rate: 0.0008300, \u001b[91mTrain Loss: 0.203\n",
      "\u001b[0m\u001b[1mEpoch: [61/70], Progress: 95.70 %, Steps: 5670, Current Learning Rate: 0.0008299, \u001b[96mTrain Loss: 0.155\n",
      "\u001b[0m\u001b[1mEpoch: [61/70], Progress: 96.77 %, Steps: 5671, Current Learning Rate: 0.0008299, \u001b[96mTrain Loss: 0.154\n",
      "\u001b[0m\u001b[1mEpoch: [61/70], Progress: 97.85 %, Steps: 5672, Current Learning Rate: 0.0008298, \u001b[91mTrain Loss: 0.196\n",
      "\u001b[0m\u001b[1mEpoch: [61/70], Progress: 98.92 %, Steps: 5673, Current Learning Rate: 0.0008297, \u001b[96mTrain Loss: 0.166\n",
      "\u001b[1m\u001b[93mComputing Validation Loss...\n",
      "\u001b[1mEpoch 61 Completed! Average Train Loss: 0.182, Average Validation Loss: 0.035\n",
      "\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m= Epoch [62/70] ==========================================\n",
      "\u001b[0m\u001b[1mEpoch: [62/70], Progress: 0.00 %, Steps: 5674, Current Learning Rate: 0.0008297, \u001b[91mTrain Loss: 0.131\n",
      "\u001b[0m\u001b[1mEpoch: [62/70], Progress: 1.08 %, Steps: 5675, Current Learning Rate: 0.0008296, \u001b[91mTrain Loss: 0.140\n",
      "\u001b[0m\u001b[1mEpoch: [62/70], Progress: 2.15 %, Steps: 5676, Current Learning Rate: 0.0008295, \u001b[96mTrain Loss: 0.129\n",
      "\u001b[0m\u001b[1mEpoch: [62/70], Progress: 3.23 %, Steps: 5677, Current Learning Rate: 0.0008294, \u001b[91mTrain Loss: 0.152\n",
      "\u001b[0m\u001b[1mEpoch: [62/70], Progress: 4.30 %, Steps: 5678, Current Learning Rate: 0.0008294, \u001b[96mTrain Loss: 0.113\n",
      "\u001b[0m\u001b[1mEpoch: [62/70], Progress: 5.38 %, Steps: 5679, Current Learning Rate: 0.0008293, \u001b[91mTrain Loss: 0.162\n",
      "\u001b[0m\u001b[1mEpoch: [62/70], Progress: 6.45 %, Steps: 5680, Current Learning Rate: 0.0008292, \u001b[96mTrain Loss: 0.108\n",
      "\u001b[0m\u001b[1mEpoch: [62/70], Progress: 7.53 %, Steps: 5681, Current Learning Rate: 0.0008291, \u001b[91mTrain Loss: 0.129\n",
      "\u001b[0m\u001b[1mEpoch: [62/70], Progress: 8.60 %, Steps: 5682, Current Learning Rate: 0.0008291, \u001b[91mTrain Loss: 0.179\n",
      "\u001b[0m\u001b[1mEpoch: [62/70], Progress: 9.68 %, Steps: 5683, Current Learning Rate: 0.0008290, \u001b[96mTrain Loss: 0.161\n",
      "\u001b[0m\u001b[1mEpoch: [62/70], Progress: 10.75 %, Steps: 5684, Current Learning Rate: 0.0008289, \u001b[96mTrain Loss: 0.159\n",
      "\u001b[0m\u001b[1mEpoch: [62/70], Progress: 11.83 %, Steps: 5685, Current Learning Rate: 0.0008289, \u001b[96mTrain Loss: 0.128\n",
      "\u001b[0m\u001b[1mEpoch: [62/70], Progress: 12.90 %, Steps: 5686, Current Learning Rate: 0.0008288, \u001b[91mTrain Loss: 0.135\n",
      "\u001b[0m\u001b[1mEpoch: [62/70], Progress: 13.98 %, Steps: 5687, Current Learning Rate: 0.0008287, \u001b[91mTrain Loss: 0.148\n",
      "\u001b[0m\u001b[1mEpoch: [62/70], Progress: 15.05 %, Steps: 5688, Current Learning Rate: 0.0008286, \u001b[91mTrain Loss: 0.180\n",
      "\u001b[0m\u001b[1mEpoch: [62/70], Progress: 16.13 %, Steps: 5689, Current Learning Rate: 0.0008286, \u001b[96mTrain Loss: 0.153\n",
      "\u001b[0m\u001b[1mEpoch: [62/70], Progress: 17.20 %, Steps: 5690, Current Learning Rate: 0.0008285, \u001b[91mTrain Loss: 0.163\n",
      "\u001b[0m\u001b[1mEpoch: [62/70], Progress: 18.28 %, Steps: 5691, Current Learning Rate: 0.0008284, \u001b[91mTrain Loss: 0.171\n",
      "\u001b[0m\u001b[1mEpoch: [62/70], Progress: 19.35 %, Steps: 5692, Current Learning Rate: 0.0008283, \u001b[96mTrain Loss: 0.157\n",
      "\u001b[0m\u001b[1mEpoch: [62/70], Progress: 20.43 %, Steps: 5693, Current Learning Rate: 0.0008283, \u001b[91mTrain Loss: 0.222\n",
      "\u001b[0m\u001b[1mEpoch: [62/70], Progress: 21.51 %, Steps: 5694, Current Learning Rate: 0.0008282, \u001b[96mTrain Loss: 0.168\n",
      "\u001b[0m\u001b[1mEpoch: [62/70], Progress: 22.58 %, Steps: 5695, Current Learning Rate: 0.0008281, \u001b[96mTrain Loss: 0.136\n",
      "\u001b[0m\u001b[1mEpoch: [62/70], Progress: 23.66 %, Steps: 5696, Current Learning Rate: 0.0008281, \u001b[96mTrain Loss: 0.130\n",
      "\u001b[0m\u001b[1mEpoch: [62/70], Progress: 24.73 %, Steps: 5697, Current Learning Rate: 0.0008280, \u001b[96mTrain Loss: 0.090\n",
      "\u001b[0m\u001b[1mEpoch: [62/70], Progress: 25.81 %, Steps: 5698, Current Learning Rate: 0.0008279, \u001b[91mTrain Loss: 0.153\n",
      "\u001b[0m\u001b[1mEpoch: [62/70], Progress: 26.88 %, Steps: 5699, Current Learning Rate: 0.0008278, \u001b[91mTrain Loss: 0.185\n",
      "\u001b[0m\u001b[1mEpoch: [62/70], Progress: 27.96 %, Steps: 5700, Current Learning Rate: 0.0008278, \u001b[96mTrain Loss: 0.168\n",
      "\u001b[0m\u001b[1mEpoch: [62/70], Progress: 29.03 %, Steps: 5701, Current Learning Rate: 0.0008277, \u001b[96mTrain Loss: 0.132\n",
      "\u001b[0m\u001b[1mEpoch: [62/70], Progress: 30.11 %, Steps: 5702, Current Learning Rate: 0.0008276, \u001b[96mTrain Loss: 0.121\n",
      "\u001b[0m\u001b[1mEpoch: [62/70], Progress: 31.18 %, Steps: 5703, Current Learning Rate: 0.0008275, \u001b[91mTrain Loss: 0.188\n",
      "\u001b[0m\u001b[1mEpoch: [62/70], Progress: 32.26 %, Steps: 5704, Current Learning Rate: 0.0008275, \u001b[96mTrain Loss: 0.136\n",
      "\u001b[0m\u001b[1mEpoch: [62/70], Progress: 33.33 %, Steps: 5705, Current Learning Rate: 0.0008274, \u001b[91mTrain Loss: 0.140\n",
      "\u001b[0m\u001b[1mEpoch: [62/70], Progress: 34.41 %, Steps: 5706, Current Learning Rate: 0.0008273, \u001b[91mTrain Loss: 0.178\n",
      "\u001b[0m\u001b[1mEpoch: [62/70], Progress: 35.48 %, Steps: 5707, Current Learning Rate: 0.0008273, \u001b[96mTrain Loss: 0.136\n",
      "\u001b[0m\u001b[1mEpoch: [62/70], Progress: 36.56 %, Steps: 5708, Current Learning Rate: 0.0008272, \u001b[91mTrain Loss: 0.208\n",
      "\u001b[0m\u001b[1mEpoch: [62/70], Progress: 37.63 %, Steps: 5709, Current Learning Rate: 0.0008271, \u001b[96mTrain Loss: 0.175\n",
      "\u001b[0m\u001b[1mEpoch: [62/70], Progress: 38.71 %, Steps: 5710, Current Learning Rate: 0.0008270, \u001b[96mTrain Loss: 0.138\n",
      "\u001b[0m\u001b[1mEpoch: [62/70], Progress: 39.78 %, Steps: 5711, Current Learning Rate: 0.0008270, \u001b[91mTrain Loss: 0.158\n",
      "\u001b[0m\u001b[1mEpoch: [62/70], Progress: 40.86 %, Steps: 5712, Current Learning Rate: 0.0008269, \u001b[96mTrain Loss: 0.136\n",
      "\u001b[0m\u001b[1mEpoch: [62/70], Progress: 41.94 %, Steps: 5713, Current Learning Rate: 0.0008268, \u001b[91mTrain Loss: 0.143\n",
      "\u001b[0m\u001b[1mEpoch: [62/70], Progress: 43.01 %, Steps: 5714, Current Learning Rate: 0.0008267, \u001b[91mTrain Loss: 0.170\n",
      "\u001b[0m\u001b[1mEpoch: [62/70], Progress: 44.09 %, Steps: 5715, Current Learning Rate: 0.0008267, \u001b[96mTrain Loss: 0.156\n",
      "\u001b[0m\u001b[1mEpoch: [62/70], Progress: 45.16 %, Steps: 5716, Current Learning Rate: 0.0008266, \u001b[91mTrain Loss: 0.179\n",
      "\u001b[0m\u001b[1mEpoch: [62/70], Progress: 46.24 %, Steps: 5717, Current Learning Rate: 0.0008265, \u001b[96mTrain Loss: 0.171\n",
      "\u001b[0m\u001b[1mEpoch: [62/70], Progress: 47.31 %, Steps: 5718, Current Learning Rate: 0.0008265, \u001b[91mTrain Loss: 0.185\n",
      "\u001b[0m\u001b[1mEpoch: [62/70], Progress: 48.39 %, Steps: 5719, Current Learning Rate: 0.0008264, \u001b[96mTrain Loss: 0.159\n",
      "\u001b[0m\u001b[1mEpoch: [62/70], Progress: 49.46 %, Steps: 5720, Current Learning Rate: 0.0008263, \u001b[91mTrain Loss: 0.165\n",
      "\u001b[0m\u001b[1mEpoch: [62/70], Progress: 50.54 %, Steps: 5721, Current Learning Rate: 0.0008262, \u001b[91mTrain Loss: 0.167\n",
      "\u001b[0m\u001b[1mEpoch: [62/70], Progress: 51.61 %, Steps: 5722, Current Learning Rate: 0.0008262, \u001b[91mTrain Loss: 0.177\n",
      "\u001b[0m\u001b[1mEpoch: [62/70], Progress: 52.69 %, Steps: 5723, Current Learning Rate: 0.0008261, \u001b[96mTrain Loss: 0.134\n",
      "\u001b[0m\u001b[1mEpoch: [62/70], Progress: 53.76 %, Steps: 5724, Current Learning Rate: 0.0008260, \u001b[91mTrain Loss: 0.185\n",
      "\u001b[0m\u001b[1mEpoch: [62/70], Progress: 54.84 %, Steps: 5725, Current Learning Rate: 0.0008260, \u001b[96mTrain Loss: 0.176\n",
      "\u001b[0m\u001b[1mEpoch: [62/70], Progress: 55.91 %, Steps: 5726, Current Learning Rate: 0.0008259, \u001b[96mTrain Loss: 0.160\n",
      "\u001b[0m\u001b[1mEpoch: [62/70], Progress: 56.99 %, Steps: 5727, Current Learning Rate: 0.0008258, \u001b[91mTrain Loss: 0.171\n",
      "\u001b[0m\u001b[1mEpoch: [62/70], Progress: 58.06 %, Steps: 5728, Current Learning Rate: 0.0008257, \u001b[96mTrain Loss: 0.164\n",
      "\u001b[0m\u001b[1mEpoch: [62/70], Progress: 59.14 %, Steps: 5729, Current Learning Rate: 0.0008257, \u001b[91mTrain Loss: 0.193\n",
      "\u001b[0m\u001b[1mEpoch: [62/70], Progress: 60.22 %, Steps: 5730, Current Learning Rate: 0.0008256, \u001b[96mTrain Loss: 0.153\n",
      "\u001b[0m\u001b[1mEpoch: [62/70], Progress: 61.29 %, Steps: 5731, Current Learning Rate: 0.0008255, \u001b[91mTrain Loss: 0.195\n",
      "\u001b[0m\u001b[1mEpoch: [62/70], Progress: 62.37 %, Steps: 5732, Current Learning Rate: 0.0008254, \u001b[91mTrain Loss: 0.220\n",
      "\u001b[0m\u001b[1mEpoch: [62/70], Progress: 63.44 %, Steps: 5733, Current Learning Rate: 0.0008254, \u001b[96mTrain Loss: 0.177\n",
      "\u001b[0m\u001b[1mEpoch: [62/70], Progress: 64.52 %, Steps: 5734, Current Learning Rate: 0.0008253, \u001b[91mTrain Loss: 0.187\n",
      "\u001b[0m\u001b[1mEpoch: [62/70], Progress: 65.59 %, Steps: 5735, Current Learning Rate: 0.0008252, \u001b[91mTrain Loss: 0.204\n",
      "\u001b[0m\u001b[1mEpoch: [62/70], Progress: 66.67 %, Steps: 5736, Current Learning Rate: 0.0008252, \u001b[91mTrain Loss: 0.205\n",
      "\u001b[0m\u001b[1mEpoch: [62/70], Progress: 67.74 %, Steps: 5737, Current Learning Rate: 0.0008251, \u001b[96mTrain Loss: 0.182\n",
      "\u001b[0m\u001b[1mEpoch: [62/70], Progress: 68.82 %, Steps: 5738, Current Learning Rate: 0.0008250, \u001b[96mTrain Loss: 0.170\n",
      "\u001b[0m\u001b[1mEpoch: [62/70], Progress: 69.89 %, Steps: 5739, Current Learning Rate: 0.0008249, \u001b[91mTrain Loss: 0.184\n",
      "\u001b[0m\u001b[1mEpoch: [62/70], Progress: 70.97 %, Steps: 5740, Current Learning Rate: 0.0008249, \u001b[96mTrain Loss: 0.159\n",
      "\u001b[0m\u001b[1mEpoch: [62/70], Progress: 72.04 %, Steps: 5741, Current Learning Rate: 0.0008248, \u001b[91mTrain Loss: 0.197\n",
      "\u001b[0m\u001b[1mEpoch: [62/70], Progress: 73.12 %, Steps: 5742, Current Learning Rate: 0.0008247, \u001b[96mTrain Loss: 0.167\n",
      "\u001b[0m\u001b[1mEpoch: [62/70], Progress: 74.19 %, Steps: 5743, Current Learning Rate: 0.0008247, \u001b[91mTrain Loss: 0.207\n",
      "\u001b[0m\u001b[1mEpoch: [62/70], Progress: 75.27 %, Steps: 5744, Current Learning Rate: 0.0008246, \u001b[96mTrain Loss: 0.138\n",
      "\u001b[0m\u001b[1mEpoch: [62/70], Progress: 76.34 %, Steps: 5745, Current Learning Rate: 0.0008245, \u001b[91mTrain Loss: 0.221\n",
      "\u001b[0m\u001b[1mEpoch: [62/70], Progress: 77.42 %, Steps: 5746, Current Learning Rate: 0.0008244, \u001b[96mTrain Loss: 0.159\n",
      "\u001b[0m\u001b[1mEpoch: [62/70], Progress: 78.49 %, Steps: 5747, Current Learning Rate: 0.0008244, \u001b[91mTrain Loss: 0.180\n",
      "\u001b[0m\u001b[1mEpoch: [62/70], Progress: 79.57 %, Steps: 5748, Current Learning Rate: 0.0008243, \u001b[91mTrain Loss: 0.188\n",
      "\u001b[0m\u001b[1mEpoch: [62/70], Progress: 80.65 %, Steps: 5749, Current Learning Rate: 0.0008242, \u001b[91mTrain Loss: 0.200\n",
      "\u001b[0m\u001b[1mEpoch: [62/70], Progress: 81.72 %, Steps: 5750, Current Learning Rate: 0.0008242, \u001b[96mTrain Loss: 0.167\n",
      "\u001b[0m\u001b[1mEpoch: [62/70], Progress: 82.80 %, Steps: 5751, Current Learning Rate: 0.0008241, \u001b[96mTrain Loss: 0.126\n",
      "\u001b[0m\u001b[1mEpoch: [62/70], Progress: 83.87 %, Steps: 5752, Current Learning Rate: 0.0008240, \u001b[91mTrain Loss: 0.162\n",
      "\u001b[0m\u001b[1mEpoch: [62/70], Progress: 84.95 %, Steps: 5753, Current Learning Rate: 0.0008239, \u001b[91mTrain Loss: 0.203\n",
      "\u001b[0m\u001b[1mEpoch: [62/70], Progress: 86.02 %, Steps: 5754, Current Learning Rate: 0.0008239, \u001b[96mTrain Loss: 0.190\n",
      "\u001b[0m\u001b[1mEpoch: [62/70], Progress: 87.10 %, Steps: 5755, Current Learning Rate: 0.0008238, \u001b[91mTrain Loss: 0.228\n",
      "\u001b[0m\u001b[1mEpoch: [62/70], Progress: 88.17 %, Steps: 5756, Current Learning Rate: 0.0008237, \u001b[96mTrain Loss: 0.148\n",
      "\u001b[0m\u001b[1mEpoch: [62/70], Progress: 89.25 %, Steps: 5757, Current Learning Rate: 0.0008237, \u001b[91mTrain Loss: 0.182\n",
      "\u001b[0m\u001b[1mEpoch: [62/70], Progress: 90.32 %, Steps: 5758, Current Learning Rate: 0.0008236, \u001b[91mTrain Loss: 0.191\n",
      "\u001b[0m\u001b[1mEpoch: [62/70], Progress: 91.40 %, Steps: 5759, Current Learning Rate: 0.0008235, \u001b[96mTrain Loss: 0.182\n",
      "\u001b[0m\u001b[1mEpoch: [62/70], Progress: 92.47 %, Steps: 5760, Current Learning Rate: 0.0008234, \u001b[91mTrain Loss: 0.194\n",
      "\u001b[0m\u001b[1mEpoch: [62/70], Progress: 93.55 %, Steps: 5761, Current Learning Rate: 0.0008234, \u001b[91mTrain Loss: 0.239\n",
      "\u001b[0m\u001b[1mEpoch: [62/70], Progress: 94.62 %, Steps: 5762, Current Learning Rate: 0.0008233, \u001b[96mTrain Loss: 0.225\n",
      "\u001b[0m\u001b[1mEpoch: [62/70], Progress: 95.70 %, Steps: 5763, Current Learning Rate: 0.0008232, \u001b[96mTrain Loss: 0.192\n",
      "\u001b[0m\u001b[1mEpoch: [62/70], Progress: 96.77 %, Steps: 5764, Current Learning Rate: 0.0008232, \u001b[91mTrain Loss: 0.201\n",
      "\u001b[0m\u001b[1mEpoch: [62/70], Progress: 97.85 %, Steps: 5765, Current Learning Rate: 0.0008231, \u001b[96mTrain Loss: 0.136\n",
      "\u001b[0m\u001b[1mEpoch: [62/70], Progress: 98.92 %, Steps: 5766, Current Learning Rate: 0.0008230, \u001b[91mTrain Loss: 0.197\n",
      "\u001b[1m\u001b[93mComputing Validation Loss...\n",
      "\u001b[1mEpoch 62 Completed! Average Train Loss: 0.168, Average Validation Loss: 0.029\n",
      "Best Model saved in best_model/best_model.pt.\n",
      "\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m= Epoch [63/70] ==========================================\n",
      "\u001b[0m\u001b[1mEpoch: [63/70], Progress: 0.00 %, Steps: 5767, Current Learning Rate: 0.0008229, \u001b[91mTrain Loss: 0.145\n",
      "\u001b[0m\u001b[1mEpoch: [63/70], Progress: 1.08 %, Steps: 5768, Current Learning Rate: 0.0008229, \u001b[91mTrain Loss: 0.173\n",
      "\u001b[0m\u001b[1mEpoch: [63/70], Progress: 2.15 %, Steps: 5769, Current Learning Rate: 0.0008228, \u001b[96mTrain Loss: 0.164\n",
      "\u001b[0m\u001b[1mEpoch: [63/70], Progress: 3.23 %, Steps: 5770, Current Learning Rate: 0.0008227, \u001b[96mTrain Loss: 0.132\n",
      "\u001b[0m\u001b[1mEpoch: [63/70], Progress: 4.30 %, Steps: 5771, Current Learning Rate: 0.0008227, \u001b[91mTrain Loss: 0.161\n",
      "\u001b[0m\u001b[1mEpoch: [63/70], Progress: 5.38 %, Steps: 5772, Current Learning Rate: 0.0008226, \u001b[96mTrain Loss: 0.147\n",
      "\u001b[0m\u001b[1mEpoch: [63/70], Progress: 6.45 %, Steps: 5773, Current Learning Rate: 0.0008225, \u001b[96mTrain Loss: 0.139\n",
      "\u001b[0m\u001b[1mEpoch: [63/70], Progress: 7.53 %, Steps: 5774, Current Learning Rate: 0.0008224, \u001b[91mTrain Loss: 0.156\n",
      "\u001b[0m\u001b[1mEpoch: [63/70], Progress: 8.60 %, Steps: 5775, Current Learning Rate: 0.0008224, \u001b[96mTrain Loss: 0.128\n",
      "\u001b[0m\u001b[1mEpoch: [63/70], Progress: 9.68 %, Steps: 5776, Current Learning Rate: 0.0008223, \u001b[91mTrain Loss: 0.161\n",
      "\u001b[0m\u001b[1mEpoch: [63/70], Progress: 10.75 %, Steps: 5777, Current Learning Rate: 0.0008222, \u001b[91mTrain Loss: 0.172\n",
      "\u001b[0m\u001b[1mEpoch: [63/70], Progress: 11.83 %, Steps: 5778, Current Learning Rate: 0.0008222, \u001b[91mTrain Loss: 0.174\n",
      "\u001b[0m\u001b[1mEpoch: [63/70], Progress: 12.90 %, Steps: 5779, Current Learning Rate: 0.0008221, \u001b[96mTrain Loss: 0.134\n",
      "\u001b[0m\u001b[1mEpoch: [63/70], Progress: 13.98 %, Steps: 5780, Current Learning Rate: 0.0008220, \u001b[91mTrain Loss: 0.161\n",
      "\u001b[0m\u001b[1mEpoch: [63/70], Progress: 15.05 %, Steps: 5781, Current Learning Rate: 0.0008219, \u001b[96mTrain Loss: 0.160\n",
      "\u001b[0m\u001b[1mEpoch: [63/70], Progress: 16.13 %, Steps: 5782, Current Learning Rate: 0.0008219, \u001b[96mTrain Loss: 0.159\n",
      "\u001b[0m\u001b[1mEpoch: [63/70], Progress: 17.20 %, Steps: 5783, Current Learning Rate: 0.0008218, \u001b[96mTrain Loss: 0.144\n",
      "\u001b[0m\u001b[1mEpoch: [63/70], Progress: 18.28 %, Steps: 5784, Current Learning Rate: 0.0008217, \u001b[96mTrain Loss: 0.114\n",
      "\u001b[0m\u001b[1mEpoch: [63/70], Progress: 19.35 %, Steps: 5785, Current Learning Rate: 0.0008217, \u001b[91mTrain Loss: 0.180\n",
      "\u001b[0m\u001b[1mEpoch: [63/70], Progress: 20.43 %, Steps: 5786, Current Learning Rate: 0.0008216, \u001b[96mTrain Loss: 0.139\n",
      "\u001b[0m\u001b[1mEpoch: [63/70], Progress: 21.51 %, Steps: 5787, Current Learning Rate: 0.0008215, \u001b[96mTrain Loss: 0.120\n",
      "\u001b[0m\u001b[1mEpoch: [63/70], Progress: 22.58 %, Steps: 5788, Current Learning Rate: 0.0008214, \u001b[91mTrain Loss: 0.141\n",
      "\u001b[0m\u001b[1mEpoch: [63/70], Progress: 23.66 %, Steps: 5789, Current Learning Rate: 0.0008214, \u001b[91mTrain Loss: 0.169\n",
      "\u001b[0m\u001b[1mEpoch: [63/70], Progress: 24.73 %, Steps: 5790, Current Learning Rate: 0.0008213, \u001b[91mTrain Loss: 0.182\n",
      "\u001b[0m\u001b[1mEpoch: [63/70], Progress: 25.81 %, Steps: 5791, Current Learning Rate: 0.0008212, \u001b[96mTrain Loss: 0.154\n",
      "\u001b[0m\u001b[1mEpoch: [63/70], Progress: 26.88 %, Steps: 5792, Current Learning Rate: 0.0008212, \u001b[96mTrain Loss: 0.134\n",
      "\u001b[0m\u001b[1mEpoch: [63/70], Progress: 27.96 %, Steps: 5793, Current Learning Rate: 0.0008211, \u001b[91mTrain Loss: 0.176\n",
      "\u001b[0m\u001b[1mEpoch: [63/70], Progress: 29.03 %, Steps: 5794, Current Learning Rate: 0.0008210, \u001b[96mTrain Loss: 0.116\n",
      "\u001b[0m\u001b[1mEpoch: [63/70], Progress: 30.11 %, Steps: 5795, Current Learning Rate: 0.0008209, \u001b[96mTrain Loss: 0.111\n",
      "\u001b[0m\u001b[1mEpoch: [63/70], Progress: 31.18 %, Steps: 5796, Current Learning Rate: 0.0008209, \u001b[91mTrain Loss: 0.120\n",
      "\u001b[0m\u001b[1mEpoch: [63/70], Progress: 32.26 %, Steps: 5797, Current Learning Rate: 0.0008208, \u001b[91mTrain Loss: 0.155\n",
      "\u001b[0m\u001b[1mEpoch: [63/70], Progress: 33.33 %, Steps: 5798, Current Learning Rate: 0.0008207, \u001b[96mTrain Loss: 0.142\n",
      "\u001b[0m\u001b[1mEpoch: [63/70], Progress: 34.41 %, Steps: 5799, Current Learning Rate: 0.0008207, \u001b[96mTrain Loss: 0.119\n",
      "\u001b[0m\u001b[1mEpoch: [63/70], Progress: 35.48 %, Steps: 5800, Current Learning Rate: 0.0008206, \u001b[91mTrain Loss: 0.150\n",
      "\u001b[0m\u001b[1mEpoch: [63/70], Progress: 36.56 %, Steps: 5801, Current Learning Rate: 0.0008205, \u001b[91mTrain Loss: 0.155\n",
      "\u001b[0m\u001b[1mEpoch: [63/70], Progress: 37.63 %, Steps: 5802, Current Learning Rate: 0.0008205, \u001b[91mTrain Loss: 0.238\n",
      "\u001b[0m\u001b[1mEpoch: [63/70], Progress: 38.71 %, Steps: 5803, Current Learning Rate: 0.0008204, \u001b[96mTrain Loss: 0.112\n",
      "\u001b[0m\u001b[1mEpoch: [63/70], Progress: 39.78 %, Steps: 5804, Current Learning Rate: 0.0008203, \u001b[91mTrain Loss: 0.115\n",
      "\u001b[0m\u001b[1mEpoch: [63/70], Progress: 40.86 %, Steps: 5805, Current Learning Rate: 0.0008202, \u001b[91mTrain Loss: 0.197\n",
      "\u001b[0m\u001b[1mEpoch: [63/70], Progress: 41.94 %, Steps: 5806, Current Learning Rate: 0.0008202, \u001b[96mTrain Loss: 0.153\n",
      "\u001b[0m\u001b[1mEpoch: [63/70], Progress: 43.01 %, Steps: 5807, Current Learning Rate: 0.0008201, \u001b[96mTrain Loss: 0.137\n",
      "\u001b[0m\u001b[1mEpoch: [63/70], Progress: 44.09 %, Steps: 5808, Current Learning Rate: 0.0008200, \u001b[91mTrain Loss: 0.169\n",
      "\u001b[0m\u001b[1mEpoch: [63/70], Progress: 45.16 %, Steps: 5809, Current Learning Rate: 0.0008200, \u001b[96mTrain Loss: 0.135\n",
      "\u001b[0m\u001b[1mEpoch: [63/70], Progress: 46.24 %, Steps: 5810, Current Learning Rate: 0.0008199, \u001b[91mTrain Loss: 0.172\n",
      "\u001b[0m\u001b[1mEpoch: [63/70], Progress: 47.31 %, Steps: 5811, Current Learning Rate: 0.0008198, \u001b[91mTrain Loss: 0.236\n",
      "\u001b[0m\u001b[1mEpoch: [63/70], Progress: 48.39 %, Steps: 5812, Current Learning Rate: 0.0008197, \u001b[96mTrain Loss: 0.149\n",
      "\u001b[0m\u001b[1mEpoch: [63/70], Progress: 49.46 %, Steps: 5813, Current Learning Rate: 0.0008197, \u001b[91mTrain Loss: 0.188\n",
      "\u001b[0m\u001b[1mEpoch: [63/70], Progress: 50.54 %, Steps: 5814, Current Learning Rate: 0.0008196, \u001b[96mTrain Loss: 0.170\n",
      "\u001b[0m\u001b[1mEpoch: [63/70], Progress: 51.61 %, Steps: 5815, Current Learning Rate: 0.0008195, \u001b[96mTrain Loss: 0.160\n",
      "\u001b[0m\u001b[1mEpoch: [63/70], Progress: 52.69 %, Steps: 5816, Current Learning Rate: 0.0008195, \u001b[96mTrain Loss: 0.113\n",
      "\u001b[0m\u001b[1mEpoch: [63/70], Progress: 53.76 %, Steps: 5817, Current Learning Rate: 0.0008194, \u001b[91mTrain Loss: 0.154\n",
      "\u001b[0m\u001b[1mEpoch: [63/70], Progress: 54.84 %, Steps: 5818, Current Learning Rate: 0.0008193, \u001b[91mTrain Loss: 0.163\n",
      "\u001b[0m\u001b[1mEpoch: [63/70], Progress: 55.91 %, Steps: 5819, Current Learning Rate: 0.0008193, \u001b[96mTrain Loss: 0.096\n",
      "\u001b[0m\u001b[1mEpoch: [63/70], Progress: 56.99 %, Steps: 5820, Current Learning Rate: 0.0008192, \u001b[91mTrain Loss: 0.175\n",
      "\u001b[0m\u001b[1mEpoch: [63/70], Progress: 58.06 %, Steps: 5821, Current Learning Rate: 0.0008191, \u001b[96mTrain Loss: 0.142\n",
      "\u001b[0m\u001b[1mEpoch: [63/70], Progress: 59.14 %, Steps: 5822, Current Learning Rate: 0.0008190, \u001b[91mTrain Loss: 0.147\n",
      "\u001b[0m\u001b[1mEpoch: [63/70], Progress: 60.22 %, Steps: 5823, Current Learning Rate: 0.0008190, \u001b[91mTrain Loss: 0.176\n",
      "\u001b[0m\u001b[1mEpoch: [63/70], Progress: 61.29 %, Steps: 5824, Current Learning Rate: 0.0008189, \u001b[91mTrain Loss: 0.189\n",
      "\u001b[0m\u001b[1mEpoch: [63/70], Progress: 62.37 %, Steps: 5825, Current Learning Rate: 0.0008188, \u001b[96mTrain Loss: 0.133\n",
      "\u001b[0m\u001b[1mEpoch: [63/70], Progress: 63.44 %, Steps: 5826, Current Learning Rate: 0.0008188, \u001b[91mTrain Loss: 0.196\n",
      "\u001b[0m\u001b[1mEpoch: [63/70], Progress: 64.52 %, Steps: 5827, Current Learning Rate: 0.0008187, \u001b[96mTrain Loss: 0.171\n",
      "\u001b[0m\u001b[1mEpoch: [63/70], Progress: 65.59 %, Steps: 5828, Current Learning Rate: 0.0008186, \u001b[91mTrain Loss: 0.203\n",
      "\u001b[0m\u001b[1mEpoch: [63/70], Progress: 66.67 %, Steps: 5829, Current Learning Rate: 0.0008186, \u001b[96mTrain Loss: 0.196\n",
      "\u001b[0m\u001b[1mEpoch: [63/70], Progress: 67.74 %, Steps: 5830, Current Learning Rate: 0.0008185, \u001b[96mTrain Loss: 0.148\n",
      "\u001b[0m\u001b[1mEpoch: [63/70], Progress: 68.82 %, Steps: 5831, Current Learning Rate: 0.0008184, \u001b[91mTrain Loss: 0.219\n",
      "\u001b[0m\u001b[1mEpoch: [63/70], Progress: 69.89 %, Steps: 5832, Current Learning Rate: 0.0008183, \u001b[96mTrain Loss: 0.147\n",
      "\u001b[0m\u001b[1mEpoch: [63/70], Progress: 70.97 %, Steps: 5833, Current Learning Rate: 0.0008183, \u001b[91mTrain Loss: 0.228\n",
      "\u001b[0m\u001b[1mEpoch: [63/70], Progress: 72.04 %, Steps: 5834, Current Learning Rate: 0.0008182, \u001b[96mTrain Loss: 0.156\n",
      "\u001b[0m\u001b[1mEpoch: [63/70], Progress: 73.12 %, Steps: 5835, Current Learning Rate: 0.0008181, \u001b[91mTrain Loss: 0.173\n",
      "\u001b[0m\u001b[1mEpoch: [63/70], Progress: 74.19 %, Steps: 5836, Current Learning Rate: 0.0008181, \u001b[91mTrain Loss: 0.190\n",
      "\u001b[0m\u001b[1mEpoch: [63/70], Progress: 75.27 %, Steps: 5837, Current Learning Rate: 0.0008180, \u001b[96mTrain Loss: 0.123\n",
      "\u001b[0m\u001b[1mEpoch: [63/70], Progress: 76.34 %, Steps: 5838, Current Learning Rate: 0.0008179, \u001b[91mTrain Loss: 0.166\n",
      "\u001b[0m\u001b[1mEpoch: [63/70], Progress: 77.42 %, Steps: 5839, Current Learning Rate: 0.0008178, \u001b[96mTrain Loss: 0.151\n",
      "\u001b[0m\u001b[1mEpoch: [63/70], Progress: 78.49 %, Steps: 5840, Current Learning Rate: 0.0008178, \u001b[91mTrain Loss: 0.188\n",
      "\u001b[0m\u001b[1mEpoch: [63/70], Progress: 79.57 %, Steps: 5841, Current Learning Rate: 0.0008177, \u001b[91mTrain Loss: 0.211\n",
      "\u001b[0m\u001b[1mEpoch: [63/70], Progress: 80.65 %, Steps: 5842, Current Learning Rate: 0.0008176, \u001b[96mTrain Loss: 0.195\n",
      "\u001b[0m\u001b[1mEpoch: [63/70], Progress: 81.72 %, Steps: 5843, Current Learning Rate: 0.0008176, \u001b[91mTrain Loss: 0.246\n",
      "\u001b[0m\u001b[1mEpoch: [63/70], Progress: 82.80 %, Steps: 5844, Current Learning Rate: 0.0008175, \u001b[96mTrain Loss: 0.155\n",
      "\u001b[0m\u001b[1mEpoch: [63/70], Progress: 83.87 %, Steps: 5845, Current Learning Rate: 0.0008174, \u001b[91mTrain Loss: 0.157\n",
      "\u001b[0m\u001b[1mEpoch: [63/70], Progress: 84.95 %, Steps: 5846, Current Learning Rate: 0.0008174, \u001b[91mTrain Loss: 0.201\n",
      "\u001b[0m\u001b[1mEpoch: [63/70], Progress: 86.02 %, Steps: 5847, Current Learning Rate: 0.0008173, \u001b[91mTrain Loss: 0.211\n",
      "\u001b[0m\u001b[1mEpoch: [63/70], Progress: 87.10 %, Steps: 5848, Current Learning Rate: 0.0008172, \u001b[96mTrain Loss: 0.207\n",
      "\u001b[0m\u001b[1mEpoch: [63/70], Progress: 88.17 %, Steps: 5849, Current Learning Rate: 0.0008172, \u001b[96mTrain Loss: 0.161\n",
      "\u001b[0m\u001b[1mEpoch: [63/70], Progress: 89.25 %, Steps: 5850, Current Learning Rate: 0.0008171, \u001b[91mTrain Loss: 0.176\n",
      "\u001b[0m\u001b[1mEpoch: [63/70], Progress: 90.32 %, Steps: 5851, Current Learning Rate: 0.0008170, \u001b[91mTrain Loss: 0.199\n",
      "\u001b[0m\u001b[1mEpoch: [63/70], Progress: 91.40 %, Steps: 5852, Current Learning Rate: 0.0008169, \u001b[96mTrain Loss: 0.160\n",
      "\u001b[0m\u001b[1mEpoch: [63/70], Progress: 92.47 %, Steps: 5853, Current Learning Rate: 0.0008169, \u001b[96mTrain Loss: 0.138\n",
      "\u001b[0m\u001b[1mEpoch: [63/70], Progress: 93.55 %, Steps: 5854, Current Learning Rate: 0.0008168, \u001b[91mTrain Loss: 0.238\n",
      "\u001b[0m\u001b[1mEpoch: [63/70], Progress: 94.62 %, Steps: 5855, Current Learning Rate: 0.0008167, \u001b[96mTrain Loss: 0.182\n",
      "\u001b[0m\u001b[1mEpoch: [63/70], Progress: 95.70 %, Steps: 5856, Current Learning Rate: 0.0008167, \u001b[91mTrain Loss: 0.185\n",
      "\u001b[0m\u001b[1mEpoch: [63/70], Progress: 96.77 %, Steps: 5857, Current Learning Rate: 0.0008166, \u001b[91mTrain Loss: 0.215\n",
      "\u001b[0m\u001b[1mEpoch: [63/70], Progress: 97.85 %, Steps: 5858, Current Learning Rate: 0.0008165, \u001b[96mTrain Loss: 0.214\n",
      "\u001b[0m\u001b[1mEpoch: [63/70], Progress: 98.92 %, Steps: 5859, Current Learning Rate: 0.0008165, \u001b[96mTrain Loss: 0.176\n",
      "\u001b[1m\u001b[93mComputing Validation Loss...\n",
      "\u001b[1mEpoch 63 Completed! Average Train Loss: 0.164, Average Validation Loss: 0.027\n",
      "Best Model saved in best_model/best_model.pt.\n",
      "\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m= Epoch [64/70] ==========================================\n",
      "\u001b[0m\u001b[1mEpoch: [64/70], Progress: 0.00 %, Steps: 5860, Current Learning Rate: 0.0008164, \u001b[91mTrain Loss: 0.147\n",
      "\u001b[0m\u001b[1mEpoch: [64/70], Progress: 1.08 %, Steps: 5861, Current Learning Rate: 0.0008163, \u001b[96mTrain Loss: 0.104\n",
      "\u001b[0m\u001b[1mEpoch: [64/70], Progress: 2.15 %, Steps: 5862, Current Learning Rate: 0.0008162, \u001b[91mTrain Loss: 0.127\n",
      "\u001b[0m\u001b[1mEpoch: [64/70], Progress: 3.23 %, Steps: 5863, Current Learning Rate: 0.0008162, \u001b[96mTrain Loss: 0.089\n",
      "\u001b[0m\u001b[1mEpoch: [64/70], Progress: 4.30 %, Steps: 5864, Current Learning Rate: 0.0008161, \u001b[91mTrain Loss: 0.104\n",
      "\u001b[0m\u001b[1mEpoch: [64/70], Progress: 5.38 %, Steps: 5865, Current Learning Rate: 0.0008160, \u001b[91mTrain Loss: 0.139\n",
      "\u001b[0m\u001b[1mEpoch: [64/70], Progress: 6.45 %, Steps: 5866, Current Learning Rate: 0.0008160, \u001b[96mTrain Loss: 0.095\n",
      "\u001b[0m\u001b[1mEpoch: [64/70], Progress: 7.53 %, Steps: 5867, Current Learning Rate: 0.0008159, \u001b[91mTrain Loss: 0.115\n",
      "\u001b[0m\u001b[1mEpoch: [64/70], Progress: 8.60 %, Steps: 5868, Current Learning Rate: 0.0008158, \u001b[96mTrain Loss: 0.099\n",
      "\u001b[0m\u001b[1mEpoch: [64/70], Progress: 9.68 %, Steps: 5869, Current Learning Rate: 0.0008158, \u001b[91mTrain Loss: 0.124\n",
      "\u001b[0m\u001b[1mEpoch: [64/70], Progress: 10.75 %, Steps: 5870, Current Learning Rate: 0.0008157, \u001b[96mTrain Loss: 0.106\n",
      "\u001b[0m\u001b[1mEpoch: [64/70], Progress: 11.83 %, Steps: 5871, Current Learning Rate: 0.0008156, \u001b[91mTrain Loss: 0.123\n",
      "\u001b[0m\u001b[1mEpoch: [64/70], Progress: 12.90 %, Steps: 5872, Current Learning Rate: 0.0008155, \u001b[96mTrain Loss: 0.113\n",
      "\u001b[0m\u001b[1mEpoch: [64/70], Progress: 13.98 %, Steps: 5873, Current Learning Rate: 0.0008155, \u001b[91mTrain Loss: 0.118\n",
      "\u001b[0m\u001b[1mEpoch: [64/70], Progress: 15.05 %, Steps: 5874, Current Learning Rate: 0.0008154, \u001b[91mTrain Loss: 0.137\n",
      "\u001b[0m\u001b[1mEpoch: [64/70], Progress: 16.13 %, Steps: 5875, Current Learning Rate: 0.0008153, \u001b[96mTrain Loss: 0.112\n",
      "\u001b[0m\u001b[1mEpoch: [64/70], Progress: 17.20 %, Steps: 5876, Current Learning Rate: 0.0008153, \u001b[91mTrain Loss: 0.162\n",
      "\u001b[0m\u001b[1mEpoch: [64/70], Progress: 18.28 %, Steps: 5877, Current Learning Rate: 0.0008152, \u001b[96mTrain Loss: 0.158\n",
      "\u001b[0m\u001b[1mEpoch: [64/70], Progress: 19.35 %, Steps: 5878, Current Learning Rate: 0.0008151, \u001b[91mTrain Loss: 0.199\n",
      "\u001b[0m\u001b[1mEpoch: [64/70], Progress: 20.43 %, Steps: 5879, Current Learning Rate: 0.0008151, \u001b[96mTrain Loss: 0.182\n",
      "\u001b[0m\u001b[1mEpoch: [64/70], Progress: 21.51 %, Steps: 5880, Current Learning Rate: 0.0008150, \u001b[91mTrain Loss: 0.182\n",
      "\u001b[0m\u001b[1mEpoch: [64/70], Progress: 22.58 %, Steps: 5881, Current Learning Rate: 0.0008149, \u001b[96mTrain Loss: 0.140\n",
      "\u001b[0m\u001b[1mEpoch: [64/70], Progress: 23.66 %, Steps: 5882, Current Learning Rate: 0.0008149, \u001b[91mTrain Loss: 0.143\n",
      "\u001b[0m\u001b[1mEpoch: [64/70], Progress: 24.73 %, Steps: 5883, Current Learning Rate: 0.0008148, \u001b[96mTrain Loss: 0.137\n",
      "\u001b[0m\u001b[1mEpoch: [64/70], Progress: 25.81 %, Steps: 5884, Current Learning Rate: 0.0008147, \u001b[96mTrain Loss: 0.120\n",
      "\u001b[0m\u001b[1mEpoch: [64/70], Progress: 26.88 %, Steps: 5885, Current Learning Rate: 0.0008146, \u001b[96mTrain Loss: 0.092\n",
      "\u001b[0m\u001b[1mEpoch: [64/70], Progress: 27.96 %, Steps: 5886, Current Learning Rate: 0.0008146, \u001b[91mTrain Loss: 0.129\n",
      "\u001b[0m\u001b[1mEpoch: [64/70], Progress: 29.03 %, Steps: 5887, Current Learning Rate: 0.0008145, \u001b[96mTrain Loss: 0.118\n",
      "\u001b[0m\u001b[1mEpoch: [64/70], Progress: 30.11 %, Steps: 5888, Current Learning Rate: 0.0008144, \u001b[91mTrain Loss: 0.128\n",
      "\u001b[0m\u001b[1mEpoch: [64/70], Progress: 31.18 %, Steps: 5889, Current Learning Rate: 0.0008144, \u001b[91mTrain Loss: 0.160\n",
      "\u001b[0m\u001b[1mEpoch: [64/70], Progress: 32.26 %, Steps: 5890, Current Learning Rate: 0.0008143, \u001b[96mTrain Loss: 0.090\n",
      "\u001b[0m\u001b[1mEpoch: [64/70], Progress: 33.33 %, Steps: 5891, Current Learning Rate: 0.0008142, \u001b[91mTrain Loss: 0.136\n",
      "\u001b[0m\u001b[1mEpoch: [64/70], Progress: 34.41 %, Steps: 5892, Current Learning Rate: 0.0008142, \u001b[96mTrain Loss: 0.113\n",
      "\u001b[0m\u001b[1mEpoch: [64/70], Progress: 35.48 %, Steps: 5893, Current Learning Rate: 0.0008141, \u001b[91mTrain Loss: 0.181\n",
      "\u001b[0m\u001b[1mEpoch: [64/70], Progress: 36.56 %, Steps: 5894, Current Learning Rate: 0.0008140, \u001b[96mTrain Loss: 0.152\n",
      "\u001b[0m\u001b[1mEpoch: [64/70], Progress: 37.63 %, Steps: 5895, Current Learning Rate: 0.0008140, \u001b[96mTrain Loss: 0.149\n",
      "\u001b[0m\u001b[1mEpoch: [64/70], Progress: 38.71 %, Steps: 5896, Current Learning Rate: 0.0008139, \u001b[96mTrain Loss: 0.124\n",
      "\u001b[0m\u001b[1mEpoch: [64/70], Progress: 39.78 %, Steps: 5897, Current Learning Rate: 0.0008138, \u001b[91mTrain Loss: 0.138\n",
      "\u001b[0m\u001b[1mEpoch: [64/70], Progress: 40.86 %, Steps: 5898, Current Learning Rate: 0.0008137, \u001b[96mTrain Loss: 0.106\n",
      "\u001b[0m\u001b[1mEpoch: [64/70], Progress: 41.94 %, Steps: 5899, Current Learning Rate: 0.0008137, \u001b[91mTrain Loss: 0.122\n",
      "\u001b[0m\u001b[1mEpoch: [64/70], Progress: 43.01 %, Steps: 5900, Current Learning Rate: 0.0008136, \u001b[91mTrain Loss: 0.190\n",
      "\u001b[0m\u001b[1mEpoch: [64/70], Progress: 44.09 %, Steps: 5901, Current Learning Rate: 0.0008135, \u001b[96mTrain Loss: 0.150\n",
      "\u001b[0m\u001b[1mEpoch: [64/70], Progress: 45.16 %, Steps: 5902, Current Learning Rate: 0.0008135, \u001b[91mTrain Loss: 0.153\n",
      "\u001b[0m\u001b[1mEpoch: [64/70], Progress: 46.24 %, Steps: 5903, Current Learning Rate: 0.0008134, \u001b[96mTrain Loss: 0.111\n",
      "\u001b[0m\u001b[1mEpoch: [64/70], Progress: 47.31 %, Steps: 5904, Current Learning Rate: 0.0008133, \u001b[91mTrain Loss: 0.164\n",
      "\u001b[0m\u001b[1mEpoch: [64/70], Progress: 48.39 %, Steps: 5905, Current Learning Rate: 0.0008133, \u001b[96mTrain Loss: 0.135\n",
      "\u001b[0m\u001b[1mEpoch: [64/70], Progress: 49.46 %, Steps: 5906, Current Learning Rate: 0.0008132, \u001b[91mTrain Loss: 0.155\n",
      "\u001b[0m\u001b[1mEpoch: [64/70], Progress: 50.54 %, Steps: 5907, Current Learning Rate: 0.0008131, \u001b[96mTrain Loss: 0.121\n",
      "\u001b[0m\u001b[1mEpoch: [64/70], Progress: 51.61 %, Steps: 5908, Current Learning Rate: 0.0008131, \u001b[91mTrain Loss: 0.125\n",
      "\u001b[0m\u001b[1mEpoch: [64/70], Progress: 52.69 %, Steps: 5909, Current Learning Rate: 0.0008130, \u001b[96mTrain Loss: 0.120\n",
      "\u001b[0m\u001b[1mEpoch: [64/70], Progress: 53.76 %, Steps: 5910, Current Learning Rate: 0.0008129, \u001b[91mTrain Loss: 0.156\n",
      "\u001b[0m\u001b[1mEpoch: [64/70], Progress: 54.84 %, Steps: 5911, Current Learning Rate: 0.0008129, \u001b[91mTrain Loss: 0.203\n",
      "\u001b[0m\u001b[1mEpoch: [64/70], Progress: 55.91 %, Steps: 5912, Current Learning Rate: 0.0008128, \u001b[96mTrain Loss: 0.158\n",
      "\u001b[0m\u001b[1mEpoch: [64/70], Progress: 56.99 %, Steps: 5913, Current Learning Rate: 0.0008127, \u001b[96mTrain Loss: 0.132\n",
      "\u001b[0m\u001b[1mEpoch: [64/70], Progress: 58.06 %, Steps: 5914, Current Learning Rate: 0.0008126, \u001b[91mTrain Loss: 0.209\n",
      "\u001b[0m\u001b[1mEpoch: [64/70], Progress: 59.14 %, Steps: 5915, Current Learning Rate: 0.0008126, \u001b[96mTrain Loss: 0.190\n",
      "\u001b[0m\u001b[1mEpoch: [64/70], Progress: 60.22 %, Steps: 5916, Current Learning Rate: 0.0008125, \u001b[96mTrain Loss: 0.166\n",
      "\u001b[0m\u001b[1mEpoch: [64/70], Progress: 61.29 %, Steps: 5917, Current Learning Rate: 0.0008124, \u001b[91mTrain Loss: 0.177\n",
      "\u001b[0m\u001b[1mEpoch: [64/70], Progress: 62.37 %, Steps: 5918, Current Learning Rate: 0.0008124, \u001b[96mTrain Loss: 0.122\n",
      "\u001b[0m\u001b[1mEpoch: [64/70], Progress: 63.44 %, Steps: 5919, Current Learning Rate: 0.0008123, \u001b[91mTrain Loss: 0.166\n",
      "\u001b[0m\u001b[1mEpoch: [64/70], Progress: 64.52 %, Steps: 5920, Current Learning Rate: 0.0008122, \u001b[96mTrain Loss: 0.141\n",
      "\u001b[0m\u001b[1mEpoch: [64/70], Progress: 65.59 %, Steps: 5921, Current Learning Rate: 0.0008122, \u001b[96mTrain Loss: 0.127\n",
      "\u001b[0m\u001b[1mEpoch: [64/70], Progress: 66.67 %, Steps: 5922, Current Learning Rate: 0.0008121, \u001b[91mTrain Loss: 0.184\n",
      "\u001b[0m\u001b[1mEpoch: [64/70], Progress: 67.74 %, Steps: 5923, Current Learning Rate: 0.0008120, \u001b[91mTrain Loss: 0.189\n",
      "\u001b[0m\u001b[1mEpoch: [64/70], Progress: 68.82 %, Steps: 5924, Current Learning Rate: 0.0008120, \u001b[96mTrain Loss: 0.153\n",
      "\u001b[0m\u001b[1mEpoch: [64/70], Progress: 69.89 %, Steps: 5925, Current Learning Rate: 0.0008119, \u001b[91mTrain Loss: 0.193\n",
      "\u001b[0m\u001b[1mEpoch: [64/70], Progress: 70.97 %, Steps: 5926, Current Learning Rate: 0.0008118, \u001b[96mTrain Loss: 0.175\n",
      "\u001b[0m\u001b[1mEpoch: [64/70], Progress: 72.04 %, Steps: 5927, Current Learning Rate: 0.0008118, \u001b[91mTrain Loss: 0.192\n",
      "\u001b[0m\u001b[1mEpoch: [64/70], Progress: 73.12 %, Steps: 5928, Current Learning Rate: 0.0008117, \u001b[91mTrain Loss: 0.205\n",
      "\u001b[0m\u001b[1mEpoch: [64/70], Progress: 74.19 %, Steps: 5929, Current Learning Rate: 0.0008116, \u001b[96mTrain Loss: 0.176\n",
      "\u001b[0m\u001b[1mEpoch: [64/70], Progress: 75.27 %, Steps: 5930, Current Learning Rate: 0.0008116, \u001b[96mTrain Loss: 0.145\n",
      "\u001b[0m\u001b[1mEpoch: [64/70], Progress: 76.34 %, Steps: 5931, Current Learning Rate: 0.0008115, \u001b[91mTrain Loss: 0.176\n",
      "\u001b[0m\u001b[1mEpoch: [64/70], Progress: 77.42 %, Steps: 5932, Current Learning Rate: 0.0008114, \u001b[91mTrain Loss: 0.195\n",
      "\u001b[0m\u001b[1mEpoch: [64/70], Progress: 78.49 %, Steps: 5933, Current Learning Rate: 0.0008113, \u001b[96mTrain Loss: 0.140\n",
      "\u001b[0m\u001b[1mEpoch: [64/70], Progress: 79.57 %, Steps: 5934, Current Learning Rate: 0.0008113, \u001b[91mTrain Loss: 0.161\n",
      "\u001b[0m\u001b[1mEpoch: [64/70], Progress: 80.65 %, Steps: 5935, Current Learning Rate: 0.0008112, \u001b[91mTrain Loss: 0.197\n",
      "\u001b[0m\u001b[1mEpoch: [64/70], Progress: 81.72 %, Steps: 5936, Current Learning Rate: 0.0008111, \u001b[96mTrain Loss: 0.179\n",
      "\u001b[0m\u001b[1mEpoch: [64/70], Progress: 82.80 %, Steps: 5937, Current Learning Rate: 0.0008111, \u001b[96mTrain Loss: 0.161\n",
      "\u001b[0m\u001b[1mEpoch: [64/70], Progress: 83.87 %, Steps: 5938, Current Learning Rate: 0.0008110, \u001b[91mTrain Loss: 0.229\n",
      "\u001b[0m\u001b[1mEpoch: [64/70], Progress: 84.95 %, Steps: 5939, Current Learning Rate: 0.0008109, \u001b[96mTrain Loss: 0.137\n",
      "\u001b[0m\u001b[1mEpoch: [64/70], Progress: 86.02 %, Steps: 5940, Current Learning Rate: 0.0008109, \u001b[91mTrain Loss: 0.192\n",
      "\u001b[0m\u001b[1mEpoch: [64/70], Progress: 87.10 %, Steps: 5941, Current Learning Rate: 0.0008108, \u001b[96mTrain Loss: 0.159\n",
      "\u001b[0m\u001b[1mEpoch: [64/70], Progress: 88.17 %, Steps: 5942, Current Learning Rate: 0.0008107, \u001b[91mTrain Loss: 0.166\n",
      "\u001b[0m\u001b[1mEpoch: [64/70], Progress: 89.25 %, Steps: 5943, Current Learning Rate: 0.0008107, \u001b[96mTrain Loss: 0.148\n",
      "\u001b[0m\u001b[1mEpoch: [64/70], Progress: 90.32 %, Steps: 5944, Current Learning Rate: 0.0008106, \u001b[91mTrain Loss: 0.173\n",
      "\u001b[0m\u001b[1mEpoch: [64/70], Progress: 91.40 %, Steps: 5945, Current Learning Rate: 0.0008105, \u001b[91mTrain Loss: 0.205\n",
      "\u001b[0m\u001b[1mEpoch: [64/70], Progress: 92.47 %, Steps: 5946, Current Learning Rate: 0.0008105, \u001b[96mTrain Loss: 0.173\n",
      "\u001b[0m\u001b[1mEpoch: [64/70], Progress: 93.55 %, Steps: 5947, Current Learning Rate: 0.0008104, \u001b[96mTrain Loss: 0.154\n",
      "\u001b[0m\u001b[1mEpoch: [64/70], Progress: 94.62 %, Steps: 5948, Current Learning Rate: 0.0008103, \u001b[96mTrain Loss: 0.139\n",
      "\u001b[0m\u001b[1mEpoch: [64/70], Progress: 95.70 %, Steps: 5949, Current Learning Rate: 0.0008103, \u001b[91mTrain Loss: 0.177\n",
      "\u001b[0m\u001b[1mEpoch: [64/70], Progress: 96.77 %, Steps: 5950, Current Learning Rate: 0.0008102, \u001b[96mTrain Loss: 0.165\n",
      "\u001b[0m\u001b[1mEpoch: [64/70], Progress: 97.85 %, Steps: 5951, Current Learning Rate: 0.0008101, \u001b[96mTrain Loss: 0.122\n",
      "\u001b[0m\u001b[1mEpoch: [64/70], Progress: 98.92 %, Steps: 5952, Current Learning Rate: 0.0008101, \u001b[96mTrain Loss: 0.114\n",
      "\u001b[1m\u001b[93mComputing Validation Loss...\n",
      "\u001b[1mEpoch 64 Completed! Average Train Loss: 0.149, Average Validation Loss: 0.026\n",
      "Best Model saved in best_model/best_model.pt.\n",
      "\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m= Epoch [65/70] ==========================================\n",
      "\u001b[0m\u001b[1mEpoch: [65/70], Progress: 0.00 %, Steps: 5953, Current Learning Rate: 0.0008100, \u001b[91mTrain Loss: 0.113\n",
      "\u001b[0m\u001b[1mEpoch: [65/70], Progress: 1.08 %, Steps: 5954, Current Learning Rate: 0.0008099, \u001b[91mTrain Loss: 0.129\n",
      "\u001b[0m\u001b[1mEpoch: [65/70], Progress: 2.15 %, Steps: 5955, Current Learning Rate: 0.0008098, \u001b[96mTrain Loss: 0.113\n",
      "\u001b[0m\u001b[1mEpoch: [65/70], Progress: 3.23 %, Steps: 5956, Current Learning Rate: 0.0008098, \u001b[96mTrain Loss: 0.098\n",
      "\u001b[0m\u001b[1mEpoch: [65/70], Progress: 4.30 %, Steps: 5957, Current Learning Rate: 0.0008097, \u001b[91mTrain Loss: 0.116\n",
      "\u001b[0m\u001b[1mEpoch: [65/70], Progress: 5.38 %, Steps: 5958, Current Learning Rate: 0.0008096, \u001b[96mTrain Loss: 0.113\n",
      "\u001b[0m\u001b[1mEpoch: [65/70], Progress: 6.45 %, Steps: 5959, Current Learning Rate: 0.0008096, \u001b[96mTrain Loss: 0.110\n",
      "\u001b[0m\u001b[1mEpoch: [65/70], Progress: 7.53 %, Steps: 5960, Current Learning Rate: 0.0008095, \u001b[91mTrain Loss: 0.119\n",
      "\u001b[0m\u001b[1mEpoch: [65/70], Progress: 8.60 %, Steps: 5961, Current Learning Rate: 0.0008094, \u001b[96mTrain Loss: 0.112\n",
      "\u001b[0m\u001b[1mEpoch: [65/70], Progress: 9.68 %, Steps: 5962, Current Learning Rate: 0.0008094, \u001b[96mTrain Loss: 0.087\n",
      "\u001b[0m\u001b[1mEpoch: [65/70], Progress: 10.75 %, Steps: 5963, Current Learning Rate: 0.0008093, \u001b[91mTrain Loss: 0.119\n",
      "\u001b[0m\u001b[1mEpoch: [65/70], Progress: 11.83 %, Steps: 5964, Current Learning Rate: 0.0008092, \u001b[96mTrain Loss: 0.077\n",
      "\u001b[0m\u001b[1mEpoch: [65/70], Progress: 12.90 %, Steps: 5965, Current Learning Rate: 0.0008092, \u001b[91mTrain Loss: 0.102\n",
      "\u001b[0m\u001b[1mEpoch: [65/70], Progress: 13.98 %, Steps: 5966, Current Learning Rate: 0.0008091, \u001b[96mTrain Loss: 0.098\n",
      "\u001b[0m\u001b[1mEpoch: [65/70], Progress: 15.05 %, Steps: 5967, Current Learning Rate: 0.0008090, \u001b[96mTrain Loss: 0.087\n",
      "\u001b[0m\u001b[1mEpoch: [65/70], Progress: 16.13 %, Steps: 5968, Current Learning Rate: 0.0008090, \u001b[91mTrain Loss: 0.108\n",
      "\u001b[0m\u001b[1mEpoch: [65/70], Progress: 17.20 %, Steps: 5969, Current Learning Rate: 0.0008089, \u001b[96mTrain Loss: 0.098\n",
      "\u001b[0m\u001b[1mEpoch: [65/70], Progress: 18.28 %, Steps: 5970, Current Learning Rate: 0.0008088, \u001b[91mTrain Loss: 0.125\n",
      "\u001b[0m\u001b[1mEpoch: [65/70], Progress: 19.35 %, Steps: 5971, Current Learning Rate: 0.0008088, \u001b[91mTrain Loss: 0.160\n",
      "\u001b[0m\u001b[1mEpoch: [65/70], Progress: 20.43 %, Steps: 5972, Current Learning Rate: 0.0008087, \u001b[96mTrain Loss: 0.125\n",
      "\u001b[0m\u001b[1mEpoch: [65/70], Progress: 21.51 %, Steps: 5973, Current Learning Rate: 0.0008086, \u001b[96mTrain Loss: 0.113\n",
      "\u001b[0m\u001b[1mEpoch: [65/70], Progress: 22.58 %, Steps: 5974, Current Learning Rate: 0.0008086, \u001b[91mTrain Loss: 0.153\n",
      "\u001b[0m\u001b[1mEpoch: [65/70], Progress: 23.66 %, Steps: 5975, Current Learning Rate: 0.0008085, \u001b[91mTrain Loss: 0.166\n",
      "\u001b[0m\u001b[1mEpoch: [65/70], Progress: 24.73 %, Steps: 5976, Current Learning Rate: 0.0008084, \u001b[96mTrain Loss: 0.153\n",
      "\u001b[0m\u001b[1mEpoch: [65/70], Progress: 25.81 %, Steps: 5977, Current Learning Rate: 0.0008084, \u001b[91mTrain Loss: 0.158\n",
      "\u001b[0m\u001b[1mEpoch: [65/70], Progress: 26.88 %, Steps: 5978, Current Learning Rate: 0.0008083, \u001b[96mTrain Loss: 0.152\n",
      "\u001b[0m\u001b[1mEpoch: [65/70], Progress: 27.96 %, Steps: 5979, Current Learning Rate: 0.0008082, \u001b[96mTrain Loss: 0.131\n",
      "\u001b[0m\u001b[1mEpoch: [65/70], Progress: 29.03 %, Steps: 5980, Current Learning Rate: 0.0008082, \u001b[91mTrain Loss: 0.164\n",
      "\u001b[0m\u001b[1mEpoch: [65/70], Progress: 30.11 %, Steps: 5981, Current Learning Rate: 0.0008081, \u001b[96mTrain Loss: 0.139\n",
      "\u001b[0m\u001b[1mEpoch: [65/70], Progress: 31.18 %, Steps: 5982, Current Learning Rate: 0.0008080, \u001b[91mTrain Loss: 0.146\n",
      "\u001b[0m\u001b[1mEpoch: [65/70], Progress: 32.26 %, Steps: 5983, Current Learning Rate: 0.0008079, \u001b[96mTrain Loss: 0.142\n",
      "\u001b[0m\u001b[1mEpoch: [65/70], Progress: 33.33 %, Steps: 5984, Current Learning Rate: 0.0008079, \u001b[91mTrain Loss: 0.149\n",
      "\u001b[0m\u001b[1mEpoch: [65/70], Progress: 34.41 %, Steps: 5985, Current Learning Rate: 0.0008078, \u001b[91mTrain Loss: 0.152\n",
      "\u001b[0m\u001b[1mEpoch: [65/70], Progress: 35.48 %, Steps: 5986, Current Learning Rate: 0.0008077, \u001b[91mTrain Loss: 0.174\n",
      "\u001b[0m\u001b[1mEpoch: [65/70], Progress: 36.56 %, Steps: 5987, Current Learning Rate: 0.0008077, \u001b[96mTrain Loss: 0.136\n",
      "\u001b[0m\u001b[1mEpoch: [65/70], Progress: 37.63 %, Steps: 5988, Current Learning Rate: 0.0008076, \u001b[91mTrain Loss: 0.185\n",
      "\u001b[0m\u001b[1mEpoch: [65/70], Progress: 38.71 %, Steps: 5989, Current Learning Rate: 0.0008075, \u001b[96mTrain Loss: 0.177\n",
      "\u001b[0m\u001b[1mEpoch: [65/70], Progress: 39.78 %, Steps: 5990, Current Learning Rate: 0.0008075, \u001b[96mTrain Loss: 0.125\n",
      "\u001b[0m\u001b[1mEpoch: [65/70], Progress: 40.86 %, Steps: 5991, Current Learning Rate: 0.0008074, \u001b[91mTrain Loss: 0.154\n",
      "\u001b[0m\u001b[1mEpoch: [65/70], Progress: 41.94 %, Steps: 5992, Current Learning Rate: 0.0008073, \u001b[91mTrain Loss: 0.157\n",
      "\u001b[0m\u001b[1mEpoch: [65/70], Progress: 43.01 %, Steps: 5993, Current Learning Rate: 0.0008073, \u001b[96mTrain Loss: 0.088\n",
      "\u001b[0m\u001b[1mEpoch: [65/70], Progress: 44.09 %, Steps: 5994, Current Learning Rate: 0.0008072, \u001b[91mTrain Loss: 0.139\n",
      "\u001b[0m\u001b[1mEpoch: [65/70], Progress: 45.16 %, Steps: 5995, Current Learning Rate: 0.0008071, \u001b[96mTrain Loss: 0.139\n",
      "\u001b[0m\u001b[1mEpoch: [65/70], Progress: 46.24 %, Steps: 5996, Current Learning Rate: 0.0008071, \u001b[96mTrain Loss: 0.115\n",
      "\u001b[0m\u001b[1mEpoch: [65/70], Progress: 47.31 %, Steps: 5997, Current Learning Rate: 0.0008070, \u001b[91mTrain Loss: 0.158\n",
      "\u001b[0m\u001b[1mEpoch: [65/70], Progress: 48.39 %, Steps: 5998, Current Learning Rate: 0.0008069, \u001b[96mTrain Loss: 0.121\n",
      "\u001b[0m\u001b[1mEpoch: [65/70], Progress: 49.46 %, Steps: 5999, Current Learning Rate: 0.0008069, \u001b[91mTrain Loss: 0.185\n",
      "\u001b[0m\u001b[1mEpoch: [65/70], Progress: 50.54 %, Steps: 6000, Current Learning Rate: 0.0008068, \u001b[96mTrain Loss: 0.128\n",
      "\u001b[0m\u001b[1mEpoch: [65/70], Progress: 51.61 %, Steps: 6001, Current Learning Rate: 0.0008067, \u001b[91mTrain Loss: 0.167\n",
      "\u001b[0m\u001b[1mEpoch: [65/70], Progress: 52.69 %, Steps: 6002, Current Learning Rate: 0.0008067, \u001b[96mTrain Loss: 0.109\n",
      "\u001b[0m\u001b[1mEpoch: [65/70], Progress: 53.76 %, Steps: 6003, Current Learning Rate: 0.0008066, \u001b[91mTrain Loss: 0.112\n",
      "\u001b[0m\u001b[1mEpoch: [65/70], Progress: 54.84 %, Steps: 6004, Current Learning Rate: 0.0008065, \u001b[91mTrain Loss: 0.128\n",
      "\u001b[0m\u001b[1mEpoch: [65/70], Progress: 55.91 %, Steps: 6005, Current Learning Rate: 0.0008065, \u001b[91mTrain Loss: 0.160\n",
      "\u001b[0m\u001b[1mEpoch: [65/70], Progress: 56.99 %, Steps: 6006, Current Learning Rate: 0.0008064, \u001b[91mTrain Loss: 0.222\n",
      "\u001b[0m\u001b[1mEpoch: [65/70], Progress: 58.06 %, Steps: 6007, Current Learning Rate: 0.0008063, \u001b[96mTrain Loss: 0.144\n",
      "\u001b[0m\u001b[1mEpoch: [65/70], Progress: 59.14 %, Steps: 6008, Current Learning Rate: 0.0008063, \u001b[91mTrain Loss: 0.201\n",
      "\u001b[0m\u001b[1mEpoch: [65/70], Progress: 60.22 %, Steps: 6009, Current Learning Rate: 0.0008062, \u001b[96mTrain Loss: 0.142\n",
      "\u001b[0m\u001b[1mEpoch: [65/70], Progress: 61.29 %, Steps: 6010, Current Learning Rate: 0.0008061, \u001b[91mTrain Loss: 0.155\n",
      "\u001b[0m\u001b[1mEpoch: [65/70], Progress: 62.37 %, Steps: 6011, Current Learning Rate: 0.0008061, \u001b[96mTrain Loss: 0.151\n",
      "\u001b[0m\u001b[1mEpoch: [65/70], Progress: 63.44 %, Steps: 6012, Current Learning Rate: 0.0008060, \u001b[91mTrain Loss: 0.152\n",
      "\u001b[0m\u001b[1mEpoch: [65/70], Progress: 64.52 %, Steps: 6013, Current Learning Rate: 0.0008059, \u001b[91mTrain Loss: 0.178\n",
      "\u001b[0m\u001b[1mEpoch: [65/70], Progress: 65.59 %, Steps: 6014, Current Learning Rate: 0.0008059, \u001b[96mTrain Loss: 0.158\n",
      "\u001b[0m\u001b[1mEpoch: [65/70], Progress: 66.67 %, Steps: 6015, Current Learning Rate: 0.0008058, \u001b[96mTrain Loss: 0.156\n",
      "\u001b[0m\u001b[1mEpoch: [65/70], Progress: 67.74 %, Steps: 6016, Current Learning Rate: 0.0008057, \u001b[96mTrain Loss: 0.149\n",
      "\u001b[0m\u001b[1mEpoch: [65/70], Progress: 68.82 %, Steps: 6017, Current Learning Rate: 0.0008057, \u001b[96mTrain Loss: 0.148\n",
      "\u001b[0m\u001b[1mEpoch: [65/70], Progress: 69.89 %, Steps: 6018, Current Learning Rate: 0.0008056, \u001b[96mTrain Loss: 0.141\n",
      "\u001b[0m\u001b[1mEpoch: [65/70], Progress: 70.97 %, Steps: 6019, Current Learning Rate: 0.0008055, \u001b[91mTrain Loss: 0.207\n",
      "\u001b[0m\u001b[1mEpoch: [65/70], Progress: 72.04 %, Steps: 6020, Current Learning Rate: 0.0008055, \u001b[96mTrain Loss: 0.135\n",
      "\u001b[0m\u001b[1mEpoch: [65/70], Progress: 73.12 %, Steps: 6021, Current Learning Rate: 0.0008054, \u001b[91mTrain Loss: 0.151\n",
      "\u001b[0m\u001b[1mEpoch: [65/70], Progress: 74.19 %, Steps: 6022, Current Learning Rate: 0.0008053, \u001b[91mTrain Loss: 0.189\n",
      "\u001b[0m\u001b[1mEpoch: [65/70], Progress: 75.27 %, Steps: 6023, Current Learning Rate: 0.0008053, \u001b[96mTrain Loss: 0.134\n",
      "\u001b[0m\u001b[1mEpoch: [65/70], Progress: 76.34 %, Steps: 6024, Current Learning Rate: 0.0008052, \u001b[91mTrain Loss: 0.164\n",
      "\u001b[0m\u001b[1mEpoch: [65/70], Progress: 77.42 %, Steps: 6025, Current Learning Rate: 0.0008051, \u001b[96mTrain Loss: 0.162\n",
      "\u001b[0m\u001b[1mEpoch: [65/70], Progress: 78.49 %, Steps: 6026, Current Learning Rate: 0.0008051, \u001b[91mTrain Loss: 0.163\n",
      "\u001b[0m\u001b[1mEpoch: [65/70], Progress: 79.57 %, Steps: 6027, Current Learning Rate: 0.0008050, \u001b[91mTrain Loss: 0.193\n",
      "\u001b[0m\u001b[1mEpoch: [65/70], Progress: 80.65 %, Steps: 6028, Current Learning Rate: 0.0008049, \u001b[96mTrain Loss: 0.178\n",
      "\u001b[0m\u001b[1mEpoch: [65/70], Progress: 81.72 %, Steps: 6029, Current Learning Rate: 0.0008049, \u001b[96mTrain Loss: 0.130\n",
      "\u001b[0m\u001b[1mEpoch: [65/70], Progress: 82.80 %, Steps: 6030, Current Learning Rate: 0.0008048, \u001b[96mTrain Loss: 0.127\n",
      "\u001b[0m\u001b[1mEpoch: [65/70], Progress: 83.87 %, Steps: 6031, Current Learning Rate: 0.0008047, \u001b[91mTrain Loss: 0.152\n",
      "\u001b[0m\u001b[1mEpoch: [65/70], Progress: 84.95 %, Steps: 6032, Current Learning Rate: 0.0008047, \u001b[91mTrain Loss: 0.164\n",
      "\u001b[0m\u001b[1mEpoch: [65/70], Progress: 86.02 %, Steps: 6033, Current Learning Rate: 0.0008046, \u001b[96mTrain Loss: 0.129\n",
      "\u001b[0m\u001b[1mEpoch: [65/70], Progress: 87.10 %, Steps: 6034, Current Learning Rate: 0.0008045, \u001b[91mTrain Loss: 0.194\n",
      "\u001b[0m\u001b[1mEpoch: [65/70], Progress: 88.17 %, Steps: 6035, Current Learning Rate: 0.0008045, \u001b[96mTrain Loss: 0.185\n",
      "\u001b[0m\u001b[1mEpoch: [65/70], Progress: 89.25 %, Steps: 6036, Current Learning Rate: 0.0008044, \u001b[96mTrain Loss: 0.126\n",
      "\u001b[0m\u001b[1mEpoch: [65/70], Progress: 90.32 %, Steps: 6037, Current Learning Rate: 0.0008043, \u001b[91mTrain Loss: 0.211\n",
      "\u001b[0m\u001b[1mEpoch: [65/70], Progress: 91.40 %, Steps: 6038, Current Learning Rate: 0.0008043, \u001b[96mTrain Loss: 0.178\n",
      "\u001b[0m\u001b[1mEpoch: [65/70], Progress: 92.47 %, Steps: 6039, Current Learning Rate: 0.0008042, \u001b[96mTrain Loss: 0.171\n",
      "\u001b[0m\u001b[1mEpoch: [65/70], Progress: 93.55 %, Steps: 6040, Current Learning Rate: 0.0008041, \u001b[96mTrain Loss: 0.158\n",
      "\u001b[0m\u001b[1mEpoch: [65/70], Progress: 94.62 %, Steps: 6041, Current Learning Rate: 0.0008041, \u001b[91mTrain Loss: 0.175\n",
      "\u001b[0m\u001b[1mEpoch: [65/70], Progress: 95.70 %, Steps: 6042, Current Learning Rate: 0.0008040, \u001b[96mTrain Loss: 0.110\n",
      "\u001b[0m\u001b[1mEpoch: [65/70], Progress: 96.77 %, Steps: 6043, Current Learning Rate: 0.0008039, \u001b[91mTrain Loss: 0.197\n",
      "\u001b[0m\u001b[1mEpoch: [65/70], Progress: 97.85 %, Steps: 6044, Current Learning Rate: 0.0008039, \u001b[96mTrain Loss: 0.157\n",
      "\u001b[0m\u001b[1mEpoch: [65/70], Progress: 98.92 %, Steps: 6045, Current Learning Rate: 0.0008038, \u001b[91mTrain Loss: 0.175\n",
      "\u001b[1m\u001b[93mComputing Validation Loss...\n",
      "\u001b[1mEpoch 65 Completed! Average Train Loss: 0.145, Average Validation Loss: 0.025\n",
      "Best Model saved in best_model/best_model.pt.\n",
      "\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m= Epoch [66/70] ==========================================\n",
      "\u001b[0m\u001b[1mEpoch: [66/70], Progress: 0.00 %, Steps: 6046, Current Learning Rate: 0.0008037, \u001b[91mTrain Loss: 0.129\n",
      "\u001b[0m\u001b[1mEpoch: [66/70], Progress: 1.08 %, Steps: 6047, Current Learning Rate: 0.0008037, \u001b[91mTrain Loss: 0.129\n",
      "\u001b[0m\u001b[1mEpoch: [66/70], Progress: 2.15 %, Steps: 6048, Current Learning Rate: 0.0008036, \u001b[96mTrain Loss: 0.129\n",
      "\u001b[0m\u001b[1mEpoch: [66/70], Progress: 3.23 %, Steps: 6049, Current Learning Rate: 0.0008035, \u001b[91mTrain Loss: 0.130\n",
      "\u001b[0m\u001b[1mEpoch: [66/70], Progress: 4.30 %, Steps: 6050, Current Learning Rate: 0.0008035, \u001b[96mTrain Loss: 0.115\n",
      "\u001b[0m\u001b[1mEpoch: [66/70], Progress: 5.38 %, Steps: 6051, Current Learning Rate: 0.0008034, \u001b[96mTrain Loss: 0.108\n",
      "\u001b[0m\u001b[1mEpoch: [66/70], Progress: 6.45 %, Steps: 6052, Current Learning Rate: 0.0008033, \u001b[91mTrain Loss: 0.111\n",
      "\u001b[0m\u001b[1mEpoch: [66/70], Progress: 7.53 %, Steps: 6053, Current Learning Rate: 0.0008033, \u001b[96mTrain Loss: 0.083\n",
      "\u001b[0m\u001b[1mEpoch: [66/70], Progress: 8.60 %, Steps: 6054, Current Learning Rate: 0.0008032, \u001b[91mTrain Loss: 0.138\n",
      "\u001b[0m\u001b[1mEpoch: [66/70], Progress: 9.68 %, Steps: 6055, Current Learning Rate: 0.0008031, \u001b[96mTrain Loss: 0.087\n",
      "\u001b[0m\u001b[1mEpoch: [66/70], Progress: 10.75 %, Steps: 6056, Current Learning Rate: 0.0008031, \u001b[91mTrain Loss: 0.099\n",
      "\u001b[0m\u001b[1mEpoch: [66/70], Progress: 11.83 %, Steps: 6057, Current Learning Rate: 0.0008030, \u001b[91mTrain Loss: 0.128\n",
      "\u001b[0m\u001b[1mEpoch: [66/70], Progress: 12.90 %, Steps: 6058, Current Learning Rate: 0.0008029, \u001b[91mTrain Loss: 0.162\n",
      "\u001b[0m\u001b[1mEpoch: [66/70], Progress: 13.98 %, Steps: 6059, Current Learning Rate: 0.0008029, \u001b[96mTrain Loss: 0.090\n",
      "\u001b[0m\u001b[1mEpoch: [66/70], Progress: 15.05 %, Steps: 6060, Current Learning Rate: 0.0008028, \u001b[91mTrain Loss: 0.111\n",
      "\u001b[0m\u001b[1mEpoch: [66/70], Progress: 16.13 %, Steps: 6061, Current Learning Rate: 0.0008027, \u001b[91mTrain Loss: 0.135\n",
      "\u001b[0m\u001b[1mEpoch: [66/70], Progress: 17.20 %, Steps: 6062, Current Learning Rate: 0.0008027, \u001b[96mTrain Loss: 0.097\n",
      "\u001b[0m\u001b[1mEpoch: [66/70], Progress: 18.28 %, Steps: 6063, Current Learning Rate: 0.0008026, \u001b[91mTrain Loss: 0.135\n",
      "\u001b[0m\u001b[1mEpoch: [66/70], Progress: 19.35 %, Steps: 6064, Current Learning Rate: 0.0008025, \u001b[96mTrain Loss: 0.111\n",
      "\u001b[0m\u001b[1mEpoch: [66/70], Progress: 20.43 %, Steps: 6065, Current Learning Rate: 0.0008025, \u001b[96mTrain Loss: 0.105\n",
      "\u001b[0m\u001b[1mEpoch: [66/70], Progress: 21.51 %, Steps: 6066, Current Learning Rate: 0.0008024, \u001b[91mTrain Loss: 0.130\n",
      "\u001b[0m\u001b[1mEpoch: [66/70], Progress: 22.58 %, Steps: 6067, Current Learning Rate: 0.0008023, \u001b[96mTrain Loss: 0.106\n",
      "\u001b[0m\u001b[1mEpoch: [66/70], Progress: 23.66 %, Steps: 6068, Current Learning Rate: 0.0008023, \u001b[91mTrain Loss: 0.135\n",
      "\u001b[0m\u001b[1mEpoch: [66/70], Progress: 24.73 %, Steps: 6069, Current Learning Rate: 0.0008022, \u001b[96mTrain Loss: 0.117\n",
      "\u001b[0m\u001b[1mEpoch: [66/70], Progress: 25.81 %, Steps: 6070, Current Learning Rate: 0.0008021, \u001b[96mTrain Loss: 0.111\n",
      "\u001b[0m\u001b[1mEpoch: [66/70], Progress: 26.88 %, Steps: 6071, Current Learning Rate: 0.0008021, \u001b[96mTrain Loss: 0.098\n",
      "\u001b[0m\u001b[1mEpoch: [66/70], Progress: 27.96 %, Steps: 6072, Current Learning Rate: 0.0008020, \u001b[91mTrain Loss: 0.118\n",
      "\u001b[0m\u001b[1mEpoch: [66/70], Progress: 29.03 %, Steps: 6073, Current Learning Rate: 0.0008019, \u001b[96mTrain Loss: 0.115\n",
      "\u001b[0m\u001b[1mEpoch: [66/70], Progress: 30.11 %, Steps: 6074, Current Learning Rate: 0.0008019, \u001b[91mTrain Loss: 0.128\n",
      "\u001b[0m\u001b[1mEpoch: [66/70], Progress: 31.18 %, Steps: 6075, Current Learning Rate: 0.0008018, \u001b[91mTrain Loss: 0.144\n",
      "\u001b[0m\u001b[1mEpoch: [66/70], Progress: 32.26 %, Steps: 6076, Current Learning Rate: 0.0008017, \u001b[96mTrain Loss: 0.100\n",
      "\u001b[0m\u001b[1mEpoch: [66/70], Progress: 33.33 %, Steps: 6077, Current Learning Rate: 0.0008017, \u001b[91mTrain Loss: 0.126\n",
      "\u001b[0m\u001b[1mEpoch: [66/70], Progress: 34.41 %, Steps: 6078, Current Learning Rate: 0.0008016, \u001b[91mTrain Loss: 0.136\n",
      "\u001b[0m\u001b[1mEpoch: [66/70], Progress: 35.48 %, Steps: 6079, Current Learning Rate: 0.0008015, \u001b[96mTrain Loss: 0.100\n",
      "\u001b[0m\u001b[1mEpoch: [66/70], Progress: 36.56 %, Steps: 6080, Current Learning Rate: 0.0008015, \u001b[91mTrain Loss: 0.136\n",
      "\u001b[0m\u001b[1mEpoch: [66/70], Progress: 37.63 %, Steps: 6081, Current Learning Rate: 0.0008014, \u001b[96mTrain Loss: 0.125\n",
      "\u001b[0m\u001b[1mEpoch: [66/70], Progress: 38.71 %, Steps: 6082, Current Learning Rate: 0.0008013, \u001b[91mTrain Loss: 0.149\n",
      "\u001b[0m\u001b[1mEpoch: [66/70], Progress: 39.78 %, Steps: 6083, Current Learning Rate: 0.0008013, \u001b[96mTrain Loss: 0.136\n",
      "\u001b[0m\u001b[1mEpoch: [66/70], Progress: 40.86 %, Steps: 6084, Current Learning Rate: 0.0008012, \u001b[96mTrain Loss: 0.130\n",
      "\u001b[0m\u001b[1mEpoch: [66/70], Progress: 41.94 %, Steps: 6085, Current Learning Rate: 0.0008012, \u001b[91mTrain Loss: 0.169\n",
      "\u001b[0m\u001b[1mEpoch: [66/70], Progress: 43.01 %, Steps: 6086, Current Learning Rate: 0.0008011, \u001b[96mTrain Loss: 0.101\n",
      "\u001b[0m\u001b[1mEpoch: [66/70], Progress: 44.09 %, Steps: 6087, Current Learning Rate: 0.0008010, \u001b[91mTrain Loss: 0.111\n",
      "\u001b[0m\u001b[1mEpoch: [66/70], Progress: 45.16 %, Steps: 6088, Current Learning Rate: 0.0008010, \u001b[91mTrain Loss: 0.174\n",
      "\u001b[0m\u001b[1mEpoch: [66/70], Progress: 46.24 %, Steps: 6089, Current Learning Rate: 0.0008009, \u001b[96mTrain Loss: 0.122\n",
      "\u001b[0m\u001b[1mEpoch: [66/70], Progress: 47.31 %, Steps: 6090, Current Learning Rate: 0.0008008, \u001b[96mTrain Loss: 0.099\n",
      "\u001b[0m\u001b[1mEpoch: [66/70], Progress: 48.39 %, Steps: 6091, Current Learning Rate: 0.0008008, \u001b[91mTrain Loss: 0.121\n",
      "\u001b[0m\u001b[1mEpoch: [66/70], Progress: 49.46 %, Steps: 6092, Current Learning Rate: 0.0008007, \u001b[91mTrain Loss: 0.165\n",
      "\u001b[0m\u001b[1mEpoch: [66/70], Progress: 50.54 %, Steps: 6093, Current Learning Rate: 0.0008006, \u001b[96mTrain Loss: 0.124\n",
      "\u001b[0m\u001b[1mEpoch: [66/70], Progress: 51.61 %, Steps: 6094, Current Learning Rate: 0.0008006, \u001b[91mTrain Loss: 0.154\n",
      "\u001b[0m\u001b[1mEpoch: [66/70], Progress: 52.69 %, Steps: 6095, Current Learning Rate: 0.0008005, \u001b[96mTrain Loss: 0.144\n",
      "\u001b[0m\u001b[1mEpoch: [66/70], Progress: 53.76 %, Steps: 6096, Current Learning Rate: 0.0008004, \u001b[91mTrain Loss: 0.166\n",
      "\u001b[0m\u001b[1mEpoch: [66/70], Progress: 54.84 %, Steps: 6097, Current Learning Rate: 0.0008004, \u001b[96mTrain Loss: 0.135\n",
      "\u001b[0m\u001b[1mEpoch: [66/70], Progress: 55.91 %, Steps: 6098, Current Learning Rate: 0.0008003, \u001b[96mTrain Loss: 0.105\n",
      "\u001b[0m\u001b[1mEpoch: [66/70], Progress: 56.99 %, Steps: 6099, Current Learning Rate: 0.0008002, \u001b[91mTrain Loss: 0.162\n",
      "\u001b[0m\u001b[1mEpoch: [66/70], Progress: 58.06 %, Steps: 6100, Current Learning Rate: 0.0008002, \u001b[96mTrain Loss: 0.111\n",
      "\u001b[0m\u001b[1mEpoch: [66/70], Progress: 59.14 %, Steps: 6101, Current Learning Rate: 0.0008001, \u001b[91mTrain Loss: 0.137\n",
      "\u001b[0m\u001b[1mEpoch: [66/70], Progress: 60.22 %, Steps: 6102, Current Learning Rate: 0.0008000, \u001b[91mTrain Loss: 0.190\n",
      "\u001b[0m\u001b[1mEpoch: [66/70], Progress: 61.29 %, Steps: 6103, Current Learning Rate: 0.0008000, \u001b[96mTrain Loss: 0.177\n",
      "\u001b[0m\u001b[1mEpoch: [66/70], Progress: 62.37 %, Steps: 6104, Current Learning Rate: 0.0007999, \u001b[96mTrain Loss: 0.132\n",
      "\u001b[0m\u001b[1mEpoch: [66/70], Progress: 63.44 %, Steps: 6105, Current Learning Rate: 0.0007998, \u001b[96mTrain Loss: 0.132\n",
      "\u001b[0m\u001b[1mEpoch: [66/70], Progress: 64.52 %, Steps: 6106, Current Learning Rate: 0.0007998, \u001b[96mTrain Loss: 0.122\n",
      "\u001b[0m\u001b[1mEpoch: [66/70], Progress: 65.59 %, Steps: 6107, Current Learning Rate: 0.0007997, \u001b[91mTrain Loss: 0.185\n",
      "\u001b[0m\u001b[1mEpoch: [66/70], Progress: 66.67 %, Steps: 6108, Current Learning Rate: 0.0007996, \u001b[96mTrain Loss: 0.121\n",
      "\u001b[0m\u001b[1mEpoch: [66/70], Progress: 67.74 %, Steps: 6109, Current Learning Rate: 0.0007996, \u001b[91mTrain Loss: 0.148\n",
      "\u001b[0m\u001b[1mEpoch: [66/70], Progress: 68.82 %, Steps: 6110, Current Learning Rate: 0.0007995, \u001b[91mTrain Loss: 0.157\n",
      "\u001b[0m\u001b[1mEpoch: [66/70], Progress: 69.89 %, Steps: 6111, Current Learning Rate: 0.0007994, \u001b[96mTrain Loss: 0.140\n",
      "\u001b[0m\u001b[1mEpoch: [66/70], Progress: 70.97 %, Steps: 6112, Current Learning Rate: 0.0007994, \u001b[96mTrain Loss: 0.136\n",
      "\u001b[0m\u001b[1mEpoch: [66/70], Progress: 72.04 %, Steps: 6113, Current Learning Rate: 0.0007993, \u001b[91mTrain Loss: 0.167\n",
      "\u001b[0m\u001b[1mEpoch: [66/70], Progress: 73.12 %, Steps: 6114, Current Learning Rate: 0.0007992, \u001b[96mTrain Loss: 0.136\n",
      "\u001b[0m\u001b[1mEpoch: [66/70], Progress: 74.19 %, Steps: 6115, Current Learning Rate: 0.0007992, \u001b[91mTrain Loss: 0.166\n",
      "\u001b[0m\u001b[1mEpoch: [66/70], Progress: 75.27 %, Steps: 6116, Current Learning Rate: 0.0007991, \u001b[96mTrain Loss: 0.107\n",
      "\u001b[0m\u001b[1mEpoch: [66/70], Progress: 76.34 %, Steps: 6117, Current Learning Rate: 0.0007991, \u001b[91mTrain Loss: 0.128\n",
      "\u001b[0m\u001b[1mEpoch: [66/70], Progress: 77.42 %, Steps: 6118, Current Learning Rate: 0.0007990, \u001b[91mTrain Loss: 0.173\n",
      "\u001b[0m\u001b[1mEpoch: [66/70], Progress: 78.49 %, Steps: 6119, Current Learning Rate: 0.0007989, \u001b[96mTrain Loss: 0.140\n",
      "\u001b[0m\u001b[1mEpoch: [66/70], Progress: 79.57 %, Steps: 6120, Current Learning Rate: 0.0007989, \u001b[96mTrain Loss: 0.126\n",
      "\u001b[0m\u001b[1mEpoch: [66/70], Progress: 80.65 %, Steps: 6121, Current Learning Rate: 0.0007988, \u001b[91mTrain Loss: 0.169\n",
      "\u001b[0m\u001b[1mEpoch: [66/70], Progress: 81.72 %, Steps: 6122, Current Learning Rate: 0.0007987, \u001b[91mTrain Loss: 0.180\n",
      "\u001b[0m\u001b[1mEpoch: [66/70], Progress: 82.80 %, Steps: 6123, Current Learning Rate: 0.0007987, \u001b[96mTrain Loss: 0.133\n",
      "\u001b[0m\u001b[1mEpoch: [66/70], Progress: 83.87 %, Steps: 6124, Current Learning Rate: 0.0007986, \u001b[91mTrain Loss: 0.152\n",
      "\u001b[0m\u001b[1mEpoch: [66/70], Progress: 84.95 %, Steps: 6125, Current Learning Rate: 0.0007985, \u001b[96mTrain Loss: 0.104\n",
      "\u001b[0m\u001b[1mEpoch: [66/70], Progress: 86.02 %, Steps: 6126, Current Learning Rate: 0.0007985, \u001b[91mTrain Loss: 0.167\n",
      "\u001b[0m\u001b[1mEpoch: [66/70], Progress: 87.10 %, Steps: 6127, Current Learning Rate: 0.0007984, \u001b[96mTrain Loss: 0.153\n",
      "\u001b[0m\u001b[1mEpoch: [66/70], Progress: 88.17 %, Steps: 6128, Current Learning Rate: 0.0007983, \u001b[91mTrain Loss: 0.184\n",
      "\u001b[0m\u001b[1mEpoch: [66/70], Progress: 89.25 %, Steps: 6129, Current Learning Rate: 0.0007983, \u001b[96mTrain Loss: 0.103\n",
      "\u001b[0m\u001b[1mEpoch: [66/70], Progress: 90.32 %, Steps: 6130, Current Learning Rate: 0.0007982, \u001b[91mTrain Loss: 0.160\n",
      "\u001b[0m\u001b[1mEpoch: [66/70], Progress: 91.40 %, Steps: 6131, Current Learning Rate: 0.0007981, \u001b[91mTrain Loss: 0.188\n",
      "\u001b[0m\u001b[1mEpoch: [66/70], Progress: 92.47 %, Steps: 6132, Current Learning Rate: 0.0007981, \u001b[96mTrain Loss: 0.168\n",
      "\u001b[0m\u001b[1mEpoch: [66/70], Progress: 93.55 %, Steps: 6133, Current Learning Rate: 0.0007980, \u001b[96mTrain Loss: 0.149\n",
      "\u001b[0m\u001b[1mEpoch: [66/70], Progress: 94.62 %, Steps: 6134, Current Learning Rate: 0.0007979, \u001b[91mTrain Loss: 0.162\n",
      "\u001b[0m\u001b[1mEpoch: [66/70], Progress: 95.70 %, Steps: 6135, Current Learning Rate: 0.0007979, \u001b[96mTrain Loss: 0.109\n",
      "\u001b[0m\u001b[1mEpoch: [66/70], Progress: 96.77 %, Steps: 6136, Current Learning Rate: 0.0007978, \u001b[91mTrain Loss: 0.167\n",
      "\u001b[0m\u001b[1mEpoch: [66/70], Progress: 97.85 %, Steps: 6137, Current Learning Rate: 0.0007977, \u001b[96mTrain Loss: 0.151\n",
      "\u001b[0m\u001b[1mEpoch: [66/70], Progress: 98.92 %, Steps: 6138, Current Learning Rate: 0.0007977, \u001b[91mTrain Loss: 0.153\n",
      "\u001b[1m\u001b[93mComputing Validation Loss...\n",
      "\u001b[1mEpoch 66 Completed! Average Train Loss: 0.135, Average Validation Loss: 0.023\n",
      "Best Model saved in best_model/best_model.pt.\n",
      "\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m= Epoch [67/70] ==========================================\n",
      "\u001b[0m\u001b[1mEpoch: [67/70], Progress: 0.00 %, Steps: 6139, Current Learning Rate: 0.0007976, \u001b[91mTrain Loss: 0.084\n",
      "\u001b[0m\u001b[1mEpoch: [67/70], Progress: 1.08 %, Steps: 6140, Current Learning Rate: 0.0007976, \u001b[91mTrain Loss: 0.117\n",
      "\u001b[0m\u001b[1mEpoch: [67/70], Progress: 2.15 %, Steps: 6141, Current Learning Rate: 0.0007975, \u001b[96mTrain Loss: 0.093\n",
      "\u001b[0m\u001b[1mEpoch: [67/70], Progress: 3.23 %, Steps: 6142, Current Learning Rate: 0.0007974, \u001b[91mTrain Loss: 0.097\n",
      "\u001b[0m\u001b[1mEpoch: [67/70], Progress: 4.30 %, Steps: 6143, Current Learning Rate: 0.0007974, \u001b[91mTrain Loss: 0.155\n",
      "\u001b[0m\u001b[1mEpoch: [67/70], Progress: 5.38 %, Steps: 6144, Current Learning Rate: 0.0007973, \u001b[96mTrain Loss: 0.112\n",
      "\u001b[0m\u001b[1mEpoch: [67/70], Progress: 6.45 %, Steps: 6145, Current Learning Rate: 0.0007972, \u001b[91mTrain Loss: 0.126\n",
      "\u001b[0m\u001b[1mEpoch: [67/70], Progress: 7.53 %, Steps: 6146, Current Learning Rate: 0.0007972, \u001b[96mTrain Loss: 0.083\n",
      "\u001b[0m\u001b[1mEpoch: [67/70], Progress: 8.60 %, Steps: 6147, Current Learning Rate: 0.0007971, \u001b[91mTrain Loss: 0.103\n",
      "\u001b[0m\u001b[1mEpoch: [67/70], Progress: 9.68 %, Steps: 6148, Current Learning Rate: 0.0007970, \u001b[91mTrain Loss: 0.111\n",
      "\u001b[0m\u001b[1mEpoch: [67/70], Progress: 10.75 %, Steps: 6149, Current Learning Rate: 0.0007970, \u001b[96mTrain Loss: 0.062\n",
      "\u001b[0m\u001b[1mEpoch: [67/70], Progress: 11.83 %, Steps: 6150, Current Learning Rate: 0.0007969, \u001b[91mTrain Loss: 0.131\n",
      "\u001b[0m\u001b[1mEpoch: [67/70], Progress: 12.90 %, Steps: 6151, Current Learning Rate: 0.0007968, \u001b[96mTrain Loss: 0.088\n",
      "\u001b[0m\u001b[1mEpoch: [67/70], Progress: 13.98 %, Steps: 6152, Current Learning Rate: 0.0007968, \u001b[91mTrain Loss: 0.113\n",
      "\u001b[0m\u001b[1mEpoch: [67/70], Progress: 15.05 %, Steps: 6153, Current Learning Rate: 0.0007967, \u001b[96mTrain Loss: 0.094\n",
      "\u001b[0m\u001b[1mEpoch: [67/70], Progress: 16.13 %, Steps: 6154, Current Learning Rate: 0.0007966, \u001b[91mTrain Loss: 0.110\n",
      "\u001b[0m\u001b[1mEpoch: [67/70], Progress: 17.20 %, Steps: 6155, Current Learning Rate: 0.0007966, \u001b[91mTrain Loss: 0.119\n",
      "\u001b[0m\u001b[1mEpoch: [67/70], Progress: 18.28 %, Steps: 6156, Current Learning Rate: 0.0007965, \u001b[96mTrain Loss: 0.093\n",
      "\u001b[0m\u001b[1mEpoch: [67/70], Progress: 19.35 %, Steps: 6157, Current Learning Rate: 0.0007965, \u001b[91mTrain Loss: 0.113\n",
      "\u001b[0m\u001b[1mEpoch: [67/70], Progress: 20.43 %, Steps: 6158, Current Learning Rate: 0.0007964, \u001b[96mTrain Loss: 0.094\n",
      "\u001b[0m\u001b[1mEpoch: [67/70], Progress: 21.51 %, Steps: 6159, Current Learning Rate: 0.0007963, \u001b[91mTrain Loss: 0.120\n",
      "\u001b[0m\u001b[1mEpoch: [67/70], Progress: 22.58 %, Steps: 6160, Current Learning Rate: 0.0007963, \u001b[91mTrain Loss: 0.152\n",
      "\u001b[0m\u001b[1mEpoch: [67/70], Progress: 23.66 %, Steps: 6161, Current Learning Rate: 0.0007962, \u001b[96mTrain Loss: 0.138\n",
      "\u001b[0m\u001b[1mEpoch: [67/70], Progress: 24.73 %, Steps: 6162, Current Learning Rate: 0.0007961, \u001b[96mTrain Loss: 0.112\n",
      "\u001b[0m\u001b[1mEpoch: [67/70], Progress: 25.81 %, Steps: 6163, Current Learning Rate: 0.0007961, \u001b[96mTrain Loss: 0.103\n",
      "\u001b[0m\u001b[1mEpoch: [67/70], Progress: 26.88 %, Steps: 6164, Current Learning Rate: 0.0007960, \u001b[91mTrain Loss: 0.120\n",
      "\u001b[0m\u001b[1mEpoch: [67/70], Progress: 27.96 %, Steps: 6165, Current Learning Rate: 0.0007959, \u001b[96mTrain Loss: 0.105\n",
      "\u001b[0m\u001b[1mEpoch: [67/70], Progress: 29.03 %, Steps: 6166, Current Learning Rate: 0.0007959, \u001b[96mTrain Loss: 0.097\n",
      "\u001b[0m\u001b[1mEpoch: [67/70], Progress: 30.11 %, Steps: 6167, Current Learning Rate: 0.0007958, \u001b[96mTrain Loss: 0.084\n",
      "\u001b[0m\u001b[1mEpoch: [67/70], Progress: 31.18 %, Steps: 6168, Current Learning Rate: 0.0007957, \u001b[91mTrain Loss: 0.147\n",
      "\u001b[0m\u001b[1mEpoch: [67/70], Progress: 32.26 %, Steps: 6169, Current Learning Rate: 0.0007957, \u001b[96mTrain Loss: 0.071\n",
      "\u001b[0m\u001b[1mEpoch: [67/70], Progress: 33.33 %, Steps: 6170, Current Learning Rate: 0.0007956, \u001b[91mTrain Loss: 0.093\n",
      "\u001b[0m\u001b[1mEpoch: [67/70], Progress: 34.41 %, Steps: 6171, Current Learning Rate: 0.0007955, \u001b[91mTrain Loss: 0.150\n",
      "\u001b[0m\u001b[1mEpoch: [67/70], Progress: 35.48 %, Steps: 6172, Current Learning Rate: 0.0007955, \u001b[96mTrain Loss: 0.103\n",
      "\u001b[0m\u001b[1mEpoch: [67/70], Progress: 36.56 %, Steps: 6173, Current Learning Rate: 0.0007954, \u001b[91mTrain Loss: 0.131\n",
      "\u001b[0m\u001b[1mEpoch: [67/70], Progress: 37.63 %, Steps: 6174, Current Learning Rate: 0.0007954, \u001b[96mTrain Loss: 0.122\n",
      "\u001b[0m\u001b[1mEpoch: [67/70], Progress: 38.71 %, Steps: 6175, Current Learning Rate: 0.0007953, \u001b[96mTrain Loss: 0.092\n",
      "\u001b[0m\u001b[1mEpoch: [67/70], Progress: 39.78 %, Steps: 6176, Current Learning Rate: 0.0007952, \u001b[91mTrain Loss: 0.126\n",
      "\u001b[0m\u001b[1mEpoch: [67/70], Progress: 40.86 %, Steps: 6177, Current Learning Rate: 0.0007952, \u001b[91mTrain Loss: 0.128\n",
      "\u001b[0m\u001b[1mEpoch: [67/70], Progress: 41.94 %, Steps: 6178, Current Learning Rate: 0.0007951, \u001b[91mTrain Loss: 0.152\n",
      "\u001b[0m\u001b[1mEpoch: [67/70], Progress: 43.01 %, Steps: 6179, Current Learning Rate: 0.0007950, \u001b[96mTrain Loss: 0.102\n",
      "\u001b[0m\u001b[1mEpoch: [67/70], Progress: 44.09 %, Steps: 6180, Current Learning Rate: 0.0007950, \u001b[91mTrain Loss: 0.113\n",
      "\u001b[0m\u001b[1mEpoch: [67/70], Progress: 45.16 %, Steps: 6181, Current Learning Rate: 0.0007949, \u001b[96mTrain Loss: 0.111\n",
      "\u001b[0m\u001b[1mEpoch: [67/70], Progress: 46.24 %, Steps: 6182, Current Learning Rate: 0.0007948, \u001b[91mTrain Loss: 0.168\n",
      "\u001b[0m\u001b[1mEpoch: [67/70], Progress: 47.31 %, Steps: 6183, Current Learning Rate: 0.0007948, \u001b[96mTrain Loss: 0.145\n",
      "\u001b[0m\u001b[1mEpoch: [67/70], Progress: 48.39 %, Steps: 6184, Current Learning Rate: 0.0007947, \u001b[96mTrain Loss: 0.111\n",
      "\u001b[0m\u001b[1mEpoch: [67/70], Progress: 49.46 %, Steps: 6185, Current Learning Rate: 0.0007946, \u001b[96mTrain Loss: 0.109\n",
      "\u001b[0m\u001b[1mEpoch: [67/70], Progress: 50.54 %, Steps: 6186, Current Learning Rate: 0.0007946, \u001b[91mTrain Loss: 0.138\n",
      "\u001b[0m\u001b[1mEpoch: [67/70], Progress: 51.61 %, Steps: 6187, Current Learning Rate: 0.0007945, \u001b[96mTrain Loss: 0.124\n",
      "\u001b[0m\u001b[1mEpoch: [67/70], Progress: 52.69 %, Steps: 6188, Current Learning Rate: 0.0007945, \u001b[91mTrain Loss: 0.125\n",
      "\u001b[0m\u001b[1mEpoch: [67/70], Progress: 53.76 %, Steps: 6189, Current Learning Rate: 0.0007944, \u001b[96mTrain Loss: 0.088\n",
      "\u001b[0m\u001b[1mEpoch: [67/70], Progress: 54.84 %, Steps: 6190, Current Learning Rate: 0.0007943, \u001b[91mTrain Loss: 0.134\n",
      "\u001b[0m\u001b[1mEpoch: [67/70], Progress: 55.91 %, Steps: 6191, Current Learning Rate: 0.0007943, \u001b[91mTrain Loss: 0.139\n",
      "\u001b[0m\u001b[1mEpoch: [67/70], Progress: 56.99 %, Steps: 6192, Current Learning Rate: 0.0007942, \u001b[96mTrain Loss: 0.090\n",
      "\u001b[0m\u001b[1mEpoch: [67/70], Progress: 58.06 %, Steps: 6193, Current Learning Rate: 0.0007941, \u001b[91mTrain Loss: 0.127\n",
      "\u001b[0m\u001b[1mEpoch: [67/70], Progress: 59.14 %, Steps: 6194, Current Learning Rate: 0.0007941, \u001b[96mTrain Loss: 0.096\n",
      "\u001b[0m\u001b[1mEpoch: [67/70], Progress: 60.22 %, Steps: 6195, Current Learning Rate: 0.0007940, \u001b[91mTrain Loss: 0.124\n",
      "\u001b[0m\u001b[1mEpoch: [67/70], Progress: 61.29 %, Steps: 6196, Current Learning Rate: 0.0007939, \u001b[91mTrain Loss: 0.150\n",
      "\u001b[0m\u001b[1mEpoch: [67/70], Progress: 62.37 %, Steps: 6197, Current Learning Rate: 0.0007939, \u001b[96mTrain Loss: 0.133\n",
      "\u001b[0m\u001b[1mEpoch: [67/70], Progress: 63.44 %, Steps: 6198, Current Learning Rate: 0.0007938, \u001b[91mTrain Loss: 0.164\n",
      "\u001b[0m\u001b[1mEpoch: [67/70], Progress: 64.52 %, Steps: 6199, Current Learning Rate: 0.0007938, \u001b[91mTrain Loss: 0.174\n",
      "\u001b[0m\u001b[1mEpoch: [67/70], Progress: 65.59 %, Steps: 6200, Current Learning Rate: 0.0007937, \u001b[96mTrain Loss: 0.161\n",
      "\u001b[0m\u001b[1mEpoch: [67/70], Progress: 66.67 %, Steps: 6201, Current Learning Rate: 0.0007936, \u001b[96mTrain Loss: 0.125\n",
      "\u001b[0m\u001b[1mEpoch: [67/70], Progress: 67.74 %, Steps: 6202, Current Learning Rate: 0.0007936, \u001b[91mTrain Loss: 0.161\n",
      "\u001b[0m\u001b[1mEpoch: [67/70], Progress: 68.82 %, Steps: 6203, Current Learning Rate: 0.0007935, \u001b[96mTrain Loss: 0.161\n",
      "\u001b[0m\u001b[1mEpoch: [67/70], Progress: 69.89 %, Steps: 6204, Current Learning Rate: 0.0007934, \u001b[96mTrain Loss: 0.136\n",
      "\u001b[0m\u001b[1mEpoch: [67/70], Progress: 70.97 %, Steps: 6205, Current Learning Rate: 0.0007934, \u001b[96mTrain Loss: 0.111\n",
      "\u001b[0m\u001b[1mEpoch: [67/70], Progress: 72.04 %, Steps: 6206, Current Learning Rate: 0.0007933, \u001b[91mTrain Loss: 0.145\n",
      "\u001b[0m\u001b[1mEpoch: [67/70], Progress: 73.12 %, Steps: 6207, Current Learning Rate: 0.0007932, \u001b[96mTrain Loss: 0.129\n",
      "\u001b[0m\u001b[1mEpoch: [67/70], Progress: 74.19 %, Steps: 6208, Current Learning Rate: 0.0007932, \u001b[91mTrain Loss: 0.140\n",
      "\u001b[0m\u001b[1mEpoch: [67/70], Progress: 75.27 %, Steps: 6209, Current Learning Rate: 0.0007931, \u001b[96mTrain Loss: 0.136\n",
      "\u001b[0m\u001b[1mEpoch: [67/70], Progress: 76.34 %, Steps: 6210, Current Learning Rate: 0.0007930, \u001b[96mTrain Loss: 0.126\n",
      "\u001b[0m\u001b[1mEpoch: [67/70], Progress: 77.42 %, Steps: 6211, Current Learning Rate: 0.0007930, \u001b[91mTrain Loss: 0.180\n",
      "\u001b[0m\u001b[1mEpoch: [67/70], Progress: 78.49 %, Steps: 6212, Current Learning Rate: 0.0007929, \u001b[96mTrain Loss: 0.131\n",
      "\u001b[0m\u001b[1mEpoch: [67/70], Progress: 79.57 %, Steps: 6213, Current Learning Rate: 0.0007929, \u001b[91mTrain Loss: 0.141\n",
      "\u001b[0m\u001b[1mEpoch: [67/70], Progress: 80.65 %, Steps: 6214, Current Learning Rate: 0.0007928, \u001b[96mTrain Loss: 0.124\n",
      "\u001b[0m\u001b[1mEpoch: [67/70], Progress: 81.72 %, Steps: 6215, Current Learning Rate: 0.0007927, \u001b[91mTrain Loss: 0.128\n",
      "\u001b[0m\u001b[1mEpoch: [67/70], Progress: 82.80 %, Steps: 6216, Current Learning Rate: 0.0007927, \u001b[91mTrain Loss: 0.138\n",
      "\u001b[0m\u001b[1mEpoch: [67/70], Progress: 83.87 %, Steps: 6217, Current Learning Rate: 0.0007926, \u001b[91mTrain Loss: 0.159\n",
      "\u001b[0m\u001b[1mEpoch: [67/70], Progress: 84.95 %, Steps: 6218, Current Learning Rate: 0.0007925, \u001b[91mTrain Loss: 0.164\n",
      "\u001b[0m\u001b[1mEpoch: [67/70], Progress: 86.02 %, Steps: 6219, Current Learning Rate: 0.0007925, \u001b[91mTrain Loss: 0.180\n",
      "\u001b[0m\u001b[1mEpoch: [67/70], Progress: 87.10 %, Steps: 6220, Current Learning Rate: 0.0007924, \u001b[96mTrain Loss: 0.142\n",
      "\u001b[0m\u001b[1mEpoch: [67/70], Progress: 88.17 %, Steps: 6221, Current Learning Rate: 0.0007923, \u001b[96mTrain Loss: 0.139\n",
      "\u001b[0m\u001b[1mEpoch: [67/70], Progress: 89.25 %, Steps: 6222, Current Learning Rate: 0.0007923, \u001b[91mTrain Loss: 0.152\n",
      "\u001b[0m\u001b[1mEpoch: [67/70], Progress: 90.32 %, Steps: 6223, Current Learning Rate: 0.0007922, \u001b[96mTrain Loss: 0.117\n",
      "\u001b[0m\u001b[1mEpoch: [67/70], Progress: 91.40 %, Steps: 6224, Current Learning Rate: 0.0007922, \u001b[91mTrain Loss: 0.137\n",
      "\u001b[0m\u001b[1mEpoch: [67/70], Progress: 92.47 %, Steps: 6225, Current Learning Rate: 0.0007921, \u001b[96mTrain Loss: 0.105\n",
      "\u001b[0m\u001b[1mEpoch: [67/70], Progress: 93.55 %, Steps: 6226, Current Learning Rate: 0.0007920, \u001b[91mTrain Loss: 0.165\n",
      "\u001b[0m\u001b[1mEpoch: [67/70], Progress: 94.62 %, Steps: 6227, Current Learning Rate: 0.0007920, \u001b[96mTrain Loss: 0.129\n",
      "\u001b[0m\u001b[1mEpoch: [67/70], Progress: 95.70 %, Steps: 6228, Current Learning Rate: 0.0007919, \u001b[96mTrain Loss: 0.118\n",
      "\u001b[0m\u001b[1mEpoch: [67/70], Progress: 96.77 %, Steps: 6229, Current Learning Rate: 0.0007918, \u001b[91mTrain Loss: 0.154\n",
      "\u001b[0m\u001b[1mEpoch: [67/70], Progress: 97.85 %, Steps: 6230, Current Learning Rate: 0.0007918, \u001b[96mTrain Loss: 0.120\n",
      "\u001b[0m\u001b[1mEpoch: [67/70], Progress: 98.92 %, Steps: 6231, Current Learning Rate: 0.0007917, \u001b[91mTrain Loss: 0.125\n",
      "\u001b[1m\u001b[93mComputing Validation Loss...\n",
      "\u001b[1mEpoch 67 Completed! Average Train Loss: 0.125, Average Validation Loss: 0.019\n",
      "Best Model saved in best_model/best_model.pt.\n",
      "\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m= Epoch [68/70] ==========================================\n",
      "\u001b[0m\u001b[1mEpoch: [68/70], Progress: 0.00 %, Steps: 6232, Current Learning Rate: 0.0007916, \u001b[91mTrain Loss: 0.097\n",
      "\u001b[0m\u001b[1mEpoch: [68/70], Progress: 1.08 %, Steps: 6233, Current Learning Rate: 0.0007916, \u001b[91mTrain Loss: 0.111\n",
      "\u001b[0m\u001b[1mEpoch: [68/70], Progress: 2.15 %, Steps: 6234, Current Learning Rate: 0.0007915, \u001b[96mTrain Loss: 0.065\n",
      "\u001b[0m\u001b[1mEpoch: [68/70], Progress: 3.23 %, Steps: 6235, Current Learning Rate: 0.0007915, \u001b[91mTrain Loss: 0.113\n",
      "\u001b[0m\u001b[1mEpoch: [68/70], Progress: 4.30 %, Steps: 6236, Current Learning Rate: 0.0007914, \u001b[91mTrain Loss: 0.121\n",
      "\u001b[0m\u001b[1mEpoch: [68/70], Progress: 5.38 %, Steps: 6237, Current Learning Rate: 0.0007913, \u001b[96mTrain Loss: 0.062\n",
      "\u001b[0m\u001b[1mEpoch: [68/70], Progress: 6.45 %, Steps: 6238, Current Learning Rate: 0.0007913, \u001b[91mTrain Loss: 0.082\n",
      "\u001b[0m\u001b[1mEpoch: [68/70], Progress: 7.53 %, Steps: 6239, Current Learning Rate: 0.0007912, \u001b[91mTrain Loss: 0.116\n",
      "\u001b[0m\u001b[1mEpoch: [68/70], Progress: 8.60 %, Steps: 6240, Current Learning Rate: 0.0007911, \u001b[91mTrain Loss: 0.129\n",
      "\u001b[0m\u001b[1mEpoch: [68/70], Progress: 9.68 %, Steps: 6241, Current Learning Rate: 0.0007911, \u001b[96mTrain Loss: 0.111\n",
      "\u001b[0m\u001b[1mEpoch: [68/70], Progress: 10.75 %, Steps: 6242, Current Learning Rate: 0.0007910, \u001b[96mTrain Loss: 0.101\n",
      "\u001b[0m\u001b[1mEpoch: [68/70], Progress: 11.83 %, Steps: 6243, Current Learning Rate: 0.0007909, \u001b[91mTrain Loss: 0.133\n",
      "\u001b[0m\u001b[1mEpoch: [68/70], Progress: 12.90 %, Steps: 6244, Current Learning Rate: 0.0007909, \u001b[96mTrain Loss: 0.092\n",
      "\u001b[0m\u001b[1mEpoch: [68/70], Progress: 13.98 %, Steps: 6245, Current Learning Rate: 0.0007908, \u001b[91mTrain Loss: 0.104\n",
      "\u001b[0m\u001b[1mEpoch: [68/70], Progress: 15.05 %, Steps: 6246, Current Learning Rate: 0.0007908, \u001b[91mTrain Loss: 0.160\n",
      "\u001b[0m\u001b[1mEpoch: [68/70], Progress: 16.13 %, Steps: 6247, Current Learning Rate: 0.0007907, \u001b[96mTrain Loss: 0.061\n",
      "\u001b[0m\u001b[1mEpoch: [68/70], Progress: 17.20 %, Steps: 6248, Current Learning Rate: 0.0007906, \u001b[91mTrain Loss: 0.088\n",
      "\u001b[0m\u001b[1mEpoch: [68/70], Progress: 18.28 %, Steps: 6249, Current Learning Rate: 0.0007906, \u001b[96mTrain Loss: 0.087\n",
      "\u001b[0m\u001b[1mEpoch: [68/70], Progress: 19.35 %, Steps: 6250, Current Learning Rate: 0.0007905, \u001b[96mTrain Loss: 0.070\n",
      "\u001b[0m\u001b[1mEpoch: [68/70], Progress: 20.43 %, Steps: 6251, Current Learning Rate: 0.0007904, \u001b[91mTrain Loss: 0.111\n",
      "\u001b[0m\u001b[1mEpoch: [68/70], Progress: 21.51 %, Steps: 6252, Current Learning Rate: 0.0007904, \u001b[96mTrain Loss: 0.110\n",
      "\u001b[0m\u001b[1mEpoch: [68/70], Progress: 22.58 %, Steps: 6253, Current Learning Rate: 0.0007903, \u001b[96mTrain Loss: 0.101\n",
      "\u001b[0m\u001b[1mEpoch: [68/70], Progress: 23.66 %, Steps: 6254, Current Learning Rate: 0.0007903, \u001b[96mTrain Loss: 0.082\n",
      "\u001b[0m\u001b[1mEpoch: [68/70], Progress: 24.73 %, Steps: 6255, Current Learning Rate: 0.0007902, \u001b[91mTrain Loss: 0.093\n",
      "\u001b[0m\u001b[1mEpoch: [68/70], Progress: 25.81 %, Steps: 6256, Current Learning Rate: 0.0007901, \u001b[96mTrain Loss: 0.091\n",
      "\u001b[0m\u001b[1mEpoch: [68/70], Progress: 26.88 %, Steps: 6257, Current Learning Rate: 0.0007901, \u001b[91mTrain Loss: 0.106\n",
      "\u001b[0m\u001b[1mEpoch: [68/70], Progress: 27.96 %, Steps: 6258, Current Learning Rate: 0.0007900, \u001b[96mTrain Loss: 0.072\n",
      "\u001b[0m\u001b[1mEpoch: [68/70], Progress: 29.03 %, Steps: 6259, Current Learning Rate: 0.0007899, \u001b[91mTrain Loss: 0.146\n",
      "\u001b[0m\u001b[1mEpoch: [68/70], Progress: 30.11 %, Steps: 6260, Current Learning Rate: 0.0007899, \u001b[96mTrain Loss: 0.114\n",
      "\u001b[0m\u001b[1mEpoch: [68/70], Progress: 31.18 %, Steps: 6261, Current Learning Rate: 0.0007898, \u001b[91mTrain Loss: 0.115\n",
      "\u001b[0m\u001b[1mEpoch: [68/70], Progress: 32.26 %, Steps: 6262, Current Learning Rate: 0.0007897, \u001b[96mTrain Loss: 0.091\n",
      "\u001b[0m\u001b[1mEpoch: [68/70], Progress: 33.33 %, Steps: 6263, Current Learning Rate: 0.0007897, \u001b[91mTrain Loss: 0.150\n",
      "\u001b[0m\u001b[1mEpoch: [68/70], Progress: 34.41 %, Steps: 6264, Current Learning Rate: 0.0007896, \u001b[96mTrain Loss: 0.101\n",
      "\u001b[0m\u001b[1mEpoch: [68/70], Progress: 35.48 %, Steps: 6265, Current Learning Rate: 0.0007896, \u001b[91mTrain Loss: 0.161\n",
      "\u001b[0m\u001b[1mEpoch: [68/70], Progress: 36.56 %, Steps: 6266, Current Learning Rate: 0.0007895, \u001b[91mTrain Loss: 0.172\n",
      "\u001b[0m\u001b[1mEpoch: [68/70], Progress: 37.63 %, Steps: 6267, Current Learning Rate: 0.0007894, \u001b[96mTrain Loss: 0.091\n",
      "\u001b[0m\u001b[1mEpoch: [68/70], Progress: 38.71 %, Steps: 6268, Current Learning Rate: 0.0007894, \u001b[91mTrain Loss: 0.128\n",
      "\u001b[0m\u001b[1mEpoch: [68/70], Progress: 39.78 %, Steps: 6269, Current Learning Rate: 0.0007893, \u001b[96mTrain Loss: 0.109\n",
      "\u001b[0m\u001b[1mEpoch: [68/70], Progress: 40.86 %, Steps: 6270, Current Learning Rate: 0.0007892, \u001b[91mTrain Loss: 0.136\n",
      "\u001b[0m\u001b[1mEpoch: [68/70], Progress: 41.94 %, Steps: 6271, Current Learning Rate: 0.0007892, \u001b[91mTrain Loss: 0.157\n",
      "\u001b[0m\u001b[1mEpoch: [68/70], Progress: 43.01 %, Steps: 6272, Current Learning Rate: 0.0007891, \u001b[96mTrain Loss: 0.154\n",
      "\u001b[0m\u001b[1mEpoch: [68/70], Progress: 44.09 %, Steps: 6273, Current Learning Rate: 0.0007891, \u001b[96mTrain Loss: 0.117\n",
      "\u001b[0m\u001b[1mEpoch: [68/70], Progress: 45.16 %, Steps: 6274, Current Learning Rate: 0.0007890, \u001b[91mTrain Loss: 0.185\n",
      "\u001b[0m\u001b[1mEpoch: [68/70], Progress: 46.24 %, Steps: 6275, Current Learning Rate: 0.0007889, \u001b[96mTrain Loss: 0.092\n",
      "\u001b[0m\u001b[1mEpoch: [68/70], Progress: 47.31 %, Steps: 6276, Current Learning Rate: 0.0007889, \u001b[91mTrain Loss: 0.127\n",
      "\u001b[0m\u001b[1mEpoch: [68/70], Progress: 48.39 %, Steps: 6277, Current Learning Rate: 0.0007888, \u001b[91mTrain Loss: 0.137\n",
      "\u001b[0m\u001b[1mEpoch: [68/70], Progress: 49.46 %, Steps: 6278, Current Learning Rate: 0.0007887, \u001b[91mTrain Loss: 0.150\n",
      "\u001b[0m\u001b[1mEpoch: [68/70], Progress: 50.54 %, Steps: 6279, Current Learning Rate: 0.0007887, \u001b[96mTrain Loss: 0.106\n",
      "\u001b[0m\u001b[1mEpoch: [68/70], Progress: 51.61 %, Steps: 6280, Current Learning Rate: 0.0007886, \u001b[91mTrain Loss: 0.133\n",
      "\u001b[0m\u001b[1mEpoch: [68/70], Progress: 52.69 %, Steps: 6281, Current Learning Rate: 0.0007886, \u001b[96mTrain Loss: 0.120\n",
      "\u001b[0m\u001b[1mEpoch: [68/70], Progress: 53.76 %, Steps: 6282, Current Learning Rate: 0.0007885, \u001b[96mTrain Loss: 0.110\n",
      "\u001b[0m\u001b[1mEpoch: [68/70], Progress: 54.84 %, Steps: 6283, Current Learning Rate: 0.0007884, \u001b[91mTrain Loss: 0.124\n",
      "\u001b[0m\u001b[1mEpoch: [68/70], Progress: 55.91 %, Steps: 6284, Current Learning Rate: 0.0007884, \u001b[91mTrain Loss: 0.138\n",
      "\u001b[0m\u001b[1mEpoch: [68/70], Progress: 56.99 %, Steps: 6285, Current Learning Rate: 0.0007883, \u001b[96mTrain Loss: 0.113\n",
      "\u001b[0m\u001b[1mEpoch: [68/70], Progress: 58.06 %, Steps: 6286, Current Learning Rate: 0.0007882, \u001b[91mTrain Loss: 0.129\n",
      "\u001b[0m\u001b[1mEpoch: [68/70], Progress: 59.14 %, Steps: 6287, Current Learning Rate: 0.0007882, \u001b[96mTrain Loss: 0.119\n",
      "\u001b[0m\u001b[1mEpoch: [68/70], Progress: 60.22 %, Steps: 6288, Current Learning Rate: 0.0007881, \u001b[91mTrain Loss: 0.154\n",
      "\u001b[0m\u001b[1mEpoch: [68/70], Progress: 61.29 %, Steps: 6289, Current Learning Rate: 0.0007881, \u001b[96mTrain Loss: 0.121\n",
      "\u001b[0m\u001b[1mEpoch: [68/70], Progress: 62.37 %, Steps: 6290, Current Learning Rate: 0.0007880, \u001b[96mTrain Loss: 0.105\n",
      "\u001b[0m\u001b[1mEpoch: [68/70], Progress: 63.44 %, Steps: 6291, Current Learning Rate: 0.0007879, \u001b[96mTrain Loss: 0.082\n",
      "\u001b[0m\u001b[1mEpoch: [68/70], Progress: 64.52 %, Steps: 6292, Current Learning Rate: 0.0007879, \u001b[91mTrain Loss: 0.093\n",
      "\u001b[0m\u001b[1mEpoch: [68/70], Progress: 65.59 %, Steps: 6293, Current Learning Rate: 0.0007878, \u001b[96mTrain Loss: 0.072\n",
      "\u001b[0m\u001b[1mEpoch: [68/70], Progress: 66.67 %, Steps: 6294, Current Learning Rate: 0.0007877, \u001b[91mTrain Loss: 0.140\n",
      "\u001b[0m\u001b[1mEpoch: [68/70], Progress: 67.74 %, Steps: 6295, Current Learning Rate: 0.0007877, \u001b[96mTrain Loss: 0.102\n",
      "\u001b[0m\u001b[1mEpoch: [68/70], Progress: 68.82 %, Steps: 6296, Current Learning Rate: 0.0007876, \u001b[96mTrain Loss: 0.090\n",
      "\u001b[0m\u001b[1mEpoch: [68/70], Progress: 69.89 %, Steps: 6297, Current Learning Rate: 0.0007876, \u001b[91mTrain Loss: 0.093\n",
      "\u001b[0m\u001b[1mEpoch: [68/70], Progress: 70.97 %, Steps: 6298, Current Learning Rate: 0.0007875, \u001b[91mTrain Loss: 0.113\n",
      "\u001b[0m\u001b[1mEpoch: [68/70], Progress: 72.04 %, Steps: 6299, Current Learning Rate: 0.0007874, \u001b[96mTrain Loss: 0.102\n",
      "\u001b[0m\u001b[1mEpoch: [68/70], Progress: 73.12 %, Steps: 6300, Current Learning Rate: 0.0007874, \u001b[91mTrain Loss: 0.121\n",
      "\u001b[0m\u001b[1mEpoch: [68/70], Progress: 74.19 %, Steps: 6301, Current Learning Rate: 0.0007873, \u001b[96mTrain Loss: 0.104\n",
      "\u001b[0m\u001b[1mEpoch: [68/70], Progress: 75.27 %, Steps: 6302, Current Learning Rate: 0.0007872, \u001b[91mTrain Loss: 0.153\n",
      "\u001b[0m\u001b[1mEpoch: [68/70], Progress: 76.34 %, Steps: 6303, Current Learning Rate: 0.0007872, \u001b[96mTrain Loss: 0.097\n",
      "\u001b[0m\u001b[1mEpoch: [68/70], Progress: 77.42 %, Steps: 6304, Current Learning Rate: 0.0007871, \u001b[91mTrain Loss: 0.138\n",
      "\u001b[0m\u001b[1mEpoch: [68/70], Progress: 78.49 %, Steps: 6305, Current Learning Rate: 0.0007871, \u001b[96mTrain Loss: 0.136\n",
      "\u001b[0m\u001b[1mEpoch: [68/70], Progress: 79.57 %, Steps: 6306, Current Learning Rate: 0.0007870, \u001b[91mTrain Loss: 0.144\n",
      "\u001b[0m\u001b[1mEpoch: [68/70], Progress: 80.65 %, Steps: 6307, Current Learning Rate: 0.0007869, \u001b[91mTrain Loss: 0.147\n",
      "\u001b[0m\u001b[1mEpoch: [68/70], Progress: 81.72 %, Steps: 6308, Current Learning Rate: 0.0007869, \u001b[91mTrain Loss: 0.169\n",
      "\u001b[0m\u001b[1mEpoch: [68/70], Progress: 82.80 %, Steps: 6309, Current Learning Rate: 0.0007868, \u001b[96mTrain Loss: 0.119\n",
      "\u001b[0m\u001b[1mEpoch: [68/70], Progress: 83.87 %, Steps: 6310, Current Learning Rate: 0.0007867, \u001b[96mTrain Loss: 0.113\n",
      "\u001b[0m\u001b[1mEpoch: [68/70], Progress: 84.95 %, Steps: 6311, Current Learning Rate: 0.0007867, \u001b[91mTrain Loss: 0.194\n",
      "\u001b[0m\u001b[1mEpoch: [68/70], Progress: 86.02 %, Steps: 6312, Current Learning Rate: 0.0007866, \u001b[96mTrain Loss: 0.129\n",
      "\u001b[0m\u001b[1mEpoch: [68/70], Progress: 87.10 %, Steps: 6313, Current Learning Rate: 0.0007866, \u001b[91mTrain Loss: 0.134\n",
      "\u001b[0m\u001b[1mEpoch: [68/70], Progress: 88.17 %, Steps: 6314, Current Learning Rate: 0.0007865, \u001b[91mTrain Loss: 0.143\n",
      "\u001b[0m\u001b[1mEpoch: [68/70], Progress: 89.25 %, Steps: 6315, Current Learning Rate: 0.0007864, \u001b[91mTrain Loss: 0.144\n",
      "\u001b[0m\u001b[1mEpoch: [68/70], Progress: 90.32 %, Steps: 6316, Current Learning Rate: 0.0007864, \u001b[96mTrain Loss: 0.144\n",
      "\u001b[0m\u001b[1mEpoch: [68/70], Progress: 91.40 %, Steps: 6317, Current Learning Rate: 0.0007863, \u001b[91mTrain Loss: 0.164\n",
      "\u001b[0m\u001b[1mEpoch: [68/70], Progress: 92.47 %, Steps: 6318, Current Learning Rate: 0.0007862, \u001b[96mTrain Loss: 0.161\n",
      "\u001b[0m\u001b[1mEpoch: [68/70], Progress: 93.55 %, Steps: 6319, Current Learning Rate: 0.0007862, \u001b[91mTrain Loss: 0.191\n",
      "\u001b[0m\u001b[1mEpoch: [68/70], Progress: 94.62 %, Steps: 6320, Current Learning Rate: 0.0007861, \u001b[96mTrain Loss: 0.136\n",
      "\u001b[0m\u001b[1mEpoch: [68/70], Progress: 95.70 %, Steps: 6321, Current Learning Rate: 0.0007861, \u001b[91mTrain Loss: 0.160\n",
      "\u001b[0m\u001b[1mEpoch: [68/70], Progress: 96.77 %, Steps: 6322, Current Learning Rate: 0.0007860, \u001b[96mTrain Loss: 0.149\n",
      "\u001b[0m\u001b[1mEpoch: [68/70], Progress: 97.85 %, Steps: 6323, Current Learning Rate: 0.0007859, \u001b[91mTrain Loss: 0.151\n",
      "\u001b[0m\u001b[1mEpoch: [68/70], Progress: 98.92 %, Steps: 6324, Current Learning Rate: 0.0007859, \u001b[91mTrain Loss: 0.152\n",
      "\u001b[1m\u001b[93mComputing Validation Loss...\n",
      "\u001b[1mEpoch 68 Completed! Average Train Loss: 0.121, Average Validation Loss: 0.019\n",
      "Best Model saved in best_model/best_model.pt.\n",
      "\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m= Epoch [69/70] ==========================================\n",
      "\u001b[0m\u001b[1mEpoch: [69/70], Progress: 0.00 %, Steps: 6325, Current Learning Rate: 0.0007858, \u001b[91mTrain Loss: 0.085\n",
      "\u001b[0m\u001b[1mEpoch: [69/70], Progress: 1.08 %, Steps: 6326, Current Learning Rate: 0.0007857, \u001b[91mTrain Loss: 0.087\n",
      "\u001b[0m\u001b[1mEpoch: [69/70], Progress: 2.15 %, Steps: 6327, Current Learning Rate: 0.0007857, \u001b[91mTrain Loss: 0.105\n",
      "\u001b[0m\u001b[1mEpoch: [69/70], Progress: 3.23 %, Steps: 6328, Current Learning Rate: 0.0007856, \u001b[96mTrain Loss: 0.079\n",
      "\u001b[0m\u001b[1mEpoch: [69/70], Progress: 4.30 %, Steps: 6329, Current Learning Rate: 0.0007856, \u001b[96mTrain Loss: 0.076\n",
      "\u001b[0m\u001b[1mEpoch: [69/70], Progress: 5.38 %, Steps: 6330, Current Learning Rate: 0.0007855, \u001b[91mTrain Loss: 0.104\n",
      "\u001b[0m\u001b[1mEpoch: [69/70], Progress: 6.45 %, Steps: 6331, Current Learning Rate: 0.0007854, \u001b[96mTrain Loss: 0.078\n",
      "\u001b[0m\u001b[1mEpoch: [69/70], Progress: 7.53 %, Steps: 6332, Current Learning Rate: 0.0007854, \u001b[91mTrain Loss: 0.122\n",
      "\u001b[0m\u001b[1mEpoch: [69/70], Progress: 8.60 %, Steps: 6333, Current Learning Rate: 0.0007853, \u001b[96mTrain Loss: 0.121\n",
      "\u001b[0m\u001b[1mEpoch: [69/70], Progress: 9.68 %, Steps: 6334, Current Learning Rate: 0.0007852, \u001b[96mTrain Loss: 0.064\n",
      "\u001b[0m\u001b[1mEpoch: [69/70], Progress: 10.75 %, Steps: 6335, Current Learning Rate: 0.0007852, \u001b[91mTrain Loss: 0.081\n",
      "\u001b[0m\u001b[1mEpoch: [69/70], Progress: 11.83 %, Steps: 6336, Current Learning Rate: 0.0007851, \u001b[91mTrain Loss: 0.112\n",
      "\u001b[0m\u001b[1mEpoch: [69/70], Progress: 12.90 %, Steps: 6337, Current Learning Rate: 0.0007851, \u001b[96mTrain Loss: 0.099\n",
      "\u001b[0m\u001b[1mEpoch: [69/70], Progress: 13.98 %, Steps: 6338, Current Learning Rate: 0.0007850, \u001b[91mTrain Loss: 0.104\n",
      "\u001b[0m\u001b[1mEpoch: [69/70], Progress: 15.05 %, Steps: 6339, Current Learning Rate: 0.0007849, \u001b[91mTrain Loss: 0.115\n",
      "\u001b[0m\u001b[1mEpoch: [69/70], Progress: 16.13 %, Steps: 6340, Current Learning Rate: 0.0007849, \u001b[96mTrain Loss: 0.107\n",
      "\u001b[0m\u001b[1mEpoch: [69/70], Progress: 17.20 %, Steps: 6341, Current Learning Rate: 0.0007848, \u001b[96mTrain Loss: 0.090\n",
      "\u001b[0m\u001b[1mEpoch: [69/70], Progress: 18.28 %, Steps: 6342, Current Learning Rate: 0.0007848, \u001b[96mTrain Loss: 0.083\n",
      "\u001b[0m\u001b[1mEpoch: [69/70], Progress: 19.35 %, Steps: 6343, Current Learning Rate: 0.0007847, \u001b[91mTrain Loss: 0.134\n",
      "\u001b[0m\u001b[1mEpoch: [69/70], Progress: 20.43 %, Steps: 6344, Current Learning Rate: 0.0007846, \u001b[96mTrain Loss: 0.123\n",
      "\u001b[0m\u001b[1mEpoch: [69/70], Progress: 21.51 %, Steps: 6345, Current Learning Rate: 0.0007846, \u001b[96mTrain Loss: 0.088\n",
      "\u001b[0m\u001b[1mEpoch: [69/70], Progress: 22.58 %, Steps: 6346, Current Learning Rate: 0.0007845, \u001b[91mTrain Loss: 0.090\n",
      "\u001b[0m\u001b[1mEpoch: [69/70], Progress: 23.66 %, Steps: 6347, Current Learning Rate: 0.0007844, \u001b[91mTrain Loss: 0.122\n",
      "\u001b[0m\u001b[1mEpoch: [69/70], Progress: 24.73 %, Steps: 6348, Current Learning Rate: 0.0007844, \u001b[96mTrain Loss: 0.092\n",
      "\u001b[0m\u001b[1mEpoch: [69/70], Progress: 25.81 %, Steps: 6349, Current Learning Rate: 0.0007843, \u001b[91mTrain Loss: 0.113\n",
      "\u001b[0m\u001b[1mEpoch: [69/70], Progress: 26.88 %, Steps: 6350, Current Learning Rate: 0.0007843, \u001b[96mTrain Loss: 0.104\n",
      "\u001b[0m\u001b[1mEpoch: [69/70], Progress: 27.96 %, Steps: 6351, Current Learning Rate: 0.0007842, \u001b[96mTrain Loss: 0.103\n",
      "\u001b[0m\u001b[1mEpoch: [69/70], Progress: 29.03 %, Steps: 6352, Current Learning Rate: 0.0007841, \u001b[96mTrain Loss: 0.103\n",
      "\u001b[0m\u001b[1mEpoch: [69/70], Progress: 30.11 %, Steps: 6353, Current Learning Rate: 0.0007841, \u001b[91mTrain Loss: 0.103\n",
      "\u001b[0m\u001b[1mEpoch: [69/70], Progress: 31.18 %, Steps: 6354, Current Learning Rate: 0.0007840, \u001b[96mTrain Loss: 0.101\n",
      "\u001b[0m\u001b[1mEpoch: [69/70], Progress: 32.26 %, Steps: 6355, Current Learning Rate: 0.0007839, \u001b[91mTrain Loss: 0.105\n",
      "\u001b[0m\u001b[1mEpoch: [69/70], Progress: 33.33 %, Steps: 6356, Current Learning Rate: 0.0007839, \u001b[91mTrain Loss: 0.133\n",
      "\u001b[0m\u001b[1mEpoch: [69/70], Progress: 34.41 %, Steps: 6357, Current Learning Rate: 0.0007838, \u001b[91mTrain Loss: 0.143\n",
      "\u001b[0m\u001b[1mEpoch: [69/70], Progress: 35.48 %, Steps: 6358, Current Learning Rate: 0.0007838, \u001b[96mTrain Loss: 0.098\n",
      "\u001b[0m\u001b[1mEpoch: [69/70], Progress: 36.56 %, Steps: 6359, Current Learning Rate: 0.0007837, \u001b[96mTrain Loss: 0.085\n",
      "\u001b[0m\u001b[1mEpoch: [69/70], Progress: 37.63 %, Steps: 6360, Current Learning Rate: 0.0007836, \u001b[96mTrain Loss: 0.083\n",
      "\u001b[0m\u001b[1mEpoch: [69/70], Progress: 38.71 %, Steps: 6361, Current Learning Rate: 0.0007836, \u001b[91mTrain Loss: 0.119\n",
      "\u001b[0m\u001b[1mEpoch: [69/70], Progress: 39.78 %, Steps: 6362, Current Learning Rate: 0.0007835, \u001b[96mTrain Loss: 0.108\n",
      "\u001b[0m\u001b[1mEpoch: [69/70], Progress: 40.86 %, Steps: 6363, Current Learning Rate: 0.0007835, \u001b[96mTrain Loss: 0.089\n",
      "\u001b[0m\u001b[1mEpoch: [69/70], Progress: 41.94 %, Steps: 6364, Current Learning Rate: 0.0007834, \u001b[91mTrain Loss: 0.109\n",
      "\u001b[0m\u001b[1mEpoch: [69/70], Progress: 43.01 %, Steps: 6365, Current Learning Rate: 0.0007833, \u001b[91mTrain Loss: 0.123\n",
      "\u001b[0m\u001b[1mEpoch: [69/70], Progress: 44.09 %, Steps: 6366, Current Learning Rate: 0.0007833, \u001b[96mTrain Loss: 0.109\n",
      "\u001b[0m\u001b[1mEpoch: [69/70], Progress: 45.16 %, Steps: 6367, Current Learning Rate: 0.0007832, \u001b[91mTrain Loss: 0.130\n",
      "\u001b[0m\u001b[1mEpoch: [69/70], Progress: 46.24 %, Steps: 6368, Current Learning Rate: 0.0007831, \u001b[96mTrain Loss: 0.107\n",
      "\u001b[0m\u001b[1mEpoch: [69/70], Progress: 47.31 %, Steps: 6369, Current Learning Rate: 0.0007831, \u001b[91mTrain Loss: 0.161\n",
      "\u001b[0m\u001b[1mEpoch: [69/70], Progress: 48.39 %, Steps: 6370, Current Learning Rate: 0.0007830, \u001b[96mTrain Loss: 0.090\n",
      "\u001b[0m\u001b[1mEpoch: [69/70], Progress: 49.46 %, Steps: 6371, Current Learning Rate: 0.0007830, \u001b[91mTrain Loss: 0.141\n",
      "\u001b[0m\u001b[1mEpoch: [69/70], Progress: 50.54 %, Steps: 6372, Current Learning Rate: 0.0007829, \u001b[96mTrain Loss: 0.084\n",
      "\u001b[0m\u001b[1mEpoch: [69/70], Progress: 51.61 %, Steps: 6373, Current Learning Rate: 0.0007828, \u001b[91mTrain Loss: 0.122\n",
      "\u001b[0m\u001b[1mEpoch: [69/70], Progress: 52.69 %, Steps: 6374, Current Learning Rate: 0.0007828, \u001b[91mTrain Loss: 0.127\n",
      "\u001b[0m\u001b[1mEpoch: [69/70], Progress: 53.76 %, Steps: 6375, Current Learning Rate: 0.0007827, \u001b[96mTrain Loss: 0.115\n",
      "\u001b[0m\u001b[1mEpoch: [69/70], Progress: 54.84 %, Steps: 6376, Current Learning Rate: 0.0007827, \u001b[91mTrain Loss: 0.141\n",
      "\u001b[0m\u001b[1mEpoch: [69/70], Progress: 55.91 %, Steps: 6377, Current Learning Rate: 0.0007826, \u001b[96mTrain Loss: 0.112\n",
      "\u001b[0m\u001b[1mEpoch: [69/70], Progress: 56.99 %, Steps: 6378, Current Learning Rate: 0.0007825, \u001b[91mTrain Loss: 0.128\n",
      "\u001b[0m\u001b[1mEpoch: [69/70], Progress: 58.06 %, Steps: 6379, Current Learning Rate: 0.0007825, \u001b[96mTrain Loss: 0.096\n",
      "\u001b[0m\u001b[1mEpoch: [69/70], Progress: 59.14 %, Steps: 6380, Current Learning Rate: 0.0007824, \u001b[91mTrain Loss: 0.103\n",
      "\u001b[0m\u001b[1mEpoch: [69/70], Progress: 60.22 %, Steps: 6381, Current Learning Rate: 0.0007824, \u001b[91mTrain Loss: 0.170\n",
      "\u001b[0m\u001b[1mEpoch: [69/70], Progress: 61.29 %, Steps: 6382, Current Learning Rate: 0.0007823, \u001b[96mTrain Loss: 0.104\n",
      "\u001b[0m\u001b[1mEpoch: [69/70], Progress: 62.37 %, Steps: 6383, Current Learning Rate: 0.0007822, \u001b[91mTrain Loss: 0.168\n",
      "\u001b[0m\u001b[1mEpoch: [69/70], Progress: 63.44 %, Steps: 6384, Current Learning Rate: 0.0007822, \u001b[96mTrain Loss: 0.121\n",
      "\u001b[0m\u001b[1mEpoch: [69/70], Progress: 64.52 %, Steps: 6385, Current Learning Rate: 0.0007821, \u001b[91mTrain Loss: 0.152\n",
      "\u001b[0m\u001b[1mEpoch: [69/70], Progress: 65.59 %, Steps: 6386, Current Learning Rate: 0.0007820, \u001b[96mTrain Loss: 0.115\n",
      "\u001b[0m\u001b[1mEpoch: [69/70], Progress: 66.67 %, Steps: 6387, Current Learning Rate: 0.0007820, \u001b[96mTrain Loss: 0.109\n",
      "\u001b[0m\u001b[1mEpoch: [69/70], Progress: 67.74 %, Steps: 6388, Current Learning Rate: 0.0007819, \u001b[91mTrain Loss: 0.182\n",
      "\u001b[0m\u001b[1mEpoch: [69/70], Progress: 68.82 %, Steps: 6389, Current Learning Rate: 0.0007819, \u001b[96mTrain Loss: 0.146\n",
      "\u001b[0m\u001b[1mEpoch: [69/70], Progress: 69.89 %, Steps: 6390, Current Learning Rate: 0.0007818, \u001b[96mTrain Loss: 0.139\n",
      "\u001b[0m\u001b[1mEpoch: [69/70], Progress: 70.97 %, Steps: 6391, Current Learning Rate: 0.0007817, \u001b[96mTrain Loss: 0.130\n",
      "\u001b[0m\u001b[1mEpoch: [69/70], Progress: 72.04 %, Steps: 6392, Current Learning Rate: 0.0007817, \u001b[96mTrain Loss: 0.100\n",
      "\u001b[0m\u001b[1mEpoch: [69/70], Progress: 73.12 %, Steps: 6393, Current Learning Rate: 0.0007816, \u001b[96mTrain Loss: 0.090\n",
      "\u001b[0m\u001b[1mEpoch: [69/70], Progress: 74.19 %, Steps: 6394, Current Learning Rate: 0.0007816, \u001b[91mTrain Loss: 0.114\n",
      "\u001b[0m\u001b[1mEpoch: [69/70], Progress: 75.27 %, Steps: 6395, Current Learning Rate: 0.0007815, \u001b[96mTrain Loss: 0.114\n",
      "\u001b[0m\u001b[1mEpoch: [69/70], Progress: 76.34 %, Steps: 6396, Current Learning Rate: 0.0007814, \u001b[91mTrain Loss: 0.150\n",
      "\u001b[0m\u001b[1mEpoch: [69/70], Progress: 77.42 %, Steps: 6397, Current Learning Rate: 0.0007814, \u001b[96mTrain Loss: 0.125\n",
      "\u001b[0m\u001b[1mEpoch: [69/70], Progress: 78.49 %, Steps: 6398, Current Learning Rate: 0.0007813, \u001b[96mTrain Loss: 0.086\n",
      "\u001b[0m\u001b[1mEpoch: [69/70], Progress: 79.57 %, Steps: 6399, Current Learning Rate: 0.0007813, \u001b[91mTrain Loss: 0.108\n",
      "\u001b[0m\u001b[1mEpoch: [69/70], Progress: 80.65 %, Steps: 6400, Current Learning Rate: 0.0007812, \u001b[91mTrain Loss: 0.138\n",
      "\u001b[0m\u001b[1mEpoch: [69/70], Progress: 81.72 %, Steps: 6401, Current Learning Rate: 0.0007811, \u001b[91mTrain Loss: 0.161\n",
      "\u001b[0m\u001b[1mEpoch: [69/70], Progress: 82.80 %, Steps: 6402, Current Learning Rate: 0.0007811, \u001b[91mTrain Loss: 0.173\n",
      "\u001b[0m\u001b[1mEpoch: [69/70], Progress: 83.87 %, Steps: 6403, Current Learning Rate: 0.0007810, \u001b[96mTrain Loss: 0.147\n",
      "\u001b[0m\u001b[1mEpoch: [69/70], Progress: 84.95 %, Steps: 6404, Current Learning Rate: 0.0007809, \u001b[96mTrain Loss: 0.128\n",
      "\u001b[0m\u001b[1mEpoch: [69/70], Progress: 86.02 %, Steps: 6405, Current Learning Rate: 0.0007809, \u001b[91mTrain Loss: 0.133\n",
      "\u001b[0m\u001b[1mEpoch: [69/70], Progress: 87.10 %, Steps: 6406, Current Learning Rate: 0.0007808, \u001b[96mTrain Loss: 0.087\n",
      "\u001b[0m\u001b[1mEpoch: [69/70], Progress: 88.17 %, Steps: 6407, Current Learning Rate: 0.0007808, \u001b[91mTrain Loss: 0.120\n",
      "\u001b[0m\u001b[1mEpoch: [69/70], Progress: 89.25 %, Steps: 6408, Current Learning Rate: 0.0007807, \u001b[96mTrain Loss: 0.108\n",
      "\u001b[0m\u001b[1mEpoch: [69/70], Progress: 90.32 %, Steps: 6409, Current Learning Rate: 0.0007806, \u001b[91mTrain Loss: 0.120\n",
      "\u001b[0m\u001b[1mEpoch: [69/70], Progress: 91.40 %, Steps: 6410, Current Learning Rate: 0.0007806, \u001b[91mTrain Loss: 0.121\n",
      "\u001b[0m\u001b[1mEpoch: [69/70], Progress: 92.47 %, Steps: 6411, Current Learning Rate: 0.0007805, \u001b[96mTrain Loss: 0.093\n",
      "\u001b[0m\u001b[1mEpoch: [69/70], Progress: 93.55 %, Steps: 6412, Current Learning Rate: 0.0007805, \u001b[91mTrain Loss: 0.159\n",
      "\u001b[0m\u001b[1mEpoch: [69/70], Progress: 94.62 %, Steps: 6413, Current Learning Rate: 0.0007804, \u001b[96mTrain Loss: 0.144\n",
      "\u001b[0m\u001b[1mEpoch: [69/70], Progress: 95.70 %, Steps: 6414, Current Learning Rate: 0.0007803, \u001b[96mTrain Loss: 0.129\n",
      "\u001b[0m\u001b[1mEpoch: [69/70], Progress: 96.77 %, Steps: 6415, Current Learning Rate: 0.0007803, \u001b[91mTrain Loss: 0.143\n",
      "\u001b[0m\u001b[1mEpoch: [69/70], Progress: 97.85 %, Steps: 6416, Current Learning Rate: 0.0007802, \u001b[91mTrain Loss: 0.160\n",
      "\u001b[0m\u001b[1mEpoch: [69/70], Progress: 98.92 %, Steps: 6417, Current Learning Rate: 0.0007802, \u001b[96mTrain Loss: 0.093\n",
      "\u001b[1m\u001b[93mComputing Validation Loss...\n",
      "\u001b[1mEpoch 69 Completed! Average Train Loss: 0.115, Average Validation Loss: 0.022\n",
      "\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m=\u001b[1m\u001b[92m= Epoch [70/70] ==========================================\n",
      "\u001b[0m\u001b[1mEpoch: [70/70], Progress: 0.00 %, Steps: 6418, Current Learning Rate: 0.0007801, \u001b[91mTrain Loss: 0.095\n",
      "\u001b[0m\u001b[1mEpoch: [70/70], Progress: 1.08 %, Steps: 6419, Current Learning Rate: 0.0007800, \u001b[96mTrain Loss: 0.085\n",
      "\u001b[0m\u001b[1mEpoch: [70/70], Progress: 2.15 %, Steps: 6420, Current Learning Rate: 0.0007800, \u001b[96mTrain Loss: 0.083\n",
      "\u001b[0m\u001b[1mEpoch: [70/70], Progress: 3.23 %, Steps: 6421, Current Learning Rate: 0.0007799, \u001b[96mTrain Loss: 0.083\n",
      "\u001b[0m\u001b[1mEpoch: [70/70], Progress: 4.30 %, Steps: 6422, Current Learning Rate: 0.0007798, \u001b[91mTrain Loss: 0.093\n",
      "\u001b[0m\u001b[1mEpoch: [70/70], Progress: 5.38 %, Steps: 6423, Current Learning Rate: 0.0007798, \u001b[96mTrain Loss: 0.076\n",
      "\u001b[0m\u001b[1mEpoch: [70/70], Progress: 6.45 %, Steps: 6424, Current Learning Rate: 0.0007797, \u001b[91mTrain Loss: 0.099\n",
      "\u001b[0m\u001b[1mEpoch: [70/70], Progress: 7.53 %, Steps: 6425, Current Learning Rate: 0.0007797, \u001b[96mTrain Loss: 0.070\n",
      "\u001b[0m\u001b[1mEpoch: [70/70], Progress: 8.60 %, Steps: 6426, Current Learning Rate: 0.0007796, \u001b[91mTrain Loss: 0.114\n",
      "\u001b[0m\u001b[1mEpoch: [70/70], Progress: 9.68 %, Steps: 6427, Current Learning Rate: 0.0007795, \u001b[96mTrain Loss: 0.102\n",
      "\u001b[0m\u001b[1mEpoch: [70/70], Progress: 10.75 %, Steps: 6428, Current Learning Rate: 0.0007795, \u001b[96mTrain Loss: 0.082\n",
      "\u001b[0m\u001b[1mEpoch: [70/70], Progress: 11.83 %, Steps: 6429, Current Learning Rate: 0.0007794, \u001b[96mTrain Loss: 0.073\n",
      "\u001b[0m\u001b[1mEpoch: [70/70], Progress: 12.90 %, Steps: 6430, Current Learning Rate: 0.0007794, \u001b[91mTrain Loss: 0.079\n",
      "\u001b[0m\u001b[1mEpoch: [70/70], Progress: 13.98 %, Steps: 6431, Current Learning Rate: 0.0007793, \u001b[91mTrain Loss: 0.105\n",
      "\u001b[0m\u001b[1mEpoch: [70/70], Progress: 15.05 %, Steps: 6432, Current Learning Rate: 0.0007792, \u001b[96mTrain Loss: 0.095\n",
      "\u001b[0m\u001b[1mEpoch: [70/70], Progress: 16.13 %, Steps: 6433, Current Learning Rate: 0.0007792, \u001b[91mTrain Loss: 0.123\n",
      "\u001b[0m\u001b[1mEpoch: [70/70], Progress: 17.20 %, Steps: 6434, Current Learning Rate: 0.0007791, \u001b[96mTrain Loss: 0.105\n",
      "\u001b[0m\u001b[1mEpoch: [70/70], Progress: 18.28 %, Steps: 6435, Current Learning Rate: 0.0007791, \u001b[96mTrain Loss: 0.093\n",
      "\u001b[0m\u001b[1mEpoch: [70/70], Progress: 19.35 %, Steps: 6436, Current Learning Rate: 0.0007790, \u001b[91mTrain Loss: 0.107\n",
      "\u001b[0m\u001b[1mEpoch: [70/70], Progress: 20.43 %, Steps: 6437, Current Learning Rate: 0.0007789, \u001b[96mTrain Loss: 0.082\n",
      "\u001b[0m\u001b[1mEpoch: [70/70], Progress: 21.51 %, Steps: 6438, Current Learning Rate: 0.0007789, \u001b[91mTrain Loss: 0.110\n",
      "\u001b[0m\u001b[1mEpoch: [70/70], Progress: 22.58 %, Steps: 6439, Current Learning Rate: 0.0007788, \u001b[96mTrain Loss: 0.108\n",
      "\u001b[0m\u001b[1mEpoch: [70/70], Progress: 23.66 %, Steps: 6440, Current Learning Rate: 0.0007788, \u001b[96mTrain Loss: 0.108\n",
      "\u001b[0m\u001b[1mEpoch: [70/70], Progress: 24.73 %, Steps: 6441, Current Learning Rate: 0.0007787, \u001b[91mTrain Loss: 0.109\n",
      "\u001b[0m\u001b[1mEpoch: [70/70], Progress: 25.81 %, Steps: 6442, Current Learning Rate: 0.0007786, \u001b[96mTrain Loss: 0.100\n",
      "\u001b[0m\u001b[1mEpoch: [70/70], Progress: 26.88 %, Steps: 6443, Current Learning Rate: 0.0007786, \u001b[91mTrain Loss: 0.101\n",
      "\u001b[0m\u001b[1mEpoch: [70/70], Progress: 27.96 %, Steps: 6444, Current Learning Rate: 0.0007785, \u001b[91mTrain Loss: 0.102\n",
      "\u001b[0m\u001b[1mEpoch: [70/70], Progress: 29.03 %, Steps: 6445, Current Learning Rate: 0.0007785, \u001b[91mTrain Loss: 0.112\n",
      "\u001b[0m\u001b[1mEpoch: [70/70], Progress: 30.11 %, Steps: 6446, Current Learning Rate: 0.0007784, \u001b[96mTrain Loss: 0.097\n",
      "\u001b[0m\u001b[1mEpoch: [70/70], Progress: 31.18 %, Steps: 6447, Current Learning Rate: 0.0007783, \u001b[96mTrain Loss: 0.086\n",
      "\u001b[0m\u001b[1mEpoch: [70/70], Progress: 32.26 %, Steps: 6448, Current Learning Rate: 0.0007783, \u001b[91mTrain Loss: 0.103\n",
      "\u001b[0m\u001b[1mEpoch: [70/70], Progress: 33.33 %, Steps: 6449, Current Learning Rate: 0.0007782, \u001b[96mTrain Loss: 0.094\n",
      "\u001b[0m\u001b[1mEpoch: [70/70], Progress: 34.41 %, Steps: 6450, Current Learning Rate: 0.0007782, \u001b[91mTrain Loss: 0.096\n",
      "\u001b[0m\u001b[1mEpoch: [70/70], Progress: 35.48 %, Steps: 6451, Current Learning Rate: 0.0007781, \u001b[91mTrain Loss: 0.102\n",
      "\u001b[0m\u001b[1mEpoch: [70/70], Progress: 36.56 %, Steps: 6452, Current Learning Rate: 0.0007780, \u001b[96mTrain Loss: 0.099\n",
      "\u001b[0m\u001b[1mEpoch: [70/70], Progress: 37.63 %, Steps: 6453, Current Learning Rate: 0.0007780, \u001b[91mTrain Loss: 0.177\n",
      "\u001b[0m\u001b[1mEpoch: [70/70], Progress: 38.71 %, Steps: 6454, Current Learning Rate: 0.0007779, \u001b[96mTrain Loss: 0.093\n",
      "\u001b[0m\u001b[1mEpoch: [70/70], Progress: 39.78 %, Steps: 6455, Current Learning Rate: 0.0007779, \u001b[91mTrain Loss: 0.108\n",
      "\u001b[0m\u001b[1mEpoch: [70/70], Progress: 40.86 %, Steps: 6456, Current Learning Rate: 0.0007778, \u001b[96mTrain Loss: 0.096\n",
      "\u001b[0m\u001b[1mEpoch: [70/70], Progress: 41.94 %, Steps: 6457, Current Learning Rate: 0.0007777, \u001b[91mTrain Loss: 0.128\n",
      "\u001b[0m\u001b[1mEpoch: [70/70], Progress: 43.01 %, Steps: 6458, Current Learning Rate: 0.0007777, \u001b[96mTrain Loss: 0.085\n",
      "\u001b[0m\u001b[1mEpoch: [70/70], Progress: 44.09 %, Steps: 6459, Current Learning Rate: 0.0007776, \u001b[91mTrain Loss: 0.094\n",
      "\u001b[0m\u001b[1mEpoch: [70/70], Progress: 45.16 %, Steps: 6460, Current Learning Rate: 0.0007776, \u001b[91mTrain Loss: 0.123\n",
      "\u001b[0m\u001b[1mEpoch: [70/70], Progress: 46.24 %, Steps: 6461, Current Learning Rate: 0.0007775, \u001b[96mTrain Loss: 0.113\n",
      "\u001b[0m\u001b[1mEpoch: [70/70], Progress: 47.31 %, Steps: 6462, Current Learning Rate: 0.0007774, \u001b[91mTrain Loss: 0.158\n",
      "\u001b[0m\u001b[1mEpoch: [70/70], Progress: 48.39 %, Steps: 6463, Current Learning Rate: 0.0007774, \u001b[96mTrain Loss: 0.089\n",
      "\u001b[0m\u001b[1mEpoch: [70/70], Progress: 49.46 %, Steps: 6464, Current Learning Rate: 0.0007773, \u001b[91mTrain Loss: 0.120\n",
      "\u001b[0m\u001b[1mEpoch: [70/70], Progress: 50.54 %, Steps: 6465, Current Learning Rate: 0.0007773, \u001b[91mTrain Loss: 0.146\n",
      "\u001b[0m\u001b[1mEpoch: [70/70], Progress: 51.61 %, Steps: 6466, Current Learning Rate: 0.0007772, \u001b[96mTrain Loss: 0.112\n",
      "\u001b[0m\u001b[1mEpoch: [70/70], Progress: 52.69 %, Steps: 6467, Current Learning Rate: 0.0007771, \u001b[91mTrain Loss: 0.118\n",
      "\u001b[0m\u001b[1mEpoch: [70/70], Progress: 53.76 %, Steps: 6468, Current Learning Rate: 0.0007771, \u001b[96mTrain Loss: 0.117\n",
      "\u001b[0m\u001b[1mEpoch: [70/70], Progress: 54.84 %, Steps: 6469, Current Learning Rate: 0.0007770, \u001b[96mTrain Loss: 0.116\n",
      "\u001b[0m\u001b[1mEpoch: [70/70], Progress: 55.91 %, Steps: 6470, Current Learning Rate: 0.0007770, \u001b[91mTrain Loss: 0.148\n",
      "\u001b[0m\u001b[1mEpoch: [70/70], Progress: 56.99 %, Steps: 6471, Current Learning Rate: 0.0007769, \u001b[96mTrain Loss: 0.119\n",
      "\u001b[0m\u001b[1mEpoch: [70/70], Progress: 58.06 %, Steps: 6472, Current Learning Rate: 0.0007768, \u001b[96mTrain Loss: 0.107\n",
      "\u001b[0m\u001b[1mEpoch: [70/70], Progress: 59.14 %, Steps: 6473, Current Learning Rate: 0.0007768, \u001b[91mTrain Loss: 0.107\n",
      "\u001b[0m\u001b[1mEpoch: [70/70], Progress: 60.22 %, Steps: 6474, Current Learning Rate: 0.0007767, \u001b[91mTrain Loss: 0.119\n",
      "\u001b[0m\u001b[1mEpoch: [70/70], Progress: 61.29 %, Steps: 6475, Current Learning Rate: 0.0007767, \u001b[96mTrain Loss: 0.096\n",
      "\u001b[0m\u001b[1mEpoch: [70/70], Progress: 62.37 %, Steps: 6476, Current Learning Rate: 0.0007766, \u001b[91mTrain Loss: 0.120\n",
      "\u001b[0m\u001b[1mEpoch: [70/70], Progress: 63.44 %, Steps: 6477, Current Learning Rate: 0.0007765, \u001b[96mTrain Loss: 0.098\n",
      "\u001b[0m\u001b[1mEpoch: [70/70], Progress: 64.52 %, Steps: 6478, Current Learning Rate: 0.0007765, \u001b[91mTrain Loss: 0.141\n",
      "\u001b[0m\u001b[1mEpoch: [70/70], Progress: 65.59 %, Steps: 6479, Current Learning Rate: 0.0007764, \u001b[96mTrain Loss: 0.114\n",
      "\u001b[0m\u001b[1mEpoch: [70/70], Progress: 66.67 %, Steps: 6480, Current Learning Rate: 0.0007764, \u001b[96mTrain Loss: 0.091\n",
      "\u001b[0m\u001b[1mEpoch: [70/70], Progress: 67.74 %, Steps: 6481, Current Learning Rate: 0.0007763, \u001b[91mTrain Loss: 0.144\n",
      "\u001b[0m\u001b[1mEpoch: [70/70], Progress: 68.82 %, Steps: 6482, Current Learning Rate: 0.0007762, \u001b[96mTrain Loss: 0.101\n",
      "\u001b[0m\u001b[1mEpoch: [70/70], Progress: 69.89 %, Steps: 6483, Current Learning Rate: 0.0007762, \u001b[91mTrain Loss: 0.113\n",
      "\u001b[0m\u001b[1mEpoch: [70/70], Progress: 70.97 %, Steps: 6484, Current Learning Rate: 0.0007761, \u001b[96mTrain Loss: 0.107\n",
      "\u001b[0m\u001b[1mEpoch: [70/70], Progress: 72.04 %, Steps: 6485, Current Learning Rate: 0.0007761, \u001b[96mTrain Loss: 0.092\n",
      "\u001b[0m\u001b[1mEpoch: [70/70], Progress: 73.12 %, Steps: 6486, Current Learning Rate: 0.0007760, \u001b[91mTrain Loss: 0.156\n",
      "\u001b[0m\u001b[1mEpoch: [70/70], Progress: 74.19 %, Steps: 6487, Current Learning Rate: 0.0007759, \u001b[96mTrain Loss: 0.146\n",
      "\u001b[0m\u001b[1mEpoch: [70/70], Progress: 75.27 %, Steps: 6488, Current Learning Rate: 0.0007759, \u001b[91mTrain Loss: 0.148\n",
      "\u001b[0m\u001b[1mEpoch: [70/70], Progress: 76.34 %, Steps: 6489, Current Learning Rate: 0.0007758, \u001b[96mTrain Loss: 0.105\n",
      "\u001b[0m\u001b[1mEpoch: [70/70], Progress: 77.42 %, Steps: 6490, Current Learning Rate: 0.0007758, \u001b[91mTrain Loss: 0.124\n",
      "\u001b[0m\u001b[1mEpoch: [70/70], Progress: 78.49 %, Steps: 6491, Current Learning Rate: 0.0007757, \u001b[91mTrain Loss: 0.126\n",
      "\u001b[0m\u001b[1mEpoch: [70/70], Progress: 79.57 %, Steps: 6492, Current Learning Rate: 0.0007756, \u001b[91mTrain Loss: 0.164\n",
      "\u001b[0m\u001b[1mEpoch: [70/70], Progress: 80.65 %, Steps: 6493, Current Learning Rate: 0.0007756, \u001b[96mTrain Loss: 0.096\n",
      "\u001b[0m\u001b[1mEpoch: [70/70], Progress: 81.72 %, Steps: 6494, Current Learning Rate: 0.0007755, \u001b[91mTrain Loss: 0.097\n",
      "\u001b[0m\u001b[1mEpoch: [70/70], Progress: 82.80 %, Steps: 6495, Current Learning Rate: 0.0007755, \u001b[91mTrain Loss: 0.111\n",
      "\u001b[0m\u001b[1mEpoch: [70/70], Progress: 83.87 %, Steps: 6496, Current Learning Rate: 0.0007754, \u001b[96mTrain Loss: 0.111\n",
      "\u001b[0m\u001b[1mEpoch: [70/70], Progress: 84.95 %, Steps: 6497, Current Learning Rate: 0.0007753, \u001b[96mTrain Loss: 0.109\n",
      "\u001b[0m\u001b[1mEpoch: [70/70], Progress: 86.02 %, Steps: 6498, Current Learning Rate: 0.0007753, \u001b[91mTrain Loss: 0.111\n",
      "\u001b[0m\u001b[1mEpoch: [70/70], Progress: 87.10 %, Steps: 6499, Current Learning Rate: 0.0007752, \u001b[96mTrain Loss: 0.080\n",
      "\u001b[0m\u001b[1mEpoch: [70/70], Progress: 88.17 %, Steps: 6500, Current Learning Rate: 0.0007752, \u001b[91mTrain Loss: 0.135\n",
      "\u001b[0m\u001b[1mEpoch: [70/70], Progress: 89.25 %, Steps: 6501, Current Learning Rate: 0.0007751, \u001b[96mTrain Loss: 0.087\n",
      "\u001b[0m\u001b[1mEpoch: [70/70], Progress: 90.32 %, Steps: 6502, Current Learning Rate: 0.0007750, \u001b[91mTrain Loss: 0.109\n",
      "\u001b[0m\u001b[1mEpoch: [70/70], Progress: 91.40 %, Steps: 6503, Current Learning Rate: 0.0007750, \u001b[91mTrain Loss: 0.180\n",
      "\u001b[0m\u001b[1mEpoch: [70/70], Progress: 92.47 %, Steps: 6504, Current Learning Rate: 0.0007749, \u001b[91mTrain Loss: 0.193\n",
      "\u001b[0m\u001b[1mEpoch: [70/70], Progress: 93.55 %, Steps: 6505, Current Learning Rate: 0.0007749, \u001b[96mTrain Loss: 0.107\n",
      "\u001b[0m\u001b[1mEpoch: [70/70], Progress: 94.62 %, Steps: 6506, Current Learning Rate: 0.0007748, \u001b[91mTrain Loss: 0.115\n",
      "\u001b[0m\u001b[1mEpoch: [70/70], Progress: 95.70 %, Steps: 6507, Current Learning Rate: 0.0007747, \u001b[96mTrain Loss: 0.105\n",
      "\u001b[0m\u001b[1mEpoch: [70/70], Progress: 96.77 %, Steps: 6508, Current Learning Rate: 0.0007747, \u001b[91mTrain Loss: 0.138\n",
      "\u001b[0m\u001b[1mEpoch: [70/70], Progress: 97.85 %, Steps: 6509, Current Learning Rate: 0.0007746, \u001b[96mTrain Loss: 0.095\n",
      "\u001b[0m\u001b[1mEpoch: [70/70], Progress: 98.92 %, Steps: 6510, Current Learning Rate: 0.0007746, \u001b[91mTrain Loss: 0.107\n",
      "\u001b[1m\u001b[93mComputing Validation Loss...\n",
      "\u001b[1mEpoch 70 Completed! Average Train Loss: 0.110, Average Validation Loss: 0.016\n",
      "Best Model saved in best_model/best_model.pt.\n"
     ]
    }
   ],
   "source": [
    "y_train, y_eval = [], []  # Lists for Visualization\n",
    "best_validation_loss: float = float(\"inf\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"\\033[1m\\033[92m=\" * 42 + f\" Epoch [{epoch + 1}/{epochs}] \" + \"=\" * 42)\n",
    "    train_loss = train(\n",
    "        epoch=epoch,\n",
    "        epochs=epochs,\n",
    "        model=model,\n",
    "        iterator=QA_train_dataloader,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        criterion=criterion,\n",
    "        clip=clip,\n",
    "    )\n",
    "    y_train.append(train_loss)\n",
    "\n",
    "    print(\"\\033[1m\\033[93mComputing Validation Loss...\")\n",
    "    validation_loss = evaluate(\n",
    "        model=model,\n",
    "        iterator=QA_validation_dataloader,\n",
    "        criterion=criterion,\n",
    "    )\n",
    "    y_eval.append(validation_loss)\n",
    "\n",
    "    print(\n",
    "        f\"\\033[1mEpoch {epoch + 1} Completed! Average Train Loss: {train_loss:.3f}, Average Validation Loss: {validation_loss:.3f}\"\n",
    "    )\n",
    "\n",
    "    if validation_loss <= best_validation_loss:\n",
    "        best_validation_loss = validation_loss\n",
    "        f: str = \"best_model/best_model.pt\"\n",
    "        torch.save(obj=model.state_dict(), f=f)\n",
    "        print(f\"Best Model saved in {f}.\")\n",
    "\n",
    "    torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def visualization(x: List[int], y_train: List[float], y_eval: List[float]) -> plt.plot:\n",
    "    plt.plot(x, y_train, label=\"Train Loss\")\n",
    "    plt.plot(x, y_eval, label=\"Validation Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.title(\"Loss Visualization\")\n",
    "    plt.tight_layout()\n",
    "    plt.grid()\n",
    "\n",
    "    return plt.show()\n",
    "\n",
    "visualization(x=list(range(1, epochs + 1)), y_train=y_train, y_eval=y_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess User Input and Predict the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_user_input(sentence: str) -> torch.IntTensor:\n",
    "    sentence = sub(pattern=r\"([?.!,])\", repl=r\" \\1\", string=sentence)\n",
    "    sentence = sentence.strip()\n",
    "    encoded_sentence = tokenizer.encode(sentence)\n",
    "    sentence_id = torch.IntTensor(encoded_sentence)\n",
    "    \n",
    "    return sentence_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: \n",
      "A:   .\n"
     ]
    }
   ],
   "source": [
    "def predict(sentence: str) -> None:\n",
    "    print(f\"Q: {sentence}\")\n",
    "\n",
    "    model.load_state_dict(\n",
    "        torch.load(\"best_model/best_model.pt\")\n",
    "    )  # Call the best validation loss model.\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        sentence = preprocess_user_input(sentence=sentence)\n",
    "        sentence = sentence.unsqueeze(dim=0).to(device=device)  # Encoder's input\n",
    "\n",
    "        output = (\n",
    "            torch.IntTensor([SOS]).unsqueeze(dim=0).to(device=device)\n",
    "        )  # Decoder's input\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            predictions = model(encoder_input=sentence, decoder_input=output)\n",
    "            predictions = predictions[:, -1:, :]\n",
    "            predicted_id = torch.argmax(input=predictions, dim=-1)\n",
    "            predicted_id = predicted_id[-1]  # Take only last portion of prediction\n",
    "\n",
    "            if torch.equal(\n",
    "                input=torch.IntTensor([predicted_id]).to(device=device),\n",
    "                other=torch.IntTensor([EOS]).to(device=device),\n",
    "            ):\n",
    "                break\n",
    "\n",
    "            output = torch.cat(\n",
    "                [output, torch.IntTensor([[predicted_id]]).to(device=device)], dim=1\n",
    "            )\n",
    "\n",
    "        output = output[:, 1:]  # Exclude SOS token\n",
    "        prediction = torch.squeeze(input=output, dim=0)\n",
    "        predicted_sentence = tokenizer.decode(prediction)\n",
    "\n",
    "    print(f\"A: {predicted_sentence}\")\n",
    "\n",
    "predict(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

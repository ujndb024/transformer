{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\") # Make module visible across folders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "from tokenizers.implementations import BertWordPieceTokenizer\n",
    "\n",
    "from re import sub\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam, Optimizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from typing import List, Tuple\n",
    "\n",
    "from transformer.module.transformer import Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and Save the Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2004: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15124"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = \"./train_data/ChatbotData.csv\"\n",
    "train_data_csv = pd.read_csv(train_data)\n",
    "\n",
    "tokenizer = BertWordPieceTokenizer(lowercase=False, strip_accents=False)\n",
    "\n",
    "# tokenizer.train(\n",
    "#     files=train_data,\n",
    "#     vocab_size=2**14,\n",
    "#     min_frequency=2,\n",
    "#     special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\", \"[SOS]\", \"[EOS]\"],\n",
    "# )\n",
    "\n",
    "# tokenizer.save_model(\"train_data/\")\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"train_data/vocab.txt\", do_basic_tokenize=False)\n",
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set the Maximum Length of the Sequence, Get the Index of the `SOS`, `EOS`, `PAD`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 6 0\n"
     ]
    }
   ],
   "source": [
    "max_length: int = 40\n",
    "\n",
    "SOS = tokenizer.get_vocab()[\"[SOS]\"]\n",
    "EOS = tokenizer.get_vocab()[\"[EOS]\"]\n",
    "PAD = tokenizer.get_vocab()[\"[PAD]\"]\n",
    "\n",
    "print(SOS, EOS, PAD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Punctuations, Integer Encoding, Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = []\n",
    "for sentence in train_data_csv[\"Q\"]:\n",
    "    sentence = sub(pattern=r\"([?.!,])\", repl=r\" \\1\", string=sentence)\n",
    "    sentence = sentence.strip()\n",
    "    encoded_sentence = tokenizer.encode(sentence)\n",
    "    encoded_sentence = encoded_sentence[1:-1]\n",
    "    sentence = torch.IntTensor(encoded_sentence) # Encoder's input. No need to add SOS or EOS tokens.\n",
    "    padding = nn.ZeroPad1d((0, max_length - 1 - sentence.shape[0]))(sentence)\n",
    "    questions.append(padding.tolist())\n",
    "\n",
    "questions = torch.IntTensor(questions)\n",
    "\n",
    "answers = []\n",
    "for sentence in train_data_csv[\"A\"]:\n",
    "    sentence = sub(pattern=r\"([?.!,])\", repl=r\" \\1\", string=sentence)\n",
    "    sentence = sentence.strip()\n",
    "    encoded_sentence = tokenizer.encode(sentence)\n",
    "    encoded_sentence = encoded_sentence[1:-1]\n",
    "    sentence = torch.IntTensor([SOS] + encoded_sentence + [EOS]) # Decoder's input And Output. Decoder's input only contains SOS token and Output only contains EOS token.\n",
    "    padding = nn.ZeroPad1d((0, max_length - sentence.shape[0]))(sentence)\n",
    "    answers.append(padding.tolist())\n",
    "\n",
    "answers = torch.IntTensor(answers)\n",
    "\n",
    "# print(questions[:5], answers[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define `batch_size`, Make Several Datasets, Load with DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size: int = 128\n",
    "\n",
    "# QADataset in GitHub\n",
    "# Link: https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData.csv\n",
    "\n",
    "\n",
    "class QADataset(Dataset):\n",
    "    def __init__(self, questions: torch.IntTensor, answers: torch.IntTensor) -> None:\n",
    "        self.inputs = questions\n",
    "        self.dec_inputs = answers[:, :-1]\n",
    "        self.outputs = answers[:, 1:]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[dict, dict]:\n",
    "        return {\"inputs\": self.inputs[idx], \"dec_inputs\": self.dec_inputs[idx]}, {\n",
    "            \"outputs\": self.outputs[idx]\n",
    "        }\n",
    "\n",
    "\n",
    "dataset = QADataset(questions, answers)\n",
    "\n",
    "QA_train_dataloader = DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "QA_validation_dataloader = DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set the Hyperparameter of the `Transformer` Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15124 15124\n"
     ]
    }
   ],
   "source": [
    "encoder_vocab_size = tokenizer.vocab_size\n",
    "decoder_vocab_size = tokenizer.vocab_size\n",
    "pad_token = PAD\n",
    "num_layers: int = 2\n",
    "d_model: int = 256\n",
    "num_heads: int = 8\n",
    "max_len: int = max_length - 1\n",
    "dff: int = 512\n",
    "dropout: float = 0.1\n",
    "device = torch.device(\n",
    "    \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    ")  # if CPU is used, you should remove .to(device) in all train and validation datasets otherwise error will be occurred due to different placeholder storage.\n",
    "print(encoder_vocab_size, decoder_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set the Parameter of the `Transformer` Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(\n",
    "    pad_token=pad_token,\n",
    "    encoder_vocab_size=encoder_vocab_size,\n",
    "    decoder_vocab_size=decoder_vocab_size,\n",
    "    num_layers=num_layers,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    max_len=max_len,\n",
    "    dff=dff,\n",
    "    dropout=dropout,\n",
    "    device=device,\n",
    ").to(device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Number of Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14266132"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_parameters(model: Transformer) -> int:\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize the Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): TransformerEmbedding(\n",
       "      (token_embedding): Embedding(15124, 256)\n",
       "      (positional_encoding): PositionalEncoding(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x EncoderLayer(\n",
       "        (attention): MultiHeadAttention(\n",
       "          (attention): ScaledDotProductAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (query_dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (key_dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (value_dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (layer_norm1): LayerNorm()\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (ffnn): PositionWiseFeedForward(\n",
       "          (weight_tensor1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (weight_tensor2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (layer_norm2): LayerNorm()\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): TransformerEmbedding(\n",
       "      (token_embedding): Embedding(15124, 256)\n",
       "      (positional_encoding): PositionalEncoding(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x DecoderLayer(\n",
       "        (self_attention): MultiHeadAttention(\n",
       "          (attention): ScaledDotProductAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (query_dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (key_dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (value_dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (layer_norm1): LayerNorm()\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (encoder_decoder_attention): MultiHeadAttention(\n",
       "          (attention): ScaledDotProductAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (query_dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (key_dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (value_dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (layer_norm2): LayerNorm()\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (ffnn): PositionWiseFeedForward(\n",
       "          (weight_tensor1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (weight_tensor2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (layer_norm3): LayerNorm()\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (linear): Linear(in_features=256, out_features=15124, bias=True)\n",
       "  )\n",
       "  (padding_mask): CreatePaddingMask()\n",
       "  (look_ahead_mask): CreateLookAheadMask()\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def initialize_weights(model) -> None:\n",
    "    if hasattr(model, \"weight\") and model.weight.dim() > 1:\n",
    "        nn.init.kaiming_uniform_(model.weight.data) # Use Kaiming uniform due to ReLU activation\n",
    "\n",
    "model.apply(initialize_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make `TransformerScheduler`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerScheduler(Optimizer):\n",
    "    \"\"\"\n",
    "    Custom Scheduler for Transformer which was introduced from the original paper \"Attention is all you need, 2017\".\n",
    "\n",
    "    Parameters:\n",
    "        optimizer: Optimizer\n",
    "            optimizer for model training.\n",
    "        d_model: int, required\n",
    "            the embed dimension of the model.\n",
    "        warmup_steps: int, optional (default=4000)\n",
    "            boundary step between linear increase and proportional decrease.\n",
    "\n",
    "    Returns:\n",
    "        lr: float\n",
    "            corresponding learning rate.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        optimizer: Optimizer,\n",
    "        d_model: int,\n",
    "        warmup_steps: int = 4000, # Paper-based value\n",
    "    ) -> None:\n",
    "        self.optimizer = optimizer\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def get_lr(self, steps: int) -> float:\n",
    "        scale = self.d_model**-0.5\n",
    "        if steps < self.warmup_steps:\n",
    "            lr = scale * (steps + 1) * (self.warmup_steps**-1.5)\n",
    "        else:\n",
    "            lr = scale * ((steps + 1) ** -0.5)\n",
    "        return lr\n",
    "\n",
    "    def step(self, steps: int) -> None:\n",
    "        lr = self.get_lr(steps)\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group[\"lr\"] = lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set the Parameters required for optimizer and criterion(loss)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr: float = 1e-5 # There was no initial rate written in the based paper, I manually set it to 1e-5. You can adjust it if you want.\n",
    "betas: Tuple[float, float] = [0.9, 0.98]\n",
    "eps: float = 1e-9\n",
    "epochs: int = 70\n",
    "clip: float = 1.0\n",
    "total_steps: int = 0\n",
    "\n",
    "optimizer = Adam(params=model.parameters(), lr=lr, betas=betas, eps=eps)\n",
    "scheduler = TransformerScheduler(optimizer=optimizer, d_model=d_model) # Adam Optimizer is wrapped in the `TransformerScheduler` Class.\n",
    "criterion = CrossEntropyLoss(ignore_index=PAD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    epoch: int,\n",
    "    epochs: int,\n",
    "    model: Transformer,\n",
    "    iterator: DataLoader,\n",
    "    optimizer: Optimizer,\n",
    "    scheduler: TransformerScheduler,\n",
    "    criterion: CrossEntropyLoss,\n",
    "    clip: int,\n",
    ") -> float:\n",
    "    model.train()\n",
    "    training_total_loss: float = 0.0\n",
    "    previous_loss: float = 0.0\n",
    "    global total_steps\n",
    "\n",
    "    for i, (all_inputs, outputs) in enumerate(iterator):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = model(\n",
    "            encoder_input=all_inputs[\"inputs\"].to(device=device),\n",
    "            decoder_input=all_inputs[\"dec_inputs\"].to(device=device),\n",
    "        )\n",
    "        logits = logits.contiguous().view(-1, logits.shape[-1]).to(device=device)\n",
    "        outputs = outputs[\"outputs\"].view(-1).type(torch.LongTensor).to(device=device)\n",
    "\n",
    "        loss = criterion(input=logits, target=outputs).to(device=device)\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=clip)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        total_steps += 1\n",
    "        scheduler.step(steps=total_steps)\n",
    "\n",
    "        training_total_loss += loss.item()\n",
    "\n",
    "        if max(previous_loss, loss.item()) == loss.item():\n",
    "            print(\n",
    "                f\"\\033[0m\\033[1mEpoch: [{epoch + 1}/{epochs}], Progress: {i / len(iterator) * 100:.2f} %, Steps: {total_steps}, Current Learning Rate: {scheduler.get_lr(total_steps):.7f}, \\033[91mTrain Loss: {loss.item():.3f}\"\n",
    "            )\n",
    "            previous_loss = loss.item()\n",
    "        else:\n",
    "            print(\n",
    "                f\"\\033[0m\\033[1mEpoch: [{epoch + 1}/{epochs}], Progress: {i / len(iterator) * 100:.2f} %, Steps: {total_steps}, Current Learning Rate: {scheduler.get_lr(total_steps):.7f}, \\033[96mTrain Loss: {loss.item():.3f}\"\n",
    "            )\n",
    "            previous_loss = loss.item()\n",
    "\n",
    "    return training_total_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(\n",
    "    model: Transformer,\n",
    "    iterator: DataLoader,\n",
    "    criterion: CrossEntropyLoss,\n",
    ") -> float:\n",
    "    model.eval()\n",
    "    validation_total_loss: float = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, (all_inputs, outputs) in enumerate(iterator):\n",
    "            logits = model(\n",
    "                encoder_input=all_inputs[\"inputs\"].to(device=device),\n",
    "                decoder_input=all_inputs[\"dec_inputs\"].to(device=device),\n",
    "            )\n",
    "            logits = logits.contiguous().view(-1, logits.shape[-1]).to(device=device)\n",
    "            outputs = (\n",
    "                outputs[\"outputs\"].view(-1).type(torch.LongTensor).to(device=device)\n",
    "            )\n",
    "\n",
    "            loss = criterion(input=logits, target=outputs).to(device=device)\n",
    "            validation_total_loss += loss.item()\n",
    "\n",
    "    return validation_total_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, y_eval = [], []  # Lists for Visualization\n",
    "best_validation_loss: float = float(\"inf\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"\\033[1m\\033[92m=\" * 42 + f\" Epoch [{epoch + 1}/{epochs}] \" + \"=\" * 42)\n",
    "    train_loss = train(\n",
    "        epoch=epoch,\n",
    "        epochs=epochs,\n",
    "        model=model,\n",
    "        iterator=QA_train_dataloader,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        criterion=criterion,\n",
    "        clip=clip,\n",
    "    )\n",
    "    y_train.append(train_loss)\n",
    "\n",
    "    print(\"\\033[1m\\033[93mComputing Validation Loss...\")\n",
    "    validation_loss = evaluate(\n",
    "        model=model,\n",
    "        iterator=QA_validation_dataloader,\n",
    "        criterion=criterion,\n",
    "    )\n",
    "    y_eval.append(validation_loss)\n",
    "\n",
    "    print(\n",
    "        f\"\\033[1mEpoch {epoch + 1} Completed! Average Train Loss: {train_loss:.3f}, Average Validation Loss: {validation_loss:.3f}\"\n",
    "    )\n",
    "\n",
    "    if validation_loss <= best_validation_loss:\n",
    "        best_validation_loss = validation_loss\n",
    "        f: str = \"best_model/best_model.pt\"\n",
    "        torch.save(obj=model.state_dict(), f=f)\n",
    "        print(f\"Best Model saved in {f}.\")\n",
    "\n",
    "    torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualization(x: List[int], y_train: List[float], y_eval: List[float]) -> plt.plot:\n",
    "    plt.plot(x, y_train, label=\"Train Loss\")\n",
    "    plt.plot(x, y_eval, label=\"Validation Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.title(\"Loss Visualization\")\n",
    "    plt.tight_layout()\n",
    "    plt.grid()\n",
    "\n",
    "    return plt.show()\n",
    "\n",
    "visualization(x=list(range(1, epochs + 1)), y_train=y_train, y_eval=y_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess User Input and Predict the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_user_input(sentence: str) -> torch.IntTensor:\n",
    "    sentence = sub(pattern=r\"([?.!,])\", repl=r\" \\1\", string=sentence)\n",
    "    sentence = sentence.strip()\n",
    "    encoded_sentence = tokenizer.encode(sentence)\n",
    "    sentence_id = torch.IntTensor(encoded_sentence)\n",
    "    \n",
    "    return sentence_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: 여행 갈까?\n",
      "A: 같이 가서 놀아보면 깰 거예요.\n"
     ]
    }
   ],
   "source": [
    "def predict(sentence: str) -> None:\n",
    "    print(f\"Q: {sentence}\")\n",
    "\n",
    "    model.load_state_dict(\n",
    "        torch.load(\"best_model/best_model.pt\")\n",
    "    )  # Call the best validation loss model.\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        sentence = preprocess_user_input(sentence=sentence)\n",
    "        sentence = sentence.unsqueeze(dim=0).to(device=device)  # Encoder's input\n",
    "\n",
    "        output = (\n",
    "            torch.IntTensor([SOS]).unsqueeze(dim=0).to(device=device)\n",
    "        )  # Decoder's input\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            predictions = model(encoder_input=sentence, decoder_input=output)\n",
    "            predictions = predictions[:, -1:, :]\n",
    "            predicted_id = torch.argmax(input=predictions, dim=-1)\n",
    "            predicted_id = predicted_id[-1]  # Take only last portion of prediction\n",
    "\n",
    "            if torch.equal(\n",
    "                input=torch.IntTensor([predicted_id]).to(device=device),\n",
    "                other=torch.IntTensor([EOS]).to(device=device),\n",
    "            ):\n",
    "                break\n",
    "\n",
    "            output = torch.cat(\n",
    "                [output, torch.IntTensor([[predicted_id]]).to(device=device)], dim=1\n",
    "            )\n",
    "\n",
    "        output = output[:, 1:]  # Exclude SOS token\n",
    "        prediction = torch.squeeze(input=output, dim=0)\n",
    "        predicted_sentence = tokenizer.decode(prediction)\n",
    "\n",
    "    print(f\"A: {predicted_sentence}\")\n",
    "\n",
    "predict(\"여행 갈까?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
